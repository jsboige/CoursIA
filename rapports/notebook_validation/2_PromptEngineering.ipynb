{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c7e9ec",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e375cd",
   "metadata": {
    "papermill": {
     "duration": 0.003057,
     "end_time": "2025-12-15T19:42:26.386999",
     "exception": false,
     "start_time": "2025-12-15T19:42:26.383942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prompt Engineering : Advanced Prompting avec OpenAI\n",
    "\n",
    "Dans ce notebook, nous allons tester différentes techniques avancées de **prompt engineering**:\n",
    "- **Zero-shot prompting**\n",
    "- **Few-shot prompting**\n",
    "- **Chain-of-thought** (CoT)\n",
    "- **Self-refine** (ou auto-amélioration)\n",
    "\n",
    "Nous utiliserons la **nouvelle API** de la bibliothèque `openai` (>=1.0.0) via la classe `OpenAI` et ses méthodes de chat (`client.chat.completions.create`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd4071b",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2025-12-15T19:42:26.393913Z",
     "iopub.status.busy": "2025-12-15T19:42:26.393488Z",
     "iopub.status.idle": "2025-12-15T19:42:28.526835Z",
     "shell.execute_reply": "2025-12-15T19:42:28.526199Z"
    },
    "papermill": {
     "duration": 2.138026,
     "end_time": "2025-12-15T19:42:28.527922",
     "exception": false,
     "start_time": "2025-12-15T19:42:26.389896",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.77.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python313\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in c:\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\python313\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: colorama in c:\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 1 : Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd9400",
   "metadata": {
    "papermill": {
     "duration": 0.002265,
     "end_time": "2025-12-15T19:42:28.534077",
     "exception": false,
     "start_time": "2025-12-15T19:42:28.531812",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a190bfaa",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c905cb",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2025-12-15T19:42:28.540489Z",
     "iopub.status.busy": "2025-12-15T19:42:28.540127Z",
     "iopub.status.idle": "2025-12-15T19:42:28.821949Z",
     "shell.execute_reply": "2025-12-15T19:42:28.820946Z"
    },
    "papermill": {
     "duration": 0.286156,
     "end_time": "2025-12-15T19:42:28.823099",
     "exception": true,
     "start_time": "2025-12-15T19:42:28.536943",
     "status": "failed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Clé API introuvable. Vérifie ton fichier .env.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m----------\u001b[39m",
      "\u001b[31mValueError\u001b[39mTraceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m api_key = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mClé API introuvable. Vérifie ton fichier .env.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Clé API introuvable. Vérifie ton fichier .env."
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 2 : Configuration\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# On suppose que ton .env contient :\n",
    "# OPENAI_API_KEY=sk-xxxxxx\n",
    "# (ou autre variable si tu utilises Azure)\n",
    "#\n",
    "# Récupère la clé d'API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Clé API introuvable. Vérifie ton fichier .env.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339ca57",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Client OpenAI\n",
    "# ============================\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Pour l'exemple, on définit le modèle par défaut\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "# Instanciation du client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    # Tu peux configurer d'autres options si besoin\n",
    ")\n",
    "\n",
    "print(\"Client OpenAI initialisé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d285f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Rappel des différences entre Zero-shot, Few-shot, Chain-of-thought et Self-refine\n",
    "\n",
    "1. **Zero-shot Prompting**  \n",
    "   - Aucune instruction ou exemple préalable (à part la demande de l'utilisateur).  \n",
    "   - Simple et direct, mais parfois moins précis ou cohérent.\n",
    "\n",
    "2. **Few-shot Prompting**  \n",
    "   - Fournir quelques exemples “input → output” pour guider la réponse.  \n",
    "   - Permet de **spécifier le format**, le style, ou le contenu souhaité.  \n",
    "   - Améliore significativement la qualité des réponses sur des tâches complexes.\n",
    "\n",
    "3. **Chain-of-thought (CoT)**  \n",
    "   - On **incite** le modèle à détailler son raisonnement étape par étape.  \n",
    "   - Souvent utile pour des questions de logique, mathématiques, programmation ou raisonnement complexe.  \n",
    "   - Peut **augmenter** la cohérence et la justesse de la réponse (mais attention à ne pas divulguer ces “étapes” si elles sont confidentielles).\n",
    "\n",
    "4. **Self-refine**  \n",
    "   - Demander au modèle de s’auto-critiquer puis de proposer une réponse améliorée.  \n",
    "   - Mise en œuvre en plusieurs appels (réponse initiale, re-demande d’analyse, ré-énoncé final).  \n",
    "   - Intéressant pour du code, des textes longs, ou des situations nécessitant un contrôle qualité.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27abbd0",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Zero-shot\n",
    "# ============================\n",
    "\n",
    "prompt_1 = \"Donne-moi 3 idées de recettes végétariennes à base de tomates.\"\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt_1}\n",
    "    ],\n",
    "    # Contrôle du style\n",
    "    max_tokens=200,\n",
    "    temperature=0.7  # plus la température est haute, plus c'est créatif\n",
    ")\n",
    "\n",
    "print(\"=== Zero-shot Prompt ===\")\n",
    "print(f\"Prompt: {prompt_1}\\n\")\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_1.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116417f4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Ici, pas d’exemples ni d’instructions détaillées, on se contente d’un prompt direct.\n",
    "\n",
    "\n",
    "\n",
    "###  Exemple Few-shot prompting (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b503e",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Few-shot supplémentaire\n",
    "# ============================\n",
    "\n",
    "few_shot_prompt_2 = \"\"\"\n",
    "Tu es un assistant spécialisé en rédaction d'e-mails professionnels.\n",
    "Voici quelques exemples de style :\n",
    "\n",
    "Exemple 1:\n",
    "Q: Rédige un e-mail pour informer un client d'un retard de livraison\n",
    "A: \n",
    "Sujet: Information concernant le retard de votre livraison\n",
    "\n",
    "Bonjour [Nom du Client],\n",
    "\n",
    "Nous tenions à vous informer que votre commande #1234 a pris du retard...\n",
    "[...suite du mail...]\n",
    "\n",
    "Exemple 2:\n",
    "Q: Envoie un e-mail de remerciement pour un entretien d'embauche\n",
    "A:\n",
    "Sujet: Remerciements suite à notre entretien\n",
    "\n",
    "Bonjour [Nom du Contact],\n",
    "\n",
    "Je tiens à vous remercier pour le temps que vous m'avez accordé...\n",
    "[...suite du mail...]\n",
    "\n",
    "Maintenant, voici ma demande:\n",
    "\n",
    "Q: Écris un e-mail pour informer un collaborateur d'un changement de planning et l'inviter à une réunion de suivi.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response_few_shot_2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,  # ex. \"gpt-4o-mini\"\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt_2}\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "print(\"=== Exemple Few-shot (e-mail professionnel) ===\")\n",
    "print(response_few_shot_2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936e3b6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Ici, nous donnons au modèle **deux exemples** de questions/réponses avant la **véritable question**. Cela oriente le style et le contexte.\n",
    "\n",
    "\n",
    "\n",
    "###  7 : Exemple Chain-of-thought (Code)\n",
    "\n",
    "On va demander un **calcul** simple, en guidant le modèle à réfléchir pas à pas :\n",
    "\n",
    "\n",
    "\n",
    "- **Masquer le raisonnement si nécessaire** :  \n",
    "  Parfois, on ne souhaite pas afficher au client final les étapes du raisonnement. Il existe des techniques pour “cacher” ce raisonnement ou n’afficher qu’un résumé. Par exemple :  \n",
    "  1. Dans le prompt, on peut demander : “Don’t reveal your chain-of-thought. Provide only the final concise answer to the user.”  \n",
    "  2. Ou effectuer un 2e appel API où l’on transmet l’enchaînement de pensées, mais on n'affiche que la conclusion.\n",
    "\n",
    "- **Adapter la température** :  \n",
    "  Pour un problème logique ou mathématique, une température trop élevée peut introduire des dérives ou des incohérences. Une température entre 0.0 et 0.3 est souvent recommandée pour les questions de calcul.\n",
    "\n",
    "- **Chain-of-thought partiel** :  \n",
    "  On peut encourager un raisonnement **intermédiaire** (quelques étapes clés) au lieu d’un raisonnement hyper détaillé, afin d’éviter que le texte devienne trop long ou difficile à comprendre.\n",
    "\n",
    "- **Expliquer la démarche** :  \n",
    "  On peut terminer par un résumé du raisonnement en 2-3 phrases, pour rendre la solution plus lisible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2afec6",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Chain-of-thought\n",
    "# ============================\n",
    "\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "\n",
    "Explique ton raisonnement étape par étape, puis donne la réponse finale.\n",
    "\"\"\"\n",
    "\n",
    "response_3 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": cot_prompt}\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.2  # on réduit la température pour moins de fantaisie\n",
    ")\n",
    "\n",
    "print(\"=== Chain-of-thought Prompt ===\")\n",
    "print(\"Réponse du modèle (avec raisonnement) :\\n\")\n",
    "print(response_3.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf51ee61",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "On demande explicitement « explique ton raisonnement ». Cela **n’oblige** pas le modèle à le faire, mais en pratique, GPT-4o-mini (ou tout modèle qui gère le CoT) fournit souvent une solution pas-à-pas.\n",
    "\n",
    "---\n",
    "\n",
    "###  8 : Exemple Self-refine (Code)\n",
    "\n",
    "L’idée : on fait **une première demande** (première réponse) et ensuite **on redemande** au modèle de s’auto-corriger.\n",
    "\n",
    "#### 8a. Premier prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa9ad1",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 8a : Self-refine Step 1\n",
    "# ============================\n",
    "\n",
    "prompt_sr1 = \"\"\"\n",
    "Ecris une courte fonction Python pour calculer la somme d'une liste. \n",
    "Ajoute un bug volontaire dans le code. \n",
    "\"\"\"\n",
    "\n",
    "response_sr1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr1}],\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "buggy_code = response_sr1.choices[0].message.content\n",
    "\n",
    "print(\"=== Self-refine (1) : Code buggy ===\\n\")\n",
    "print(buggy_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d408f70",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 8b. Self-critique et amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a62718",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 8b : Self-refine Step 2\n",
    "# ============================\n",
    "\n",
    "prompt_sr2 = f\"\"\"\n",
    "Voici un code Python qui contient un bug:\n",
    "\n",
    "{buggy_code}\n",
    "\n",
    "Peux-tu l'analyser, détecter le bug, proposer un correctif et une version améliorée du code ? \n",
    "Explique la correction.\n",
    "\"\"\"\n",
    "\n",
    "response_sr2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr2}],\n",
    "    max_tokens=400,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine (2) : Correction ===\\n\")\n",
    "print(response_sr2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626cfdf8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Self-refine avec developer role\n",
    "# ============================\n",
    "\n",
    "messages_sr = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": (\n",
    "            \"You are a self-improving coding assistant. Whenever you provide code, \"\n",
    "            \"you will automatically search for potential bugs or improvements \"\n",
    "            \"and refine your output.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Écris une fonction Python qui calcule la factorielle d'un nombre entier. \"\n",
    "            \"Ensuite, relis-toi et corrige d'éventuels bugs.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "response_self_refine = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages_sr,\n",
    "    max_tokens=300,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine avec 'developer role' ===\")\n",
    "print(response_self_refine.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f887e22",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Ici, on utilise la première réponse pour nourrir le second prompt, demandant au modèle de **critiquer** et **améliorer** la réponse initiale.\n",
    "\n",
    "---\n",
    "\n",
    "### Cellule 9 : Interactive Prompt (Code)\n",
    "\n",
    "Enfin, on peut proposer une **cellule interactive** : l’utilisateur peut saisir un prompt, et on envoie la requête au modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ec6cb",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif\n",
    "# ============================\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Fin de l'interaction.\")\n",
    "        break\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\":\"user\",\"content\":user_input}],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Réponse du modèle ===\")\n",
    "    print(resp.choices[0].message.content)\n",
    "    print(\"---------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324286d9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif avec mémoire de chat\n",
    "# ============================\n",
    "user_input = \"\";\n",
    "current_messages=[]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Fin de l'interaction.\")\n",
    "        break\n",
    "    print(\"\\n=== message de l'utilisateur ===\")\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages = current_messages + [{\"role\":\"user\",\"content\":user_input}],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Réponse du modèle ===\")\n",
    "    assistant_message = resp.choices[0].message.content\n",
    "    print(assistant_message)\n",
    "    print(\"---------------------------------------------------\\n\")\n",
    "    current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a3136",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Maintenant, tu peux saisir n’importe quel prompt, et tu verras la réponse du modèle.  \n",
    "Tape `exit` pour quitter la boucle.\n",
    "\n",
    "----\n",
    "\n",
    "## Conclusion et Ressources Supplémentaires\n",
    "\n",
    "Dans ce notebook, nous avons approfondi diverses techniques de **prompt engineering** : \n",
    "- Zero-shot  \n",
    "- Few-shot  \n",
    "- Chain-of-thought  \n",
    "- Self-refine  \n",
    "- Interactions multi-messages (avec les rôles `system`, `developer`, `user`, `assistant`)  \n",
    "\n",
    "### Ressources conseillées\n",
    "- [**OpenAI Cookbook**](https://github.com/openai/openai-cookbook) : Recettes et astuces pour résoudre des problèmes concrets (prompt engineering, RAG, etc.).  \n",
    "- [**Prompt Engineering Guide**](https://www.promptingguide.ai/) : Conseils de rédaction de prompts, cas d’usages, bonnes pratiques.  \n",
    "- [**Chaine d’outils** (LangChain, LlamaIndex, etc.)](https://github.com/hwchase17/langchain) : Facilite la création de pipelines complexes (RAG, function calling, mémoire de conversation).  \n",
    "\n",
    "> **Idées d’exercices**  \n",
    "> 1. Adapter la technique Few-shot à d’autres cas (e.g., Q&R sur la finance, la santé ou le marketing).  \n",
    "> 2. Utiliser la Self-refine pour générer un texte marketing, puis le réécrire en style “plus formel” ou “plus humoristique”.  \n",
    "> 3. Tester la **combinaison** de techniques : un prompt Few-shot + Chain-of-thought + un re-run Self-refine.\n",
    "\n",
    "---\n",
    "\n",
    "Merci d'avoir suivi ce notebook sur le **prompt engineering avancé** !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.086246,
   "end_time": "2025-12-15T19:42:29.055458",
   "environment_variables": {},
   "exception": true,
   "input_path": "MyIA.AI.Notebooks\\GenAI\\2_PromptEngineering.ipynb",
   "output_path": "rapports\\notebook_validation\\2_PromptEngineering.ipynb",
   "parameters": {},
   "start_time": "2025-12-15T19:42:24.969212",
   "version": "2.6.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}