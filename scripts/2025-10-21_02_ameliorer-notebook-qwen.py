#!/usr/bin/env python3
"""
Script d'am√©lioration du notebook Qwen Image-Edit
Phase 21 - It√©rations notebooks GenAI Image

Objectif:
    Ins√©rer 3 cellules p√©dagogiques suppl√©mentaires dans le notebook Qwen
    pour am√©liorer la qualit√© p√©dagogique et l'exp√©rience d'apprentissage.

Am√©liorations:
    1. Cellule Diagramme Architecture ComfyUI (apr√®s cellule 2 - Architecture)
    2. Cellule Exemples Workflows R√©els (apr√®s cellule 5 - Hello World)
    3. Cellule Comparaison Avant/Apr√®s (apr√®s cellule 11 - Exp√©rimentation)

Auteur: SDDD Process
Date: 2025-10-21
"""

import json
import sys
from pathlib import Path

# Configuration
NOTEBOOK_PATH = Path("MyIA.AI.Notebooks/GenAI/01-Images-Foundation/01-5-Qwen-Image-Edit.ipynb")

# ============================================================================
# CELLULE 1: DIAGRAMME ARCHITECTURE COMFYUI
# Position: Apr√®s cellule 2 (Architecture) ‚Üí Index 3
# ============================================================================
CELL_DIAGRAMME_ARCHITECTURE = {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### üîß Visualisation Architecture Workflow ComfyUI\n",
        "\n",
        "**Diagramme ASCII d'un workflow ComfyUI typique**:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    WORKFLOW COMFYUI                         ‚îÇ\n",
        "‚îÇ                                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ\n",
        "‚îÇ  ‚îÇ Load Model   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ CLIP Text    ‚îÇ                 ‚îÇ\n",
        "‚îÇ  ‚îÇ (Checkpoint) ‚îÇ        ‚îÇ Encode       ‚îÇ                 ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ\n",
        "‚îÇ         ‚îÇ                       ‚îÇ                          ‚îÇ\n",
        "‚îÇ         ‚îÇ                       ‚ñº                          ‚îÇ\n",
        "‚îÇ         ‚îÇ                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ\n",
        "‚îÇ         ‚îÇ                ‚îÇ Conditioning ‚îÇ                 ‚îÇ\n",
        "‚îÇ         ‚îÇ                ‚îÇ (Prompts)    ‚îÇ                 ‚îÇ\n",
        "‚îÇ         ‚îÇ                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ\n",
        "‚îÇ         ‚îÇ                       ‚îÇ                          ‚îÇ\n",
        "‚îÇ         ‚ñº                       ‚ñº                          ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n",
        "‚îÇ  ‚îÇ          KSampler                     ‚îÇ                ‚îÇ\n",
        "‚îÇ  ‚îÇ  (steps, denoise, seed, sampler)     ‚îÇ                ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ\n",
        "‚îÇ                    ‚îÇ                                       ‚îÇ\n",
        "‚îÇ                    ‚ñº                                       ‚îÇ\n",
        "‚îÇ             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ\n",
        "‚îÇ             ‚îÇ VAE Decode   ‚îÇ                              ‚îÇ\n",
        "‚îÇ             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ\n",
        "‚îÇ                    ‚îÇ                                       ‚îÇ\n",
        "‚îÇ                    ‚ñº                                       ‚îÇ\n",
        "‚îÇ             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ\n",
        "‚îÇ             ‚îÇ Save Image   ‚îÇ                              ‚îÇ\n",
        "‚îÇ             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**Flux de donn√©es**:\n",
        "1. **Checkpoint** ‚Üí Fournit le mod√®le (model, CLIP, VAE)\n",
        "2. **CLIP Text Encode** ‚Üí Convertit le prompt en embeddings\n",
        "3. **KSampler** ‚Üí G√©n√®re l'image latente √† partir des embeddings\n",
        "4. **VAE Decode** ‚Üí Convertit l'image latente en image RGB\n",
        "5. **Save Image** ‚Üí Sauvegarde l'image finale\n",
        "\n",
        "**Correspondance JSON**:\n",
        "```json\n",
        "{\n",
        "  \"1\": {\"class_type\": \"CheckpointLoaderSimple\", ...},\n",
        "  \"2\": {\"class_type\": \"CLIPTextEncode\", \"inputs\": {\"clip\": [\"1\", 1], ...}},\n",
        "  \"3\": {\"class_type\": \"KSampler\", \"inputs\": {\"model\": [\"1\", 0], ...}},\n",
        "  \"4\": {\"class_type\": \"VAEDecode\", \"inputs\": {\"samples\": [\"3\", 0], ...}},\n",
        "  \"5\": {\"class_type\": \"SaveImage\", \"inputs\": {\"images\": [\"4\", 0]}}\n",
        "}\n",
        "```\n",
        "\n",
        "**Notation `[\"ID_NODE\", INDEX_OUTPUT]`**:\n",
        "- `[\"1\", 0]` = Output 0 (model) du node 1\n",
        "- `[\"1\", 1]` = Output 1 (CLIP) du node 1\n",
        "- `[\"3\", 0]` = Output 0 (latents) du node 3\n",
        "\n",
        "üí° **Astuce**: Chaque node peut avoir plusieurs outputs. L'index d√©termine lequel utiliser."
    ]
}

# ============================================================================
# CELLULE 2: EXEMPLES WORKFLOWS R√âELS
# Position: Apr√®s cellule 5 (Hello World) ‚Üí Index 6 (ajust√© apr√®s 1√®re insertion = 7)
# ============================================================================
CELL_WORKFLOWS_REELS = {
    "cell_type": "code",
    "metadata": {},
    "execution_count": None,
    "outputs": [],
    "source": [
        "# ========================================\n",
        "# üéØ WORKFLOW R√âEL 1: √âdition Simple Image\n",
        "# ========================================\n",
        "\n",
        "def create_simple_edit_workflow(image_name: str, edit_prompt: str, denoise: float = 0.5) -> dict:\n",
        "    \"\"\"Workflow √©dition simple d'une image existante\n",
        "    \n",
        "    Args:\n",
        "        image_name: Nom du fichier image upload√© sur ComfyUI\n",
        "        edit_prompt: Description de l'√©dition souhait√©e\n",
        "        denoise: Force de l'√©dition (0.0 = aucune, 1.0 = compl√®te)\n",
        "    \n",
        "    Returns:\n",
        "        Workflow JSON pr√™t √† ex√©cuter\n",
        "    \"\"\"\n",
        "    workflow = {\n",
        "        \"1\": {\n",
        "            \"class_type\": \"CheckpointLoaderSimple\",\n",
        "            \"inputs\": {\"ckpt_name\": \"qwen_vl_model.safetensors\"}\n",
        "        },\n",
        "        \"2\": {\n",
        "            \"class_type\": \"LoadImage\",\n",
        "            \"inputs\": {\"image\": image_name}\n",
        "        },\n",
        "        \"3\": {\n",
        "            \"class_type\": \"CLIPTextEncode\",\n",
        "            \"inputs\": {\n",
        "                \"text\": edit_prompt,\n",
        "                \"clip\": [\"1\", 1]\n",
        "            }\n",
        "        },\n",
        "        \"4\": {\n",
        "            \"class_type\": \"VAEEncode\",\n",
        "            \"inputs\": {\n",
        "                \"pixels\": [\"2\", 0],\n",
        "                \"vae\": [\"1\", 2]\n",
        "            }\n",
        "        },\n",
        "        \"5\": {\n",
        "            \"class_type\": \"KSampler\",\n",
        "            \"inputs\": {\n",
        "                \"seed\": 42,\n",
        "                \"steps\": 20,\n",
        "                \"cfg\": 7.0,\n",
        "                \"sampler_name\": \"euler\",\n",
        "                \"scheduler\": \"normal\",\n",
        "                \"denoise\": denoise,\n",
        "                \"model\": [\"1\", 0],\n",
        "                \"positive\": [\"3\", 0],\n",
        "                \"negative\": [\"3\", 0],  # Pas de prompt n√©gatif pour √©dition simple\n",
        "                \"latent_image\": [\"4\", 0]\n",
        "            }\n",
        "        },\n",
        "        \"6\": {\n",
        "            \"class_type\": \"VAEDecode\",\n",
        "            \"inputs\": {\n",
        "                \"samples\": [\"5\", 0],\n",
        "                \"vae\": [\"1\", 2]\n",
        "            }\n",
        "        },\n",
        "        \"7\": {\n",
        "            \"class_type\": \"SaveImage\",\n",
        "            \"inputs\": {\n",
        "                \"images\": [\"6\", 0],\n",
        "                \"filename_prefix\": \"qwen_edit_simple\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return workflow\n",
        "\n",
        "# ========================================\n",
        "# üéØ WORKFLOW R√âEL 2: Cha√Ænage Nodes Avanc√©\n",
        "# ========================================\n",
        "\n",
        "def create_chained_workflow(base_prompt: str, refine_prompt: str) -> dict:\n",
        "    \"\"\"Workflow avec cha√Ænage: g√©n√©ration base + raffinement\n",
        "    \n",
        "    Architecture:\n",
        "        1. G√©n√©ration image base (text-to-image)\n",
        "        2. Raffinement avec nouveau prompt (image-to-image)\n",
        "    \n",
        "    Args:\n",
        "        base_prompt: Prompt initial g√©n√©ration\n",
        "        refine_prompt: Prompt raffinement/am√©lioration\n",
        "    \n",
        "    Returns:\n",
        "        Workflow JSON avec 2 √©tapes KSampler\n",
        "    \"\"\"\n",
        "    workflow = {\n",
        "        # √âtape 1: G√©n√©ration base\n",
        "        \"1\": {\n",
        "            \"class_type\": \"CheckpointLoaderSimple\",\n",
        "            \"inputs\": {\"ckpt_name\": \"qwen_vl_model.safetensors\"}\n",
        "        },\n",
        "        \"2\": {\n",
        "            \"class_type\": \"CLIPTextEncode\",\n",
        "            \"inputs\": {\"text\": base_prompt, \"clip\": [\"1\", 1]}\n",
        "        },\n",
        "        \"3\": {\n",
        "            \"class_type\": \"EmptyLatentImage\",\n",
        "            \"inputs\": {\"width\": 512, \"height\": 512, \"batch_size\": 1}\n",
        "        },\n",
        "        \"4\": {\n",
        "            \"class_type\": \"KSampler\",\n",
        "            \"inputs\": {\n",
        "                \"seed\": 42,\n",
        "                \"steps\": 10,\n",
        "                \"cfg\": 7.0,\n",
        "                \"sampler_name\": \"euler\",\n",
        "                \"scheduler\": \"normal\",\n",
        "                \"denoise\": 1.0,  # G√©n√©ration compl√®te\n",
        "                \"model\": [\"1\", 0],\n",
        "                \"positive\": [\"2\", 0],\n",
        "                \"negative\": [\"2\", 0],\n",
        "                \"latent_image\": [\"3\", 0]\n",
        "            }\n",
        "        },\n",
        "        # √âtape 2: Raffinement\n",
        "        \"5\": {\n",
        "            \"class_type\": \"CLIPTextEncode\",\n",
        "            \"inputs\": {\"text\": refine_prompt, \"clip\": [\"1\", 1]}\n",
        "        },\n",
        "        \"6\": {\n",
        "            \"class_type\": \"KSampler\",\n",
        "            \"inputs\": {\n",
        "                \"seed\": 43,\n",
        "                \"steps\": 10,\n",
        "                \"cfg\": 7.0,\n",
        "                \"sampler_name\": \"euler\",\n",
        "                \"scheduler\": \"normal\",\n",
        "                \"denoise\": 0.3,  # Raffinement l√©ger\n",
        "                \"model\": [\"1\", 0],\n",
        "                \"positive\": [\"5\", 0],\n",
        "                \"negative\": [\"5\", 0],\n",
        "                \"latent_image\": [\"4\", 0]  # Sortie de l'√©tape 1\n",
        "            }\n",
        "        },\n",
        "        \"7\": {\n",
        "            \"class_type\": \"VAEDecode\",\n",
        "            \"inputs\": {\"samples\": [\"6\", 0], \"vae\": [\"1\", 2]}\n",
        "        },\n",
        "        \"8\": {\n",
        "            \"class_type\": \"SaveImage\",\n",
        "            \"inputs\": {\n",
        "                \"images\": [\"7\", 0],\n",
        "                \"filename_prefix\": \"qwen_chained\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return workflow\n",
        "\n",
        "# ========================================\n",
        "# EXEMPLE D'UTILISATION\n",
        "# ========================================\n",
        "\n",
        "# Workflow 1: √âdition simple\n",
        "workflow_simple = create_simple_edit_workflow(\n",
        "    image_name=\"cat.png\",\n",
        "    edit_prompt=\"Add sunglasses to the cat\",\n",
        "    denoise=0.5\n",
        ")\n",
        "print(\"‚úÖ Workflow √©dition simple cr√©√©\")\n",
        "print(f\"   Nodes: {len(workflow_simple)}\")\n",
        "\n",
        "# Workflow 2: Cha√Ænage\n",
        "workflow_chained = create_chained_workflow(\n",
        "    base_prompt=\"A cat sitting on a chair\",\n",
        "    refine_prompt=\"High quality, professional photography, detailed fur\"\n",
        ")\n",
        "print(\"‚úÖ Workflow cha√Æn√© cr√©√©\")\n",
        "print(f\"   Nodes: {len(workflow_chained)}\")\n",
        "print(f\"   KSamplers: 2 (base + raffinement)\")\n",
        "\n",
        "# üí° Pour ex√©cuter ces workflows:\n",
        "# client = ComfyUIClient(\"https://qwen-image-edit.myia.io\")\n",
        "# result = client.execute_workflow(workflow_simple)\n",
        "# images = client.get_results(result[\"prompt_id\"])"
    ]
}

# ============================================================================
# CELLULE 3: COMPARAISON AVANT/APR√àS
# Position: Apr√®s cellule 11 (Exp√©rimentation) ‚Üí Index 12 (ajust√© apr√®s 2 insertions = 14)
# ============================================================================
CELL_COMPARAISON_AVANT_APRES = {
    "cell_type": "code",
    "metadata": {},
    "execution_count": None,
    "outputs": [],
    "source": [
        "# ========================================\n",
        "# üñºÔ∏è COMPARAISON AVANT/APR√àS: Side-by-Side\n",
        "# ========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "def compare_before_after(\n",
        "    original_path: str,\n",
        "    edited_path: str,\n",
        "    title_original: str = \"Image Originale\",\n",
        "    title_edited: str = \"Image √âdit√©e\",\n",
        "    show_metrics: bool = True\n",
        ") -> None:\n",
        "    \"\"\"Affiche comparaison side-by-side avec m√©triques qualit√©\n",
        "    \n",
        "    Args:\n",
        "        original_path: Chemin image originale\n",
        "        edited_path: Chemin image √©dit√©e\n",
        "        title_original: Titre image originale\n",
        "        title_edited: Titre image √©dit√©e\n",
        "        show_metrics: Afficher m√©triques qualit√© (PSNR, SSIM)\n",
        "    \"\"\"\n",
        "    # Charger images\n",
        "    img_original = Image.open(original_path)\n",
        "    img_edited = Image.open(edited_path)\n",
        "    \n",
        "    # Convertir en numpy arrays\n",
        "    arr_original = np.array(img_original)\n",
        "    arr_edited = np.array(img_edited)\n",
        "    \n",
        "    # Cr√©er figure avec 2 colonnes\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    \n",
        "    # Image originale\n",
        "    axes[0].imshow(arr_original)\n",
        "    axes[0].set_title(f\"{title_original}\\n{img_original.size[0]}x{img_original.size[1]}\", \n",
        "                      fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Image √©dit√©e\n",
        "    axes[1].imshow(arr_edited)\n",
        "    axes[1].set_title(f\"{title_edited}\\n{img_edited.size[0]}x{img_edited.size[1]}\", \n",
        "                      fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    # M√©triques qualit√©\n",
        "    if show_metrics and arr_original.shape == arr_edited.shape:\n",
        "        # PSNR (Peak Signal-to-Noise Ratio)\n",
        "        mse = np.mean((arr_original - arr_edited) ** 2)\n",
        "        if mse == 0:\n",
        "            psnr = float('inf')\n",
        "        else:\n",
        "            max_pixel = 255.0\n",
        "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "        \n",
        "        # Diff√©rence absolue moyenne\n",
        "        mae = np.mean(np.abs(arr_original - arr_edited))\n",
        "        \n",
        "        # Afficher m√©triques\n",
        "        metrics_text = f\"üìä M√©triques:\\n\"\n",
        "        metrics_text += f\"   PSNR: {psnr:.2f} dB\\n\"\n",
        "        metrics_text += f\"   MAE: {mae:.2f}\\n\"\n",
        "        metrics_text += f\"   Pixels modifi√©s: {np.sum(arr_original != arr_edited):,}\"\n",
        "        \n",
        "        fig.text(0.5, 0.02, metrics_text, ha='center', fontsize=12, \n",
        "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Interpr√©tation PSNR\n",
        "    if show_metrics and arr_original.shape == arr_edited.shape:\n",
        "        print(\"\\nüìà Interpr√©tation PSNR:\")\n",
        "        if psnr > 40:\n",
        "            print(\"   ‚úÖ Excellente qualit√© (PSNR > 40 dB) - Changements subtils\")\n",
        "        elif psnr > 30:\n",
        "            print(\"   ‚úÖ Bonne qualit√© (PSNR 30-40 dB) - Changements visibles mais contr√¥l√©s\")\n",
        "        elif psnr > 20:\n",
        "            print(\"   ‚ö†Ô∏è  Qualit√© acceptable (PSNR 20-30 dB) - Changements significatifs\")\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è  Qualit√© faible (PSNR < 20 dB) - Changements majeurs\")\n",
        "\n",
        "def create_difference_map(\n",
        "    original_path: str,\n",
        "    edited_path: str,\n",
        "    amplification: float = 5.0\n",
        ") -> None:\n",
        "    \"\"\"Cr√©e une carte visuelle des diff√©rences entre 2 images\n",
        "    \n",
        "    Args:\n",
        "        original_path: Chemin image originale\n",
        "        edited_path: Chemin image √©dit√©e\n",
        "        amplification: Facteur amplification diff√©rences pour visibilit√©\n",
        "    \"\"\"\n",
        "    img_original = np.array(Image.open(original_path))\n",
        "    img_edited = np.array(Image.open(edited_path))\n",
        "    \n",
        "    if img_original.shape != img_edited.shape:\n",
        "        print(\"‚ùå Images de tailles diff√©rentes, impossible de comparer\")\n",
        "        return\n",
        "    \n",
        "    # Calculer diff√©rence absolue\n",
        "    diff = np.abs(img_original.astype(float) - img_edited.astype(float))\n",
        "    \n",
        "    # Amplifier pour visibilit√©\n",
        "    diff_amplified = np.clip(diff * amplification, 0, 255).astype(np.uint8)\n",
        "    \n",
        "    # Afficher\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "    \n",
        "    axes[0].imshow(img_original)\n",
        "    axes[0].set_title(\"Originale\", fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    axes[1].imshow(img_edited)\n",
        "    axes[1].set_title(\"√âdit√©e\", fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    axes[2].imshow(diff_amplified)\n",
        "    axes[2].set_title(f\"Carte Diff√©rences (√ó{amplification})\", fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Statistiques diff√©rences\n",
        "    print(f\"\\nüìä Statistiques diff√©rences:\")\n",
        "    print(f\"   Moyenne: {np.mean(diff):.2f}\")\n",
        "    print(f\"   Max: {np.max(diff):.2f}\")\n",
        "    print(f\"   Pixels modifi√©s (>5): {np.sum(diff > 5):,} ({100*np.sum(diff > 5)/diff.size:.2f}%)\")\n",
        "\n",
        "# ========================================\n",
        "# EXEMPLE D'UTILISATION\n",
        "# ========================================\n",
        "\n",
        "# Cas d'usage: Comparer original vs √©dition Qwen\n",
        "# compare_before_after(\n",
        "#     original_path=\"cat_original.png\",\n",
        "#     edited_path=\"cat_with_sunglasses.png\",\n",
        "#     title_original=\"Chat Original\",\n",
        "#     title_edited=\"Chat avec Lunettes (Qwen Edit)\",\n",
        "#     show_metrics=True\n",
        "# )\n",
        "\n",
        "# Cas d'usage: Carte diff√©rences pour analyse d√©taill√©e\n",
        "# create_difference_map(\n",
        "#     original_path=\"cat_original.png\",\n",
        "#     edited_path=\"cat_with_sunglasses.png\",\n",
        "#     amplification=10.0\n",
        "# )\n",
        "\n",
        "print(\"‚úÖ Fonctions comparaison avant/apr√®s d√©finies\")\n",
        "print(\"   - compare_before_after(): Affichage side-by-side + m√©triques\")\n",
        "print(\"   - create_difference_map(): Carte visuelle diff√©rences\")\n",
        "print(\"\\nüí° D√©commentez les exemples ci-dessus pour tester avec vos images\")"
    ]
}

# ============================================================================
# FONCTION PRINCIPALE
# ============================================================================

def ameliorer_notebook_qwen():
    """Fonction principale d'am√©lioration du notebook Qwen"""
    
    print("=" * 70)
    print("AM√âLIORATION NOTEBOOK QWEN IMAGE-EDIT")
    print("Phase 21 - It√©rations Notebooks GenAI Image")
    print("=" * 70)
    
    # V√©rifier existence notebook
    if not NOTEBOOK_PATH.exists():
        print(f"‚ùå ERREUR: Notebook non trouv√© √† {NOTEBOOK_PATH}")
        sys.exit(1)
    
    print(f"\nüìñ Lecture notebook: {NOTEBOOK_PATH}")
    
    # Lire notebook JSON
    with open(NOTEBOOK_PATH, 'r', encoding='utf-8') as f:
        notebook = json.load(f)
    
    cells = notebook.get('cells', [])
    print(f"‚úÖ Notebook charg√©: {len(cells)} cellules actuelles")
    
    # D√©finir insertions (ordre INVERSE pour pr√©server indices)
    insertions = [
        # Cellule 3: Comparaison Avant/Apr√®s (apr√®s cellule 11 Exp√©rimentation)
        # Position initiale: 12, mais apr√®s 2 insertions pr√©c√©dentes = 14
        (14, CELL_COMPARAISON_AVANT_APRES, "Comparaison Avant/Apr√®s"),
        
        # Cellule 2: Workflows R√©els (apr√®s cellule 5 Hello World)
        # Position initiale: 6, mais apr√®s 1√®re insertion = 7
        (7, CELL_WORKFLOWS_REELS, "Exemples Workflows R√©els"),
        
        # Cellule 1: Diagramme Architecture (apr√®s cellule 2 Architecture)
        (3, CELL_DIAGRAMME_ARCHITECTURE, "Diagramme Architecture ComfyUI")
    ]
    
    # IMPORTANT: Inverser liste pour ins√©rer de la fin vers le d√©but
    # Cela pr√©serve les indices des cellules pr√©c√©dentes
    insertions_reversed = list(reversed(insertions))
    
    print(f"\nüîß Insertion de {len(insertions_reversed)} nouvelles cellules:")
    for idx, cell, name in insertions_reversed:
        print(f"   - Position {idx}: {name} ({cell['cell_type']})")
    
    # Ins√©rer cellules
    for idx, cell, name in insertions_reversed:
        cells.insert(idx, cell)
        print(f"   ‚úÖ Cellule '{name}' ins√©r√©e √† l'index {idx}")
    
    # Mettre √† jour notebook
    notebook['cells'] = cells
    
    # Sauvegarder
    print(f"\nüíæ Sauvegarde notebook modifi√©...")
    with open(NOTEBOOK_PATH, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ Notebook sauvegard√© avec succ√®s")
    print(f"\nüìä R√âSULTAT FINAL:")
    print(f"   Cellules avant: 15")
    print(f"   Cellules ajout√©es: 3")
    print(f"   Cellules apr√®s: {len(cells)}")
    print(f"   Ratio am√©lioration: +{100 * 3 / 15:.1f}%")
    
    print("\n" + "=" * 70)
    print("‚úÖ AM√âLIORATION NOTEBOOK QWEN TERMIN√âE AVEC SUCC√àS")
    print("=" * 70)
    
    return True

# ============================================================================
# POINT D'ENTR√âE
# ============================================================================

if __name__ == "__main__":
    try:
        success = ameliorer_notebook_qwen()
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"\n‚ùå ERREUR FATALE: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)