# Stable Diffusion 3.5 Service

## üìã Description

Service de g√©n√©ration d'images bas√© sur Stable Diffusion 3.5 Large.
Optimis√© pour la g√©n√©ration polyvalente production avec API FastAPI.

## üéØ R√¥le du Service

- G√©n√©ration d'images polyvalente haute qualit√©
- API REST FastAPI performante
- Support multi-r√©solutions et styles
- Interface optimis√©e pour int√©gration production

## üìÅ Structure des Fichiers

```
stable-diffusion-35/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.json               # Configuration du mod√®le et API
‚îú‚îÄ‚îÄ models/                       # Cache des mod√®les Hugging Face
‚îÇ   ‚îî‚îÄ‚îÄ (t√©l√©chargement auto depuis HF)
‚îî‚îÄ‚îÄ README.md                     # Ce fichier
```

## üì¶ Mod√®les

### T√©l√©chargement Automatique

Le mod√®le **Stable Diffusion 3.5 Large** est t√©l√©charg√© automatiquement depuis Hugging Face au premier d√©marrage.

- **Mod√®le**: `stabilityai/stable-diffusion-3.5-large`
- **Taille**: ~6.9 GB
- **Cache**: `/models` dans le container
- **Source**: https://huggingface.co/stabilityai/stable-diffusion-3.5-large

‚ö†Ô∏è **Token Hugging Face**: Peut n√©cessiter un token HF si mod√®le priv√©/gated.

### Configuration Token HF (si n√©cessaire)

```bash
# Ajouter dans .env.docker
HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxxxxxxxxx
```

## üîß Configuration

### Fichier `config/config.json`

```json
{
  "model_id": "stabilityai/stable-diffusion-3.5-large",
  "torch_dtype": "float16",
  "device": "cuda",
  "cache_dir": "/models",
  "generation_defaults": {
    "num_inference_steps": 50,
    "guidance_scale": 7.5,
    "width": 1024,
    "height": 1024
  }
}
```

### Variables d'Environnement Docker

- `CUDA_VISIBLE_DEVICES=0` : GPU √† utiliser
- `MODEL_NAME` : Nom du mod√®le HuggingFace
- `CACHE_DIR` : R√©pertoire de cache des mod√®les
- `TORCH_COMPILE=1` : Active compilation PyTorch (perf++)

## üöÄ Utilisation

### D√©marrage du Service

```powershell
# Via docker-compose principal
docker compose up stable-diffusion-35 -d

# Suivre le t√©l√©chargement du mod√®le (premier d√©marrage)
docker compose logs -f stable-diffusion-35
```

### Acc√®s √† l'API

- **API Endpoint**: http://localhost:8190
- **Health Check**: http://localhost:8190/health
- **Documentation API**: http://localhost:8190/docs (Swagger UI)

### Exemple d'Utilisation API

```python
import requests

# Test de g√©n√©ration
response = requests.post(
    "http://localhost:8190/generate",
    json={
        "prompt": "A beautiful sunset over mountains",
        "negative_prompt": "blurry, low quality",
        "width": 1024,
        "height": 1024,
        "num_inference_steps": 50,
        "guidance_scale": 7.5
    }
)

# R√©cup√©rer l'image
if response.status_code == 200:
    image_data = response.json()["image"]
    # Traiter l'image (base64 ou URL)
```

## üß™ Commandes de Test

```powershell
# V√©rifier que le service d√©marre
docker compose logs stable-diffusion-35

# V√©rifier l'utilisation GPU
docker exec stable-diffusion-35 nvidia-smi

# Test health check
curl http://localhost:8190/health

# Tester la documentation Swagger
# Ouvrir: http://localhost:8190/docs

# V√©rifier les mod√®les t√©l√©charg√©s
docker exec stable-diffusion-35 ls -lh /models/
docker exec stable-diffusion-35 du -sh /models/*
```

## üìä Ressources Syst√®me

### Configuration Minimale
- **RAM**: 12 GB
- **GPU VRAM**: 10 GB (RTX 3080 minimum)
- **Espace Disque**: 15 GB (mod√®le + cache)

### Configuration Recommand√©e
- **RAM**: 24 GB
- **GPU VRAM**: 16 GB+ (RTX 3090 / 4090)
- **Espace Disque**: 30 GB

### Temps de G√©n√©ration Estim√©s

| R√©solution | RTX 3080 Ti | RTX 3090 |
|------------|-------------|----------|
| 512x512    | ~8s         | ~6s      |
| 1024x1024  | ~15s        | ~12s     |
| 1536x1536  | ~30s        | ~25s     |

## üêõ D√©pannage

### Le mod√®le ne se t√©l√©charge pas

```powershell
# V√©rifier les logs
docker compose logs stable-diffusion-35 | Select-String -Pattern "download|error"

# V√©rifier la connexion Hugging Face
docker exec stable-diffusion-35 curl -I https://huggingface.co
```

### Erreur "Token required"

Le mod√®le peut √™tre gated. Solution:
1. Cr√©er un token sur https://huggingface.co/settings/tokens
2. Ajouter `HUGGINGFACE_TOKEN` dans `.env.docker`
3. Red√©marrer le service

### GPU non d√©tect√©

```powershell
# V√©rifier NVIDIA Docker runtime
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### M√©moire GPU insuffisante

Solutions:
- R√©duire la r√©solution (512x512 au lieu de 1024x1024)
- Activer `enable_attention_slicing` dans config.json
- Activer `enable_cpu_offload` (plus lent mais utilise moins de VRAM)
- Utiliser le GPU avec plus de VRAM (RTX 3090)

### G√©n√©ration trop lente

Optimisations:
- V√©rifier que `TORCH_COMPILE=1` est activ√©
- R√©duire `num_inference_steps` (25-30 au lieu de 50)
- Utiliser `fp16` precision (d√©j√† par d√©faut)

## üìö Ressources

- [Stable Diffusion 3.5 Hugging Face](https://huggingface.co/stabilityai/stable-diffusion-3.5-large)
- [Diffusers Documentation](https://huggingface.co/docs/diffusers)
- [API FastAPI Docs](http://localhost:8190/docs) (une fois service d√©marr√©)