services:
  vllm-zimage:
    image: vllm/vllm-omni:v0.12.0rc1
    container_name: vllm-zimage
    ipc: host
    ports:
      - "${VLLM_ZIMAGE_PORT:-8001}:8000"
    volumes:
      - ../../shared/cache/huggingface:/root/.cache/huggingface
      - ../../shared/models:/models:ro
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
      - NCCL_P2P_DISABLE=1
    # vLLM Omni serve command for Z-Image-Turbo text-to-image
    # API endpoint: /v1/chat/completions (OpenAI-compatible)
    # Supported resolutions: 1328x1328, 1664x928, 928x1664, 1472x1140, etc.
    # Note: L'entrypoint de l'image est déjà "vllm serve --omni", on passe juste les arguments
    # Optimizations: GPU memory utilization, max sequences, VAE slicing/tiling
    command:
      - "Tongyi-MAI/Z-Image-Turbo"
      - "--served-model-name"
      - "z-image"
      - "--port"
      - "8000"
      - "--dtype"
      - "bfloat16"
      - "--trust-remote-code"
      - "--gpu-memory-utilization"
      - "0.3"
      - "--max-num-seqs"
      - "4"
      - "--vae-use-slicing"
      - "--vae-use-tiling"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s
    restart: unless-stopped
    networks:
      - genai-network

networks:
  genai-network:
    external: true
    name: genai-network
