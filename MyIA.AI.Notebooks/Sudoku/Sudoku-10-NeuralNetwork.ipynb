{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-nav",
   "metadata": {},
   "source": "# Sudoku-10 : Résolution par Réseaux de Neurones\n\n**Navigation** : [<< Sudoku-9-HumanStrategies](Sudoku-9-HumanStrategies.ipynb) | [Index](README.md) | [Sudoku-11-Comparison >>](Sudoku-11-Comparison.ipynb)\n\n**Voir aussi** : [GenAI](../GenAI/README.md) pour les réseaux de neurones\n\n## Objectifs d'apprentissage\n\nA la fin de ce notebook, vous saurez :\n1. Encoder une grille de Sudoku sous forme de tenseur adapté à un réseau de neurones\n2. Concevoir et entraîner un réseau dense (MLP) pour prédire les chiffres manquants\n3. Concevoir et entraîner un réseau convolutif (CNN) qui exploite la structure spatiale\n4. Utiliser la prédiction itérative pour améliorer drastiquement la précision\n5. Comparer les limites des approches neuronales avec les solveurs algorithmiques\n\n### Prérequis\n- Python 3.10+, notions de base en deep learning (couches, loss, backpropagation)\n- Familiarité avec NumPy et matplotlib\n- Pas besoin de GPU : les modèles sont volontairement légers\n\n### Durée estimée : 55 minutes\n\n### Contexte\nCe notebook s'inspire du projet [jsboigeEpita/2024-EPITA-SCIA-PPC-Sudoku-CV](https://github.com/jsboigeEpita/2024-EPITA-SCIA-PPC-Sudoku-CV) (module `Sudoku.NeuralNetwork`) qui explore 4 architectures de réseaux de neurones entraînés sur 17 millions de puzzles. Ici, nous reproduisons les idées clés sur un dataset réduit pour rester exécutable en quelques minutes."
  },
  {
   "cell_type": "markdown",
   "id": "intro-section",
   "metadata": {},
   "source": [
    "## 1. Introduction : le Sudoku vu comme un probleme de reconnaissance de motifs\n",
    "\n",
    "Les notebooks precedents de cette serie resolvent le Sudoku par des approches algorithmiques : backtracking, programmation par contraintes, recherche locale. Mais peut-on **apprendre** a resoudre un Sudoku ?\n",
    "\n",
    "### Pourquoi un reseau de neurones ?\n",
    "\n",
    "Un reseau de neurones ne \"comprend\" pas les regles du Sudoku. Il apprend des **correlations statistiques** entre les grilles partielles et leurs solutions a partir de milliers d'exemples. C'est une approche fondamentalement differente :\n",
    "\n",
    "| Approche | Mecanisme | Garantie de solution valide |\n",
    "|----------|-----------|-----------------------------|\n",
    "| **Backtracking** | Exploration exhaustive des possibilites | Oui |\n",
    "| **CP-SAT / Z3** | Satisfaction de contraintes formelles | Oui |\n",
    "| **Reseau de neurones** | Apprentissage de motifs a partir de donnees | Non |\n",
    "\n",
    "### Formulation du probleme\n",
    "\n",
    "- **Entree** : une grille 9x9 avec des cases vides (valeur 0)\n",
    "- **Sortie** : pour chaque case, une distribution de probabilite sur les chiffres 1-9\n",
    "- **Encodage** : one-hot encoding pour transformer les entiers en tenseurs\n",
    "\n",
    "```\n",
    "Grille brute :     Tenseur one-hot (9x9x10) :\n",
    "[5, 0, 0, ...]  -> canal 0 = masque vide\n",
    "                    canal 5 = 1 en position (0,0)\n",
    "                    etc.\n",
    "```\n",
    "\n",
    "L'idee cle est que le reseau apprend a \"deviner\" le bon chiffre pour chaque case en s'appuyant sur le contexte spatial (ligne, colonne, bloc 3x3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Mode batch pour execution automatisee (Papermill)\n",
    "BATCH_MODE = os.environ.get('BATCH_MODE', 'false').lower() == 'true'\n",
    "\n",
    "# Reproductibilite\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch {torch.__version__}\")\n",
    "print(f\"Device : {device}\")\n",
    "print(f\"BATCH_MODE : {BATCH_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "## 2. Preparation des donnees\n",
    "\n",
    "Pour entrainer un reseau de neurones, il faut un **dataset** de paires (puzzle, solution). Plutot que de telecharger un fichier externe, nous generons nos donnees de maniere programmatique grace a un solveur par backtracking.\n",
    "\n",
    "### Strategie de generation\n",
    "\n",
    "1. **Generer une grille complete valide** : remplir une grille vide par backtracking avec un ordre aleatoire des chiffres\n",
    "2. **Creer le puzzle** : retirer aleatoirement entre 40 et 55 cases\n",
    "3. **Stocker la paire** : (puzzle, solution_complete)\n",
    "\n",
    "### Taille du dataset\n",
    "\n",
    "Le projet de reference utilise 17 millions de puzzles. Ici, nous travaillons avec **1000 puzzles** pour permettre un entrainement rapide (~1-2 min). C'est suffisant pour observer les comportements fondamentaux des architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-intro",
   "metadata": {},
   "source": [
    "### 2.1 Generateur de puzzles\n",
    "\n",
    "Le generateur utilise un backtracking classique avec une astuce : les chiffres sont testes dans un **ordre aleatoire** a chaque appel, ce qui produit une grille complete differente a chaque execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(grid: np.ndarray, row: int, col: int, num: int) -> bool:\n",
    "    \"\"\"Verifie si placer num a (row, col) est valide.\"\"\"\n",
    "    # Ligne\n",
    "    if num in grid[row, :]:\n",
    "        return False\n",
    "    # Colonne\n",
    "    if num in grid[:, col]:\n",
    "        return False\n",
    "    # Bloc 3x3\n",
    "    br, bc = 3 * (row // 3), 3 * (col // 3)\n",
    "    if num in grid[br:br+3, bc:bc+3]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def solve_random(grid: np.ndarray) -> bool:\n",
    "    \"\"\"Resout par backtracking avec ordre aleatoire des chiffres.\"\"\"\n",
    "    for r in range(9):\n",
    "        for c in range(9):\n",
    "            if grid[r, c] == 0:\n",
    "                digits = list(range(1, 10))\n",
    "                np.random.shuffle(digits)\n",
    "                for num in digits:\n",
    "                    if is_valid(grid, r, c, num):\n",
    "                        grid[r, c] = num\n",
    "                        if solve_random(grid):\n",
    "                            return True\n",
    "                        grid[r, c] = 0\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def generate_puzzle(n_remove_min: int = 40, n_remove_max: int = 55) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Genere un puzzle et sa solution.\n",
    "    \n",
    "    Returns:\n",
    "        (puzzle, solution) : grilles 9x9 en entiers 0-9\n",
    "    \"\"\"\n",
    "    # Generer une grille complete\n",
    "    solution = np.zeros((9, 9), dtype=int)\n",
    "    solve_random(solution)\n",
    "    \n",
    "    # Creer le puzzle en retirant des cases\n",
    "    puzzle = solution.copy()\n",
    "    n_remove = np.random.randint(n_remove_min, n_remove_max + 1)\n",
    "    indices = np.random.choice(81, size=n_remove, replace=False)\n",
    "    for idx in indices:\n",
    "        puzzle[idx // 9, idx % 9] = 0\n",
    "    \n",
    "    return puzzle, solution\n",
    "\n",
    "\n",
    "# Test : generer un puzzle\n",
    "test_puzzle, test_solution = generate_puzzle()\n",
    "n_empty = np.sum(test_puzzle == 0)\n",
    "print(f\"Puzzle genere : {n_empty} cases vides\")\n",
    "print(\"\\nPuzzle :\")\n",
    "print(test_puzzle)\n",
    "print(\"\\nSolution :\")\n",
    "print(test_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-intro",
   "metadata": {},
   "source": [
    "### 2.2 Construction du dataset\n",
    "\n",
    "Nous generons 1000 puzzles et les separons en 800 pour l'entrainement et 200 pour le test.\n",
    "\n",
    "**Encodage one-hot** :\n",
    "- **Puzzle** : tenseur `(9, 9, 10)` -- le canal 0 represente \"case vide\", les canaux 1-9 representent les chiffres\n",
    "- **Solution** : tenseur `(9, 9)` avec les chiffres 1-9 decales en indices 0-8 pour la cross-entropy\n",
    "\n",
    "Cet encodage permet au reseau de distinguer explicitement une case vide (a predire) d'une case contenant le chiffre 0 (qui n'existe pas en Sudoku)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_puzzle(puzzle: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Encode un puzzle en one-hot (9, 9, 10).\n",
    "    Canal 0 = case vide, canaux 1-9 = chiffres.\"\"\"\n",
    "    encoded = np.zeros((9, 9, 10), dtype=np.float32)\n",
    "    for r in range(9):\n",
    "        for c in range(9):\n",
    "            encoded[r, c, puzzle[r, c]] = 1.0\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def encode_solution(solution: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Encode la solution en indices 0-8 (pour cross-entropy).\"\"\"\n",
    "    return (solution - 1).astype(np.int64)  # chiffres 1-9 -> indices 0-8\n",
    "\n",
    "\n",
    "# Generer le dataset\n",
    "N_SAMPLES = 1000\n",
    "print(f\"Generation de {N_SAMPLES} puzzles...\")\n",
    "\n",
    "start = time.time()\n",
    "puzzles_raw = []\n",
    "solutions_raw = []\n",
    "\n",
    "for i in range(N_SAMPLES):\n",
    "    p, s = generate_puzzle()\n",
    "    puzzles_raw.append(p)\n",
    "    solutions_raw.append(s)\n",
    "    if (i + 1) % 200 == 0:\n",
    "        print(f\"  {i + 1}/{N_SAMPLES} generes...\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"Generation terminee en {elapsed:.1f}s\")\n",
    "\n",
    "# Encoder\n",
    "X_all = np.array([encode_puzzle(p) for p in puzzles_raw])    # (N, 9, 9, 10)\n",
    "y_all = np.array([encode_solution(s) for s in solutions_raw]) # (N, 9, 9)\n",
    "\n",
    "# Split train/test\n",
    "N_TRAIN = 800\n",
    "X_train, X_test = X_all[:N_TRAIN], X_all[N_TRAIN:]\n",
    "y_train, y_test = y_all[:N_TRAIN], y_all[N_TRAIN:]\n",
    "puzzles_test = puzzles_raw[N_TRAIN:]\n",
    "solutions_test = solutions_raw[N_TRAIN:]\n",
    "\n",
    "print(f\"\\nDataset :\")\n",
    "print(f\"  Train : {X_train.shape[0]} puzzles\")\n",
    "print(f\"  Test  : {X_test.shape[0]} puzzles\")\n",
    "print(f\"  X shape : {X_train.shape}\")\n",
    "print(f\"  y shape : {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-interp",
   "metadata": {},
   "source": [
    "### Interpretation : encodage des donnees\n",
    "\n",
    "| Element | Shape | Description |\n",
    "|---------|-------|-------------|\n",
    "| Puzzle brut | `(9, 9)` | Entiers 0-9, 0 = case vide |\n",
    "| Puzzle encode | `(9, 9, 10)` | One-hot, canal 0 = vide |\n",
    "| Solution cible | `(9, 9)` | Indices 0-8 (chiffre - 1) |\n",
    "\n",
    "Le one-hot encoding est essentiel : sans lui, le reseau traiterait les chiffres comme des valeurs ordonnees (5 > 3) alors qu'en Sudoku ils sont simplement des **etiquettes** sans relation d'ordre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "torch-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudokuDataset(Dataset):\n",
    "    \"\"\"Dataset PyTorch pour les puzzles de Sudoku.\"\"\"\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        # Reorganiser en (N, channels, H, W) pour PyTorch\n",
    "        self.X = torch.FloatTensor(X).permute(0, 3, 1, 2)  # (N, 10, 9, 9)\n",
    "        self.y = torch.LongTensor(y)                        # (N, 9, 9)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Creer les DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = SudokuDataset(X_train, y_train)\n",
    "test_dataset = SudokuDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Verifier les dimensions\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "print(f\"Batch X : {X_batch.shape}\")  # (64, 10, 9, 9)\n",
    "print(f\"Batch y : {y_batch.shape}\")  # (64, 9, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-section",
   "metadata": {},
   "source": [
    "## 3. Architecture 1 : Reseau Dense (MLP)\n",
    "\n",
    "La premiere approche est la plus naive : aplatir la grille en un vecteur et utiliser des couches denses (fully connected). Le reseau traite la grille comme un vecteur de 810 valeurs (9 x 9 x 10) et produit 729 sorties (9 x 9 x 9 = une distribution sur 9 chiffres pour chaque case).\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Input(810) -> Dense(512) -> ReLU -> Dense(512) -> ReLU -> Dense(729) -> Reshape(9, 9, 9)\n",
    "```\n",
    "\n",
    "### Limitations attendues\n",
    "\n",
    "Le MLP n'a **aucune notion de structure spatiale**. Il ne sait pas que la case (0,0) et la case (0,8) sont dans la meme ligne, ni que les cases forment des blocs 3x3. Tout est un vecteur plat. Malgre cela, il peut apprendre certains motifs statistiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseModel(nn.Module):\n",
    "    \"\"\"Reseau dense (MLP) pour la resolution de Sudoku.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(9 * 9 * 10, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 9 * 9 * 9)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 10, 9, 9)\n",
    "        x = self.flatten(x)              # (batch, 810)\n",
    "        x = self.network(x)              # (batch, 729)\n",
    "        x = x.view(-1, 9, 9, 9)          # (batch, 9, 9, 9)\n",
    "        return x\n",
    "\n",
    "\n",
    "dense_model = DenseModel().to(device)\n",
    "n_params = sum(p.numel() for p in dense_model.parameters())\n",
    "print(f\"DenseModel : {n_params:,} parametres\")\n",
    "print(dense_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-fn-intro",
   "metadata": {},
   "source": [
    "### Fonction d'entrainement et d'evaluation\n",
    "\n",
    "Nous utilisons la **cross-entropy** comme fonction de perte : pour chaque case, le reseau produit 9 logits et la perte mesure l'ecart avec le bon chiffre. Deux metriques sont calculees :\n",
    "\n",
    "- **Precision par case** : pourcentage de cases individuelles correctement predites\n",
    "- **Precision par grille** : pourcentage de grilles entierement correctes (les 81 cases justes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, n_epochs=20, lr=1e-3):\n",
    "    \"\"\"Entraine un modele et retourne l'historique.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'test_loss': [],\n",
    "        'cell_acc': [], 'grid_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # --- Entrainement ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)       # (batch, 9, 9, 9)\n",
    "            # Reshape pour cross-entropy : (batch*81, 9) vs (batch*81)\n",
    "            loss = criterion(\n",
    "                output.view(-1, 9),\n",
    "                y_batch.view(-1)\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        train_loss /= n_batches\n",
    "        \n",
    "        # --- Evaluation ---\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct_cells = 0\n",
    "        total_cells = 0\n",
    "        correct_grids = 0\n",
    "        total_grids = 0\n",
    "        n_batches_test = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output.view(-1, 9), y_batch.view(-1))\n",
    "                test_loss += loss.item()\n",
    "                n_batches_test += 1\n",
    "                \n",
    "                # Predictions\n",
    "                preds = output.argmax(dim=-1)  # (batch, 9, 9)\n",
    "                correct_cells += (preds == y_batch).sum().item()\n",
    "                total_cells += y_batch.numel()\n",
    "                \n",
    "                # Grilles entierement correctes\n",
    "                grid_correct = (preds == y_batch).all(dim=-1).all(dim=-1)\n",
    "                correct_grids += grid_correct.sum().item()\n",
    "                total_grids += y_batch.shape[0]\n",
    "        \n",
    "        test_loss /= n_batches_test\n",
    "        cell_acc = correct_cells / total_cells\n",
    "        grid_acc = correct_grids / total_grids\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['cell_acc'].append(cell_acc)\n",
    "        history['grid_acc'].append(grid_acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:2d}/{n_epochs} | \"\n",
    "                  f\"Loss train={train_loss:.4f} test={test_loss:.4f} | \"\n",
    "                  f\"Cell acc={cell_acc:.3f} | Grid acc={grid_acc:.3f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def plot_history(history, title=\"\"):\n",
    "    \"\"\"Affiche les courbes d'entrainement.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].plot(history['train_loss'], label='Train')\n",
    "    axes[0].plot(history['test_loss'], label='Test')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Fonction de perte')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(history['cell_acc'])\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Precision')\n",
    "    axes[1].set_title('Precision par case')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].plot(history['grid_acc'])\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Precision')\n",
    "    axes[2].set_title('Precision par grille complete')\n",
    "    axes[2].set_ylim(0, 1)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-train-intro",
   "metadata": {},
   "source": [
    "### Entrainement du MLP\n",
    "\n",
    "Nous entrainons le reseau dense pendant 20 epochs avec l'optimiseur Adam. Observez l'ecart entre la precision par case (relativement elevee) et la precision par grille (beaucoup plus basse) : il suffit d'une seule case fausse pour qu'une grille entiere soit comptee comme incorrecte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "print(\"=== Entrainement du reseau dense (MLP) ===\")\n",
    "dense_model = DenseModel().to(device)\n",
    "\n",
    "start = time.time()\n",
    "dense_history = train_model(dense_model, train_loader, test_loader, n_epochs=N_EPOCHS)\n",
    "dense_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTemps d'entrainement : {dense_time:.1f}s\")\n",
    "plot_history(dense_history, \"Reseau Dense (MLP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-interp",
   "metadata": {},
   "source": [
    "### Interpretation : performances du MLP\n",
    "\n",
    "**Observations attendues** :\n",
    "\n",
    "| Metrique | Valeur typique | Explication |\n",
    "|----------|---------------|-------------|\n",
    "| Precision par case | ~70-85% | Le reseau apprend les motifs frequents |\n",
    "| Precision par grille | ~0-5% | Tres peu de grilles entierement correctes |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le MLP peut atteindre une bonne precision par case, mais cela ne suffit pas : chaque erreur se propage et invalide la grille\n",
    "2. Sans notion de structure spatiale, le reseau ne peut pas exploiter les contraintes de lignes, colonnes et blocs\n",
    "3. La precision par grille reste tres faible car 81 predictions doivent etre simultanement correctes\n",
    "\n",
    "> **Note technique** : sur 1000 puzzles d'entrainement, le MLP apprend essentiellement les cases les plus faciles (celles contraintes par beaucoup d'indices). Les cases ambigues restent difficiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn-section",
   "metadata": {},
   "source": [
    "## 4. Architecture 2 : Reseau Convolutif (CNN)\n",
    "\n",
    "Les reseaux convolutifs sont connus pour leur capacite a capturer les **motifs spatiaux locaux**. En traitant la grille de Sudoku comme une \"image\" a 10 canaux, les filtres de convolution peuvent apprendre les interactions entre cases voisines.\n",
    "\n",
    "### Pourquoi la convolution est pertinente\n",
    "\n",
    "Les contraintes du Sudoku sont **locales** :\n",
    "- Les cases d'un meme bloc 3x3 interagissent (filtre 3x3 capture exactement un bloc)\n",
    "- Les cases d'une ligne/colonne interagissent (filtres empiles capturent des contextes plus larges)\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Input(10, 9, 9)\n",
    "  -> Conv2D(64, 3x3, pad=same) -> BN -> ReLU\n",
    "  -> Conv2D(128, 3x3, pad=same) -> BN -> ReLU\n",
    "  -> Conv2D(128, 3x3, pad=same) -> BN -> ReLU\n",
    "  -> Conv2D(256, 3x3, pad=same) -> BN -> ReLU\n",
    "  -> Conv2D(256, 3x3, pad=same) -> BN -> ReLU\n",
    "  -> Conv2D(9, 1x1) -> Permute\n",
    "Output(9, 9, 9)\n",
    "```\n",
    "\n",
    "Le `padding='same'` preserve la resolution spatiale (9x9) tout au long du reseau. La derniere couche `1x1` produit 9 canaux (un par chiffre possible) pour chaque case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cnn-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    \"\"\"Reseau convolutif pour la resolution de Sudoku.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Couche 1 : capturer les motifs locaux\n",
    "            nn.Conv2d(10, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Couche 2 : combiner les motifs\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Couche 3 : contexte plus large\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Couche 4 : representation riche\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Couche 5 : affiner les predictions\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Couche de sortie : 1x1 conv pour produire 9 classes\n",
    "            nn.Conv2d(256, 9, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 10, 9, 9)\n",
    "        x = self.conv_layers(x)           # (batch, 9, 9, 9)\n",
    "        x = x.permute(0, 2, 3, 1)         # (batch, 9, 9, 9) - dernier dim = classes\n",
    "        return x\n",
    "\n",
    "\n",
    "cnn_model = CNNModel().to(device)\n",
    "n_params = sum(p.numel() for p in cnn_model.parameters())\n",
    "print(f\"CNNModel : {n_params:,} parametres\")\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn-train-intro",
   "metadata": {},
   "source": [
    "### Entrainement du CNN\n",
    "\n",
    "Le CNN est entraine dans les memes conditions que le MLP (20 epochs, meme dataset). L'objectif est de comparer equitablement les deux architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cnn-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Entrainement du reseau convolutif (CNN) ===\")\n",
    "cnn_model = CNNModel().to(device)\n",
    "\n",
    "start = time.time()\n",
    "cnn_history = train_model(cnn_model, train_loader, test_loader, n_epochs=N_EPOCHS)\n",
    "cnn_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTemps d'entrainement : {cnn_time:.1f}s\")\n",
    "plot_history(cnn_history, \"Reseau Convolutif (CNN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn-interp",
   "metadata": {},
   "source": [
    "### Interpretation : CNN vs MLP\n",
    "\n",
    "**Comparaison directe** :\n",
    "\n",
    "| Metrique | MLP | CNN | Avantage CNN |\n",
    "|----------|-----|-----|-------|\n",
    "| Precision par case | ~70-85% | ~80-92% | Structure spatiale exploitee |\n",
    "| Precision par grille | ~0-5% | ~5-15% | Meilleur mais toujours faible |\n",
    "| Nombre de parametres | ~700K | ~500K | CNN plus compact |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le CNN surpasse le MLP grace aux filtres 3x3 qui capturent naturellement les blocs du Sudoku\n",
    "2. Avec 5 couches de convolution et `padding=same`, le champ receptif couvre une grande partie de la grille\n",
    "3. Malgre l'amelioration, la precision par grille reste modeste : le reseau fait encore des erreurs occasionnelles\n",
    "\n",
    "> **Note technique** : avec le dataset de reference de 17M puzzles, ces architectures atteignent des precisions par case de 95%+. Notre dataset reduit (1000 puzzles) limite la generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iterative-section",
   "metadata": {},
   "source": [
    "## 5. Prediction iterative : resoudre case par case\n",
    "\n",
    "L'idee cle pour ameliorer drastiquement la precision est d'imiter la strategie humaine : **predire la case la plus certaine, la remplir, puis re-predire**.\n",
    "\n",
    "### Principe\n",
    "\n",
    "Au lieu de predire les 81 cases d'un coup, on procede iterativement :\n",
    "\n",
    "1. Le reseau predit les probabilites pour toutes les cases vides\n",
    "2. On selectionne la prediction la plus confiante (probabilite maximale la plus elevee)\n",
    "3. On remplit cette case dans le puzzle\n",
    "4. On re-encode le puzzle mis a jour et on recommence\n",
    "5. On repete jusqu'a ce que toutes les cases soient remplies\n",
    "\n",
    "### Pourquoi cela fonctionne\n",
    "\n",
    "Chaque case remplie fournit un **indice supplementaire** au reseau pour les cases restantes. C'est un mecanisme d'auto-regression : les predictions les plus sures servent de base pour les predictions suivantes.\n",
    "\n",
    "```\n",
    "Iteration 1 : 45 cases vides -> predire la plus certaine (confiance 99%)\n",
    "Iteration 2 : 44 cases vides -> plus d'indices, predictions plus sures\n",
    "...\n",
    "Iteration 45 : 1 case vide -> presque triviale\n",
    "```\n",
    "\n",
    "**Risque** : si une prediction intermediaire est fausse, les erreurs se propagent. Le seuil de confiance peut attenuer ce risque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iterative-predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_predict(model, puzzle: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    \"\"\"Prediction iterative : remplir une case a la fois.\n",
    "    \n",
    "    Args:\n",
    "        model: Modele entraine\n",
    "        puzzle: Grille 9x9 (0 = vide)\n",
    "        verbose: Afficher les etapes intermediaires\n",
    "    \n",
    "    Returns:\n",
    "        Grille 9x9 completee\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    current = puzzle.copy()\n",
    "    steps = []\n",
    "    \n",
    "    while np.any(current == 0):\n",
    "        # Encoder le puzzle courant\n",
    "        encoded = encode_puzzle(current)\n",
    "        x = torch.FloatTensor(encoded).permute(2, 0, 1).unsqueeze(0).to(device)  # (1, 10, 9, 9)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(x)  # (1, 9, 9, 9)\n",
    "            probs = torch.softmax(output[0], dim=-1)  # (9, 9, 9)\n",
    "        \n",
    "        probs = probs.cpu().numpy()\n",
    "        \n",
    "        # Trouver la case vide avec la prediction la plus confiante\n",
    "        best_conf = -1\n",
    "        best_pos = None\n",
    "        best_digit = None\n",
    "        \n",
    "        for r in range(9):\n",
    "            for c in range(9):\n",
    "                if current[r, c] == 0:\n",
    "                    max_prob = probs[r, c].max()\n",
    "                    if max_prob > best_conf:\n",
    "                        best_conf = max_prob\n",
    "                        best_pos = (r, c)\n",
    "                        best_digit = probs[r, c].argmax() + 1  # indice 0-8 -> chiffre 1-9\n",
    "        \n",
    "        if best_pos is None:\n",
    "            break\n",
    "        \n",
    "        current[best_pos[0], best_pos[1]] = best_digit\n",
    "        steps.append((best_pos, best_digit, best_conf))\n",
    "        \n",
    "        if verbose and len(steps) <= 5:\n",
    "            print(f\"  Etape {len(steps):2d} : case ({best_pos[0]},{best_pos[1]}) \"\n",
    "                  f\"= {best_digit} (confiance {best_conf:.3f})\")\n",
    "    \n",
    "    if verbose:\n",
    "        if len(steps) > 5:\n",
    "            print(f\"  ... ({len(steps) - 5} etapes supplementaires)\")\n",
    "        print(f\"  Total : {len(steps)} cases remplies\")\n",
    "    \n",
    "    return current, steps\n",
    "\n",
    "\n",
    "# Test sur quelques puzzles\n",
    "print(\"=== Prediction iterative avec le CNN ===\")\n",
    "print()\n",
    "\n",
    "for i in range(3):\n",
    "    puzzle = puzzles_test[i]\n",
    "    solution = solutions_test[i]\n",
    "    n_empty = np.sum(puzzle == 0)\n",
    "    \n",
    "    print(f\"Puzzle {i+1} ({n_empty} cases vides) :\")\n",
    "    \n",
    "    # Prediction directe (en une passe)\n",
    "    encoded = encode_puzzle(puzzle)\n",
    "    x = torch.FloatTensor(encoded).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        direct_output = cnn_model(x)\n",
    "        direct_preds = direct_output[0].argmax(dim=-1).cpu().numpy() + 1\n",
    "    direct_correct = np.sum(direct_preds == solution)\n",
    "    \n",
    "    # Prediction iterative\n",
    "    iterative_result, steps = iterative_predict(cnn_model, puzzle, verbose=True)\n",
    "    iter_correct = np.sum(iterative_result == solution)\n",
    "    \n",
    "    print(f\"  Directe   : {direct_correct}/81 cases correctes ({direct_correct == 81})\")\n",
    "    print(f\"  Iterative : {iter_correct}/81 cases correctes ({iter_correct == 81})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iterative-interp",
   "metadata": {},
   "source": [
    "### Interpretation : impact de la prediction iterative\n",
    "\n",
    "La prediction iterative ameliore considerablement les resultats car :\n",
    "\n",
    "| Aspect | Prediction directe | Prediction iterative |\n",
    "|--------|-------------------|---------------------|\n",
    "| Contexte utilise | Uniquement les indices initiaux | Indices + cases deja remplies |\n",
    "| Risque d'erreur | Independant par case | Risque de propagation d'erreurs |\n",
    "| Precision par grille | Faible | Nettement superieure |\n",
    "\n",
    "**Points cles** :\n",
    "1. La confiance du reseau est un bon indicateur : les premieres cases remplies sont souvent correctes\n",
    "2. L'amelioration est d'autant plus forte que le modele est precis par case\n",
    "3. C'est exactement le mecanisme utilise par les humains : resoudre d'abord les cases evidentes\n",
    "\n",
    "> **Note technique** : cette approche est similaire aux modeles auto-regressifs en NLP (GPT) qui generent un token a la fois en conditionnant sur les tokens precedents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-section",
   "metadata": {},
   "source": [
    "## 6. Evaluation comparative et analyse\n",
    "\n",
    "Evaluons systematiquement les deux architectures (MLP et CNN) avec les deux strategies de prediction (directe et iterative) sur le jeu de test complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, puzzles, solutions, use_iterative=False):\n",
    "    \"\"\"Evalue un modele sur un ensemble de puzzles.\n",
    "    \n",
    "    Returns:\n",
    "        dict avec cell_acc, grid_acc, et details par puzzle\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_correct_cells = 0\n",
    "    total_cells = 0\n",
    "    correct_grids = 0\n",
    "    n_puzzles = len(puzzles)\n",
    "    \n",
    "    for puzzle, solution in zip(puzzles, solutions):\n",
    "        if use_iterative:\n",
    "            prediction, _ = iterative_predict(model, puzzle)\n",
    "        else:\n",
    "            encoded = encode_puzzle(puzzle)\n",
    "            x = torch.FloatTensor(encoded).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(x)\n",
    "                prediction = output[0].argmax(dim=-1).cpu().numpy() + 1\n",
    "        \n",
    "        n_correct = np.sum(prediction == solution)\n",
    "        total_correct_cells += n_correct\n",
    "        total_cells += 81\n",
    "        if n_correct == 81:\n",
    "            correct_grids += 1\n",
    "    \n",
    "    return {\n",
    "        'cell_acc': total_correct_cells / total_cells,\n",
    "        'grid_acc': correct_grids / n_puzzles,\n",
    "        'correct_grids': correct_grids,\n",
    "        'total': n_puzzles\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluation complete\n",
    "print(\"=== Evaluation sur le jeu de test (200 puzzles) ===\")\n",
    "print()\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"MLP - prediction directe...\")\n",
    "results['MLP direct'] = evaluate_model(dense_model, puzzles_test, solutions_test, use_iterative=False)\n",
    "\n",
    "print(\"MLP - prediction iterative...\")\n",
    "results['MLP iteratif'] = evaluate_model(dense_model, puzzles_test, solutions_test, use_iterative=True)\n",
    "\n",
    "print(\"CNN - prediction directe...\")\n",
    "results['CNN direct'] = evaluate_model(cnn_model, puzzles_test, solutions_test, use_iterative=False)\n",
    "\n",
    "print(\"CNN - prediction iterative...\")\n",
    "results['CNN iteratif'] = evaluate_model(cnn_model, puzzles_test, solutions_test, use_iterative=True)\n",
    "\n",
    "# Tableau recapitulatif\n",
    "print()\n",
    "print(f\"{'Modele':<20} {'Precision/case':>15} {'Precision/grille':>18} {'Grilles OK':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for name, r in results.items():\n",
    "    print(f\"{name:<20} {r['cell_acc']:>14.1%} {r['grid_acc']:>17.1%} {r['correct_grids']:>8}/{r['total']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-interp",
   "metadata": {},
   "source": [
    "### Interpretation : tableau comparatif\n",
    "\n",
    "| Configuration | Precision/case | Precision/grille | Observation |\n",
    "|---------------|---------------|-----------------|-------------|\n",
    "| MLP direct | ~75% | ~0-2% | Baseline naive |\n",
    "| MLP iteratif | ~85% | ~5-15% | Amelioration modeste |\n",
    "| CNN direct | ~85% | ~5-10% | Structure spatiale aide |\n",
    "| CNN iteratif | ~90-95% | ~20-50% | Meilleure combinaison |\n",
    "\n",
    "**Points cles** :\n",
    "1. La prediction iterative apporte un gain significatif pour les deux architectures\n",
    "2. Le CNN beneficie davantage de l'iteratif car ses predictions individuelles sont plus fiables\n",
    "3. Meme le meilleur resultat est loin de 100% -- un solveur algorithmique (backtracking, CP-SAT) resout 100% des puzzles valides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation : grille predite vs solution\n",
    "def plot_sudoku_comparison(puzzle, prediction, solution, title=\"\"):\n",
    "    \"\"\"Affiche puzzle, prediction et solution cote a cote.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    titles = [\"Puzzle\", \"Prediction NN\", \"Solution\"]\n",
    "    grids = [puzzle, prediction, solution]\n",
    "    \n",
    "    for ax, grid, t in zip(axes, grids, titles):\n",
    "        ax.set_xlim(0, 9)\n",
    "        ax.set_ylim(0, 9)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title(t, fontsize=12)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # Lignes de grille\n",
    "        for i in range(10):\n",
    "            lw = 2 if i % 3 == 0 else 0.5\n",
    "            ax.axhline(i, color='black', linewidth=lw)\n",
    "            ax.axvline(i, color='black', linewidth=lw)\n",
    "        \n",
    "        # Chiffres\n",
    "        for r in range(9):\n",
    "            for c in range(9):\n",
    "                val = grid[r, c]\n",
    "                if val != 0:\n",
    "                    # Couleur : noir=indice, bleu=predit correct, rouge=predit faux\n",
    "                    if puzzle[r, c] != 0:\n",
    "                        color = 'black'\n",
    "                    elif grid is prediction:\n",
    "                        color = 'blue' if val == solution[r, c] else 'red'\n",
    "                    else:\n",
    "                        color = 'blue'\n",
    "                    \n",
    "                    ax.text(c + 0.5, 8.5 - r, str(val),\n",
    "                           ha='center', va='center', fontsize=11, color=color,\n",
    "                           fontweight='bold' if puzzle[r, c] != 0 else 'normal')\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Afficher 2 exemples\n",
    "for i in range(2):\n",
    "    puzzle = puzzles_test[i]\n",
    "    solution = solutions_test[i]\n",
    "    \n",
    "    prediction, _ = iterative_predict(cnn_model, puzzle)\n",
    "    n_correct = np.sum(prediction == solution)\n",
    "    n_empty = np.sum(puzzle == 0)\n",
    "    \n",
    "    plot_sudoku_comparison(\n",
    "        puzzle, prediction, solution,\n",
    "        f\"Puzzle {i+1} : {n_correct}/81 cases correctes ({n_empty} vides)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des erreurs : quelles cases sont les plus difficiles ?\n",
    "error_map = np.zeros((9, 9), dtype=int)\n",
    "\n",
    "for puzzle, solution in zip(puzzles_test, solutions_test):\n",
    "    prediction, _ = iterative_predict(cnn_model, puzzle)\n",
    "    errors = (prediction != solution).astype(int)\n",
    "    # Ne compter les erreurs que sur les cases vides du puzzle\n",
    "    mask_empty = (puzzle == 0).astype(int)\n",
    "    error_map += errors * mask_empty\n",
    "\n",
    "# Heatmap des erreurs\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(error_map, cmap='YlOrRd', aspect='equal')\n",
    "ax.set_title(\"Carte des erreurs de prediction (CNN iteratif)\", fontsize=12)\n",
    "ax.set_xlabel(\"Colonne\")\n",
    "ax.set_ylabel(\"Ligne\")\n",
    "\n",
    "# Ajouter les valeurs dans les cases\n",
    "for r in range(9):\n",
    "    for c in range(9):\n",
    "        ax.text(c, r, str(error_map[r, c]),\n",
    "               ha='center', va='center', fontsize=10,\n",
    "               color='white' if error_map[r, c] > error_map.max() * 0.5 else 'black')\n",
    "\n",
    "# Lignes de bloc\n",
    "for i in [3, 6]:\n",
    "    ax.axhline(i - 0.5, color='black', linewidth=2)\n",
    "    ax.axvline(i - 0.5, color='black', linewidth=2)\n",
    "\n",
    "plt.colorbar(im, label=\"Nombre d'erreurs\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal erreurs : {error_map.sum()}\")\n",
    "print(f\"Position la plus difficile : ligne {error_map.argmax() // 9}, colonne {error_map.argmax() % 9} ({error_map.max()} erreurs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-interp",
   "metadata": {},
   "source": [
    "### Interpretation : carte des erreurs\n",
    "\n",
    "La heatmap revele les positions ou le reseau commet le plus d'erreurs :\n",
    "\n",
    "**Points cles** :\n",
    "1. Les erreurs ne sont pas uniformement reparties : certaines positions sont systematiquement plus difficiles\n",
    "2. Les cases au centre de la grille sont souvent plus difficiles car elles sont contraintes par plus de voisins\n",
    "3. Le reseau n'a pas de mecanisme pour **verifier la validite** de ses predictions (pas de contrainte AllDifferent)\n",
    "\n",
    "### Limitations fondamentales\n",
    "\n",
    "| Limitation | Consequence | Solution possible |\n",
    "|-----------|-------------|------------------|\n",
    "| Pas de contraintes explicites | Grilles invalides possibles | Hybrid NN + verificateur |\n",
    "| Dataset limite (1000) | Generalisation insuffisante | Plus de donnees (17M) |\n",
    "| Propagation d'erreurs | Erreur precoce = grille fausse | Seuil de confiance + backtrack |\n",
    "\n",
    "> **Vers une approche hybride** : combiner la prediction neuronale avec un verificateur de contraintes permet de garantir la validite. Le reseau propose, le solveur dispose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises-section",
   "metadata": {},
   "source": [
    "## 7. Exercices\n",
    "\n",
    "### Exercice 1 : Experimenter avec l'architecture CNN\n",
    "\n",
    "Modifiez l'architecture du CNN pour observer l'impact :\n",
    "- Ajouter ou retirer des couches de convolution\n",
    "- Modifier le nombre de filtres (32, 64, 128, 256)\n",
    "- Tester des kernel sizes differents (1x1, 3x3, 5x5)\n",
    "\n",
    "**Question** : quel est le nombre minimal de couches pour atteindre un champ receptif couvrant toute la grille 9x9 avec des filtres 3x3 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1 : Experimentez ici\n",
    "# Indice : le champ receptif d'un filtre 3x3 apres n couches est (2n+1) x (2n+1)\n",
    "# Pour couvrir 9x9, il faut 2n+1 >= 9, soit n >= 4 couches\n",
    "\n",
    "# Exemple : CNN plus profond\n",
    "# class DeepCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         layers = []\n",
    "#         in_channels = 10\n",
    "#         for out_channels in [64, 64, 128, 128, 256, 256, 256]:\n",
    "#             layers.extend([\n",
    "#                 nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "#                 nn.BatchNorm2d(out_channels),\n",
    "#                 nn.ReLU()\n",
    "#             ])\n",
    "#             in_channels = out_channels\n",
    "#         layers.append(nn.Conv2d(in_channels, 9, 1))\n",
    "#         self.net = nn.Sequential(*layers)\n",
    "#     \n",
    "#     def forward(self, x):\n",
    "#         return self.net(x).permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2-intro",
   "metadata": {},
   "source": [
    "### Exercice 2 : Dropout et regularisation\n",
    "\n",
    "Ajoutez du dropout et de la batch normalization au MLP et au CNN. Comparez les courbes de train/test loss pour detecter le sur-apprentissage.\n",
    "\n",
    "**Questions** :\n",
    "- Le dropout ameliore-t-il la precision sur le jeu de test ?\n",
    "- A partir de combien d'epochs observe-t-on du sur-apprentissage ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2 : Experimentez ici\n",
    "# Indice : ajoutez nn.Dropout2d(0.2) apres les couches ReLU du CNN\n",
    "# Comparez les courbes train_loss et test_loss\n",
    "\n",
    "# Exemple :\n",
    "# class CNNWithDropout(nn.Module):\n",
    "#     def __init__(self, dropout_rate=0.2):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Conv2d(10, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "#             nn.Dropout2d(dropout_rate),\n",
    "#             nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "#             nn.Dropout2d(dropout_rate),\n",
    "#             nn.Conv2d(128, 9, 1)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x).permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3-intro",
   "metadata": {},
   "source": [
    "### Exercice 3 : Approche hybride NN + backtracking\n",
    "\n",
    "Implementez une approche hybride :\n",
    "1. Utilisez le CNN pour predire les chiffres les plus probables\n",
    "2. Remplissez les cases ou la confiance depasse un seuil (ex: 0.95)\n",
    "3. Pour les cases restantes, utilisez le backtracking classique\n",
    "\n",
    "**Question** : cette approche garantit-elle une solution valide ? Quel est l'avantage par rapport au backtracking seul ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3 : Approche hybride\n",
    "# Indice : le NN reduit le nombre de cases a explorer par backtracking\n",
    "\n",
    "def hybrid_solve(model, puzzle: np.ndarray, confidence_threshold: float = 0.95) -> np.ndarray:\n",
    "    \"\"\"Resout un Sudoku par approche hybride NN + backtracking.\"\"\"\n",
    "    current = puzzle.copy()\n",
    "    \n",
    "    # Phase 1 : remplir les cases avec haute confiance\n",
    "    model.eval()\n",
    "    nn_filled = 0\n",
    "    \n",
    "    while True:\n",
    "        encoded = encode_puzzle(current)\n",
    "        x = torch.FloatTensor(encoded).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "            probs = torch.softmax(output[0], dim=-1).cpu().numpy()\n",
    "        \n",
    "        # Trouver la case vide la plus confiante\n",
    "        best_conf = -1\n",
    "        best_pos = None\n",
    "        best_digit = None\n",
    "        \n",
    "        for r in range(9):\n",
    "            for c in range(9):\n",
    "                if current[r, c] == 0:\n",
    "                    max_prob = probs[r, c].max()\n",
    "                    if max_prob > best_conf:\n",
    "                        best_conf = max_prob\n",
    "                        best_pos = (r, c)\n",
    "                        best_digit = probs[r, c].argmax() + 1\n",
    "        \n",
    "        if best_pos is None or best_conf < confidence_threshold:\n",
    "            break\n",
    "        \n",
    "        current[best_pos[0], best_pos[1]] = best_digit\n",
    "        nn_filled += 1\n",
    "    \n",
    "    # Phase 2 : backtracking pour les cases restantes\n",
    "    remaining = np.sum(current == 0)\n",
    "    \n",
    "    def backtrack(grid):\n",
    "        for r in range(9):\n",
    "            for c in range(9):\n",
    "                if grid[r, c] == 0:\n",
    "                    for num in range(1, 10):\n",
    "                        if is_valid(grid, r, c, num):\n",
    "                            grid[r, c] = num\n",
    "                            if backtrack(grid):\n",
    "                                return True\n",
    "                            grid[r, c] = 0\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    backtrack(current)\n",
    "    print(f\"  NN a rempli {nn_filled} cases (seuil={confidence_threshold}), \"\n",
    "          f\"backtracking pour les {remaining} restantes\")\n",
    "    \n",
    "    return current\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"=== Approche hybride CNN + Backtracking ===\")\n",
    "for i in range(3):\n",
    "    puzzle = puzzles_test[i]\n",
    "    solution = solutions_test[i]\n",
    "    print(f\"\\nPuzzle {i+1} :\")\n",
    "    result = hybrid_solve(cnn_model, puzzle, confidence_threshold=0.90)\n",
    "    n_correct = np.sum(result == solution)\n",
    "    print(f\"  Resultat : {n_correct}/81 cases correctes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "### Recapitulatif\n",
    "\n",
    "| Aspect | MLP | CNN | CNN iteratif | Hybride NN+BT |\n",
    "|--------|-----|-----|-------------|----------------|\n",
    "| Structure spatiale | Non | Oui | Oui | Oui |\n",
    "| Precision/case | ~75% | ~85% | ~90-95% | 100%* |\n",
    "| Precision/grille | ~0-2% | ~5-10% | ~20-50% | 100%* |\n",
    "| Garantie solution | Non | Non | Non | Oui |\n",
    "| Temps | ~ms | ~ms | ~100ms | Variable |\n",
    "\n",
    "\\* L'approche hybride garantit une solution valide grace au backtracking de secours.\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "1. **Les reseaux de neurones peuvent apprendre les motifs du Sudoku** mais ne garantissent pas des solutions valides\n",
    "2. **La structure spatiale compte** : le CNN surpasse le MLP grace aux convolutions\n",
    "3. **La prediction iterative est cruciale** : elle transforme un classificateur moyen en un solveur potable\n",
    "4. **L'approche hybride est le meilleur compromis** : la vitesse du NN + la fiabilite du backtracking\n",
    "\n",
    "### Comparaison avec les autres approches de la serie\n",
    "\n",
    "| Solveur | Type | Fiabilite | Rapidite |\n",
    "|---------|------|-----------|----------|\n",
    "| Backtracking | Algorithmique | 100% | Rapide |\n",
    "| OR-Tools/Z3 | Contraintes | 100% | Tres rapide |\n",
    "| Dancing Links | Couverture exacte | 100% | Optimal |\n",
    "| Norvig | Propagation + BT | 100% | Rapide |\n",
    "| Genetique | Metaheuristique | ~50% | Lent |\n",
    "| **Reseau de neurones** | **Apprentissage** | **Variable** | **Rapide** |\n",
    "\n",
    "Les reseaux de neurones ne sont pas la meilleure approche pour resoudre le Sudoku (les solveurs par contraintes sont plus fiables et souvent plus rapides). Leur interet est **pedagogique** : ils illustrent comment le deep learning peut etre applique a des problemes combinatoires, et quelles sont ses limites fondamentales.\n",
    "\n",
    "### Pour aller plus loin\n",
    "\n",
    "- [Projet de reference : 4 architectures sur 17M puzzles](https://github.com/jsboigeEpita/2024-EPITA-SCIA-PPC-Sudoku-CV)\n",
    "- [Park (2018) : Relational recurrent neural network for Sudoku](https://arxiv.org/abs/1802.04310)\n",
    "- [Graph Neural Networks for combinatorial optimization](https://arxiv.org/abs/2012.01806)\n",
    "- Approches par reinforcement learning : apprendre a resoudre par essai-erreur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer-nav",
   "metadata": {},
   "source": "---\n\n**Navigation** : [<< Sudoku-9-HumanStrategies](Sudoku-9-HumanStrategies.ipynb) | [Index](README.md) | [Sudoku-11-Comparison >>](Sudoku-11-Comparison.ipynb)\n\n**Voir aussi** : \n- [GenAI](../GenAI/README.md) - Série sur l'IA générative et les réseaux de neurones\n- [Sudoku-11-Comparison](Sudoku-11-Comparison.ipynb) - Benchmark comparatif de tous les solveurs\n\n### Notebooks associés\n- [Sudoku-1-Backtracking](Sudoku-1-Backtracking.ipynb) : solveur algorithmique de référence\n- [Sudoku-Python-Backtracking](Sudoku-Python-Backtracking.ipynb) : backtracking en Python (meme SudokuGrid)\n- [Sudoku-7-Norvig](Sudoku-7-Norvig.ipynb) : propagation de contraintes\n- [Sudoku-Python-Genetic](Sudoku-Python-Genetic.ipynb) : autre approche non-déterministe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}