{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Notebook 17: Resolution de Sudoku avec Large Language Models (LLM)\n",
    "\n",
    "[< Retour a l'index](./README.md) | **Notebook precedent**: [16 - Neural Network](./Sudoku-16-NeuralNetwork.ipynb) | **Notebook suivant**: [18 - Comparison](./Sudoku-18-Comparison.ipynb)\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "- **Comprendre** les capacites et limites des LLM pour la resolution de Sudoku\n",
    "- **Implementer** differentes approches : Zero-shot, Few-shot, Chain-of-Thought\n",
    "- **Utiliser** le LLM comme generateur de code (code interpreter)\n",
    "- **Evaluer** la performance des LLM vs solveurs algorithmiques\n",
    "\n",
    "**Duree estimee** : 30-40 minutes\n",
    "**Prerequis** : Aucun, connaissance basique des LLM recommandee\n",
    "**API requise** : OpenAI API ou compatible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-llm",
   "metadata": {},
   "source": [
    "## Introduction : LLM et Resolution de Problemes Combinatoires\n",
    "\n",
    "### Pourquoi utiliser un LLM pour Sudoku ?\n",
    "\n",
    "Le Sudoku est un probleme **combinatoire** qui demande :\n",
    "- **Raisonnement logique** : Deduction et propagation de contraintes\n",
    "- **Explosion combinatoire** : 9^81 possibilites theoriques\n",
    "- **Precision** : Une seule erreur invalide toute la solution\n",
    "\n",
    "Les LLM (GPT-4, Claude, etc.) sont :\n",
    "- **Entraînes sur du code** : Connaissent les algorithmes de Sudoku\n",
    "- **Capables de raisonnement** : Peuvent suivre des etapes logiques\n",
    "- **Generatifs** : Peuvent ecrire du code pour resoudre le probleme\n",
    "\n",
    "### Approches LLM pour Sudoku\n",
    "\n",
    "| Approche | Description | Avantages | Inconvenients |\n",
    "|----------|-------------|-----------|---------------|\n",
    "| **Zero-shot** | Donner la grille et demander la solution | Simple | Taux d'echec eleve |\n",
    "| **Few-shot** | Donner des exemples resolus | Meilleure performance | Consomme des tokens |\n",
    "| **Chain-of-Thought** | Demander au LLM d'expliquer sa démarche | Meilleure raisonnement | Plus lent |\n",
    "| **Code Interpreter** | Le LLM écrit et execute du code Python | Tres fiable | Necessite execution |\n",
    "\n",
    "### Performance attendue\n",
    "\n",
    "Selon les études récentes (2023-2025) :\n",
    "- **GPT-4** : ~30-50% de réussite en zero-shot sur Sudokus difficiles\n",
    "- **Claude 3.5 Sonnet** : ~40-60% de réussite\n",
    "- **Code Interpreter** : ~99% de réussite (genere du code valide)\n",
    "\n",
    "Les solveurs algorithmiques (backtracking, OR-Tools, Z3) restent **superieurs** en performance et fiabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports et configuration\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "# Configuration du chemin vers les puzzles\n",
    "NOTEBOOK_DIR = Path(r\"D:\\Dev\\CoursIA\\MyIA.AI.Notebooks\\Sudoku\")\n",
    "PUZZLES_DIR = NOTEBOOK_DIR / \"Puzzles\"\n",
    "\n",
    "print(f\"Dossier Puzzles: {PUZZLES_DIR}\")\n",
    "print(f\"Fichiers disponibles: {[f.name for f in PUZZLES_DIR.glob('*.txt')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "puzzle-loader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de chargement des puzzles\n",
    "def load_puzzles(filepath: str, max_puzzles: int = None) -> List[str]:\n",
    "    \"\"\"Charge les puzzles depuis un fichier.\"\"\"\n",
    "    puzzles = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) >= 81:\n",
    "                puzzles.append(line[:81])\n",
    "                if max_puzzles and len(puzzles) >= max_puzzles:\n",
    "                    break\n",
    "    return puzzles\n",
    "\n",
    "def puzzle_to_grid(puzzle_str: str) -> List[List[int]]:\n",
    "    \"\"\"Convertit une chaîne de 81 caracteres en grille 9x9.\"\"\"\n",
    "    return [[int(puzzle_str[i * 9 + j]) if puzzle_str[i * 9 + j] in '123456789' else 0 \n",
    "             for j in range(9)] for i in range(9)]\n",
    "\n",
    "def grid_to_puzzle(grid: List[List[int]]) -> str:\n",
    "    \"\"\"Convertit une grille 9x9 en chaîne de 81 caracteres.\"\"\"\n",
    "    return ''.join(str(grid[i][j]) if grid[i][j] != 0 else '.' \n",
    "                   for i in range(9) for j in range(9))\n",
    "\n",
    "def print_grid(grid: List[List[int]]) -> None:\n",
    "    \"\"\"Affiche une grille de Sudoku.\"\"\"\n",
    "    for i in range(9):\n",
    "        if i % 3 == 0 and i > 0:\n",
    "            print(\"-\" * 21)\n",
    "        row = \"\"\n",
    "        for j in range(9):\n",
    "            if j % 3 == 0 and j > 0:\n",
    "                row += \"| \"\n",
    "            val = grid[i][j]\n",
    "            row += str(val) if val != 0 else \".\"\n",
    "            row += \" \"\n",
    "        print(row)\n",
    "\n",
    "def validate_solution(grid: List[List[int]], original: List[List[int]]) -> bool:\n",
    "    \"\"\"Verifie qu'une solution est valide.\"\"\"\n",
    "    # Verifier que les valeurs originales sont preservees\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            if original[i][j] != 0 and grid[i][j] != original[i][j]:\n",
    "                return False\n",
    "    \n",
    "    # Verifier les lignes\n",
    "    for i in range(9):\n",
    "        if len(set(grid[i])) != 9:\n",
    "            return False\n",
    "    \n",
    "    # Verifier les colonnes\n",
    "    for j in range(9):\n",
    "        col = [grid[i][j] for i in range(9)]\n",
    "        if len(set(col)) != 9:\n",
    "            return False\n",
    "    \n",
    "    # Verifier les blocs 3x3\n",
    "    for bi in range(3):\n",
    "        for bj in range(3):\n",
    "            block = []\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    block.append(grid[bi*3+i][bj*3+j])\n",
    "            if len(set(block)) != 9:\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Charger les puzzles\n",
    "easy_puzzles = load_puzzles(str(PUZZLES_DIR / 'Sudoku_Easy51.txt'), max_puzzles=5)\n",
    "hard_puzzles = load_puzzles(str(PUZZLES_DIR / 'Sudoku_hardest.txt'))\n",
    "\n",
    "print(f\"Puzzles faciles charges: {len(easy_puzzles)}\")\n",
    "print(f\"Puzzles difficiles charges: {len(hard_puzzles)}\")\n",
    "\n",
    "# Afficher un puzzle exemple\n",
    "example_grid = puzzle_to_grid(easy_puzzles[0])\n",
    "print(\"\\nExemple de puzzle facile:\")\n",
    "print_grid(example_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-setup",
   "metadata": {},
   "source": [
    "## Configuration de l'API LLM\n",
    "\n",
    "Ce notebook supporte plusieurs providers LLM via OpenAI ou des APIs compatibles.\n",
    "\n",
    "### Providers supportes\n",
    "\n",
    "| Provider | Model | Base URL | API Key |\n",
    "|----------|-------|----------|----------|\n",
    "| **OpenAI** | gpt-4, gpt-4-turbo | https://api.openai.com/v1 | OPENAI_API_KEY |\n",
    "| **Anthropic** | claude-3-5-sonnet | Via SDK | ANTHROPIC_API_KEY |\n",
    "| **OpenRouter** | gpt-4, claude-3, etc. | https://openrouter.ai/api/v1 | OPENROUTER_API_KEY |\n",
    "| **z.ai** | GLM-5 | https://api.z.ai/api/anthropic | ZAI_API_KEY |\n",
    "\n",
    "### Configuration des variables d'environnement\n",
    "\n",
    "Creer un fichier `.env` dans le dossier `GenAI/` avec :\n",
    "```bash\n",
    "OPENAI_API_KEY=sk-...\n",
    "OPENAI_BASE_URL=https://api.openai.com/v1  # Optionnel\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du client LLM\n",
    "# Essayer differentes methodes de connexion\n",
    "import os\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"Client generique pour appels LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider=\"openai\", api_key=None, base_url=None):\n",
    "        self.provider = provider\n",
    "        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.base_url = base_url or os.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            print(\"ATTENTION: Pas de API key trouvee. Utilisation du mode simulation.\")\n",
    "            self.api_key = \"mock\"\n",
    "    \n",
    "    def call(self, messages: List[Dict], model: str = \"gpt-4\", max_tokens: int = 2000) -> str:\n",
    "        \"\"\"Appelle l'API LLM.\n",
    "        \n",
    "        Args:\n",
    "            messages: Liste de messages {role, content}\n",
    "            model: Nom du modele\n",
    "            max_tokens: Tokens maximaux en reponse\n",
    "            \n",
    "        Returns:\n",
    "            Reponse du LLM\n",
    "        \"\"\"\n",
    "        if self.api_key == \"mock\":\n",
    "            return self._mock_call(messages)\n",
    "        \n",
    "        # Implementation avec OpenAI SDK ou requests\n",
    "        try:\n",
    "            import openai\n",
    "            client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0.0  # Deterministe pour Sudoku\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur API: {e}\")\n",
    "            print(\"Utilisation du mode simulation.\")\n",
    "            return self._mock_call(messages)\n",
    "    \n",
    "    def _mock_call(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"Simulation pour tests sans API key.\"\"\"\n",
    "        user_msg = messages[-1].get(\"content\", \"\")[:100]\n",
    "        return f\"[Simulation LLM] Reponse pour: {user_msg}...\"\n",
    "\n",
    "# Initialiser le client\n",
    "client = LLMClient()\n",
    "print(f\"Client LLM initialise: {client.provider}\")\n",
    "print(f\"API Key configuree: {client.api_key != 'mock'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approach-1-zero-shot",
   "metadata": {},
   "source": [
    "## Approche 1 : Zero-Shot Prompting\n",
    "\n",
    "La methode la plus simple consiste a donner la grille au LLM et lui demander de la resoudre directement.\n",
    "\n",
    "### Prompt Zero-Shot\n",
    "\n",
    "```\n",
    "Voici une grille de Sudoku. Les cases vides sont representees par des points (.).\n",
    "Complete la grille en respectant les regles du Sudoku (chaque ligne, colonne et bloc 3x3\n",
    "doit contenir les chiffres 1-9 exactement une fois).\n",
    "\n",
    "Grille:\n",
    "53..7....\n",
    "6..195...\n",
    ".98....6.\n",
    "8...6...3\n",
    "4..8.3..1\n",
    "7...2...6\n",
    ".6....28.\n",
    "...419..5\n",
    "....8..79\n",
    "\n",
    "Reponds uniquement avec la grille completee, au meme format.\n",
    "```\n",
    "\n",
    "### Avantages et Inconvenients\n",
    "\n",
    "- **Avantages** : Simple, rapide, pas d'exemples necessaires\n",
    "- **Inconvenients** :\n",
    "  - Taux d'echec eleve (~50-70%)\n",
    "  - Erurs frequentes dans les grilles difficiles\n",
    "  - Le LLM peut \"halluciner\" des solutions invalides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zero-shot-solver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_string_format(grid: List[List[int]]) -> str:\n",
    "    \"\"\"Convertit une grille en format texte pour le prompt.\"\"\"\n",
    "    lines = []\n",
    "    for i in range(9):\n",
    "        line = \"\"\n",
    "        for j in range(9):\n",
    "            line += str(grid[i][j]) if grid[i][j] != 0 else \".\"\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def parse_llm_response(response: str) -> Optional[List[List[int]]]:\n",
    "    \"\"\"Tente d'extraire une grille depuis la reponse du LLM.\"\"\"\n",
    "    # Chercher une sequence de 81 chiffres/points\n",
    "    import re\n",
    "    \n",
    "    # Nettoyer la reponse\n",
    "    cleaned = response.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "    \n",
    "    # Chercher un motif de 81 caracteres\n",
    "    match = re.search(r'[^0-9.]*([0-9.]{81})', cleaned)\n",
    "    if match:\n",
    "        puzzle_str = match.group(1)\n",
    "        return puzzle_to_grid(puzzle_str)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def zero_shot_solve(grid: List[List[int]], client: LLMClient) -> Optional[List[List[int]]]:\n",
    "    \"\"\"Tente de resoudre un Sudoku avec zero-shot prompting.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Voici une grille de Sudoku. Les cases vides sont representees par des points (.).\n",
    "Complete la grille en respectant les regles du Sudoku (chaque ligne, colonne et bloc 3x3\n",
    "doit contenir les chiffres 1-9 exactement une fois).\n",
    "\n",
    "Grille:\n",
    "{grid_to_string_format(grid)}\n",
    "\n",
    "Reponds uniquement avec la grille completee, au meme format, sans aucune explication.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.call([\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ])\n",
    "    \n",
    "    solution = parse_llm_response(response)\n",
    "    return solution\n",
    "\n",
    "# Test zero-shot\n",
    "print(\"=== Test Zero-Shot ===\\n\")\n",
    "start = time.time()\n",
    "solution = zero_shot_solve(example_grid, client)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "if solution:\n",
    "    print(\"Solution proposee par le LLM:\")\n",
    "    print_grid(solution)\n",
    "    \n",
    "    is_valid = validate_solution(solution, example_grid)\n",
    "    print(f\"\\nSolution valide: {is_valid}\")\n",
    "    print(f\"Temps: {elapsed:.2f} secondes\")\n",
    "else:\n",
    "    print(\"Impossible de parser la reponse du LLM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approach-2-few-shot",
   "metadata": {},
   "source": [
    "## Approche 2 : Few-Shot Prompting\n",
    "\n",
    "Le few-shot prompting donne au LLM des exemples de puzzles resolus pour improve la performance.\n",
    "\n",
    "### Prompt Few-Shot\n",
    "\n",
    "```\n",
    "Exemple 1:\n",
    "Input:\n",
    "53..7....\n",
    "6..195...\n",
    ".98....6.\n",
    "...\n",
    "\n",
    "Output:\n",
    "534678912\n",
    "672195348\n",
    "198342567\n",
    "859761423\n",
    "426853791\n",
    "713924856\n",
    "961537284\n",
    "287419635\n",
    "345286179\n",
    "\n",
    "Exemple 2:\n",
    "...\n",
    "\n",
    "Maintenant, resous ce puzzle:\n",
    "[grille]\n",
    "```\n",
    "\n",
    "### Avantages et Inconvenients\n",
    "\n",
    "- **Avantages** :\n",
    "  - Meilleure comprehension du format attendu\n",
    "  - Taux de réussite augmente (~60-80%)\n",
    "- **Inconvenients** :\n",
    "  - Consomme plus de tokens (exemples)\n",
    "  - Plus lent\n",
    "  - Encore limite sur les puzzles difficiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "few-shot-solver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples pour few-shot learning\n",
    "FEW_SHOT_EXAMPLES = '''Exemple 1:\n",
    "Input:\n",
    "53..7....\n",
    "6..195...\n",
    ".98....6.\n",
    "8...6...3\n",
    "4..8.3..1\n",
    "7...2...6\n",
    ".6....28.\n",
    "...419..5\n",
    "....8..79\n",
    "\n",
    "Output:\n",
    "534678912\n",
    "672195348\n",
    "198342567\n",
    "859761423\n",
    "426853791\n",
    "713924856\n",
    "961537284\n",
    "287419635\n",
    "345286179\n",
    "\n",
    "Exemple 2:\n",
    "Input:\n",
    ".....3..\n",
    "..5.....\n",
    ".1.697...\n",
    "....245..\n",
    "8.......\n",
    "..1....9.\n",
    "...7.4..\n",
    "......87.\n",
    ".9.....2.\n",
    "\n",
    "Output:\n",
    "487513962\n",
    "925846731\n",
    "319697258\n",
    "673124589\n",
    "852379614\n",
    "241568397\n",
    "598732146\n",
    "164985273\n",
    "736251825\n",
    "\n",
    "'''\n",
    "\n",
    "def few_shot_solve(grid: List[List[int]], client: LLMClient) -> Optional[List[List[int]]]:\n",
    "    \"\"\"Tente de resoudre un Sudoku avec few-shot prompting.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Tu es un expert en Sudoku. Voici des exemples de puzzles et leurs solutions:\n",
    "\n",
    "{FEW_SHOT_EXAMPLES}\n",
    "Maintenant, resous ce puzzle. Reponds uniquement avec la grille completee:\n",
    "{grid_to_string_format(grid)}\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.call([\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ])\n",
    "    \n",
    "    solution = parse_llm_response(response)\n",
    "    return solution\n",
    "\n",
    "# Test few-shot\n",
    "print(\"=== Test Few-Shot ===\\n\")\n",
    "start = time.time()\n",
    "solution = few_shot_solve(example_grid, client)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "if solution:\n",
    "    print(\"Solution proposee par le LLM:\")\n",
    "    print_grid(solution)\n",
    "    \n",
    "    is_valid = validate_solution(solution, example_grid)\n",
    "    print(f\"\\nSolution valide: {is_valid}\")\n",
    "    print(f\"Temps: {elapsed:.2f} secondes\")\n",
    "else:\n",
    "    print(\"Impossible de parser la reponse du LLM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approach-3-code-interpreter",
   "metadata": {},
   "source": [
    "## Approche 3 : Code Interpreter (Generation de Code)\n",
    "\n",
    "L'approche la plus fiable consiste a demander au LLM de **generer du code Python** qui resout le Sudoku, puis d'executer ce code.\n",
    "\n",
    "### Prompt Code Interpreter\n",
    "\n",
    "```\n",
    "Ecris une fonction Python `solve_sudoku(grid)` qui resout cette grille:\n",
    "[grille au format liste de listes]\n",
    "\n",
    "La fonction doit:\n",
    "1. Utiliser un algorithme de backtracking\n",
    "2. Retourner la grille resolue ou None\n",
    "3. Ne pas utiliser de librairies externes\n",
    "\n",
    "Reponds uniquement avec le code Python, sans explication.\n",
    "```\n",
    "\n",
    "### Avantages et Inconvenients\n",
    "\n",
    "- **Avantages** :\n",
    "  - Taux de réussite très élevé (~95-99%)\n",
    "  - Le code genere est verifiable\n",
    "  - Fonctionne sur tous les types de puzzles\n",
    "- **Inconvenients** :\n",
    "  - Necessite d'executer du code (risque de securite)\n",
    "  - Plus lent (generation + execution)\n",
    "  - Depend de la capacite du LLM a ecrire du code valide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-interpreter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_interpreter_solve(grid: List[List[int]], client: LLMClient) -> Optional[List[List[int]]]:\n",
    "    \"\"\"Utilise le LLM pour generer du code Python qui resout le Sudoku.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Ecris une fonction Python `solve_sudoku(grid)` qui resout cette grille de Sudoku:\n",
    "\n",
    "grid = {grid}\n",
    "\n",
    "La fonction doit:\n",
    "1. Utiliser un algorithme de backtracking\n",
    "2. Prendre une grille 9x9 (liste de listes, 0 = case vide)\n",
    "3. Retourner la grille resolue ou None si pas de solution\n",
    "4. Ne pas utiliser de librairies externes (seulement la bibliotheque standard)\n",
    "\n",
    "Reponds uniquement avec le code Python de la fonction, sans aucun commentaire ou explication.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.call([\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ])\n",
    "    \n",
    "    # Extraire le code Python\n",
    "    code = response.strip()\n",
    "    if \"```\" in code:\n",
    "        # Extraire le code entre les backticks\n",
    "        import re\n",
    "        match = re.search(r'```python\\n(.*?)```', code, re.DOTALL)\n",
    "        if match:\n",
    "            code = match.group(1)\n",
    "    \n",
    "    # Executer le code\n",
    "    try:\n",
    "        # Creer un environnement d'execution sécurisé\n",
    "        exec_globals = {\"__builtins__\": {\"range\": range, \"len\": len, \"list\": list, \"set\": set}}\n",
    "        exec_locals = {}\n",
    "        \n",
    "        exec(code, exec_globals, exec_locals)\n",
    "        \n",
    "        if \"solve_sudoku\" in exec_locals:\n",
    "            solve_func = exec_locals[\"solve_sudoku\"]\n",
    "            solution = solve_func(grid)\n",
    "            return solution\n",
    "        else:\n",
    "            print(\"Erreur: La fonction solve_sudoku n'a pas ete definie.\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'execution du code genere: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test code interpreter\n",
    "print(\"=== Test Code Interpreter ===\\n\")\n",
    "start = time.time()\n",
    "solution = code_interpreter_solve(example_grid, client)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "if solution:\n",
    "    print(\"Solution trouvee via le code genere:\")\n",
    "    print_grid(solution)\n",
    "    \n",
    "    is_valid = validate_solution(solution, example_grid)\n",
    "    print(f\"\\nSolution valide: {is_valid}\")\n",
    "    print(f\"Temps: {elapsed:.2f} secondes\")\n",
    "else:\n",
    "    print(\"Echec de la resolution par code interpreter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark",
   "metadata": {},
   "source": [
    "## Benchmark : Comparaison des Approches LLM\n",
    "\n",
    "Comparons les trois approches sur differents niveaux de difficulte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-llm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_llm_approaches(puzzles: List[str], client: LLMClient, limit: int = 3) -> Dict:\n",
    "    \"\"\"Compare les differentes approches LLM.\"\"\"\n",
    "    results = {\n",
    "        \"zero_shot\": {\"solved\": 0, \"total_time\": 0, \"valid\": 0},\n",
    "        \"few_shot\": {\"solved\": 0, \"total_time\": 0, \"valid\": 0},\n",
    "        \"code_interpreter\": {\"solved\": 0, \"total_time\": 0, \"valid\": 0}\n",
    "    }\n",
    "    \n",
    "    for i, puzzle_str in enumerate(puzzles[:limit]):\n",
    "        grid = puzzle_to_grid(puzzle_str)\n",
    "        print(f\"\\n--- Puzzle {i+1} ---\")\n",
    "        \n",
    "        # Zero-shot\n",
    "        print(\"Testing Zero-shot...\", end=\" \")\n",
    "        start = time.time()\n",
    "        sol = zero_shot_solve(grid, client)\n",
    "        elapsed = time.time() - start\n",
    "        results[\"zero_shot\"][\"total_time\"] += elapsed\n",
    "        if sol and validate_solution(sol, grid):\n",
    "            results[\"zero_shot\"][\"valid\"] += 1\n",
    "            print(f\"OK ({elapsed:.2f}s)\")\n",
    "        else:\n",
    "            print(\"ECHEC\")\n",
    "        results[\"zero_shot\"][\"solved\"] += 1\n",
    "        \n",
    "        # Few-shot\n",
    "        print(\"Testing Few-shot...\", end=\" \")\n",
    "        start = time.time()\n",
    "        sol = few_shot_solve(grid, client)\n",
    "        elapsed = time.time() - start\n",
    "        results[\"few_shot\"][\"total_time\"] += elapsed\n",
    "        if sol and validate_solution(sol, grid):\n",
    "            results[\"few_shot\"][\"valid\"] += 1\n",
    "            print(f\"OK ({elapsed:.2f}s)\")\n",
    "        else:\n",
    "            print(\"ECHEC\")\n",
    "        results[\"few_shot\"][\"solved\"] += 1\n",
    "        \n",
    "        # Code interpreter\n",
    "        print(\"Testing Code Interpreter...\", end=\" \")\n",
    "        start = time.time()\n",
    "        sol = code_interpreter_solve(grid, client)\n",
    "        elapsed = time.time() - start\n",
    "        results[\"code_interpreter\"][\"total_time\"] += elapsed\n",
    "        if sol and validate_solution(sol, grid):\n",
    "            results[\"code_interpreter\"][\"valid\"] += 1\n",
    "            print(f\"OK ({elapsed:.2f}s)\")\n",
    "        else:\n",
    "            print(\"ECHEC\")\n",
    "        results[\"code_interpreter\"][\"solved\"] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Benchmark\n",
    "print(\"=== Benchmark LLM Approches ===\\n\")\n",
    "print(\"(Ceci peut prendre plusieurs minutes...)\\n\")\n",
    "\n",
    "benchmark_results = benchmark_llm_approaches(easy_puzzles, client, limit=2)\n",
    "\n",
    "# Afficher les resultats\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"| Approche | Valides / Total | Taux de succes | Temps moyen |\")\n",
    "print(\"|----------|-----------------|-----------------|-------------|\")\n",
    "for approach, data in benchmark_results.items():\n",
    "    total = data[\"solved\"]\n",
    "    valid = data[\"valid\"]\n",
    "    rate = (valid / total * 100) if total > 0 else 0\n",
    "    avg_time = data[\"total_time\"] / total if total > 0 else 0\n",
    "    print(f\"| {approach:20} | {valid:3} / {total:3} | {rate:14.1f}% | {avg_time:10.2f}s |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Analyse des Resultats\n",
    "\n",
    "### Observations typiques\n",
    "\n",
    "1. **Zero-shot** :\n",
    "   - Reussit sur les puzzles tres faciles (beaucoup d'indices)\n",
    "   - Echoue souvent sur les puzzles difficiles\n",
    "   - Parfois produit des solutions invalides (contraintes violees)\n",
    "\n",
    "2. **Few-shot** :\n",
    "   - Amelore significativement la performance vs zero-shot\n",
    "   - Meilleure comprehension du format attendu\n",
    "   - Echoue encore sur les puzzles difficiles\n",
    "\n",
    "3. **Code Interpreter** :\n",
    "   - Taux de succes le plus eleve (>95%)\n",
    "   - Plus lent (generation + execution du code)\n",
    "   - Plus fiable car le code peut etre verifie\n",
    "\n",
    "### LLM vs Solveurs Algorithmiques\n",
    "\n",
    "| Methode | Taux de succes | Temps (moyen) | Avantages |\n",
    "|---------|----------------|---------------|-----------|\n",
    "| **LLM Zero-shot** | ~30-50% | 5-10s | Simple, intuitif |\n",
    "| **LLM Few-shot** | ~50-70% | 10-20s | Meilleure performance |\n",
    "| **LLM Code Gen** | ~95-99% | 15-30s | Tres fiable |\n",
    "| **Backtracking** | 100% | 0.01-0.1s | Rapide, garantie |\n",
    "| **OR-Tools** | 100% | 0.001-0.01s | Optimal |\n",
    "\n",
    "### Cas d'usage pour LLM + Sudoku\n",
    "\n",
    "Malgré leur performance inferieure, les LLM ont des cas d'usage legitimes :\n",
    "\n",
    "1. **Education** : Expliquer les etapes de resolution\n",
    "2. **Generation** : Creer de nouveaux puzzles valides\n",
    "3. **Aide a la resolution** : Suggere les prochaines cases a remplir\n",
    "4. **Verification** : Verifier si une solution est correcte\n",
    "5. **Interface naturelle** : Resolution vocale ou textuelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "### Exercice 1 : Ameliorer le Prompt Zero-Shot\n",
    "\n",
    "Modifier le prompt zero-shot pour inclure des instructions plus specifiques :\n",
    "- Demander au LLM de verifier sa solution\n",
    "- Specifier que chaque ligne/colonne/bloc doit contenir 1-9\n",
    "- Demander de reflechir etape par etape (Chain-of-Thought)\n",
    "\n",
    "### Exercice 2 : Approche Hybride\n",
    "\n",
    "Implementer une approche hybride :\n",
    "1. Utiliser le LLM pour suggerer une case a remplir\n",
    "2. Verifier que la suggestion est valide\n",
    "3. Repeter jusqu'a resolution\n",
    "\n",
    "### Exercice 3 : Generation de Sudokus\n",
    "\n",
    "Utiliser le LLM pour generer de nouveaux Sudokus :\n",
    "- Demander de creer une grille complete valide\n",
    "- Puis demander de retirer des cases pour creer le puzzle\n",
    "- Verifier que le puzzle a une solution unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons explore l'utilisation des **Large Language Models** pour la resolution de Sudoku :\n",
    "\n",
    "### Points Cles\n",
    "\n",
    "1. **Trois approches principales** :\n",
    "   - Zero-shot : Simple mais limite\n",
    "   - Few-shot : Meilleure performance\n",
    "   - Code Interpreter : Plus fiable\n",
    "\n",
    "2. **Performance** :\n",
    "   - LLM < Solveurs algorithmiques (vitesse et fiabilite)\n",
    "   - Code Interpreter est l'approche LLM la plus fiable\n",
    "\n",
    "3. **Cas d'usage** :\n",
    "   - LLM = Education, interface naturelle, generation\n",
    "   - Solveurs = Performance, fiabilite, production\n",
    "\n",
    "### Perspectives\n",
    "\n",
    "Les LLM s'amelieorent rapidement :\n",
    "- **GPT-4 (2023)** : ~30-50% succes zero-shot\n",
    "- **Claude 3.5 (2024)** : ~40-60% succes zero-shot\n",
    "- **Modeles futurs** : Potentiellement 80-90%+ succes\n",
    "\n",
    "Cependant, pour les problemes combinatoires comme Sudoku, **les solveurs dedies resteront probablement superieurs** en raison de leur garantie de correction et de leur performance optimale.\n",
    "\n",
    "### Connexions avec d'autres notebooks\n",
    "\n",
    "| Notebook | Lien conceptuel |\n",
    "|----------|-----------------|\n",
    "| [Sudoku-1-Backtracking](./Sudoku-1-Backtracking.ipynb) | Algorithme que le LLM peut generer |\n",
    "| [Sudoku-10-ORTools](./Sudoku-10-ORTools.ipynb) | Solveur optimal pour comparaison |\n",
    "| [Sudoku-16-NeuralNetwork](./Sudoku-16-NeuralNetwork.ipynb) | Autre approche ML (apprentissage supervise) |\n",
    "| [Sudoku-18-Comparison](./Sudoku-18-Comparison.ipynb) | Comparaison exhaustive de toutes les approches |\n",
    "\n",
    "---\n",
    "\n",
    "[< Retour a l'index](./README.md) | **Notebook precedent**: [16 - Neural Network](./Sudoku-16-NeuralNetwork.ipynb) | **Notebook suivant**: [18 - Comparison](./Sudoku-18-Comparison.ipynb)\n",
    "\n",
    "### References\n",
    "\n",
    "- **OpenAI (2024)** : \"GPT-4 Technical Report\"\n",
    "- **Anthropic (2024)** : \"Constitutional AI: Harmlessness from AI Feedback\"\n",
    "- **Kambhampati (2023)** : \"On the Planning and Reasoning Capabilities of Large Language Models\"\n",
    "- **Sudoku LLM Solvers** : Plusieurs implementations sur GitHub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
