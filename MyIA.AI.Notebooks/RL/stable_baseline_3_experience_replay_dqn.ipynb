{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 3 – Hindsight Experience Replay (HER) et Sauvegarde Avancée\n",
        "\n",
        "Dans ce troisième notebook, nous allons aborder plusieurs sujets avancés :\n",
        "\n",
        "1. **Hindsight Experience Replay (HER)**, une technique permettant d'entraîner un agent sur des tâches de type \"Goal-Conditioned RL\" (où l’agent doit atteindre un objectif paramétrable),\n",
        "2. un **exemple pratique** avec l’environnement `parking-v0` (issu de la bibliothèque [highway-env](https://github.com/eleurent/highway-env)),\n",
        "3. la **sauvegarde/chargement avancés** d’un modèle, incluant la sauvegarde et le rechargement de la buffer d’expérience (replay buffer).\n",
        "\n",
        "Cet environnement `parking-v0` est un cas classique d’apprentissage par renforcement à but : la récompense dépend d’un objectif (ici, se garer à un endroit précis, avec la bonne orientation).\n",
        "\n",
        "Nous verrons comment utiliser HER avec différents algorithmes (SAC, DDPG), comment **sauvegarder/recharger** un modèle **et** sa mémoire de rejouage, et comment **évaluer** l’agent.\n",
        "\n",
        "\n",
        "**Rappel sur le “Goal-Conditioned RL”**  \n",
        "- Dans certaines tâches, un “goal” (objectif) fait partie de l’état désiré. Par exemple, la position finale d’un bras robotique, la place de parking à occuper, etc.  \n",
        "- Les observations se présentent souvent comme un dictionnaire : `{'observation': ..., 'desired_goal': ..., 'achieved_goal': ...}`.  \n",
        "- HER (Hindsight Experience Replay) ré-étiquette des transitions a posteriori pour rendre l’apprentissage plus efficace, surtout quand la récompense est très clairsemée (sparse).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation des dépendances\n",
        "\n",
        "Sous Windows, nous n’utilisons pas de commandes `apt-get`. Nous procédons uniquement par `pip` pour installer :\n",
        "\n",
        "- Stable-Baselines3 (avec le `[extra]`),\n",
        "- highway-env pour l’environnement `parking-v0`,\n",
        "- (Optionnel) `moviepy` pour l’enregistrement de vidéos.\n",
        "\n",
        "```bash\n",
        "pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
        "pip install highway-env\n",
        "pip install moviepy\n",
        "```\n",
        "\n",
        "Dans un notebook Python, on peut faire :\n",
        "```python\n",
        "%pip install \"stable-baselines3[extra]>=2.0.0a4\" highway-env moviepy\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null
      },
      "outputs": [],
      "source": [
        "# Installation par commande magique Notebook (Windows-friendly, pas de apt-get)\n",
        "%pip install \"stable-baselines3[extra]>=2.0.0a4\" highway-env moviepy --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports Essentiels\n",
        "\n",
        "Nous importons :\n",
        "- `HerReplayBuffer` : le buffer de rejouage spécialisé pour HER,\n",
        "- des algorithmes (SAC, DDPG) compatibles avec HER (il faut un algo off-policy pour combiner avec HER),\n",
        "- l’environnement `parking-v0` depuis highway_env,\n",
        "- NumPy, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import highway_env\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3 import HerReplayBuffer, SAC, DDPG\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environnement Parking\n",
        "\n",
        "[`parking-v0`](https://github.com/eleurent/highway-env#parking-env) est un environnement « goal-conditioned » : la position et l’orientation cibles font partie de l’`info['goal']`. Pour résoudre cette tâche, on doit apprendre à manœuvrer la voiture pour qu’elle se gare.\n",
        "\n",
        "![parking-env](https://raw.githubusercontent.com/eleurent/highway-env/gh-media/docs/media/parking-env.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Création de l’environnement Gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"parking-v0\")\n",
        "obs, _ = env.reset()\n",
        "print(\"Observation :\", obs.keys())\n",
        "print(\"Exemple d'observation['observation']:\", obs[\"observation\"].shape)\n",
        "print(\"Exemple d'observation['desired_goal']:\", obs[\"desired_goal\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Par défaut, l’action est continue (2 dimensions : accélération et direction). On peut vérifier en imprimant `env.action_space` ou `env.observation_space`.\n",
        "\n",
        "**Structure de l’observation**  \n",
        "- `obs['observation']`: informations sur la voiture (position, vitesse, angle...).  \n",
        "- `obs['desired_goal']`: position/angle cible (le “parking spot”).  \n",
        "- `obs['achieved_goal']`: l’état effectivement atteint par la voiture.  \n",
        "\n",
        "La récompense dépend souvent de la distance entre `achieved_goal` et `desired_goal`. HER va ré-étiqueter certains buts pour générer des transitions artificiellement “réussies”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entraîner un agent SAC avec HER\n",
        "\n",
        "La configuration de `HerReplayBuffer` est centrale ici. Nous choisissons :\n",
        "- `goal_selection_strategy=\"future\"` (la stratégie la plus courante, on va remplacer le but original par un but futur observé dans le même épisode),\n",
        "- `n_sampled_goal=4` (on crée 4 transitions artificielles par transition réelle),\n",
        "- des hyperparamètres un peu custom pour *SAC* : `batch_size`, `policy_kwargs`, etc.\n",
        "\n",
        "Au final, l’entraînement dure un certain temps (on peut ajuster le `total_timesteps` en fonction de la machine).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null
      },
      "outputs": [],
      "source": [
        "model_sac = SAC(\n",
        "    \"MultiInputPolicy\",\n",
        "    env,\n",
        "    replay_buffer_class=HerReplayBuffer,\n",
        "    replay_buffer_kwargs=dict(\n",
        "        n_sampled_goal=4,\n",
        "        goal_selection_strategy=\"future\",\n",
        "    ),\n",
        "    # on attend 1000 pas avant d'entraîner,\n",
        "    # afin d'avoir au moins un épisode complet stocké.\n",
        "    learning_starts=1000,  \n",
        "    buffer_size=50000,\n",
        "    batch_size=64,\n",
        "    policy_kwargs=dict(net_arch=[64, 64]),\n",
        "    train_freq=1,\n",
        "    gradient_steps=1,\n",
        "    verbose=1,\n",
        ")\n",
        "model_sac.learn(total_timesteps=5000, log_interval=100)\n",
        "\n",
        "\n",
        "\n",
        "# Sauvegarde du modèle ET de la replay buffer avant suppression\n",
        "model_sac.save(\"her_sac_parking\")\n",
        "model_sac.save_replay_buffer(\"her_sac_parking_replay_buffer\")\n",
        "del model_sac  # On supprime de la RAM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Focus sur la stratégie `goal_selection_strategy=\\\"future\\\"`**  \n",
        "- “future” signifie qu’on va remplacer le but initial par un but échantillonné **plus tard** dans la même trajectoire.  \n",
        "- Cela favorise l’apprentissage, car beaucoup d’états futurs atteints sont convertis en “objectifs cibles”.  \n",
        "- Alternatives : “final”, “episode”, “random” — à tester selon l’environnement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rechargement du modèle et évaluation\n",
        "\n",
        "Nous rechargeons ensuite le modèle, et on peut l’évaluer sur quelques épisodes :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "# Rechargement\n",
        "model_sac = SAC.load(\"her_sac_parking\", env=env)\n",
        "\n",
        "# Évaluation\n",
        "\n",
        "eval_env = Monitor(env)  # Ajout du Monitor pour éviter les warnings\n",
        "mean_reward, std_reward = evaluate_policy(model_sac, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "\n",
        "print(f\"SAC Parking : reward moyen={mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La notion de « récompense » dans un environnement `goal-conditioned` (HER) reflète la distance à l’objectif et la réussite/échec à se garer. On peut inspecter `info.get(\"is_success\", False)` pour savoir si l’épisode est terminé avec succès."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exemple avec DDPG\n",
        "\n",
        "Nous pouvons reproduire la même idée avec un autre algorithme off-policy (DDPG). On ajoute souvent un bruit d’exploration, `NormalActionNoise` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null
      },
      "outputs": [],
      "source": [
        "# On crée un bruit gaussien pour l’action\n",
        "n_actions = env.action_space.shape[0]  # en général = 2\n",
        "noise_std = 0.2\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions))\n",
        "\n",
        "model_ddpg = DDPG(\n",
        "    \"MultiInputPolicy\",\n",
        "    env,\n",
        "    replay_buffer_class=HerReplayBuffer,\n",
        "    replay_buffer_kwargs=dict(\n",
        "        n_sampled_goal=4,\n",
        "        goal_selection_strategy=\"future\",\n",
        "    ),\n",
        "    verbose=1,\n",
        "    # On réduit la taille de la buffer\n",
        "    buffer_size=50_000,\n",
        "    learning_rate=1e-3,\n",
        "    action_noise=action_noise,\n",
        "    gamma=0.95,\n",
        "    # batch_size plus petit\n",
        "    batch_size=64,\n",
        "    # Réseau plus léger\n",
        "    policy_kwargs=dict(net_arch=[64, 64]),\n",
        "    # On attend un peu avant d'entraîner\n",
        "    learning_starts=1000,\n",
        "    # On fait 1 step d'entraînement par step environnement\n",
        "    train_freq=1,\n",
        "    gradient_steps=1,\n",
        ")\n",
        "\n",
        "# On ne va pas jusqu'à 2e5 steps\n",
        "# mais 5000 ou 10 000 pour une démo rapide\n",
        "model_ddpg.learn(10_000)  # par exemple\n",
        "\n",
        "# Sauvegarde\n",
        "model_ddpg.save(\"her_ddpg_parking\")\n",
        "del model_ddpg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_ddpg = DDPG.load(\"her_ddpg_parking\", env=env)\n",
        "mean_reward, std_reward = evaluate_policy(model_ddpg, env, n_eval_episodes=10, deterministic=True)\n",
        "print(f\"DDPG Parking : reward moyen={mean_reward:.2f} +/- {std_reward:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sauvegarde et Chargement de la Replay Buffer\n",
        "\n",
        "Une fonctionnalité avancée de Stable-Baselines3 est la **possibilité de sauvegarder aussi la buffer de rejouage** (replay buffer) :\n",
        "\n",
        "- Par défaut, `model.save(...)` **ne** sauvegarde **pas** la replay buffer, car celle-ci peut être très volumineuse (plusieurs Go si on utilise des images, par ex.).\n",
        "- Mais on peut la sauvegarder à part avec `model.save_replay_buffer(path)`, puis la recharger avec `model.load_replay_buffer(path)`.\n",
        "\n",
        "Cela permet de **reprendre un entraînement** où on l’avait laissé, en conservant tout l’historique d’expériences collectées. Sur des environnements complexes, c’est très utile pour éviter de tout recommencer !\n",
        "\n",
        "Exemple :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Sauvegarde\n",
        "model_sac.save(\"her_sac_parking\")  # ne sauvegarde pas la replay buffer\n",
        "model_sac.save_replay_buffer(\"her_sac_parking_replay_buffer\")\n",
        "\n",
        "# 2) Rechargement du modèle + de la buffer\n",
        "model_sac_2 = SAC.load(\"her_sac_parking\", env=env)\n",
        "print(\"Taille de la replay buffer AVANT rechargement :\", model_sac_2.replay_buffer.size())\n",
        "\n",
        "model_sac_2.load_replay_buffer(\"her_sac_parking_replay_buffer\")\n",
        "print(\"Taille de la replay buffer APRÈS rechargement :\", model_sac_2.replay_buffer.size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Reprendre l’entraînement**  \n",
        "- Après avoir chargé le modèle et la replay buffer, on peut appeler `model_sac_2.learn(N)` pour continuer exactement là où l’on s’était arrêté.  \n",
        "- Utile pour *checkpoint* l’entraînement de temps en temps, sans perdre l’historique des transitions passées (particulièrement en off-policy).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enregistrement vidéo sous Windows\n",
        "\n",
        "Comme évoqué dans les notebooks précédents, sous Windows, pas besoin de démarrer un display virtuel via `xvfb`. On peut simplement enregistrer en mode `rgb_array`. \n",
        "\n",
        "Voici une fonction utilitaire pour enregistrer une vidéo d’un agent (la même que dans les notebooks précédents, adaptée pour Windows) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=1000, prefix=\"\", video_folder=\"videos/\"):\n",
        "    # On crée un DummyVecEnv pour enregistrer\n",
        "    eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "    \n",
        "    # On active le recorder vidéo :\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix,\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, _, done, info = eval_env.step(action)\n",
        "\n",
        "    eval_env.close()\n",
        "\n",
        "def show_videos(video_path=\"videos\", prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Affiche toutes les vidéos mp4 dans le dossier spécifié.\n",
        "    \"\"\"\n",
        "    mp4_list = list(Path(video_path).glob(f\"{prefix}*.mp4\"))\n",
        "    if len(mp4_list) == 0:\n",
        "        print(\"Aucune vidéo trouvée.\")\n",
        "        return\n",
        "\n",
        "    html_video = \"\"\n",
        "    for mp4 in mp4_list:\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes()).decode(\"ascii\")\n",
        "        html_video += f\"<video alt='{mp4}' autoplay loop controls style='height: 400px;'>\\n\" \\\n",
        "                      f\"<source src='data:video/mp4;base64,{video_b64}' type='video/mp4' />\\n\" \\\n",
        "                      \"</video>\\n\"\n",
        "    return HTML(html_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour tester, nous pouvons enregistrer une vidéo de `model_sac` ou `model_ddpg`. (Attention : `parking-v0` peut boucler un peu longtemps si on met un `video_length` trop grand.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null
      },
      "outputs": [],
      "source": [
        "record_video(\"parking-v0\", model_sac, video_length=500, prefix=\"sac-parking\")\n",
        "show_videos(\"videos\", prefix=\"sac-parking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vous devriez voir la voiture tenter de se garer…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Dans ce troisième notebook, nous avons abordé des fonctionnalités avancées de Stable-Baselines3 :\n",
        "\n",
        "- **Hindsight Experience Replay (HER)**, qui permet d’apprendre efficacement sur des tâches à but (objectif) en ré-étiquetant des transitions passées,\n",
        "- l’utilisation de **SAC** ou **DDPG** avec HER (algorithmes off-policy),\n",
        "- la **sauvegarde et le rechargement** de la replay buffer pour reprendre un entraînement ultérieurement,\n",
        "- l’enregistrement vidéo « friendly pour Windows », sans dépendances `apt-get`.\n",
        "\n",
        "Avec cela, vous disposez d’une base solide pour traiter des tâches plus complexes en Apprentissage par Renforcement, où l’agent doit atteindre des objectifs spécifiques."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "3_HER_Advanced_FR.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
