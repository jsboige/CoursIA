{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2 – Wrappers Gym, Sauvegarde/Chargement, Multiprocessing, Callbacks et Environnements Personnalisés\n",
        "\n",
        "Ce notebook propose un survol des fonctionnalités avancées de la librairie [Stable Baselines3](https://github.com/DLR-RM/stable-baselines3) :\n",
        "1. **Utilisation de wrappers Gym** (limitation du nombre d’étapes, normalisation des actions, etc.)\n",
        "2. **Sauvegarde et chargement de modèles**\n",
        "3. **Multiprocessing** et environnements vectorisés\n",
        "4. **Callbacks** : enregistrement automatique, traçage en temps réel, etc.\n",
        "5. **Création d’un environnement Gym personnalisé**\n",
        "\n",
        "Nous utilisons un environnement sous Windows, donc nous évitons certaines dépendances (`xvfb`, `freeglut3-dev`) et nous n’utilisons pas de commandes `apt-get`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation et imports essentiels\n",
        "Dans un environnement Python classique, on installera Stable Baselines3 (et les dépendances gym) via :\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```\n",
        "ou bien :\n",
        "```\n",
        "%pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
        "```\n",
        "si vous utilisez un notebook. \n",
        "\n",
        "Ensuite, on importe les principales classes dont on aura besoin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null,
        "id": "InstallCell"
      },
      "outputs": [],
      "source": [
        "%pip install \"stable-baselines3[extra]>=2.0.0a4\"  \n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Wrappers Gym\n",
        "Les **Gym wrappers** permettent d’ajouter des transformations aux environnements (limiter la durée d’un épisode, normaliser les actions, etc.). Ci-dessous un exemple très condensé :\n",
        "\n",
        "**Autres wrappers utiles**  \n",
        "- `Monitor`: Enregistre automatiquement la récompense et la durée de chaque épisode, ce qui facilite l’analyse.  \n",
        "- `ClipAction`: S’assure que l’action est bien bornée à `[low, high]`.  \n",
        "- `FlattenObservation`: Convertit une observation complexe (dict, tuple, etc.) en un simple vecteur Numpy.\n",
        "\n",
        "*Exemple* : \n",
        "```python\n",
        "import gymnasium as gym\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env = Monitor(env)  # suivi auto des rewards, durées d’épisodes\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null,
        "id": "GymWrappersExample"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class LimitEpisodeSteps(gym.Wrapper):\n",
        "    def __init__(self, env, max_steps=100):\n",
        "        super().__init__(env)\n",
        "        self.max_steps = max_steps\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.current_step = 0\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.current_step += 1\n",
        "        if self.current_step >= self.max_steps:\n",
        "            truncated = True\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "# Exemple d'utilisation :\n",
        "base_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "wrapped_env = LimitEpisodeSteps(base_env, max_steps=50)\n",
        "\n",
        "obs, _ = wrapped_env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = wrapped_env.action_space.sample()\n",
        "    obs, reward, terminated, truncated, _ = wrapped_env.step(action)\n",
        "    done = terminated or truncated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Sauvegarde et chargement de modèles\n",
        "Chaque algorithme de Stable-Baselines3 dispose de méthodes `.save(path)` et `.load(path)`. On peut ainsi conserver un modèle partiellement entraîné ou final, et le recharger pour continuer l’apprentissage ou faire de l’inférence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null,
        "id": "SaveLoadCell"
      },
      "outputs": [],
      "source": [
        "# Entraînement basique sur CartPole\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "model = PPO(\"MlpPolicy\", env, verbose=0)\n",
        "model.learn(5000)\n",
        "\n",
        "# Sauvegarde\n",
        "model.save(\"ppo_cartpole\")\n",
        "del model  # on supprime le modèle de la mémoire\n",
        "\n",
        "# Rechargement\n",
        "model = PPO.load(\"ppo_cartpole\", env=env)  # on précise l'env si on veut continuer\n",
        "# Test rapide\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"Reprise du modèle chargé : récompense moyenne={mean_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Attention**  \n",
        "- `model.save(path)` enregistre uniquement les poids du réseau et l’architecture.  \n",
        "- Pour **off-policy** (DQN, SAC, TD3...), si vous voulez sauvegarder **la replay buffer** (mémoire d’expériences), il faut utiliser en plus `model.save_replay_buffer(path_replay)`.  \n",
        "- Cela peut s’avérer lourd en mémoire : prenez garde à la taille de `buffer_size` et à l’espace disque.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Multiprocessing\n",
        "Pour accélérer l’apprentissage ou pour avoir une meilleure exploration, on peut exécuter plusieurs environnements en parallèle. Cela se fait via `DummyVecEnv` (qui reste sur un seul process) ou `SubprocVecEnv` (plusieurs processus). Souvent, `DummyVecEnv` est plus rapide pour un petit nombre d’environnements, car la communication inter-processus coûte cher.\n",
        "\n",
        "On définit une fonction `make_env(env_id, rank, seed=0)` qui crée un environnement, puis on construit un `SubprocVecEnv` (ou un `DummyVecEnv`) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null,
        "id": "MultiprocCell"
      },
      "outputs": [],
      "source": [
        "def make_env(env_id, rank, seed=0):\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        env.reset(seed=seed + rank)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "# Exemple : 4 environnements en parallèle\n",
        "n_procs = 4\n",
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "vec_env = SubprocVecEnv([make_env(env_id, i) for i in range(n_procs)])\n",
        "\n",
        "model_mp = A2C(\"MlpPolicy\", vec_env, verbose=0)\n",
        "model_mp.learn(5000)\n",
        "\n",
        "# Évaluation finale sur un seul env\n",
        "test_env = gym.make(env_id)\n",
        "mean_reward, _ = evaluate_policy(model_mp, test_env, n_eval_episodes=10)\n",
        "print(\"Récompense moyenne sur 10 épisodes:\", mean_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Callbacks\n",
        "Les **callbacks** permettent d’intervenir pendant l’entraînement (pour sauvegarder, tracer en temps réel, etc.). Ils héritent de `BaseCallback`. Quelques exemples :\n",
        "\n",
        "**Callbacks fournis par Stable-Baselines3**  \n",
        "- `EvalCallback`: évalue régulièrement le modèle sur un environnement de test (distinct de l’env d’entraînement).  \n",
        "- `CheckpointCallback`: sauvegarde périodiquement le modèle.  \n",
        "- `StopTrainingOnRewardThreshold`: arrête l’apprentissage si une récompense cible est atteinte.  \n",
        "\n",
        "*Tip* : En combinant `EvalCallback` et `CheckpointCallback`, vous pouvez automatiquement enregistrer le “meilleur” modèle selon une métrique d’évaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null,
        "id": "CallbacksCell"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class SimpleCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "        self._called_once = False\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if not self._called_once:\n",
        "            print(\"Callback: Première fois !\")\n",
        "            self._called_once = True\n",
        "            return True\n",
        "        print(\"Callback: Deuxième fois, on arrête l'entraînement.\")\n",
        "        return False  # on interrompt l'apprentissage\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model_cb = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=0)\n",
        "model_cb.learn(total_timesteps=2000, callback=SimpleCallback())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exemple de callback pour sauvegarder le meilleur modèle\n",
        "On peut observer la récompense d’entraînement (monitor) et sauvegarder le modèle lorsqu’on obtient une récompense moyenne record. (Pour un usage plus robuste, on conseille d’utiliser un environnement d’évaluation séparé.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null,
        "id": "SaveBestCallbackCell"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "\n",
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    def __init__(self, check_freq, log_dir, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
        "            if len(x) > 0:\n",
        "                mean_reward = np.mean(y[-100:])\n",
        "                if self.verbose > 0:\n",
        "                    print(f\"Timestep: {self.num_timesteps}\")\n",
        "                    print(\n",
        "                        f\"Meilleur reward: {self.best_mean_reward:.2f}  |  Derniers 100 épisodes: {mean_reward:.2f}\"\n",
        "                    )\n",
        "                if mean_reward > self.best_mean_reward:\n",
        "                    self.best_mean_reward = mean_reward\n",
        "                    if self.verbose > 0:\n",
        "                        print(\"Nouveau meilleur modèle! Sauvegarde...\")\n",
        "                    self.model.save(self.save_path)\n",
        "        return True\n",
        "\n",
        "# Exemple d'utilisation\n",
        "log_dir = \"./logs/\"  # dossier de logs\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env = Monitor(env, log_dir)\n",
        "\n",
        "model_saver = A2C(\"MlpPolicy\", env, verbose=0)\n",
        "callback_saver = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
        "\n",
        "model_saver.learn(total_timesteps=5000, callback=callback_saver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Créer un environnement Gym personnalisé\n",
        "Enfin, voici un **exemple minimal** d’environnement Gym personnalisé. Il faut définir :\n",
        "\n",
        "- `__init__`: définit `self.observation_space` et `self.action_space`.  \n",
        "- `reset()`: renvoie `(obs, info)` où `obs` ∈ `observation_space`.  \n",
        "- `step(action)`: renvoie `(obs, reward, terminated, truncated, info)`.  \n",
        "- Assurez-vous que `obs` respecte la forme indiquée par `observation_space`.  \n",
        "\n",
        "*Note* : Dans Gym 0.26+ et Gymnasium, on a deux indicateurs de fin : `terminated` et `truncated`.  \n",
        "- `terminated`: la tâche est terminée parce qu’on est allé au bout (victoire/défaite).  \n",
        "- `truncated`: la tâche s’arrête par limite de temps ou autre contrainte.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution_count": null,
        "id": "CustomEnvCell"
      },
      "outputs": [],
      "source": [
        "from gymnasium import spaces\n",
        "\n",
        "class MyCustomEnv(gym.Env):\n",
        "    def __init__(self, grid_size=5):\n",
        "        super().__init__()\n",
        "        self.grid_size = grid_size\n",
        "        # On définit l'action_space et l'observation_space\n",
        "        self.action_space = spaces.Discrete(2)  # ex: 0 = gauche, 1 = droite\n",
        "        self.observation_space = spaces.Box(low=0, high=self.grid_size, shape=(1,), dtype=np.float32)\n",
        "        self.agent_pos = None\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed, options=options)\n",
        "        self.agent_pos = np.random.randint(low=0, high=self.grid_size)\n",
        "        return np.array([self.agent_pos], dtype=np.float32), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:  # gauche\n",
        "            self.agent_pos -= 1\n",
        "        else:            # droite\n",
        "            self.agent_pos += 1\n",
        "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
        "\n",
        "        reward = 1.0 if self.agent_pos == 0 else 0.0  # ex : on favorise d'aller à 0\n",
        "        terminated = bool(self.agent_pos == 0)\n",
        "        truncated = False\n",
        "        info = {}\n",
        "        return np.array([self.agent_pos], dtype=np.float32), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass  # Optionnel\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "# Validation\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "env_custom = MyCustomEnv()\n",
        "check_env(env_custom, warn=True)\n",
        "\n",
        "# Test rapide\n",
        "model_custom = PPO(\"MlpPolicy\", env_custom, verbose=0)\n",
        "model_custom.learn(2000)\n",
        "mean_reward, _ = evaluate_policy(model_custom, env_custom, n_eval_episodes=10)\n",
        "print(\"Récompense moyenne :\", mean_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n",
        "Dans ce second notebook, nous avons parcouru :\n",
        "\n",
        "- L’usage de **wrappers Gym** pour modifier un environnement (limiter la durée, normaliser, etc.).\n",
        "- Les **fonctions de sauvegarde/chargement** de modèles (`.save()` / `.load()`).\n",
        "- Le **multiprocessing** via `SubprocVecEnv` ou `DummyVecEnv` pour accélérer (ou diversifier) l’apprentissage.\n",
        "- Les **callbacks**, permettant d’intervenir pendant l’entraînement (sauvegarde automatique du meilleur modèle, monitoring, etc.).\n",
        "- La **création d’un environnement Gym personnalisé**, validé ensuite par la fonction `check_env` et compatible avec tout algorithme Stable-Baselines3.\n",
        "\n",
        "Vous pouvez maintenant adapter et combiner ces techniques pour vos propres projets d’Apprentissage par Renforcement !"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2_wrappers_saving_loading_FR_synthesis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
