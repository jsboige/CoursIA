{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Kernel : Chat, Planners, Memory, Hugging Face, Groundedness & Multi-Result\n",
    "\n",
    "Dans ce notebook, nous allons explorer plusieurs fonctionnalités avancées de **Semantic Kernel** :\n",
    "\n",
    "1. **Chat basique** avec Kernel Arguments (historique et contexte via un objet `KernelArguments`).\n",
    "2. **Planners** (Sequential, Stepwise Function Calling) pour orchestrer dynamiquement des actions selon un but donné.\n",
    "3. **Mémoire** & Embeddings (VolatileMemoryStore ou connecteurs externes) pour stocker des informations sémantiques.\n",
    "4. **Hugging Face** : Intégration de modèles (texte, embeddings) en local ou depuis le Hub.\n",
    "5. **Groundedness Checking** : vérification et ajustement du contenu pour éviter des “fabrications non justifiées”.\n",
    "6. **Multi-Result** : récupération de plusieurs réponses pour un même prompt (OpenAI, Azure, Hugging Face).\n",
    "\n",
    "Ce notebook est une **synthèse** des exemples dispersés dans différents notebooks, présentés sous forme de cellules Markdown et Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Installation & Imports\n",
    "# ============================\n",
    "\n",
    "# N'installez qu'une seule fois si nécessaire\n",
    "%pip install -U semantic-kernel\n",
    "\n",
    "# Imports de base\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "print(\"Imports et installation OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Bloc Markdown – Configuration du Kernel & .env\n",
    "\n",
    "```markdown\n",
    "### Configuration du Kernel\n",
    "\n",
    "Pour exécuter les exemples, on suppose que vous avez un fichier `.env` comportant vos clés d'API OpenAI / Azure OpenAI / Hugging Face.  \n",
    "Exemple `.env` :\n",
    "\n",
    "```\n",
    "GLOBAL_LLM_SERVICE=\"OpenAI\"        # ou AzureOpenAI, HuggingFace\n",
    "OPENAI_API_KEY=\"sk-...\"\n",
    "OPENAI_CHAT_MODEL_ID=\"gpt-3.5-turbo\"\n",
    "...\n",
    "```\n",
    "\n",
    "Le `Kernel` lira ces informations pour décider quel connecteur LLM utiliser.  \n",
    "Ensuite, nous ajouterons nos services (`OpenAIChatCompletion`, `AzureChatCompletion`, etc.) selon la variable `GLOBAL_LLM_SERVICE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Initialisation du Kernel\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    AzureChatCompletion\n",
    ")\n",
    "\n",
    "# On charge le .env\n",
    "load_dotenv()\n",
    "global_llm_service = os.getenv(\"GLOBAL_LLM_SERVICE\", \"AzureOpenAI\")\n",
    "\n",
    "# Initialisation du Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "service_id = \"default\"\n",
    "\n",
    "if global_llm_service.lower() == \"openai\":\n",
    "    # Ajout du service OpenAI\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service OpenAI configuré.\")\n",
    "elif global_llm_service.lower() == \"huggingface\":\n",
    "    # Ajout du service HuggingFace\n",
    "    from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "    kernel.add_service(\n",
    "        HuggingFaceTextCompletion(\n",
    "            service_id=service_id,\n",
    "            ai_model_id=\"distilgpt2\", # ex. pour text-generation\n",
    "            task=\"text-generation\"\n",
    "        ),\n",
    "    )\n",
    "    print(\"Service Hugging Face configuré.\")\n",
    "else:\n",
    "    # Par défaut : Azure OpenAI\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service Azure OpenAI configuré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Basique avec KernelArguments\n",
    "\n",
    "L'idée : on crée une fonction de chat qui prend :\n",
    "- l’historique de conversation (objet `ChatHistory`)\n",
    "- un `user_input`\n",
    "et on stocke le tout dans un `KernelArguments`.  \n",
    "\n",
    "Cela permet d'alimenter un prompt (via un template) qui contient la variable `history` et `user_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Extrait d'un Chat Minimal\n",
    "# ============================\n",
    "\n",
    "# Exemple de prompt\n",
    "chat_prompt = \"\"\"\n",
    "{{$history}}\n",
    "User: {{$user_input}}\n",
    "ChatBot:\n",
    "\"\"\"\n",
    "\n",
    "# Création d'une fonction sémantique \"chat\"\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig, InputVariable\n",
    "\n",
    "pt_config = PromptTemplateConfig(\n",
    "    template=chat_prompt,\n",
    "    name=\"chatFunction\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"user_input\", description=\"User's message\"),\n",
    "        InputVariable(name=\"history\", description=\"Conversation history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    function_name=\"chat\",\n",
    "    plugin_name=\"myChatPlugin\",\n",
    "    prompt_template_config=pt_config\n",
    ")\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_system_message(\"Vous êtes un chatbot utile spécialisé en recommandations de livres.\")\n",
    "\n",
    "async def chat_kernel(input_text: str):\n",
    "    print(f\"[Utilisateur] : {input_text}\")\n",
    "    response = await kernel.invoke(\n",
    "        chat_function,\n",
    "        KernelArguments(user_input=input_text, history=chat_history)\n",
    "    )\n",
    "    print(f\"[ChatBot] : {response}\")\n",
    "    # Mise à jour de l'historique\n",
    "    chat_history.add_user_message(input_text)\n",
    "    chat_history.add_assistant_message(str(response))\n",
    "\n",
    "\n",
    "# Test\n",
    "await chat_kernel(\"Salut, peux-tu me conseiller un livre sur la philosophie antique ?\")\n",
    "await chat_kernel(\"Merci, tu peux détailler un peu plus la période concernée ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planners : Orchestration Dynamique\n",
    "\n",
    "**SequentialPlanner** : Génère un plan sous forme de liste d’étapes (XML), chaque étape étant une fonction existante.  \n",
    "**FunctionCallingStepwisePlanner** : S'appuie sur OpenAI function-calling pour exécuter “pas à pas” (ReAct, MRKL).\n",
    "\n",
    "**Exemple d'usage** :\n",
    "1. Le *user* fournit un `goal`.\n",
    "2. Le planner trouve les fonctions (plugins sémantiques ou natifs) permettant d'accomplir ce but.\n",
    "3. Il exécute les étapes, éventuellement enchaînant *function calls*.\n",
    "\n",
    "Ci-dessous, un extrait minimal de code pour un `SequentialPlanner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Sequential Planner\n",
    "# ============================\n",
    "from semantic_kernel.core_plugins.text_plugin import TextPlugin\n",
    "from semantic_kernel.functions.kernel_function_from_prompt import KernelFunctionFromPrompt\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\n",
    "\n",
    "\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "summarize_plugin = kernel.add_plugin(plugin_name=\"SummarizePlugin\", parent_directory=plugins_directory)\n",
    "writer_plugin = kernel.add_plugin(\n",
    "    plugin_name=\"WriterPlugin\",\n",
    "    parent_directory=plugins_directory,\n",
    ")\n",
    "text_plugin = kernel.add_plugin(plugin=TextPlugin(), plugin_name=\"TextPlugin\")\n",
    "\n",
    "shakespeare_func = KernelFunctionFromPrompt(\n",
    "    function_name=\"Shakespeare\",\n",
    "    plugin_name=\"WriterPlugin\",\n",
    "    prompt=\"\"\"\n",
    "{{$input}}\n",
    "\n",
    "Rewrite the above in the style of Shakespeare.\n",
    "\"\"\",\n",
    "    prompt_execution_settings=OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.8,\n",
    "    ),\n",
    "    description=\"Rewrite the input in the style of Shakespeare.\",\n",
    ")\n",
    "kernel.add_function(plugin_name=\"WriterPlugin\", function=shakespeare_func)\n",
    "\n",
    "for plugin_name, plugin in kernel.plugins.items():\n",
    "    for function_name, function in plugin.functions.items():\n",
    "        print(f\"Plugin: {plugin_name}, Function: {function_name}\")\n",
    "\n",
    "\n",
    "\n",
    "from semantic_kernel.planners import SequentialPlanner\n",
    "\n",
    "planner = SequentialPlanner(kernel, service_id=service_id)  # adapter selon ton service\n",
    "user_goal = \"\"\"\n",
    "Demain c'est la Saint-Valentin. Je veux composer un poème\n",
    "dans le style de Shakespeare, en français, puis convertir\n",
    "le texte en majuscules.\n",
    "\"\"\"\n",
    "\n",
    "seq_plan = await planner.create_plan(user_goal)\n",
    "print(\"Plan généré par le SequentialPlanner :\")\n",
    "for idx, step in enumerate(seq_plan._steps):\n",
    "    print(f\"Étape {idx+1} => {step.description} // {step.metadata.fully_qualified_name}\")\n",
    "\n",
    "# Exécuter le plan\n",
    "execution_result = await seq_plan.invoke(kernel)\n",
    "print(\"\\n=== Résultat du plan ===\")\n",
    "print(execution_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mémoire & Embeddings\n",
    "\n",
    "**SemanticTextMemory** permet de stocker des textes (avec un embedding) dans un store :\n",
    "- `VolatileMemoryStore` (en mémoire)\n",
    "- ou connecteurs vers Pinecone, Azure Cognitive Search, Qdrant, etc.\n",
    "\n",
    "On peut ensuite effectuer des requêtes sémantiques :  \n",
    "`await memory.search(\"MaCollection\", \"Quelle est mon budget pour 2024 ?\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Extrait Mémoire\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.memory import SemanticTextMemory, VolatileMemoryStore\n",
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n",
    "\n",
    "# Embedding service (ex : openai text-embedding-ada)\n",
    "embedding_service = OpenAITextEmbedding(\n",
    "    service_id=\"embeddingService\",\n",
    "    ai_model_id=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# Volatile in-memory store\n",
    "store = VolatileMemoryStore()\n",
    "memory = SemanticTextMemory(store, embeddings_generator=embedding_service)\n",
    "\n",
    "# Ajouter ce plugin au kernel\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")\n",
    "\n",
    "# Stocker quelques infos\n",
    "collection_name = \"testCollection\"\n",
    "await memory.save_information(collection=collection_name, id=\"info1\", text=\"Budget 2024 = 100k€\")\n",
    "await memory.save_information(collection=collection_name, id=\"info2\", text=\"Budget 2023 = 70k€\")\n",
    "\n",
    "# Requête\n",
    "results = await memory.search(collection_name, \"Quel est mon budget pour 2024 ?\", limit=1)\n",
    "print(\"Réponse potentielle : \", results[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Intégration\n",
    "\n",
    "Semantic Kernel peut se connecter à Hugging Face localement ou via API.  \n",
    "Exemple :  \n",
    "```python\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "\n",
    "hf_service = HuggingFaceTextCompletion(\n",
    "    service_id=\"textHF\", ai_model_id=\"distilgpt2\", task=\"text-generation\"\n",
    ")\n",
    "kernel.add_service(hf_service)\n",
    "```\n",
    "\n",
    "Ensuite, on peut enregistrer une fonction sémantique ou invoquer directement `hf_service.get_text_contents(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundedness Checking\n",
    "\n",
    "Pour éviter les “hallucinations” d’un résumé :  \n",
    "1. Extrait la liste d'entités du résumé.  \n",
    "2. Vérifie la correspondance de chaque entité avec le texte source (référence).  \n",
    "3. Retire ou corrige les entités non-fondées.\n",
    "\n",
    "Cela se fait via un plugin “GroundingPlugin” (ex. ExtraitEntities, ReferenceCheckEntities, ExciseEntities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Groundedness Checking\n",
    "# ============================\n",
    "\n",
    "# Suppose qu'on a un \"grounding_text\" = un texte source\n",
    "grounding_text = \"\"\"\n",
    "Votre budget 2024 est de 100k euros.\n",
    "Vous vivez à Genève.\n",
    "Vous avez investi 50k en actions.\n",
    "\"\"\"\n",
    "\n",
    "# Suppose qu'on a un résumé \"faux\"\n",
    "summary_text = \"\"\"\n",
    "Mon budget 2024 est de 200k euros.\n",
    "J'habite à Milan.\n",
    "\"\"\"\n",
    "\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "\n",
    "# On appelle un plugin (hypothétique) \"GroundingPlugin\" comportant 3 fonctions\n",
    "#   - \"ExtractEntities\"\n",
    "#   - \"ReferenceCheckEntities\"\n",
    "#   - \"ExciseEntities\"\n",
    "grounding_plugin = kernel.add_plugin(parent_directory= plugins_directory, plugin_name=\"GroundingPlugin\")\n",
    "\n",
    "extract_entities = grounding_plugin[\"ExtractEntities\"]\n",
    "check_entities = grounding_plugin[\"ReferenceCheckEntities\"]\n",
    "excise_entities = grounding_plugin[\"ExciseEntities\"]\n",
    "\n",
    "# 1) Extraire entités\n",
    "ext_result = await kernel.invoke(extract_entities, input=summary_text)\n",
    "print(\"Entités détectées:\", ext_result)\n",
    "\n",
    "# 2) Vérifier correspondance\n",
    "check_result = await kernel.invoke(check_entities, input=ext_result.value, reference_context=grounding_text)\n",
    "print(\"Entités non-fondées:\", check_result)\n",
    "\n",
    "# 3) Retirer entités non-fondées du summary\n",
    "excision = await kernel.invoke(excise_entities, input=summary_text, ungrounded_entities=check_result.value)\n",
    "print(\"Summary corrigé:\", excision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Result\n",
    "\n",
    "OpenAI (ou Azure) peut renvoyer plusieurs complétions pour un même prompt.  \n",
    "On paramètre `number_of_responses=3` dans les settings.  \n",
    "Ensuite, `get_text_contents(...)` ou `get_chat_message_contents(...)` renvoie un *tableau* de résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Multi-Result\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextPromptExecutionSettings\n",
    "\n",
    "settings = OpenAITextPromptExecutionSettings(\n",
    "    extension_data={\n",
    "        \"max_tokens\": 60,\n",
    "        \"temperature\": 0.7,\n",
    "        \"number_of_responses\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextCompletion\n",
    "text_service = OpenAITextCompletion(service_id=\"multiResult\", ai_model_id=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "prompt = \"Donne-moi une brève blague sur les chats :\"\n",
    "\n",
    "# get_text_contents() => liste de réponses\n",
    "responses = await text_service.get_text_contents(prompt, settings=settings)\n",
    "\n",
    "for i, r in enumerate(responses):\n",
    "    print(f\"Réponse n°{i+1}:\\n{r}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Nous avons vu :\n",
    "1. Un **Chat** basique avec `KernelArguments`.\n",
    "2. Les **Planners** (Sequential, Stepwise) pour orchestrer dynamiquement des steps.\n",
    "3. La **Mémoire** & embeddings pour stocker/rechercher des informations sémantiques.\n",
    "4. L’**intégration Hugging Face** pour exécuter localement des modèles open-source.\n",
    "5. Un **Groundedness Checking** minimal pour éviter les hallucinations.\n",
    "6. La gestion de **plusieurs réponses** (Multi-Result) avec un seul appel.\n",
    "\n",
    "Ces fonctionnalités permettent de créer des scénarios complexes : chat évolué, question-answering avec mémoire persistante, planification automatique, usage local ou cloud, etc. Bonne exploration !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
