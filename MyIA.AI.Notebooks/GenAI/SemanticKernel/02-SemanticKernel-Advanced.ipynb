{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Kernel : Chat, Planners, Memory, Hugging Face, Groundedness & Multi-Result\n",
    "\n",
    "Dans ce notebook, nous allons explorer plusieurs fonctionnalités avancées de **Semantic Kernel** :\n",
    "\n",
    "1. **Chat basique** avec Kernel Arguments (historique et contexte via un objet `KernelArguments`).\n",
    "2. **Planners** (Sequential, Stepwise Function Calling) pour orchestrer dynamiquement des actions selon un but donné.\n",
    "3. **Mémoire** & Embeddings (VolatileMemoryStore ou connecteurs externes) pour stocker des informations sémantiques.\n",
    "4. **Hugging Face** : Intégration de modèles (texte, embeddings) en local ou depuis le Hub.\n",
    "5. **Groundedness Checking** : vérification et ajustement du contenu pour éviter des “fabrications non justifiées”.\n",
    "6. **Multi-Result** : récupération de plusieurs réponses pour un même prompt (OpenAI, Azure, Hugging Face).\n",
    "\n",
    "Ce notebook est une **synthèse** des exemples dispersés dans différents notebooks, présentés sous forme de cellules Markdown et Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semantic-kernel in c:\\python313\\lib\\site-packages (1.22.1)\n",
      "Requirement already satisfied: aiohttp~=3.8 in c:\\python313\\lib\\site-packages (from semantic-kernel) (3.11.12)\n",
      "Requirement already satisfied: cloudevents~=1.0 in c:\\python313\\lib\\site-packages (from semantic-kernel) (1.11.0)\n",
      "Requirement already satisfied: pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.11,>=2.0 in c:\\python313\\lib\\site-packages (from semantic-kernel) (2.10.6)\n",
      "Requirement already satisfied: pydantic-settings~=2.0 in c:\\python313\\lib\\site-packages (from semantic-kernel) (2.7.1)\n",
      "Requirement already satisfied: defusedxml~=0.7 in c:\\python313\\lib\\site-packages (from semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: azure-identity~=1.13 in c:\\python313\\lib\\site-packages (from semantic-kernel) (1.19.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python313\\lib\\site-packages (from semantic-kernel) (2.2.2)\n",
      "Requirement already satisfied: openai~=1.61 in c:\\python313\\lib\\site-packages (from semantic-kernel) (1.65.2)\n",
      "Requirement already satisfied: openapi_core<0.20,>=0.18 in c:\\python313\\lib\\site-packages (from semantic-kernel) (0.19.4)\n",
      "Requirement already satisfied: opentelemetry-api~=1.24 in c:\\python313\\lib\\site-packages (from semantic-kernel) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.24 in c:\\python313\\lib\\site-packages (from semantic-kernel) (1.29.0)\n",
      "Requirement already satisfied: prance~=23.6.21.0 in c:\\python313\\lib\\site-packages (from semantic-kernel) (23.6.21.0)\n",
      "Requirement already satisfied: pybars4~=0.9 in c:\\python313\\lib\\site-packages (from semantic-kernel) (0.9.13)\n",
      "Requirement already satisfied: jinja2~=3.1 in c:\\python313\\lib\\site-packages (from semantic-kernel) (3.1.5)\n",
      "Requirement already satisfied: nest-asyncio~=1.6 in c:\\python313\\lib\\site-packages (from semantic-kernel) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.15.1 in c:\\python313\\lib\\site-packages (from semantic-kernel) (1.15.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.18.3)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in c:\\python313\\lib\\site-packages (from azure-identity~=1.13->semantic-kernel) (1.32.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\python313\\lib\\site-packages (from azure-identity~=1.13->semantic-kernel) (44.0.0)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\python313\\lib\\site-packages (from azure-identity~=1.13->semantic-kernel) (1.31.1)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\python313\\lib\\site-packages (from azure-identity~=1.13->semantic-kernel) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\python313\\lib\\site-packages (from azure-identity~=1.13->semantic-kernel) (4.12.2)\n",
      "Requirement already satisfied: deprecation<3.0,>=2.0 in c:\\python313\\lib\\site-packages (from cloudevents~=1.0->semantic-kernel) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python313\\lib\\site-packages (from jinja2~=3.1->semantic-kernel) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python313\\lib\\site-packages (from openai~=1.61->semantic-kernel) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python313\\lib\\site-packages (from openai~=1.61->semantic-kernel) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python313\\lib\\site-packages (from openai~=1.61->semantic-kernel) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\python313\\lib\\site-packages (from openai~=1.61->semantic-kernel) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\python313\\lib\\site-packages (from openai~=1.61->semantic-kernel) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\python313\\lib\\site-packages (from openai~=1.61->semantic-kernel) (4.67.1)\n",
      "Requirement already satisfied: isodate in c:\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.7.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in c:\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (4.23.0)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in c:\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.3.4)\n",
      "Requirement already satisfied: more-itertools in c:\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (10.6.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in c:\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.6.3)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in c:\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: parse in c:\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (1.20.2)\n",
      "Requirement already satisfied: werkzeug in c:\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (3.1.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\python313\\lib\\site-packages (from opentelemetry-api~=1.24->semantic-kernel) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\python313\\lib\\site-packages (from opentelemetry-api~=1.24->semantic-kernel) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in c:\\python313\\lib\\site-packages (from opentelemetry-sdk~=1.24->semantic-kernel) (0.50b0)\n",
      "Requirement already satisfied: chardet>=3.0 in c:\\python313\\lib\\site-packages (from prance~=23.6.21.0->semantic-kernel) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in c:\\python313\\lib\\site-packages (from prance~=23.6.21.0->semantic-kernel) (0.18.10)\n",
      "Requirement already satisfied: requests>=2.25 in c:\\python313\\lib\\site-packages (from prance~=23.6.21.0->semantic-kernel) (2.32.3)\n",
      "Requirement already satisfied: six~=1.15 in c:\\python313\\lib\\site-packages (from prance~=23.6.21.0->semantic-kernel) (1.17.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\python313\\lib\\site-packages (from prance~=23.6.21.0->semantic-kernel) (24.2)\n",
      "Requirement already satisfied: PyMeta3>=0.5.1 in c:\\python313\\lib\\site-packages (from pybars4~=0.9->semantic-kernel) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python313\\lib\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.11,>=2.0->semantic-kernel) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\python313\\lib\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.11,>=2.0->semantic-kernel) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\python313\\lib\\site-packages (from pydantic-settings~=2.0->semantic-kernel) (1.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai~=1.61->semantic-kernel) (3.10)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\python313\\lib\\site-packages (from cryptography>=2.5->azure-identity~=1.13->semantic-kernel) (1.17.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\python313\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api~=1.24->semantic-kernel) (1.17.2)\n",
      "Requirement already satisfied: certifi in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai~=1.61->semantic-kernel) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai~=1.61->semantic-kernel) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.61->semantic-kernel) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\python313\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api~=1.24->semantic-kernel) (3.21.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\python313\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\python313\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\python313\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.22.3)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\python313\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (6.0.2)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in c:\\python313\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (0.4.4)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\python313\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity~=1.13->semantic-kernel) (2.10.1)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in c:\\python313\\lib\\site-packages (from msal-extensions>=1.2.0->azure-identity~=1.13->semantic-kernel) (2.10.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\python313\\lib\\site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.1.4)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in c:\\python313\\lib\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel) (1.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python313\\lib\\site-packages (from requests>=2.25->prance~=23.6.21.0->semantic-kernel) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python313\\lib\\site-packages (from requests>=2.25->prance~=23.6.21.0->semantic-kernel) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\python313\\lib\\site-packages (from tqdm>4->openai~=1.61->semantic-kernel) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\python313\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity~=1.13->semantic-kernel) (2.22)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\python313\\lib\\site-packages (from portalocker<3,>=1.4->msal-extensions>=1.2.0->azure-identity~=1.13->semantic-kernel) (308)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Imports et installation OK.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Installation & Imports\n",
    "# ============================\n",
    "\n",
    "# N'installez qu'une seule fois si nécessaire\n",
    "%pip install -U semantic-kernel\n",
    "\n",
    "# Imports de base\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "print(\"Imports et installation OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Bloc Markdown – Configuration du Kernel & .env\n",
    "\n",
    "```markdown\n",
    "### Configuration du Kernel\n",
    "\n",
    "Pour exécuter les exemples, on suppose que vous avez un fichier `.env` comportant vos clés d'API OpenAI / Azure OpenAI / Hugging Face.  \n",
    "Exemple `.env` :\n",
    "\n",
    "```\n",
    "GLOBAL_LLM_SERVICE=\"OpenAI\"        # ou AzureOpenAI, HuggingFace\n",
    "OPENAI_API_KEY=\"sk-...\"\n",
    "OPENAI_CHAT_MODEL_ID=\"gpt-3.5-turbo\"\n",
    "...\n",
    "```\n",
    "\n",
    "Le `Kernel` lira ces informations pour décider quel connecteur LLM utiliser.  \n",
    "Ensuite, nous ajouterons nos services (`OpenAIChatCompletion`, `AzureChatCompletion`, etc.) selon la variable `GLOBAL_LLM_SERVICE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service OpenAI configuré.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Initialisation du Kernel\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    AzureChatCompletion\n",
    ")\n",
    "\n",
    "# On charge le .env\n",
    "load_dotenv()\n",
    "global_llm_service = os.getenv(\"GLOBAL_LLM_SERVICE\", \"AzureOpenAI\")\n",
    "\n",
    "# Initialisation du Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "service_id = \"default\"\n",
    "\n",
    "if global_llm_service.lower() == \"openai\":\n",
    "    # Ajout du service OpenAI\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service OpenAI configuré.\")\n",
    "elif global_llm_service.lower() == \"huggingface\":\n",
    "    # Ajout du service HuggingFace\n",
    "    from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "    kernel.add_service(\n",
    "        HuggingFaceTextCompletion(\n",
    "            service_id=service_id,\n",
    "            ai_model_id=\"distilgpt2\", # ex. pour text-generation\n",
    "            task=\"text-generation\"\n",
    "        ),\n",
    "    )\n",
    "    print(\"Service Hugging Face configuré.\")\n",
    "else:\n",
    "    # Par défaut : Azure OpenAI\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service Azure OpenAI configuré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Basique avec KernelArguments\n",
    "\n",
    "L'idée : on crée une fonction de chat qui prend :\n",
    "- l’historique de conversation (objet `ChatHistory`)\n",
    "- un `user_input`\n",
    "et on stocke le tout dans un `KernelArguments`.  \n",
    "\n",
    "Cela permet d'alimenter un prompt (via un template) qui contient la variable `history` et `user_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Utilisateur] : Salut, peux-tu me conseiller un livre sur la philosophie antique ?\n",
      "[ChatBot] : Salut ! Je te recommande \"La République\" de Platon. C'est une œuvre incontournable qui aborde des thèmes comme la justice, la politique, et la nature de l'âme. Si tu cherches quelque chose de plus centré sur les Stoïciens, \"Méditations\" de Marc Aurèle est également une excellente sélection, offrant des réflexions personnelles sur la vertu et la résilience. Tu veux des suggestions supplémentaires ?\n",
      "[Utilisateur] : Merci, tu peux détailler un peu plus la période concernée ?\n",
      "[ChatBot] : Bien sûr ! La philosophie antique couvre principalement la période qui s'étend du VIe siècle av. J.-C. jusqu'au Ve siècle apr. J.-C. Cette période se divise généralement en plusieurs phases :\n",
      "\n",
      "1. **La période présocratique (VIe - Ve siècles av. J.-C.)** : C'est à cette époque que des penseurs comme Thalès, Anaximène, et Héraclite commencent à explorer des questions sur la nature de l'univers et l'origine des choses. Ils jettent les bases de la philosophie en cherchant des explications rationnelles plutôt que mythologiques.\n",
      "\n",
      "2. **La période classique (Ve - IVe siècles av. J.-C.)** : Cette période est marquée par des figures emblématiques comme Socrate, Platon et Aristote. Socrate est connu pour sa méthode dialectique et son questionnement de l'éthique. Platon, son disciple, développe des idées sur les formes et la justice dans \"La République\". Aristote, à son tour, élargit la philosophie en abordant divers domaines tels que la logique, la métaphysique et la biologie.\n",
      "\n",
      "3. **La période hellénistique (IVe siècle av. J.-C. - IIIe siècle apr. J.-C.)** : Après la mort d'Alexandre le Grand, la philosophie se diversifie en plusieurs écoles, notamment le stoïcisme (avec Sénèque et Épictète), l'épicurisme (avec Épicure) et le scepticisme. Chacune propose des réponses différentes aux questions de la vie, de la vertu et du bonheur.\n",
      "\n",
      "4. **La période romaine (IIIe siècle av. J.-C. - Ve siècle apr. J.-C.)** : La philosophie se développe principalement autour de la pensée stoïcienne et du néoplatonisme. Des penseurs comme Marc Aurèle et Plotin influencent la manière dont la philosophie est intégrée dans la culture romaine, avec un fort accent sur l'éthique et la spiritualité.\n",
      "\n",
      "Ces différentes périodes montrent l'évolution de la pensée philosophique et comment elle a influencé la culture et la société de l'Antiquité. Si tu es intéressé par une période ou un penseur en particulier, fais-le moi savoir !\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Extrait d'un Chat Minimal\n",
    "# ============================\n",
    "\n",
    "# Exemple de prompt\n",
    "chat_prompt = \"\"\"\n",
    "{{$history}}\n",
    "User: {{$user_input}}\n",
    "ChatBot:\n",
    "\"\"\"\n",
    "\n",
    "# Création d'une fonction sémantique \"chat\"\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig, InputVariable\n",
    "\n",
    "pt_config = PromptTemplateConfig(\n",
    "    template=chat_prompt,\n",
    "    name=\"chatFunction\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"user_input\", description=\"User's message\"),\n",
    "        InputVariable(name=\"history\", description=\"Conversation history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    function_name=\"chat\",\n",
    "    plugin_name=\"myChatPlugin\",\n",
    "    prompt_template_config=pt_config\n",
    ")\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_system_message(\"Vous êtes un chatbot utile spécialisé en recommandations de livres.\")\n",
    "\n",
    "async def chat_kernel(input_text: str):\n",
    "    print(f\"[Utilisateur] : {input_text}\")\n",
    "    response = await kernel.invoke(\n",
    "        chat_function,\n",
    "        KernelArguments(user_input=input_text, history=chat_history)\n",
    "    )\n",
    "    print(f\"[ChatBot] : {response}\")\n",
    "    # Mise à jour de l'historique\n",
    "    chat_history.add_user_message(input_text)\n",
    "    chat_history.add_assistant_message(str(response))\n",
    "\n",
    "\n",
    "# Test\n",
    "await chat_kernel(\"Salut, peux-tu me conseiller un livre sur la philosophie antique ?\")\n",
    "await chat_kernel(\"Merci, tu peux détailler un peu plus la période concernée ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planners : Orchestration Dynamique\n",
    "\n",
    "**SequentialPlanner** : Génère un plan sous forme de liste d’étapes (XML), chaque étape étant une fonction existante.  \n",
    "**FunctionCallingStepwisePlanner** : S'appuie sur OpenAI function-calling pour exécuter “pas à pas” (ReAct, MRKL).\n",
    "\n",
    "**Exemple d'usage** :\n",
    "1. Le *user* fournit un `goal`.\n",
    "2. Le planner trouve les fonctions (plugins sémantiques ou natifs) permettant d'accomplir ce but.\n",
    "3. Il exécute les étapes, éventuellement enchaînant *function calls*.\n",
    "\n",
    "Ci-dessous, un extrait minimal de code pour un `SequentialPlanner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plugin: myChatPlugin, Function: chat\n",
      "Plugin: SummarizePlugin, Function: MakeAbstractReadable\n",
      "Plugin: SummarizePlugin, Function: Notegen\n",
      "Plugin: SummarizePlugin, Function: Summarize\n",
      "Plugin: SummarizePlugin, Function: Topics\n",
      "Plugin: WriterPlugin, Function: Acronym\n",
      "Plugin: WriterPlugin, Function: AcronymGenerator\n",
      "Plugin: WriterPlugin, Function: AcronymReverse\n",
      "Plugin: WriterPlugin, Function: Brainstorm\n",
      "Plugin: WriterPlugin, Function: EmailGen\n",
      "Plugin: WriterPlugin, Function: EmailTo\n",
      "Plugin: WriterPlugin, Function: EnglishImprover\n",
      "Plugin: WriterPlugin, Function: NovelChapter\n",
      "Plugin: WriterPlugin, Function: NovelChapterWithNotes\n",
      "Plugin: WriterPlugin, Function: NovelOutline\n",
      "Plugin: WriterPlugin, Function: Rewrite\n",
      "Plugin: WriterPlugin, Function: ShortPoem\n",
      "Plugin: WriterPlugin, Function: StoryGen\n",
      "Plugin: WriterPlugin, Function: TellMeMore\n",
      "Plugin: WriterPlugin, Function: Translate\n",
      "Plugin: WriterPlugin, Function: TwoSentenceSummary\n",
      "Plugin: WriterPlugin, Function: Shakespeare\n",
      "Plugin: TextPlugin, Function: lowercase\n",
      "Plugin: TextPlugin, Function: trim\n",
      "Plugin: TextPlugin, Function: trim_end\n",
      "Plugin: TextPlugin, Function: trim_start\n",
      "Plugin: TextPlugin, Function: uppercase\n",
      "Plugin: SequentialPlanner_Excluded, Function: SequentialPlanner_Excluded\n",
      "Plan généré par le SequentialPlanner :\n",
      "Étape 1 => Turn a scenario into a short and entertaining poem. // WriterPlugin-ShortPoem\n",
      "Étape 2 => Rewrite the input in the style of Shakespeare. // WriterPlugin-Shakespeare\n",
      "Étape 3 => Convert a string to uppercase. // TextPlugin-uppercase\n",
      "\n",
      "=== Résultat du plan ===\n",
      "TOMORROW'S MORN SHALL BE FOR LOVERS' CHEER,  \n",
      "WITH CONFECTIONS SWEET AND BLOOMS OF FAIREST HUE;  \n",
      "YET MY COMPANION DOTH TARRY, I FEAR,  \n",
      "PRAY, 'TIS NOT DESTINY'S CRUEL ADO.  \n",
      "FOR HERE AM I, WITH MINE FELINE IN SIGHT,  \n",
      "WHO DOTH NIBBLE MY HEART-SHAPED JOY, 'TIS TRUE.  \n",
      "\n",
      "WHILE I AWAIT THAT WHICH MY HEART DOTH DESIRE,  \n",
      "MY CAT DOTH PRANCE, AND MY HOPES HE DOTH TIRE.  \n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Sequential Planner\n",
    "# ============================\n",
    "from semantic_kernel.core_plugins.text_plugin import TextPlugin\n",
    "from semantic_kernel.functions.kernel_function_from_prompt import KernelFunctionFromPrompt\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\n",
    "\n",
    "\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "summarize_plugin = kernel.add_plugin(plugin_name=\"SummarizePlugin\", parent_directory=plugins_directory)\n",
    "writer_plugin = kernel.add_plugin(\n",
    "    plugin_name=\"WriterPlugin\",\n",
    "    parent_directory=plugins_directory,\n",
    ")\n",
    "text_plugin = kernel.add_plugin(plugin=TextPlugin(), plugin_name=\"TextPlugin\")\n",
    "\n",
    "shakespeare_func = KernelFunctionFromPrompt(\n",
    "    function_name=\"Shakespeare\",\n",
    "    plugin_name=\"WriterPlugin\",\n",
    "    prompt=\"\"\"\n",
    "{{$input}}\n",
    "\n",
    "Rewrite the above in the style of Shakespeare.\n",
    "\"\"\",\n",
    "    prompt_execution_settings=OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.8,\n",
    "    ),\n",
    "    description=\"Rewrite the input in the style of Shakespeare.\",\n",
    ")\n",
    "kernel.add_function(plugin_name=\"WriterPlugin\", function=shakespeare_func)\n",
    "\n",
    "for plugin_name, plugin in kernel.plugins.items():\n",
    "    for function_name, function in plugin.functions.items():\n",
    "        print(f\"Plugin: {plugin_name}, Function: {function_name}\")\n",
    "\n",
    "\n",
    "\n",
    "from semantic_kernel.planners import SequentialPlanner\n",
    "\n",
    "planner = SequentialPlanner(kernel, service_id=service_id)  # adapter selon ton service\n",
    "user_goal = \"\"\"\n",
    "Demain c'est la Saint-Valentin. Je veux composer un poème\n",
    "dans le style de Shakespeare, en français, puis convertir\n",
    "le texte en majuscules.\n",
    "\"\"\"\n",
    "\n",
    "seq_plan = await planner.create_plan(user_goal)\n",
    "print(\"Plan généré par le SequentialPlanner :\")\n",
    "for idx, step in enumerate(seq_plan._steps):\n",
    "    print(f\"Étape {idx+1} => {step.description} // {step.metadata.fully_qualified_name}\")\n",
    "\n",
    "# Exécuter le plan\n",
    "execution_result = await seq_plan.invoke(kernel)\n",
    "print(\"\\n=== Résultat du plan ===\")\n",
    "print(execution_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mémoire & Embeddings\n",
    "\n",
    "**SemanticTextMemory** permet de stocker des textes (avec un embedding) dans un store :\n",
    "- `VolatileMemoryStore` (en mémoire)\n",
    "- ou connecteurs vers Pinecone, Azure Cognitive Search, Qdrant, etc.\n",
    "\n",
    "On peut ensuite effectuer des requêtes sémantiques :  \n",
    "`await memory.search(\"MaCollection\", \"Quelle est mon budget pour 2024 ?\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse potentielle :  Budget 2024 = 100k€\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Extrait Mémoire\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.memory import SemanticTextMemory, VolatileMemoryStore\n",
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n",
    "\n",
    "# Embedding service (ex : openai text-embedding-ada)\n",
    "embedding_service = OpenAITextEmbedding(\n",
    "    service_id=\"embeddingService\",\n",
    "    ai_model_id=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# Volatile in-memory store\n",
    "store = VolatileMemoryStore()\n",
    "memory = SemanticTextMemory(store, embeddings_generator=embedding_service)\n",
    "\n",
    "# Ajouter ce plugin au kernel\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")\n",
    "\n",
    "# Stocker quelques infos\n",
    "collection_name = \"testCollection\"\n",
    "await memory.save_information(collection=collection_name, id=\"info1\", text=\"Budget 2024 = 100k€\")\n",
    "await memory.save_information(collection=collection_name, id=\"info2\", text=\"Budget 2023 = 70k€\")\n",
    "\n",
    "# Requête\n",
    "results = await memory.search(collection_name, \"Quel est mon budget pour 2024 ?\", limit=1)\n",
    "print(\"Réponse potentielle : \", results[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Intégration\n",
    "\n",
    "Semantic Kernel peut se connecter à Hugging Face localement ou via API.  \n",
    "Exemple :  \n",
    "```python\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "\n",
    "hf_service = HuggingFaceTextCompletion(\n",
    "    service_id=\"textHF\", ai_model_id=\"distilgpt2\", task=\"text-generation\"\n",
    ")\n",
    "kernel.add_service(hf_service)\n",
    "```\n",
    "\n",
    "Ensuite, on peut enregistrer une fonction sémantique ou invoquer directement `hf_service.get_text_contents(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundedness Checking\n",
    "\n",
    "Pour éviter les “hallucinations” d’un résumé :  \n",
    "1. Extrait la liste d'entités du résumé.  \n",
    "2. Vérifie la correspondance de chaque entité avec le texte source (référence).  \n",
    "3. Retire ou corrige les entités non-fondées.\n",
    "\n",
    "Cela se fait via un plugin “GroundingPlugin” (ex. ExtraitEntities, ReferenceCheckEntities, ExciseEntities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Variable `Symbols.VAR_PREFIX: topic` not found in the KernelArguments\n",
      "Variable `Symbols.VAR_PREFIX: topic` not found in the KernelArguments\n",
      "Variable `Symbols.VAR_PREFIX: example_entities` not found in the KernelArguments\n",
      "Variable `Symbols.VAR_PREFIX: topic` not found in the KernelArguments\n",
      "Variable `Symbols.VAR_PREFIX: topic` not found in the KernelArguments\n",
      "Variable `Symbols.VAR_PREFIX: topic` not found in the KernelArguments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entités détectées: <entities>\n",
      "- budget\n",
      "- euros\n",
      "- Milan\n",
      "</entities>\n",
      "Entités non-fondées: <ungrounded_entities>\n",
      "- Milan\n",
      "</ungrounded_entities>\n",
      "Summary corrigé: Mon budget 2024 est de 200k euros.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Groundedness Checking\n",
    "# ============================\n",
    "\n",
    "# Suppose qu'on a un \"grounding_text\" = un texte source\n",
    "grounding_text = \"\"\"\n",
    "Votre budget 2024 est de 100k euros.\n",
    "Vous vivez à Genève.\n",
    "Vous avez investi 50k en actions.\n",
    "\"\"\"\n",
    "\n",
    "# Suppose qu'on a un résumé \"faux\"\n",
    "summary_text = \"\"\"\n",
    "Mon budget 2024 est de 200k euros.\n",
    "J'habite à Milan.\n",
    "\"\"\"\n",
    "\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "\n",
    "# On appelle un plugin (hypothétique) \"GroundingPlugin\" comportant 3 fonctions\n",
    "#   - \"ExtractEntities\"\n",
    "#   - \"ReferenceCheckEntities\"\n",
    "#   - \"ExciseEntities\"\n",
    "grounding_plugin = kernel.add_plugin(parent_directory= plugins_directory, plugin_name=\"GroundingPlugin\")\n",
    "\n",
    "extract_entities = grounding_plugin[\"ExtractEntities\"]\n",
    "check_entities = grounding_plugin[\"ReferenceCheckEntities\"]\n",
    "excise_entities = grounding_plugin[\"ExciseEntities\"]\n",
    "\n",
    "# 1) Extraire entités\n",
    "ext_result = await kernel.invoke(extract_entities, input=summary_text)\n",
    "print(\"Entités détectées:\", ext_result)\n",
    "\n",
    "# 2) Vérifier correspondance\n",
    "check_result = await kernel.invoke(check_entities, input=ext_result.value, reference_context=grounding_text)\n",
    "print(\"Entités non-fondées:\", check_result)\n",
    "\n",
    "# 3) Retirer entités non-fondées du summary\n",
    "excision = await kernel.invoke(excise_entities, input=summary_text, ungrounded_entities=check_result.value)\n",
    "print(\"Summary corrigé:\", excision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Result\n",
    "\n",
    "OpenAI (ou Azure) peut renvoyer plusieurs complétions pour un même prompt.  \n",
    "On paramètre `number_of_responses=3` dans les settings.  \n",
    "Ensuite, `get_text_contents(...)` ou `get_chat_message_contents(...)` renvoie un *tableau* de résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse n°1:\n",
      "\n",
      "Pourquoi les chats sont-ils de mauvais menteurs ? Parce qu'ils ont toujours la queue qui les trahit !\n",
      "\n",
      "Réponse n°2:\n",
      "\n",
      "Pourquoi les chats n'aiment-ils pas les blagues ? Parce qu'ils préfèrent les griffes-à-rire !\n",
      "\n",
      "Réponse n°3:\n",
      "\n",
      "\n",
      "Pourquoi les chats n'aiment-ils pas l'eau ?\n",
      "\n",
      "Parce que dans l'eau minet, ils ont les poils tout mouillés !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Multi-Result\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextPromptExecutionSettings\n",
    "\n",
    "settings = OpenAITextPromptExecutionSettings(\n",
    "    extension_data={\n",
    "        \"max_tokens\": 60,\n",
    "        \"temperature\": 0.7,\n",
    "        \"number_of_responses\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextCompletion\n",
    "text_service = OpenAITextCompletion(service_id=\"multiResult\", ai_model_id=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "prompt = \"Donne-moi une brève blague sur les chats :\"\n",
    "\n",
    "# get_text_contents() => liste de réponses\n",
    "responses = await text_service.get_text_contents(prompt, settings=settings)\n",
    "\n",
    "for i, r in enumerate(responses):\n",
    "    print(f\"Réponse n°{i+1}:\\n{r}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Nous avons vu :\n",
    "1. Un **Chat** basique avec `KernelArguments`.\n",
    "2. Les **Planners** (Sequential, Stepwise) pour orchestrer dynamiquement des steps.\n",
    "3. La **Mémoire** & embeddings pour stocker/rechercher des informations sémantiques.\n",
    "4. L’**intégration Hugging Face** pour exécuter localement des modèles open-source.\n",
    "5. Un **Groundedness Checking** minimal pour éviter les hallucinations.\n",
    "6. La gestion de **plusieurs réponses** (Multi-Result) avec un seul appel.\n",
    "\n",
    "Ces fonctionnalités permettent de créer des scénarios complexes : chat évolué, question-answering avec mémoire persistante, planification automatique, usage local ou cloud, etc. Bonne exploration !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
