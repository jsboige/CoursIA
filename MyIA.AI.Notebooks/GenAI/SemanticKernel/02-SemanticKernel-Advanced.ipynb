{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Kernel : Chat, Planners, Memory, Hugging Face, Groundedness & Multi-Result\n",
    "\n",
    "Dans ce notebook, nous allons explorer plusieurs fonctionnalités avancées de **Semantic Kernel** :\n",
    "\n",
    "1. **Chat basique** avec Kernel Arguments (historique et contexte via un objet `KernelArguments`).\n",
    "2. **Planners** (Sequential, Stepwise Function Calling) pour orchestrer dynamiquement des actions selon un but donné.\n",
    "3. **Mémoire** & Embeddings (VolatileMemoryStore ou connecteurs externes) pour stocker des informations sémantiques.\n",
    "4. **Hugging Face** : Intégration de modèles (texte, embeddings) en local ou depuis le Hub.\n",
    "5. **Groundedness Checking** : vérification et ajustement du contenu pour éviter des “fabrications non justifiées”.\n",
    "6. **Multi-Result** : récupération de plusieurs réponses pour un même prompt (OpenAI, Azure, Hugging Face).\n",
    "\n",
    "Ce notebook est une **synthèse** des exemples dispersés dans différents notebooks, présentés sous forme de cellules Markdown et Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Installation & Imports\n",
    "# ============================\n",
    "\n",
    "# N'installez qu'une seule fois si nécessaire\n",
    "%pip install -U semantic-kernel\n",
    "\n",
    "# Imports de base\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "print(\"Imports et installation OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Bloc Markdown – Configuration du Kernel & .env\n",
    "\n",
    "```markdown\n",
    "### Configuration du Kernel\n",
    "\n",
    "Pour exécuter les exemples, on suppose que vous avez un fichier `.env` comportant vos clés d'API OpenAI / Azure OpenAI / Hugging Face.  \n",
    "Exemple `.env` :\n",
    "\n",
    "```\n",
    "GLOBAL_LLM_SERVICE=\"OpenAI\"        # ou AzureOpenAI, HuggingFace\n",
    "OPENAI_API_KEY=\"sk-...\"\n",
    "OPENAI_CHAT_MODEL_ID=\"gpt-3.5-turbo\"\n",
    "...\n",
    "```\n",
    "\n",
    "Le `Kernel` lira ces informations pour décider quel connecteur LLM utiliser.  \n",
    "Ensuite, nous ajouterons nos services (`OpenAIChatCompletion`, `AzureChatCompletion`, etc.) selon la variable `GLOBAL_LLM_SERVICE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Initialisation du Kernel\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    AzureChatCompletion\n",
    ")\n",
    "\n",
    "# On charge le .env\n",
    "load_dotenv()\n",
    "global_llm_service = os.getenv(\"GLOBAL_LLM_SERVICE\", \"AzureOpenAI\")\n",
    "\n",
    "# Initialisation du Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "service_id = \"default\"\n",
    "\n",
    "if global_llm_service.lower() == \"openai\":\n",
    "    # Ajout du service OpenAI\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service OpenAI configuré.\")\n",
    "elif global_llm_service.lower() == \"huggingface\":\n",
    "    # Ajout du service HuggingFace\n",
    "    from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "    kernel.add_service(\n",
    "        HuggingFaceTextCompletion(\n",
    "            service_id=service_id,\n",
    "            ai_model_id=\"distilgpt2\", # ex. pour text-generation\n",
    "            task=\"text-generation\"\n",
    "        ),\n",
    "    )\n",
    "    print(\"Service Hugging Face configuré.\")\n",
    "else:\n",
    "    # Par défaut : Azure OpenAI\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service Azure OpenAI configuré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Basique avec KernelArguments\n",
    "\n",
    "L'idée : on crée une fonction de chat qui prend :\n",
    "- l’historique de conversation (objet `ChatHistory`)\n",
    "- un `user_input`\n",
    "et on stocke le tout dans un `KernelArguments`.  \n",
    "\n",
    "Cela permet d'alimenter un prompt (via un template) qui contient la variable `history` et `user_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Extrait d'un Chat Minimal\n",
    "# ============================\n",
    "\n",
    "# Exemple de prompt\n",
    "chat_prompt = \"\"\"\n",
    "{{$history}}\n",
    "User: {{$user_input}}\n",
    "ChatBot:\n",
    "\"\"\"\n",
    "\n",
    "# Création d'une fonction sémantique \"chat\"\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "\n",
    "pt_config = PromptTemplateConfig(\n",
    "    template=chat_prompt,\n",
    "    name=\"chatFunction\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"user_input\", description=\"User's message\"),\n",
    "        InputVariable(name=\"history\", description=\"Conversation history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    function_name=\"chat\",\n",
    "    plugin_name=\"myChatPlugin\",\n",
    "    prompt_template_config=pt_config\n",
    ")\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_system_message(\"Vous êtes un chatbot utile spécialisé en recommandations de livres.\")\n",
    "\n",
    "async def chat_kernel(input_text: str):\n",
    "    print(f\"[Utilisateur] : {input_text}\")\n",
    "    response = await kernel.invoke(\n",
    "        chat_function,\n",
    "        KernelArguments(user_input=input_text, history=str(chat_history))\n",
    "    )\n",
    "    print(f\"[ChatBot] : {response}\")\n",
    "    # Mise à jour de l'historique\n",
    "    chat_history.add_user_message(input_text)\n",
    "    chat_history.add_assistant_message(str(response))\n",
    "\n",
    "\n",
    "# Test\n",
    "await chat_kernel(\"Salut, peux-tu me conseiller un livre sur la philosophie antique ?\")\n",
    "await chat_kernel(\"Merci, tu peux détailler un peu plus la période concernée ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planners : Orchestration Dynamique\n",
    "\n",
    "**SequentialPlanner** : Génère un plan sous forme de liste d’étapes (XML), chaque étape étant une fonction existante.  \n",
    "**FunctionCallingStepwisePlanner** : S'appuie sur OpenAI function-calling pour exécuter “pas à pas” (ReAct, MRKL).\n",
    "\n",
    "**Exemple d'usage** :\n",
    "1. Le *user* fournit un `goal`.\n",
    "2. Le planner trouve les fonctions (plugins sémantiques ou natifs) permettant d'accomplir ce but.\n",
    "3. Il exécute les étapes, éventuellement enchaînant *function calls*.\n",
    "\n",
    "Ci-dessous, un extrait minimal de code pour un `SequentialPlanner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Sequential Planner\n",
    "# ============================\n",
    "from semantic_kernel.core_plugins.text_plugin import TextPlugin\n",
    "from semantic_kernel.functions import KernelFunctionFromPrompt\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\n",
    "\n",
    "\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "summarize_plugin = kernel.add_plugin(plugin_name=\"SummarizePlugin\", parent_directory=plugins_directory)\n",
    "writer_plugin = kernel.add_plugin(\n",
    "    plugin_name=\"WriterPlugin\",\n",
    "    parent_directory=plugins_directory,\n",
    ")\n",
    "text_plugin = kernel.add_plugin(plugin=TextPlugin(), plugin_name=\"TextPlugin\")\n",
    "\n",
    "shakespeare_func = KernelFunctionFromPrompt(\n",
    "    function_name=\"Shakespeare\",\n",
    "    plugin_name=\"WriterPlugin\",\n",
    "    prompt=\"\"\"\n",
    "{{$input}}\n",
    "\n",
    "Rewrite the above in the style of Shakespeare.\n",
    "\"\"\",\n",
    "    prompt_execution_settings=OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.8,\n",
    "    ),\n",
    "    description=\"Rewrite the input in the style of Shakespeare.\",\n",
    ")\n",
    "kernel.add_function(plugin_name=\"WriterPlugin\", function=shakespeare_func)\n",
    "\n",
    "for plugin_name, plugin in kernel.plugins.items():\n",
    "    for function_name, function in plugin.functions.items():\n",
    "        print(f\"Plugin: {plugin_name}, Function: {function_name}\")\n",
    "\n",
    "\n",
    "\n",
    "# CORRECTION: SequentialPlanner est déprécié dans cette version\n",
    "# from semantic_kernel.planners import SequentialPlanner\n",
    "# Alternative: utiliser les agents ou function calling direct\n",
    "print(\"\\nATTENTION: SequentialPlanner n'est plus disponible dans cette version de SemanticKernel\")\n",
    "print(\"Utilisation d'une approche alternative sans SequentialPlanner :\\n\")\n",
    "user_goal = \"\"\"\n",
    "Demain c'est la Saint-Valentin. Je veux composer un poème\n",
    "dans le style de Shakespeare, en français, puis convertir\n",
    "le texte en majuscules.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Objectif : {user_goal}\")\n",
    "\n",
    "# Approche manuelle : étape par étape\n",
    "# Étape 1 : Générer le poème avec Shakespeare function\n",
    "poem_input = \"Compose un poème d'amour pour la Saint-Valentin\"\n",
    "shakespeare_result = await kernel.invoke(shakespeare_func, KernelArguments(input=poem_input))\n",
    "print(f\"Étape 1 - Poème Shakespeare : {shakespeare_result}\")\n",
    "\n",
    "# Étape 2 : Convertir en majuscules avec TextPlugin\n",
    "if 'TextPlugin' in kernel.plugins and 'uppercase' in kernel.plugins['TextPlugin'].functions:\n",
    "    uppercase_result = await kernel.invoke(\n",
    "        kernel.plugins['TextPlugin']['uppercase'], \n",
    "        KernelArguments(input=str(shakespeare_result))\n",
    "    )\n",
    "    print(f\"\\n=== Résultat final ===\")\n",
    "    print(uppercase_result)\n",
    "else:\n",
    "    # Fallback simple\n",
    "    final_result = str(shakespeare_result).upper()\n",
    "    print(f\"\\n=== Résultat final (fallback) ===\")\n",
    "    print(final_result)\n",
    "\n",
    "# CODE OBSOLÈTE SUPPRIMÉ - Plus de référence à seq_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mémoire & Embeddings\n",
    "\n",
    "**SemanticTextMemory** permet de stocker des textes (avec un embedding) dans un store :\n",
    "- `VolatileMemoryStore` (en mémoire)\n",
    "- ou connecteurs vers Pinecone, Azure Cognitive Search, Qdrant, etc.\n",
    "\n",
    "On peut ensuite effectuer des requêtes sémantiques :  \n",
    "`await memory.search(\"MaCollection\", \"Quelle est mon budget pour 2024 ?\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Extrait Mémoire\n",
    "# ============================\n",
    "\n",
    "# CORRECTION: Utilisation des nouvelles approches pour la mémoire vectorielle\n",
    "from semantic_kernel.connectors.in_memory import InMemoryStore\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n",
    "\n",
    "# Embedding service (ex : openai text-embedding-ada)\n",
    "embedding_service = OpenAITextEmbedding(\n",
    "    service_id=\"embeddingService\",\n",
    "    ai_model_id=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# Nouvelle approche avec InMemoryStore\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Ajout du service d'embedding au kernel\n",
    "kernel.add_service(embedding_service)\n",
    "\n",
    "# Pour la démonstration, on simule la mémoire et la recherche\n",
    "budget_2024 = \"Budget 2024 = 100k€\"\n",
    "budget_2023 = \"Budget 2023 = 70k€\"\n",
    "\n",
    "print(\"Note: API de mémoire simplifiée pour cette version.\")\n",
    "print(f\"Informations stockées :\")\n",
    "print(f\"  - {budget_2024}\")\n",
    "print(f\"  - {budget_2023}\")\n",
    "\n",
    "# Simulation de recherche\n",
    "query = \"Quel est mon budget pour 2024 ?\"\n",
    "print(f\"\\nRequête : {query}\")\n",
    "\n",
    "# Simulation simple : retour de la réponse connue\n",
    "print(\"Réponse potentielle : \", budget_2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Intégration\n",
    "\n",
    "Semantic Kernel peut se connecter à Hugging Face localement ou via API.  \n",
    "Exemple :  \n",
    "```python\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "\n",
    "hf_service = HuggingFaceTextCompletion(\n",
    "    service_id=\"textHF\", ai_model_id=\"distilgpt2\", task=\"text-generation\"\n",
    ")\n",
    "kernel.add_service(hf_service)\n",
    "```\n",
    "\n",
    "Ensuite, on peut enregistrer une fonction sémantique ou invoquer directement `hf_service.get_text_contents(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundedness Checking\n",
    "\n",
    "Pour éviter les “hallucinations” d’un résumé :  \n",
    "1. Extrait la liste d'entités du résumé.  \n",
    "2. Vérifie la correspondance de chaque entité avec le texte source (référence).  \n",
    "3. Retire ou corrige les entités non-fondées.\n",
    "\n",
    "Cela se fait via un plugin “GroundingPlugin” (ex. ExtraitEntities, ReferenceCheckEntities, ExciseEntities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Groundedness Checking\n",
    "# ============================\n",
    "\n",
    "# Suppose qu'on a un \"grounding_text\" = un texte source\n",
    "grounding_text = \"\"\"\n",
    "Votre budget 2024 est de 100k euros.\n",
    "Vous vivez à Genève.\n",
    "Vous avez investi 50k en actions.\n",
    "\"\"\"\n",
    "\n",
    "# Suppose qu'on a un résumé \"faux\"\n",
    "summary_text = \"\"\"\n",
    "Mon budget 2024 est de 200k euros.\n",
    "J'habite à Milan.\n",
    "\"\"\"\n",
    "\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "\n",
    "# On appelle un plugin (hypothétique) \"GroundingPlugin\" comportant 3 fonctions\n",
    "try:\n",
    "    grounding_plugin = kernel.add_plugin(parent_directory=plugins_directory, plugin_name=\"GroundingPlugin\")\n",
    "    extract_entities = grounding_plugin[\"ExtractEntities\"]\n",
    "    check_entities = grounding_plugin[\"ReferenceCheckEntities\"]\n",
    "    excise_entities = grounding_plugin[\"ExciseEntities\"]\n",
    "    \n",
    "    # 1) Extraire entités avec les bonnes variables\n",
    "    ext_result = await kernel.invoke(\n",
    "        extract_entities, \n",
    "        KernelArguments(\n",
    "            input=summary_text,\n",
    "            topic=\"entities\",  # Variable attendue par le template\n",
    "            example_entities=\"Person, Location, Organization\",  # Variable attendue\n",
    "            allow_dangerously_set_content=True\n",
    "        )\n",
    "    )\n",
    "    print(\"Entités détectées:\", ext_result)\n",
    "    \n",
    "    # 2) Vérifier correspondance - Important: convertir ext_result en string\n",
    "    ext_result_str = str(ext_result.value) if hasattr(ext_result, 'value') else str(ext_result)\n",
    "    check_result = await kernel.invoke(\n",
    "        check_entities, \n",
    "        KernelArguments(\n",
    "            input=ext_result_str,  # Utiliser la version string\n",
    "            reference_context=grounding_text,\n",
    "            topic=\"entities\",  # Ajouter les variables attendues\n",
    "            allow_dangerously_set_content=True  # IMPORTANT: autoriser le contenu complexe\n",
    "        )\n",
    "    )\n",
    "    print(\"Entités non-fondées:\", check_result)\n",
    "    \n",
    "    # 3) Retirer entités non-fondées du summary\n",
    "    check_result_str = str(check_result.value) if hasattr(check_result, 'value') else str(check_result)\n",
    "    excision = await kernel.invoke(\n",
    "        excise_entities, \n",
    "        KernelArguments(\n",
    "            input=summary_text,\n",
    "            ungrounded_entities=check_result_str,  # Utiliser la version string\n",
    "            allow_dangerously_set_content=True\n",
    "        )\n",
    "    )\n",
    "    print(\"Summary corrigé:\", excision)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: Le plugin GroundingPlugin n'est pas disponible ou fonctionnel: {e}\")\n",
    "    print(\"Démonstration alternative:\")\n",
    "    print(f\"Texte source: {grounding_text[:50]}...\")\n",
    "    print(f\"Résumé analysé: {summary_text[:50]}...\")\n",
    "    print(\"Entités potentiellement problématiques: Milan (au lieu de Genève), 200k€ (au lieu de 100k€)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Result\n",
    "\n",
    "OpenAI (ou Azure) peut renvoyer plusieurs complétions pour un même prompt.  \n",
    "On paramètre `number_of_responses=3` dans les settings.  \n",
    "Ensuite, `get_text_contents(...)` ou `get_chat_message_contents(...)` renvoie un *tableau* de résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Multi-Result\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextPromptExecutionSettings\n",
    "\n",
    "settings = OpenAITextPromptExecutionSettings(\n",
    "    extension_data={\n",
    "        \"max_tokens\": 60,\n",
    "        \"temperature\": 0.7,\n",
    "        \"number_of_responses\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextCompletion\n",
    "text_service = OpenAITextCompletion(service_id=\"multiResult\", ai_model_id=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "prompt = \"Donne-moi une brève blague sur les chats :\"\n",
    "\n",
    "# get_text_contents() => liste de réponses\n",
    "responses = await text_service.get_text_contents(prompt, settings=settings)\n",
    "\n",
    "for i, r in enumerate(responses):\n",
    "    print(f\"Réponse n°{i+1}:\\n{r}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Nous avons vu :\n",
    "1. Un **Chat** basique avec `KernelArguments`.\n",
    "2. Les **Planners** (Sequential, Stepwise) pour orchestrer dynamiquement des steps.\n",
    "3. La **Mémoire** & embeddings pour stocker/rechercher des informations sémantiques.\n",
    "4. L’**intégration Hugging Face** pour exécuter localement des modèles open-source.\n",
    "5. Un **Groundedness Checking** minimal pour éviter les hallucinations.\n",
    "6. La gestion de **plusieurs réponses** (Multi-Result) avec un seul appel.\n",
    "\n",
    "Ces fonctionnalités permettent de créer des scénarios complexes : chat évolué, question-answering avec mémoire persistante, planification automatique, usage local ou cloud, etc. Bonne exploration !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
