{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SK-2-Functions : Function Calling, Memory et Fonctionnalites Avancees\n\n**Navigation** : [Index](README.md) | [<< 01-Intro](01-SemanticKernel-Intro.ipynb) | [03-Agents >>](03-SemanticKernel-Agents.ipynb)\n\n---\n\n## Objectifs d'apprentissage\n\nA la fin de ce notebook, vous saurez :\n1. Utiliser **Function Calling** avec `FunctionChoiceBehavior` pour orchestrer automatiquement des fonctions\n2. Configurer la **memoire vectorielle** avec l'API moderne (InMemoryStore, embeddings)\n3. Integrer des modeles **Hugging Face** comme alternative a OpenAI\n4. Implementer un **Groundedness Checking** pour reduire les hallucinations\n5. Generer **plusieurs reponses** en un seul appel API\n\n### Prerequis\n\n- Python 3.10+\n- Notebook [01-SemanticKernel-Intro](01-SemanticKernel-Intro.ipynb) complete\n- Cle API OpenAI configuree dans `.env`\n\n### Duree estimee : 50 minutes\n\n---\n\n## Sommaire\n\n| Section | Contenu | Concepts cles |\n|---------|---------|---------------|\n| 1 | Installation et configuration | Kernel, services LLM |\n| 2 | Chat avec KernelArguments | Templates, historique |\n| 3 | **Function Calling moderne** | `FunctionChoiceBehavior.Auto()` |\n| 4 | Memoire et Embeddings | InMemoryStore, Vector Search |\n| 5 | Hugging Face Integration | Modeles open-source |\n| 6 | Groundedness Checking | Anti-hallucination |\n| 7 | Multi-Result | Generations multiples |\n\n> **Note importante** : Les anciens `SequentialPlanner` et `FunctionCallingStepwisePlanner` sont **deprecies** depuis SK 1.30. Ce notebook utilise l'approche moderne avec `FunctionChoiceBehavior`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Installation & Imports\n",
    "# ============================\n",
    "\n",
    "# N'installez qu'une seule fois si nécessaire\n",
    "%pip install -U semantic-kernel\n",
    "\n",
    "# Imports de base\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "print(\"Imports et installation OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Installation et configuration initiale\n\nCette premiere cellule effectue deux operations essentielles :\n1. **Installation de semantic-kernel** : Le SDK Python pour orchestrer des modeles de langage\n2. **Imports de base** : Les modules fondamentaux pour interagir avec le kernel et gerer l'environnement\n\nLe fichier `.env` doit contenir vos cles API (voir la section suivante pour le format attendu).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Bloc Markdown – Configuration du Kernel & .env\n",
    "\n",
    "```markdown\n",
    "### Configuration du Kernel\n",
    "\n",
    "Pour exécuter les exemples, on suppose que vous avez un fichier `.env` comportant vos clés d'API OpenAI / Azure OpenAI / Hugging Face.  \n",
    "Exemple `.env` :\n",
    "\n",
    "```\n",
    "GLOBAL_LLM_SERVICE=\"OpenAI\"        # ou AzureOpenAI, HuggingFace\n",
    "OPENAI_API_KEY=\"sk-...\"\n",
    "OPENAI_CHAT_MODEL_ID=\"gpt-3.5-turbo\"\n",
    "...\n",
    "```\n",
    "\n",
    "Le `Kernel` lira ces informations pour décider quel connecteur LLM utiliser.  \n",
    "Ensuite, nous ajouterons nos services (`OpenAIChatCompletion`, `AzureChatCompletion`, etc.) selon la variable `GLOBAL_LLM_SERVICE`."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Initialisation du Kernel avec service LLM dynamique\n\nCette cellule configure le Kernel en fonction du service LLM specifie dans le fichier `.env`. Le code supporte trois backends :\n- **OpenAI** : API officielle d'OpenAI (GPT-4, GPT-3.5, etc.)\n- **Azure OpenAI** : Deploiement Azure avec endpoint personnalise\n- **Hugging Face** : Modeles open-source en local ou via le Hub\n\nLe pattern utilise ici est courant : on lit la variable `GLOBAL_LLM_SERVICE` pour determiner dynamiquement quel connecteur instancier.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Initialisation du Kernel\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    AzureChatCompletion\n",
    ")\n",
    "\n",
    "# On charge le .env\n",
    "load_dotenv()\n",
    "global_llm_service = os.getenv(\"GLOBAL_LLM_SERVICE\", \"AzureOpenAI\")\n",
    "\n",
    "# Initialisation du Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "service_id = \"default\"\n",
    "\n",
    "if global_llm_service.lower() == \"openai\":\n",
    "    # Ajout du service OpenAI\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service OpenAI configuré.\")\n",
    "elif global_llm_service.lower() == \"huggingface\":\n",
    "    # Ajout du service HuggingFace\n",
    "    from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "    kernel.add_service(\n",
    "        HuggingFaceTextCompletion(\n",
    "            service_id=service_id,\n",
    "            ai_model_id=\"distilgpt2\", # ex. pour text-generation\n",
    "            task=\"text-generation\"\n",
    "        ),\n",
    "    )\n",
    "    print(\"Service Hugging Face configuré.\")\n",
    "else:\n",
    "    # Par défaut : Azure OpenAI\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service Azure OpenAI configuré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix du service LLM\n",
    "\n",
    "Le code ci-dessus illustre un pattern important : **la configuration dynamique du service LLM**.\n",
    "\n",
    "Selon la valeur de `GLOBAL_LLM_SERVICE` dans votre fichier `.env`, le Kernel se connectera a :\n",
    "- **OpenAI** : API officielle avec `OpenAIChatCompletion`\n",
    "- **Azure OpenAI** : Deploiement Azure avec `AzureChatCompletion`\n",
    "- **Hugging Face** : Modeles open-source avec `HuggingFaceTextCompletion`\n",
    "\n",
    "Cette flexibilite permet de switcher facilement entre fournisseurs sans modifier le code applicatif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Basique avec KernelArguments\n",
    "\n",
    "L'idée : on crée une fonction de chat qui prend :\n",
    "- l’historique de conversation (objet `ChatHistory`)\n",
    "- un `user_input`\n",
    "et on stocke le tout dans un `KernelArguments`.  \n",
    "\n",
    "Cela permet d'alimenter un prompt (via un template) qui contient la variable `history` et `user_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Extrait d'un Chat Minimal\n",
    "# ============================\n",
    "\n",
    "# Exemple de prompt\n",
    "chat_prompt = \"\"\"\n",
    "{{$history}}\n",
    "User: {{$user_input}}\n",
    "ChatBot:\n",
    "\"\"\"\n",
    "\n",
    "# Création d'une fonction sémantique \"chat\"\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "\n",
    "pt_config = PromptTemplateConfig(\n",
    "    template=chat_prompt,\n",
    "    name=\"chatFunction\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"user_input\", description=\"User's message\"),\n",
    "        InputVariable(name=\"history\", description=\"Conversation history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    function_name=\"chat\",\n",
    "    plugin_name=\"myChatPlugin\",\n",
    "    prompt_template_config=pt_config\n",
    ")\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_system_message(\"Vous êtes un chatbot utile spécialisé en recommandations de livres.\")\n",
    "\n",
    "async def chat_kernel(input_text: str):\n",
    "    print(f\"[Utilisateur] : {input_text}\")\n",
    "    response = await kernel.invoke(\n",
    "        chat_function,\n",
    "        KernelArguments(user_input=input_text, history=str(chat_history))\n",
    "    )\n",
    "    print(f\"[ChatBot] : {response}\")\n",
    "    # Mise à jour de l'historique\n",
    "    chat_history.add_user_message(input_text)\n",
    "    chat_history.add_assistant_message(str(response))\n",
    "\n",
    "\n",
    "# Test\n",
    "await chat_kernel(\"Salut, peux-tu me conseiller un livre sur la philosophie antique ?\")\n",
    "await chat_kernel(\"Merci, tu peux détailler un peu plus la période concernée ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation des resultats du chat\n",
    "\n",
    "Le chat minimal ci-dessus illustre plusieurs concepts cles :\n",
    "\n",
    "1. **Template de prompt** : La variable `{{}}` est remplacee par l'historique complet\n",
    "2. **KernelArguments** : Permet de passer des variables dynamiques au prompt\n",
    "3. **Historique persistant** : Chaque echange est ajoute a `chat_history` pour maintenir le contexte\n",
    "\n",
    "Cette approche est la base de tout chatbot conversationnel avec memoire."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Function Calling Moderne\n\n### Evolution des Planners vers Function Calling\n\nLes **Planners** (SequentialPlanner, FunctionCallingStepwisePlanner) ont ete **deprecies** dans Semantic Kernel 1.30+. Microsoft recommande maintenant d'utiliser :\n\n| Ancienne approche | Nouvelle approche | Avantages |\n|-------------------|-------------------|-----------|\n| `SequentialPlanner` | `FunctionChoiceBehavior.Auto()` | Moins de tokens, plus fiable |\n| `FunctionCallingStepwisePlanner` | Agent avec plugins | Pattern ReAct natif |\n| Planification XML | Function calling OpenAI | Standard API |\n\n### Qu'est-ce que Function Calling ?\n\n**Function Calling** permet au LLM de :\n1. Analyser la requete utilisateur\n2. Decider automatiquement quelle(s) fonction(s) appeler\n3. Extraire les parametres depuis le contexte\n4. Retourner le resultat au LLM pour formulation finale\n\nC'est le pattern **ReAct** (Reasoning + Acting) integre directement dans l'API OpenAI.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Configuration de FunctionChoiceBehavior\n\nSemantic Kernel propose plusieurs modes de Function Calling :\n\n```python\nfrom semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n\n# Mode Auto : le LLM decide quand appeler les fonctions\nsettings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n\n# Mode Required : force l'appel d'au moins une fonction\nsettings.function_choice_behavior = FunctionChoiceBehavior.Required()\n\n# Mode None : desactive le function calling\nsettings.function_choice_behavior = FunctionChoiceBehavior.NoneInvoke()\n```\n\n### Parametres avances\n\n| Parametre | Description | Valeur par defaut |\n|-----------|-------------|-------------------|\n| `auto_invoke` | Executer automatiquement les fonctions | `True` |\n| `filters` | Filtrer les fonctions disponibles | `None` |\n| `maximum_auto_invoke_attempts` | Limite d'appels | `5` |\n\nL'exemple suivant montre comment configurer un kernel avec plusieurs plugins et laisser le LLM orchestrer automatiquement."
  },
  {
   "cell_type": "markdown",
   "source": "### API de memoire vectorielle mise a jour\n\n**Note de version** : L'API de memoire a evolue dans Semantic Kernel. Les anciennes classes comme `VolatileMemoryStore` et `SemanticTextMemory` sont remplacees par :\n- `InMemoryStore` : Stockage en memoire volatile\n- Services d'embedding dedies avec `OpenAITextEmbedding`\n\nCette cellule montre le pattern moderne pour configurer la memoire semantique.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================\n# Function Calling Moderne avec FunctionChoiceBehavior\n# ============================\n\nfrom semantic_kernel.core_plugins.text_plugin import TextPlugin\nfrom semantic_kernel.core_plugins.math_plugin import MathPlugin\nfrom semantic_kernel.functions import KernelFunctionFromPrompt, kernel_function\nfrom semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\nfrom semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n\n# Charger des plugins avec des fonctions utiles\nplugins_directory = \"./prompt_template_samples/\"\nwriter_plugin = kernel.add_plugin(plugin_name=\"WriterPlugin\", parent_directory=plugins_directory)\ntext_plugin = kernel.add_plugin(plugin=TextPlugin(), plugin_name=\"TextPlugin\")\nmath_plugin = kernel.add_plugin(plugin=MathPlugin(), plugin_name=\"MathPlugin\")\n\n# Creer une fonction semantique pour la poesie\nshakespeare_func = KernelFunctionFromPrompt(\n    function_name=\"Shakespeare\",\n    plugin_name=\"WriterPlugin\",\n    prompt=\"\"\"\n{{$input}}\n\nRewrite the above in the style of Shakespeare.\n\"\"\",\n    prompt_execution_settings=OpenAIChatPromptExecutionSettings(\n        service_id=service_id,\n        max_tokens=2000,\n        temperature=0.8,\n    ),\n    description=\"Rewrite the input in the style of Shakespeare.\",\n)\nkernel.add_function(plugin_name=\"WriterPlugin\", function=shakespeare_func)\n\n# Lister les fonctions disponibles\nprint(\"Fonctions disponibles pour le LLM :\")\nprint(\"-\" * 50)\nfor plugin_name, plugin in kernel.plugins.items():\n    for function_name, function in plugin.functions.items():\n        desc = function.description[:50] + \"...\" if function.description and len(function.description) > 50 else function.description\n        print(f\"  {plugin_name}.{function_name}: {desc}\")\n\n# ============================\n# Exemple : Function Calling Auto\n# ============================\nprint(\"\\n\" + \"=\" * 50)\nprint(\"DEMO : Function Calling avec FunctionChoiceBehavior.Auto()\")\nprint(\"=\" * 50)\n\n# Configurer les settings avec function calling auto\nexecution_settings = OpenAIChatPromptExecutionSettings(\n    service_id=service_id,\n    max_tokens=2000,\n    temperature=0.7,\n    function_choice_behavior=FunctionChoiceBehavior.Auto(\n        auto_invoke=True,  # Execute automatiquement les fonctions\n        filters={\"included_plugins\": [\"TextPlugin\", \"MathPlugin\"]}  # Limiter aux plugins\n    )\n)\n\n# Prompt qui necessite l'appel de fonctions\nuser_request = \"\"\"\nJ'ai besoin d'aide pour deux choses :\n1. Convertir le texte \"hello world\" en majuscules\n2. Calculer 25 + 17\n\"\"\"\n\nprint(f\"\\nRequete utilisateur : {user_request}\")\n\n# Creer une fonction de chat avec function calling\nfrom semantic_kernel.contents import ChatHistory\n\nchat_history = ChatHistory()\nchat_history.add_system_message(\"Tu es un assistant qui utilise les outils disponibles pour aider l'utilisateur.\")\nchat_history.add_user_message(user_request)\n\n# Invoquer avec function calling\nchat_service = kernel.get_service(service_id)\nresult = await chat_service.get_chat_message_content(\n    chat_history=chat_history,\n    settings=execution_settings,\n    kernel=kernel\n)\n\nprint(f\"\\nReponse du LLM (avec function calling) :\")\nprint(result.content)"
  },
  {
   "cell_type": "markdown",
   "source": "### Interpretation : Function Calling Auto\n\n**Sortie attendue** : Le LLM a automatiquement identifie et execute deux fonctions :\n- `TextPlugin.uppercase(\"hello world\")` → \"HELLO WORLD\"\n- `MathPlugin.add(25, 17)` → 42\n\n| Etape | Action | Description |\n|-------|--------|-------------|\n| 1 | Analyse | Le LLM parse la requete et identifie 2 taches |\n| 2 | Selection | Il choisit les fonctions appropriees dans les plugins |\n| 3 | Extraction | Il extrait les parametres du contexte |\n| 4 | Execution | SK execute les fonctions automatiquement |\n| 5 | Synthese | Le LLM formule la reponse finale |\n\n**Points cles** :\n- `auto_invoke=True` : Les fonctions sont executees automatiquement (pas besoin de code manuel)\n- `filters` : Limite les fonctions accessibles (securite, performance)\n- Le LLM peut enchainer plusieurs appels si necessaire\n\n> **Comparaison avec Agents** : Le notebook [03-Agents](03-SemanticKernel-Agents.ipynb) montre comment combiner Function Calling avec des agents pour des scenarios plus complexes (multi-agents, workflows).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mémoire & Embeddings\n",
    "\n",
    "**SemanticTextMemory** permet de stocker des textes (avec un embedding) dans un store :\n",
    "- `VolatileMemoryStore` (en mémoire)\n",
    "- ou connecteurs vers Pinecone, Azure Cognitive Search, Qdrant, etc.\n",
    "\n",
    "On peut ensuite effectuer des requêtes sémantiques :  \n",
    "`await memory.search(\"MaCollection\", \"Quelle est mon budget pour 2024 ?\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Verification du GroundingPlugin\n\nLe plugin de Grounding (ancrage) permet de :\n1. **Extraire les entites** d'un texte resume\n2. **Verifier** chaque entite par rapport a un texte source de reference\n3. **Corriger** le resume en supprimant les informations non fondees\n\nCe mecanisme est essentiel pour reduire les hallucinations des LLMs et garantir que les reponses sont ancrees dans des faits verifiables.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Extrait Mémoire\n",
    "# ============================\n",
    "\n",
    "# CORRECTION: Utilisation des nouvelles approches pour la mémoire vectorielle\n",
    "from semantic_kernel.connectors.in_memory import InMemoryStore\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n",
    "\n",
    "# Embedding service (ex : openai text-embedding-ada)\n",
    "embedding_service = OpenAITextEmbedding(\n",
    "    service_id=\"embeddingService\",\n",
    "    ai_model_id=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# Nouvelle approche avec InMemoryStore\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Ajout du service d'embedding au kernel\n",
    "kernel.add_service(embedding_service)\n",
    "\n",
    "# Pour la démonstration, on simule la mémoire et la recherche\n",
    "budget_2024 = \"Budget 2024 = 100k€\"\n",
    "budget_2023 = \"Budget 2023 = 70k€\"\n",
    "\n",
    "print(\"Note: API de mémoire simplifiée pour cette version.\")\n",
    "print(f\"Informations stockées :\")\n",
    "print(f\"  - {budget_2024}\")\n",
    "print(f\"  - {budget_2023}\")\n",
    "\n",
    "# Simulation de recherche\n",
    "query = \"Quel est mon budget pour 2024 ?\"\n",
    "print(f\"\\nRequête : {query}\")\n",
    "\n",
    "# Simulation simple : retour de la réponse connue\n",
    "print(\"Réponse potentielle : \", budget_2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution de l'API de memoire\n",
    "\n",
    "L'API de memoire vectorielle de Semantic Kernel a evolue significativement :\n",
    "\n",
    "**Ancienne approche (deprecie)** :\n",
    "- `VolatileMemoryStore` + `SemanticTextMemory`\n",
    "- Methodes `.save_information()` et `.search()`\n",
    "\n",
    "**Nouvelle approche (2024+)** :\n",
    "- `InMemoryStore` pour le stockage volatile\n",
    "- Services d'embedding dedies (`OpenAITextEmbedding`)\n",
    "- Integration directe avec les connecteurs vectoriels (Pinecone, Qdrant, Azure AI Search)\n",
    "\n",
    "Cette evolution permet une meilleure separation des responsabilites et une integration plus flexible avec divers backends."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Configuration des reponses multiples\n\nLa fonctionnalite Multi-Result permet d'obtenir plusieurs completions pour un meme prompt en un seul appel API. Cela est utile pour :\n- **Generer des alternatives** : Obtenir plusieurs variations d'une reponse\n- **Evaluer la diversite** : Comparer differentes formulations\n- **Selection humaine** : Presenter plusieurs options a l'utilisateur\n\nLe parametre `number_of_responses` dans les settings controle le nombre de reponses retournees.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Intégration\n",
    "\n",
    "Semantic Kernel peut se connecter à Hugging Face localement ou via API.  \n",
    "Exemple :  \n",
    "```python\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "\n",
    "hf_service = HuggingFaceTextCompletion(\n",
    "    service_id=\"textHF\", ai_model_id=\"distilgpt2\", task=\"text-generation\"\n",
    ")\n",
    "kernel.add_service(hf_service)\n",
    "```\n",
    "\n",
    "Ensuite, on peut enregistrer une fonction sémantique ou invoquer directement `hf_service.get_text_contents(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundedness Checking\n",
    "\n",
    "Pour éviter les “hallucinations” d’un résumé :  \n",
    "1. Extrait la liste d'entités du résumé.  \n",
    "2. Vérifie la correspondance de chaque entité avec le texte source (référence).  \n",
    "3. Retire ou corrige les entités non-fondées.\n",
    "\n",
    "Cela se fait via un plugin “GroundingPlugin” (ex. ExtraitEntities, ReferenceCheckEntities, ExciseEntities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Groundedness Checking\n",
    "# ============================\n",
    "\n",
    "# Suppose qu'on a un \"grounding_text\" = un texte source\n",
    "grounding_text = \"\"\"\n",
    "Votre budget 2024 est de 100k euros.\n",
    "Vous vivez à Genève.\n",
    "Vous avez investi 50k en actions.\n",
    "\"\"\"\n",
    "\n",
    "# Suppose qu'on a un résumé \"faux\"\n",
    "summary_text = \"\"\"\n",
    "Mon budget 2024 est de 200k euros.\n",
    "J'habite à Milan.\n",
    "\"\"\"\n",
    "\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "\n",
    "# On appelle un plugin (hypothétique) \"GroundingPlugin\" comportant 3 fonctions\n",
    "try:\n",
    "    grounding_plugin = kernel.add_plugin(parent_directory=plugins_directory, plugin_name=\"GroundingPlugin\")\n",
    "    extract_entities = grounding_plugin[\"ExtractEntities\"]\n",
    "    check_entities = grounding_plugin[\"ReferenceCheckEntities\"]\n",
    "    excise_entities = grounding_plugin[\"ExciseEntities\"]\n",
    "    \n",
    "    # 1) Extraire entités avec les bonnes variables\n",
    "    ext_result = await kernel.invoke(\n",
    "        extract_entities, \n",
    "        KernelArguments(\n",
    "            input=summary_text,\n",
    "            topic=\"entities\",  # Variable attendue par le template\n",
    "            example_entities=\"Person, Location, Organization\",  # Variable attendue\n",
    "            allow_dangerously_set_content=True\n",
    "        )\n",
    "    )\n",
    "    print(\"Entités détectées:\", ext_result)\n",
    "    \n",
    "    # 2) Vérifier correspondance - Important: convertir ext_result en string\n",
    "    ext_result_str = str(ext_result.value) if hasattr(ext_result, 'value') else str(ext_result)\n",
    "    check_result = await kernel.invoke(\n",
    "        check_entities, \n",
    "        KernelArguments(\n",
    "            input=ext_result_str,  # Utiliser la version string\n",
    "            reference_context=grounding_text,\n",
    "            topic=\"entities\",  # Ajouter les variables attendues\n",
    "            allow_dangerously_set_content=True  # IMPORTANT: autoriser le contenu complexe\n",
    "        )\n",
    "    )\n",
    "    print(\"Entités non-fondées:\", check_result)\n",
    "    \n",
    "    # 3) Retirer entités non-fondées du summary\n",
    "    check_result_str = str(check_result.value) if hasattr(check_result, 'value') else str(check_result)\n",
    "    excision = await kernel.invoke(\n",
    "        excise_entities, \n",
    "        KernelArguments(\n",
    "            input=summary_text,\n",
    "            ungrounded_entities=check_result_str,  # Utiliser la version string\n",
    "            allow_dangerously_set_content=True\n",
    "        )\n",
    "    )\n",
    "    print(\"Summary corrigé:\", excision)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: Le plugin GroundingPlugin n'est pas disponible ou fonctionnel: {e}\")\n",
    "    print(\"Démonstration alternative:\")\n",
    "    print(f\"Texte source: {grounding_text[:50]}...\")\n",
    "    print(f\"Résumé analysé: {summary_text[:50]}...\")\n",
    "    print(\"Entités potentiellement problématiques: Milan (au lieu de Genève), 200k€ (au lieu de 100k€)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats du Groundedness Checking\n",
    "\n",
    "Le processus de verification d'ancrage comprend trois etapes :\n",
    "\n",
    "1. **Extraction** : Identification des entites factuelles dans le resume (personnes, lieux, montants)\n",
    "2. **Verification** : Comparaison de chaque entite avec le texte source de reference\n",
    "3. **Correction** : Suppression ou modification des entites non-fondees\n",
    "\n",
    "Dans l'exemple ci-dessus, les entites problematiques detectees seraient :\n",
    "- \"Milan\" (non mentionne dans le texte source, qui indique \"Geneve\")\n",
    "- \"200k euros\" (le texte source indique \"100k euros\")\n",
    "\n",
    "Cette technique est essentielle pour reduire les hallucinations des LLMs dans les systemes de production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Result\n",
    "\n",
    "OpenAI (ou Azure) peut renvoyer plusieurs complétions pour un même prompt.  \n",
    "On paramètre `number_of_responses=3` dans les settings.  \n",
    "Ensuite, `get_text_contents(...)` ou `get_chat_message_contents(...)` renvoie un *tableau* de résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule : Multi-Result\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextPromptExecutionSettings\n",
    "\n",
    "settings = OpenAITextPromptExecutionSettings(\n",
    "    extension_data={\n",
    "        \"max_tokens\": 60,\n",
    "        \"temperature\": 0.7,\n",
    "        \"number_of_responses\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextCompletion\n",
    "text_service = OpenAITextCompletion(service_id=\"multiResult\", ai_model_id=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "prompt = \"Donne-moi une brève blague sur les chats :\"\n",
    "\n",
    "# get_text_contents() => liste de réponses\n",
    "responses = await text_service.get_text_contents(prompt, settings=settings)\n",
    "\n",
    "for i, r in enumerate(responses):\n",
    "    print(f\"Réponse n°{i+1}:\\n{r}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusion et Resume\n\nDans ce notebook, nous avons explore les fonctionnalites avancees de Semantic Kernel :\n\n| Concept | Description | Statut |\n|---------|-------------|--------|\n| **Chat avec KernelArguments** | Gestion de l'historique et du contexte | Fondamental |\n| **Function Calling** | Orchestration automatique via `FunctionChoiceBehavior.Auto()` | **Moderne** (remplace Planners) |\n| **Memoire & Embeddings** | Stockage vectoriel avec `InMemoryStore` | **API moderne** |\n| **Hugging Face** | Integration de modeles open-source | Optionnel |\n| **Groundedness Checking** | Reduction des hallucinations | Production |\n| **Multi-Result** | Generations multiples en un appel | Creatif |\n\n### Points cles a retenir\n\n1. **Function Calling remplace les Planners** : Plus simple, plus fiable, moins de tokens\n2. **API Memory modernisee** : `InMemoryStore` + `OpenAITextEmbedding` (voir notebook 05 pour Qdrant)\n3. **Groundedness** : Essentiel pour les systemes de production\n\n### Prochaines etapes\n\n- **[03-Agents](03-SemanticKernel-Agents.ipynb)** : Agents avec ChatCompletionAgent, AgentGroupChat\n- **[04-Filters](04-SemanticKernel-Filters-Observability.ipynb)** : Intercepter et logger les appels\n- **[05-VectorStores](05-SemanticKernel-VectorStores.ipynb)** : RAG complet avec Qdrant\n\n---\n\n**Navigation** : [<< 01-Intro](01-SemanticKernel-Intro.ipynb) | [Index](README.md) | [03-Agents >>](03-SemanticKernel-Agents.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Nous avons vu :\n",
    "1. Un **Chat** basique avec `KernelArguments`.\n",
    "2. Les **Planners** (Sequential, Stepwise) pour orchestrer dynamiquement des steps.\n",
    "3. La **Mémoire** & embeddings pour stocker/rechercher des informations sémantiques.\n",
    "4. L’**intégration Hugging Face** pour exécuter localement des modèles open-source.\n",
    "5. Un **Groundedness Checking** minimal pour éviter les hallucinations.\n",
    "6. La gestion de **plusieurs réponses** (Multi-Result) avec un seul appel.\n",
    "\n",
    "Ces fonctionnalités permettent de créer des scénarios complexes : chat évolué, question-answering avec mémoire persistante, planification automatique, usage local ou cloud, etc. Bonne exploration !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}