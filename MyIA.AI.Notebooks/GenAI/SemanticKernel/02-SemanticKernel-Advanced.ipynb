{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK-2-Functions : Function Calling, Memory et Fonctionnalites Avancees\n",
    "\n",
    "**Navigation** : [Index](README.md) | [<< 01-Intro](01-SemanticKernel-Intro.ipynb) | [03-Agents >>](03-SemanticKernel-Agents.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Utiliser **Function Calling** avec `FunctionChoiceBehavior` pour orchestrer automatiquement des fonctions\n",
    "2. Configurer la **memoire vectorielle** avec l'API moderne (InMemoryStore, embeddings)\n",
    "3. Integrer des modeles **Hugging Face** comme alternative a OpenAI\n",
    "4. Implementer un **Groundedness Checking** pour reduire les hallucinations\n",
    "5. Generer **plusieurs reponses** en un seul appel API\n",
    "\n",
    "### Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Notebook [01-SemanticKernel-Intro](01-SemanticKernel-Intro.ipynb) complete\n",
    "- Cle API OpenAI configuree dans `.env`\n",
    "\n",
    "### Duree estimee : 50 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Sommaire\n",
    "\n",
    "| Section | Contenu | Concepts cles |\n",
    "|---------|---------|---------------|\n",
    "| 1 | Installation et configuration | Kernel, services LLM |\n",
    "| 2 | Chat avec KernelArguments | Templates, historique |\n",
    "| 3 | **Function Calling moderne** | `FunctionChoiceBehavior.Auto()` |\n",
    "| 4 | Memoire et Embeddings | InMemoryStore, Vector Search |\n",
    "| 5 | Hugging Face Integration | Modeles open-source |\n",
    "| 6 | Groundedness Checking | Anti-hallucination |\n",
    "| 7 | Multi-Result | Generations multiples |\n",
    "\n",
    "> **Note importante** : Les anciens `SequentialPlanner` et `FunctionCallingStepwisePlanner` sont **deprecies** depuis SK 1.30. Ce notebook utilise l'approche moderne avec `FunctionChoiceBehavior`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: semantic-kernel in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.39.3)\n",
      "Requirement already satisfied: azure-ai-projects~=1.0.0b12 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.0.0)\n",
      "Requirement already satisfied: azure-ai-agents>=1.2.0b3 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.2.0b3)\n",
      "Requirement already satisfied: aiohttp~=3.8 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (3.12.15)\n",
      "Requirement already satisfied: cloudevents~=1.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.12.0)\n",
      "Requirement already satisfied: pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (2.11.10)\n",
      "Requirement already satisfied: pydantic-settings~=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from semantic-kernel) (2.11.0)\n",
      "Requirement already satisfied: defusedxml~=0.7 in c:\\programdata\\miniconda3\\lib\\site-packages (from semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: azure-identity>=1.13 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.24.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (2.4.2)\n",
      "Requirement already satisfied: openai<2,>=1.98.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.109.1)\n",
      "Requirement already satisfied: openapi_core<0.20,>=0.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (0.19.5)\n",
      "Requirement already satisfied: websockets<16,>=13 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (15.0.1)\n",
      "Requirement already satisfied: aiortc>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.13.0)\n",
      "Requirement already satisfied: opentelemetry-api~=1.24 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.24 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.36.0)\n",
      "Requirement already satisfied: prance<25.4.9,>=23.6.21 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (23.6.21.0)\n",
      "Requirement already satisfied: pybars4~=0.9 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (0.9.13)\n",
      "Requirement already satisfied: jinja2~=3.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from semantic-kernel) (3.1.6)\n",
      "Requirement already satisfied: nest-asyncio~=1.6 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.15.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.17.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (6.31.1)\n",
      "Requirement already satisfied: typing-extensions>=4.13 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.20.1)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-ai-projects~=1.0.0b12->semantic-kernel) (0.7.2)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-ai-projects~=1.0.0b12->semantic-kernel) (1.35.0)\n",
      "Requirement already satisfied: azure-storage-blob>=12.15.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-ai-projects~=1.0.0b12->semantic-kernel) (12.26.0)\n",
      "Requirement already satisfied: deprecation<3.0,>=2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from cloudevents~=1.0->semantic-kernel) (2.1.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\miniconda3\\lib\\site-packages (from deprecation<3.0,>=2.0->cloudevents~=1.0->semantic-kernel) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jinja2~=3.1->semantic-kernel) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.5.0->openai<2,>=1.98.0->semantic-kernel) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai<2,>=1.98.0->semantic-kernel) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai<2,>=1.98.0->semantic-kernel) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2,>=1.98.0->semantic-kernel) (0.16.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (4.25.1)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.3.4)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (10.8.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.6.3)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.7.2)\n",
      "Requirement already satisfied: parse in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (1.20.2)\n",
      "Requirement already satisfied: werkzeug<3.1.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (3.1.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.27.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (6.0.3)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (0.4.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (2.32.5)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.1.4)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel) (1.12.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-api~=1.24->semantic-kernel) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api~=1.24->semantic-kernel) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-sdk~=1.24->semantic-kernel) (0.57b0)\n",
      "Requirement already satisfied: chardet>=3.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in c:\\programdata\\miniconda3\\lib\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (0.18.10)\n",
      "Requirement already satisfied: six~=1.15 in c:\\programdata\\miniconda3\\lib\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (1.17.0)\n",
      "Requirement already satisfied: PyMeta3>=0.5.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pybars4~=0.9->semantic-kernel) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic-settings~=2.0->semantic-kernel) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (2.5.0)\n",
      "Requirement already satisfied: aioice<1.0.0,>=0.10.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (0.10.1)\n",
      "Requirement already satisfied: av<15.0.0,>=14.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (14.4.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=44.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (45.0.3)\n",
      "Requirement already satisfied: google-crc32c>=1.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (1.7.1)\n",
      "Requirement already satisfied: pyee>=13.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (13.0.0)\n",
      "Requirement already satisfied: pylibsrtp>=0.10.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (0.12.0)\n",
      "Requirement already satisfied: pyopenssl>=25.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (25.1.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aioice<1.0.0,>=0.10.1->aiortc>=1.9.0->semantic-kernel) (2.7.0)\n",
      "Requirement already satisfied: ifaddr>=0.2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aioice<1.0.0,>=0.10.1->aiortc>=1.9.0->semantic-kernel) (0.2.0)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-identity>=1.13->semantic-kernel) (1.33.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-identity>=1.13->semantic-kernel) (1.3.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\miniconda3\\lib\\site-packages (from cffi>=1.0.0->aiortc>=1.9.0->semantic-kernel) (2.21)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity>=1.13->semantic-kernel) (2.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai<2,>=1.98.0->semantic-kernel) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Imports et installation OK.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Installation & Imports\n",
    "# ============================\n",
    "\n",
    "# N'installez qu'une seule fois si nécessaire\n",
    "%pip install -U semantic-kernel\n",
    "\n",
    "# Imports de base\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "print(\"Imports et installation OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation et configuration initiale\n",
    "\n",
    "Cette premiere cellule effectue deux operations essentielles :\n",
    "1. **Installation de semantic-kernel** : Le SDK Python pour orchestrer des modeles de langage\n",
    "2. **Imports de base** : Les modules fondamentaux pour interagir avec le kernel et gerer l'environnement\n",
    "\n",
    "Le fichier `.env` doit contenir vos cles API (voir la section suivante pour le format attendu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration du fichier .env\n\nPour exécuter les exemples, vous devez avoir un fichier `.env` comportant vos clés d'API OpenAI / Azure OpenAI / Hugging Face.\n\nExemple de configuration `.env` :\n\n```bash\nGLOBAL_LLM_SERVICE=\"OpenAI\"        # ou AzureOpenAI, HuggingFace\nOPENAI_API_KEY=\"sk-...\"\nOPENAI_CHAT_MODEL_ID=\"gpt-5-mini\"\n```\n\nLe `Kernel` lira ces informations pour décider quel connecteur LLM utiliser."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialisation du Kernel avec service LLM dynamique\n",
    "\n",
    "Cette cellule configure le Kernel en fonction du service LLM specifie dans le fichier `.env`. Le code supporte trois backends :\n",
    "- **OpenAI** : API officielle d'OpenAI (GPT-4, GPT-3.5, etc.)\n",
    "- **Azure OpenAI** : Deploiement Azure avec endpoint personnalise\n",
    "- **Hugging Face** : Modeles open-source en local ou via le Hub\n",
    "\n",
    "Le pattern utilise ici est courant : on lit la variable `GLOBAL_LLM_SERVICE` pour determiner dynamiquement quel connecteur instancier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service OpenAI configuré.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Initialisation du Kernel\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    AzureChatCompletion\n",
    ")\n",
    "\n",
    "# On charge le .env\n",
    "load_dotenv()\n",
    "global_llm_service = os.getenv(\"GLOBAL_LLM_SERVICE\", \"AzureOpenAI\")\n",
    "\n",
    "# Initialisation du Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "service_id = \"default\"\n",
    "\n",
    "if global_llm_service.lower() == \"openai\":\n",
    "    # Ajout du service OpenAI\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service OpenAI configuré.\")\n",
    "elif global_llm_service.lower() == \"huggingface\":\n",
    "    # Ajout du service HuggingFace\n",
    "    from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "    kernel.add_service(\n",
    "        HuggingFaceTextCompletion(\n",
    "            service_id=service_id,\n",
    "            ai_model_id=\"distilgpt2\", # ex. pour text-generation\n",
    "            task=\"text-generation\"\n",
    "        ),\n",
    "    )\n",
    "    print(\"Service Hugging Face configuré.\")\n",
    "else:\n",
    "    # Par défaut : Azure OpenAI\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service Azure OpenAI configuré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix du service LLM\n",
    "\n",
    "Le code ci-dessus illustre un pattern important : **la configuration dynamique du service LLM**.\n",
    "\n",
    "Selon la valeur de `GLOBAL_LLM_SERVICE` dans votre fichier `.env`, le Kernel se connectera a :\n",
    "- **OpenAI** : API officielle avec `OpenAIChatCompletion`\n",
    "- **Azure OpenAI** : Deploiement Azure avec `AzureChatCompletion`\n",
    "- **Hugging Face** : Modeles open-source avec `HuggingFaceTextCompletion`\n",
    "\n",
    "Cette flexibilite permet de switcher facilement entre fournisseurs sans modifier le code applicatif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Basique avec KernelArguments\n",
    "\n",
    "L'idée : on crée une fonction de chat qui prend :\n",
    "- l’historique de conversation (objet `ChatHistory`)\n",
    "- un `user_input`\n",
    "et on stocke le tout dans un `KernelArguments`.  \n",
    "\n",
    "Cela permet d'alimenter un prompt (via un template) qui contient la variable `history` et `user_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Utilisateur] : Salut, peux-tu me conseiller un livre sur la philosophie antique ?\n",
      "[ChatBot] : Salut — oui. Voici quelques suggestions selon ce que tu recherches (texte ancien vs introduction moderne, pratique vs théorique) :\n",
      "\n",
      "Recommandation rapide (si tu veux un seul livre) :\n",
      "- Pierre Hadot, Qu’est‑ce que la philosophie antique ? — Excellente introduction en français qui montre la philosophie antique comme art de vivre plutôt que simple théorie.\n",
      "\n",
      "Textes anciens (traductions) — pour lire la source :\n",
      "- Platon, La République — politique, justice, théorie des Idées. Indispensable si tu veux du philosophique théorique.\n",
      "- Aristote, Éthique à Nicomaque — fondement de l’éthique des vertus, plus dense mais central.\n",
      "- Marc Aurèle, Pensées pour moi‑même (Meditations) — court, très accessible, pratique (stoïcisme).\n",
      "- Sénèque, Lettres à Lucilius — conseils moraux et exercices stoïciens, faciles à lire.\n",
      "- Épictète, Manuel/Entretiens (Enchiridion/Discours) — très concret et pragmatique.\n",
      "\n",
      "Introductions et ouvrages modernes :\n",
      "- Julia Annas, Ancient Philosophy: A Very Short Introduction — bref et clair (en anglais).  \n",
      "- W. K. C. Guthrie, A History of Greek Philosophy — multi‑volume, très complet (pour approfondir).\n",
      "- G. S. Kirk / J. E. Raven, The Presocratic Philosophers — si tu t’intéresses aux prémices de la pensée grecque.\n",
      "\n",
      "Dis-moi :\n",
      "- préfères‑tu lire un texte ancien (traduction) ou une introduction contemporaine ?\n",
      "- veux‑tu un livre en français ou l’anglais ne te gêne pas ?\n",
      "Avec ta réponse, je te propose une édition/édition traduite adaptée et un plan de lecture.\n",
      "[Utilisateur] : Merci, tu peux détailler un peu plus la période concernée ?\n",
      "[ChatBot] : Bien sûr — voici un panorama synthétique de la période qu’on appelle « philosophie antique », avec les sous‑périodes, les thèmes principaux, quelques auteurs‑phares et une ou deux lectures recommandées pour chaque étape.\n",
      "\n",
      "Chronologie approximative\n",
      "- Présocratiques (env. 7e–5e s. av. J.-C.) — pensée naturaliste et cosmogonique.  \n",
      "- Période classique (5e–4e s. av. J.-C.) — Socrate, Platon, Aristote : éthique, politique, métaphysique, théorie de la connaissance.  \n",
      "- Période hellénistique (3e s. av. J.-C. – 1er s. av. J.-C.) — écoles pratiques : stoïcisme, épicurisme, scepticisme ; focalisation sur l’éthique comme art de vivre.  \n",
      "- Antiquité romaine / impériale (1er s. av. J.-C. – 2e s. ap. J.-C.) — appropriation et transmission des écoles grecques (Cicéron, Sénèque, Épictète, Marc Aurèle).  \n",
      "- Antiquité tardive / néoplatonisme (3e–6e s. ap. J.-C.) — systématisation métaphysique (Plotin, Proclus) et transition vers la pensée médiévale.\n",
      "\n",
      "Principaux thèmes par période\n",
      "- Présocratiques : origine et composition du cosmos, principes premiers (archê).  \n",
      "- Classiques : nature de la justice, de la vertu, des formes/Idées (Platon), logique et sciences naturelles (Aristote).  \n",
      "- Hellénistiques : comment vivre — tranquillité de l’âme, techniques morales, maîtrise des passions.  \n",
      "- Romains : pratique morale, pédagogie philosophique, rhétorique et éthique appliquée.  \n",
      "- Néoplatonisme : unité, émanation, âme et salut intellectuel/spirituel.\n",
      "\n",
      "Auteurs‑phares (quelques noms courts)\n",
      "- Présocratiques : Thalès, Héraclite, Parménide, Anaximandre.  \n",
      "- Classique : Socrate (par Platon/Xénophon), Platon, Aristote.  \n",
      "- Hellénistique : Zénon de Citium (stoïcisme), Épicure, Pyrrhon (scepticisme).  \n",
      "- Romains : Cicéron, Sénèque, Épictète, Marc Aurèle.  \n",
      "- Tardif : Plotin.\n",
      "\n",
      "Lectures recommandées (entrée facile)\n",
      "- Introduction générale (français) : Pierre Hadot, Qu’est‑ce que la philosophie antique ? — très bon pour comprendre l’esprit « art de vivre ».  \n",
      "- Texte classique pour commencer : Platon, La République (lecture politique/théorique) ou Aristote, Éthique à Nicomaque (éthique des vertus).  \n",
      "- Texte pratique/accessible : Marc Aurèle, Pensées (Meditations) ou Sénèque, Lettres à Lucilius.  \n",
      "- Si tu veux les présocratiques : recueils de fragments (ouvrages et traductions commentées) ; pour l’histoire générale approfondie : W. K. C. Guthrie, A History of Greek Philosophy (anglais).\n",
      "\n",
      "Conseils de lecture\n",
      "- Si tu veux comprendre la méthode et la visée : commence par Hadot (introduction) puis un texte classique (Platon ou Aristote).  \n",
      "- Si tu veux quelque chose de pratique et court : Marc Aurèle ou Épictète.  \n",
      "- Pour la cosmologie et l’origine de la pensée : lire les présocratiques (fragments commentés).\n",
      "\n",
      "Éditions (en français) — éditeurs sûrs pour les traductions/commentaires : Les Belles Lettres, GF/Flammarion, Vrin, Gallimard (Folio).  \n",
      "\n",
      "Souhaites‑tu que je te propose :\n",
      "- une lecture guidée (plan de lecture sur 4–6 semaines) ?  \n",
      "- une édition française précise d’un ouvrage (par ex. Platon ou Marc Aurèle) ?  \n",
      "- des suggestions selon un thème précis (éthique, politique, cosmologie) ?\n",
      "\n",
      "Dis‑moi ce qui t’attire, et je te propose la suite adaptée.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Extrait d'un Chat Minimal\n",
    "# ============================\n",
    "\n",
    "# Exemple de prompt\n",
    "chat_prompt = \"\"\"\n",
    "{{$history}}\n",
    "User: {{$user_input}}\n",
    "ChatBot:\n",
    "\"\"\"\n",
    "\n",
    "# Création d'une fonction sémantique \"chat\"\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "\n",
    "pt_config = PromptTemplateConfig(\n",
    "    template=chat_prompt,\n",
    "    name=\"chatFunction\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"user_input\", description=\"User's message\"),\n",
    "        InputVariable(name=\"history\", description=\"Conversation history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    function_name=\"chat\",\n",
    "    plugin_name=\"myChatPlugin\",\n",
    "    prompt_template_config=pt_config\n",
    ")\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_system_message(\"Vous êtes un chatbot utile spécialisé en recommandations de livres.\")\n",
    "\n",
    "async def chat_kernel(input_text: str):\n",
    "    print(f\"[Utilisateur] : {input_text}\")\n",
    "    response = await kernel.invoke(\n",
    "        chat_function,\n",
    "        KernelArguments(user_input=input_text, history=str(chat_history))\n",
    "    )\n",
    "    print(f\"[ChatBot] : {response}\")\n",
    "    # Mise à jour de l'historique\n",
    "    chat_history.add_user_message(input_text)\n",
    "    chat_history.add_assistant_message(str(response))\n",
    "\n",
    "\n",
    "# Test\n",
    "await chat_kernel(\"Salut, peux-tu me conseiller un livre sur la philosophie antique ?\")\n",
    "await chat_kernel(\"Merci, tu peux détailler un peu plus la période concernée ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation des resultats du chat\n",
    "\n",
    "Le chat minimal ci-dessus illustre plusieurs concepts cles :\n",
    "\n",
    "1. **Template de prompt** : La variable `{{}}` est remplacee par l'historique complet\n",
    "2. **KernelArguments** : Permet de passer des variables dynamiques au prompt\n",
    "3. **Historique persistant** : Chaque echange est ajoute a `chat_history` pour maintenir le contexte\n",
    "\n",
    "Cette approche est la base de tout chatbot conversationnel avec memoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Function Calling Moderne\n",
    "\n",
    "### Evolution des Planners vers Function Calling\n",
    "\n",
    "Les **Planners** (SequentialPlanner, FunctionCallingStepwisePlanner) ont ete **deprecies** dans Semantic Kernel 1.30+. Microsoft recommande maintenant d'utiliser :\n",
    "\n",
    "| Ancienne approche | Nouvelle approche | Avantages |\n",
    "|-------------------|-------------------|-----------|\n",
    "| `SequentialPlanner` | `FunctionChoiceBehavior.Auto()` | Moins de tokens, plus fiable |\n",
    "| `FunctionCallingStepwisePlanner` | Agent avec plugins | Pattern ReAct natif |\n",
    "| Planification XML | Function calling OpenAI | Standard API |\n",
    "\n",
    "### Qu'est-ce que Function Calling ?\n",
    "\n",
    "**Function Calling** permet au LLM de :\n",
    "1. Analyser la requete utilisateur\n",
    "2. Decider automatiquement quelle(s) fonction(s) appeler\n",
    "3. Extraire les parametres depuis le contexte\n",
    "4. Retourner le resultat au LLM pour formulation finale\n",
    "\n",
    "C'est le pattern **ReAct** (Reasoning + Acting) integre directement dans l'API OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration de FunctionChoiceBehavior\n",
    "\n",
    "Semantic Kernel propose plusieurs modes de Function Calling :\n",
    "\n",
    "```python\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "\n",
    "# Mode Auto : le LLM decide quand appeler les fonctions\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Mode Required : force l'appel d'au moins une fonction\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Required()\n",
    "\n",
    "# Mode None : desactive le function calling\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.NoneInvoke()\n",
    "```\n",
    "\n",
    "### Parametres avances\n",
    "\n",
    "| Parametre | Description | Valeur par defaut |\n",
    "|-----------|-------------|-------------------|\n",
    "| `auto_invoke` | Executer automatiquement les fonctions | `True` |\n",
    "| `filters` | Filtrer les fonctions disponibles | `None` |\n",
    "| `maximum_auto_invoke_attempts` | Limite d'appels | `5` |\n",
    "\n",
    "L'exemple suivant montre comment configurer un kernel avec plusieurs plugins et laisser le LLM orchestrer automatiquement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### API de memoire Semantic Kernel\n\nSemantic Kernel propose une API de memoire semantique integree qui gere automatiquement :\n- Le stockage des textes avec leurs embeddings\n- La recherche par similarite semantique\n\n**Classes principales** :\n- `VolatileMemoryStore` : Stockage en memoire (ideal pour prototypage)\n- `SemanticTextMemory` : Interface haut niveau pour save/search\n- `OpenAITextEmbedding` : Service de generation d'embeddings\n\n> **Note** : Pour la production avec persistence, voir le notebook [05-VectorStores](05-SemanticKernel-VectorStores.ipynb) qui utilise Qdrant."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================\n# Function Calling Moderne avec FunctionChoiceBehavior\n# ============================\n\nfrom semantic_kernel.core_plugins.text_plugin import TextPlugin\nfrom semantic_kernel.core_plugins.math_plugin import MathPlugin\nfrom semantic_kernel.functions import KernelFunctionFromPrompt, kernel_function\nfrom semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\nfrom semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n\n# Charger des plugins avec des fonctions utiles\nplugins_directory = \"./prompt_template_samples/\"\nwriter_plugin = kernel.add_plugin(plugin_name=\"WriterPlugin\", parent_directory=plugins_directory)\ntext_plugin = kernel.add_plugin(plugin=TextPlugin(), plugin_name=\"TextPlugin\")\nmath_plugin = kernel.add_plugin(plugin=MathPlugin(), plugin_name=\"MathPlugin\")\n\n# Creer une fonction semantique pour la poesie\nshakespeare_func = KernelFunctionFromPrompt(\n    function_name=\"Shakespeare\",\n    plugin_name=\"WriterPlugin\",\n    prompt=\"\"\"\n{{$input}}\n\nRewrite the above in the style of Shakespeare.\n\"\"\",\n    prompt_execution_settings=OpenAIChatPromptExecutionSettings(\n        service_id=service_id\n    ),\n    description=\"Rewrite the input in the style of Shakespeare.\",\n)\nkernel.add_function(plugin_name=\"WriterPlugin\", function=shakespeare_func)\n\n# Lister les fonctions disponibles\nprint(\"Fonctions disponibles pour le LLM :\")\nprint(\"-\" * 50)\nfor plugin_name, plugin in kernel.plugins.items():\n    for function_name, function in plugin.functions.items():\n        desc = function.description[:50] + \"...\" if function.description and len(function.description) > 50 else function.description\n        print(f\"  {plugin_name}.{function_name}: {desc}\")\n\n# ============================\n# Exemple : Function Calling Auto\n# ============================\nprint(\"\\n\" + \"=\" * 50)\nprint(\"DEMO : Function Calling avec FunctionChoiceBehavior.Auto()\")\nprint(\"=\" * 50)\n\n# Configurer les settings avec function calling auto\nexecution_settings = OpenAIChatPromptExecutionSettings(\n    service_id=service_id,\n    function_choice_behavior=FunctionChoiceBehavior.Auto(\n        auto_invoke=True,  # Execute automatiquement les fonctions\n        filters={\"included_plugins\": [\"TextPlugin\", \"MathPlugin\"]}  # Limiter aux plugins\n    )\n)\n\n# Prompt qui necessite l'appel de fonctions\nuser_request = \"\"\"\nJ'ai besoin d'aide pour deux choses :\n1. Convertir le texte \"hello world\" en majuscules\n2. Calculer 25 + 17\n\"\"\"\n\nprint(f\"\\nRequete utilisateur : {user_request}\")\n\n# Creer une fonction de chat avec function calling\nfrom semantic_kernel.contents import ChatHistory\n\nchat_history = ChatHistory()\nchat_history.add_system_message(\"Tu es un assistant qui utilise les outils disponibles pour aider l'utilisateur.\")\nchat_history.add_user_message(user_request)\n\n# Invoquer avec function calling\nchat_service = kernel.get_service(service_id)\nresult = await chat_service.get_chat_message_content(\n    chat_history=chat_history,\n    settings=execution_settings,\n    kernel=kernel\n)\n\nprint(f\"\\nReponse du LLM (avec function calling) :\")\nprint(result.content)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Function Calling Auto\n",
    "\n",
    "**Sortie attendue** : Le LLM a automatiquement identifie et execute deux fonctions :\n",
    "- `TextPlugin.uppercase(\"hello world\")` → \"HELLO WORLD\"\n",
    "- `MathPlugin.add(25, 17)` → 42\n",
    "\n",
    "| Etape | Action | Description |\n",
    "|-------|--------|-------------|\n",
    "| 1 | Analyse | Le LLM parse la requete et identifie 2 taches |\n",
    "| 2 | Selection | Il choisit les fonctions appropriees dans les plugins |\n",
    "| 3 | Extraction | Il extrait les parametres du contexte |\n",
    "| 4 | Execution | SK execute les fonctions automatiquement |\n",
    "| 5 | Synthese | Le LLM formule la reponse finale |\n",
    "\n",
    "**Points cles** :\n",
    "- `auto_invoke=True` : Les fonctions sont executees automatiquement (pas besoin de code manuel)\n",
    "- `filters` : Limite les fonctions accessibles (securite, performance)\n",
    "- Le LLM peut enchainer plusieurs appels si necessaire\n",
    "\n",
    "> **Comparaison avec Agents** : Le notebook [03-Agents](03-SemanticKernel-Agents.ipynb) montre comment combiner Function Calling avec des agents pour des scenarios plus complexes (multi-agents, workflows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mémoire & Embeddings\n",
    "\n",
    "**SemanticTextMemory** permet de stocker des textes (avec un embedding) dans un store :\n",
    "- `VolatileMemoryStore` (en mémoire)\n",
    "- ou connecteurs vers Pinecone, Azure Cognitive Search, Qdrant, etc.\n",
    "\n",
    "On peut ensuite effectuer des requêtes sémantiques :  \n",
    "`await memory.search(\"MaCollection\", \"Quelle est mon budget pour 2024 ?\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification du GroundingPlugin\n",
    "\n",
    "Le plugin de Grounding (ancrage) permet de :\n",
    "1. **Extraire les entites** d'un texte resume\n",
    "2. **Verifier** chaque entite par rapport a un texte source de reference\n",
    "3. **Corriger** le resume en supprimant les informations non fondees\n",
    "\n",
    "Ce mecanisme est essentiel pour reduire les hallucinations des LLMs et garantir que les reponses sont ancrees dans des faits verifiables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================\n# Cellule : Memoire Semantique avec VolatileMemoryStore\n# ============================\n\nfrom semantic_kernel.memory import SemanticTextMemory, VolatileMemoryStore\nfrom semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n\n# Service d'embedding OpenAI\nembedding_service = OpenAITextEmbedding(\n    service_id=\"embeddingService\",\n    ai_model_id=\"text-embedding-3-small\"\n)\n\n# Memoire volatile (en RAM) - ideal pour prototypage\nmemory_store = VolatileMemoryStore()\nmemory = SemanticTextMemory(storage=memory_store, embeddings_generator=embedding_service)\n\n# ============================\n# Stocker des informations\n# ============================\nprint(\"=== Stockage des informations en memoire semantique ===\\n\")\n\ncollection = \"entreprise\"\ndocuments = [\n    (\"budget_2024\", \"Le budget prevu pour l'annee 2024 est de 100 000 euros, en augmentation de 30% par rapport a 2023.\"),\n    (\"budget_2023\", \"Le budget de l'annee 2023 etait de 70 000 euros, utilise principalement pour l'infrastructure.\"),\n    (\"localisation\", \"L'entreprise est basee a Geneve, en Suisse, avec des bureaux secondaires a Lyon.\"),\n    (\"investissements\", \"En 2024, nous avons investi 50 000 euros en actions technologiques et 20 000 en obligations.\"),\n    (\"equipe\", \"L'equipe compte actuellement 15 personnes, avec 3 nouvelles embauches prevues en 2025.\"),\n]\n\nfor doc_id, text in documents:\n    await memory.save_information(collection=collection, id=doc_id, text=text)\n    print(f\"  [+] Sauvegarde: {doc_id}\")\n\n# ============================\n# Recherches semantiques\n# ============================\nprint(\"\\n=== Recherches semantiques ===\\n\")\n\nqueries = [\n    \"Quel est mon budget pour 2024 ?\",\n    \"Ou se trouve le siege social ?\",\n    \"Combien a-t-on investi cette annee ?\",\n    \"Quelle est la taille de l'equipe ?\",\n]\n\nfor query in queries:\n    print(f\"Question: {query}\")\n    results = await memory.search(collection=collection, query=query, limit=2)\n    \n    for i, result in enumerate(results, 1):\n        print(f\"  [{i}] (score: {result.relevance:.3f}) {result.text[:80]}...\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Interpretation : Memoire Semantique\n\nL'API native de Semantic Kernel simplifie considerablement la gestion de la memoire :\n\n| Composant | Role |\n|-----------|------|\n| `VolatileMemoryStore` | Stockage en RAM (non-persistent) |\n| `SemanticTextMemory` | Interface haut niveau (save/search) |\n| `OpenAITextEmbedding` | Generation des vecteurs d'embedding |\n\n**Avantages de l'API native** :\n- Code minimal (~10 lignes vs implementation manuelle)\n- Gestion automatique des embeddings et de la similarite\n- Facilement interchangeable avec d'autres stores (Qdrant, Pinecone, etc.)\n\n**Limitation** : `VolatileMemoryStore` perd les donnees a l'arret du kernel. Pour la persistence, voir le notebook [05-VectorStores](05-SemanticKernel-VectorStores.ipynb) qui utilise Qdrant."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration des reponses multiples\n",
    "\n",
    "La fonctionnalite Multi-Result permet d'obtenir plusieurs completions pour un meme prompt en un seul appel API. Cela est utile pour :\n",
    "- **Generer des alternatives** : Obtenir plusieurs variations d'une reponse\n",
    "- **Evaluer la diversite** : Comparer differentes formulations\n",
    "- **Selection humaine** : Presenter plusieurs options a l'utilisateur\n",
    "\n",
    "Le parametre `number_of_responses` dans les settings controle le nombre de reponses retournees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Intégration\n",
    "\n",
    "Semantic Kernel peut se connecter à Hugging Face localement ou via API.  \n",
    "Exemple :  \n",
    "```python\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "\n",
    "hf_service = HuggingFaceTextCompletion(\n",
    "    service_id=\"textHF\", ai_model_id=\"distilgpt2\", task=\"text-generation\"\n",
    ")\n",
    "kernel.add_service(hf_service)\n",
    "```\n",
    "\n",
    "Ensuite, on peut enregistrer une fonction sémantique ou invoquer directement `hf_service.get_text_contents(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundedness Checking\n",
    "\n",
    "Pour éviter les “hallucinations” d’un résumé :  \n",
    "1. Extrait la liste d'entités du résumé.  \n",
    "2. Vérifie la correspondance de chaque entité avec le texte source (référence).  \n",
    "3. Retire ou corrige les entités non-fondées.\n",
    "\n",
    "Cela se fait via un plugin “GroundingPlugin” (ex. ExtraitEntities, ReferenceCheckEntities, ExciseEntities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entités détectées: - 2024 — a year (date) mentioned as part of \"Mon budget 2024\".\n",
      "- 200k euros — a monetary amount (budget value) mentioned in the sentence.\n",
      "- Milan — a location (city) mentioned as the place of residence.\n",
      "\n",
      "<entities>\n",
      "- 2024\n",
      "- 200k euros\n",
      "- Milan\n",
      "</entities>\n",
      "Entités non-fondées: <ungrounded_entities>\n",
      "- 200k euros\n",
      "- Milan\n",
      "</ungrounded_entities>\n",
      "Summary corrigé: Mon budget 2024 est prévu.\n",
      "J'habite dans une grande ville.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Groundedness Checking\n",
    "# ============================\n",
    "\n",
    "# Suppose qu'on a un \"grounding_text\" = un texte source\n",
    "grounding_text = \"\"\"\n",
    "Votre budget 2024 est de 100k euros.\n",
    "Vous vivez à Genève.\n",
    "Vous avez investi 50k en actions.\n",
    "\"\"\"\n",
    "\n",
    "# Suppose qu'on a un résumé \"faux\"\n",
    "summary_text = \"\"\"\n",
    "Mon budget 2024 est de 200k euros.\n",
    "J'habite à Milan.\n",
    "\"\"\"\n",
    "\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "\n",
    "# On appelle un plugin (hypothétique) \"GroundingPlugin\" comportant 3 fonctions\n",
    "try:\n",
    "    grounding_plugin = kernel.add_plugin(parent_directory=plugins_directory, plugin_name=\"GroundingPlugin\")\n",
    "    extract_entities = grounding_plugin[\"ExtractEntities\"]\n",
    "    check_entities = grounding_plugin[\"ReferenceCheckEntities\"]\n",
    "    excise_entities = grounding_plugin[\"ExciseEntities\"]\n",
    "    \n",
    "    # 1) Extraire entités avec les bonnes variables\n",
    "    ext_result = await kernel.invoke(\n",
    "        extract_entities, \n",
    "        KernelArguments(\n",
    "            input=summary_text,\n",
    "            topic=\"entities\",  # Variable attendue par le template\n",
    "            example_entities=\"Person, Location, Organization\",  # Variable attendue\n",
    "            allow_dangerously_set_content=True\n",
    "        )\n",
    "    )\n",
    "    print(\"Entités détectées:\", ext_result)\n",
    "    \n",
    "    # 2) Vérifier correspondance - Important: convertir ext_result en string\n",
    "    ext_result_str = str(ext_result.value) if hasattr(ext_result, 'value') else str(ext_result)\n",
    "    check_result = await kernel.invoke(\n",
    "        check_entities, \n",
    "        KernelArguments(\n",
    "            input=ext_result_str,  # Utiliser la version string\n",
    "            reference_context=grounding_text,\n",
    "            topic=\"entities\",  # Ajouter les variables attendues\n",
    "            allow_dangerously_set_content=True  # IMPORTANT: autoriser le contenu complexe\n",
    "        )\n",
    "    )\n",
    "    print(\"Entités non-fondées:\", check_result)\n",
    "    \n",
    "    # 3) Retirer entités non-fondées du summary\n",
    "    check_result_str = str(check_result.value) if hasattr(check_result, 'value') else str(check_result)\n",
    "    excision = await kernel.invoke(\n",
    "        excise_entities, \n",
    "        KernelArguments(\n",
    "            input=summary_text,\n",
    "            ungrounded_entities=check_result_str,  # Utiliser la version string\n",
    "            allow_dangerously_set_content=True\n",
    "        )\n",
    "    )\n",
    "    print(\"Summary corrigé:\", excision)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: Le plugin GroundingPlugin n'est pas disponible ou fonctionnel: {e}\")\n",
    "    print(\"Démonstration alternative:\")\n",
    "    print(f\"Texte source: {grounding_text[:50]}...\")\n",
    "    print(f\"Résumé analysé: {summary_text[:50]}...\")\n",
    "    print(\"Entités potentiellement problématiques: Milan (au lieu de Genève), 200k€ (au lieu de 100k€)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats du Groundedness Checking\n",
    "\n",
    "Le processus de verification d'ancrage comprend trois etapes :\n",
    "\n",
    "1. **Extraction** : Identification des entites factuelles dans le resume (personnes, lieux, montants)\n",
    "2. **Verification** : Comparaison de chaque entite avec le texte source de reference\n",
    "3. **Correction** : Suppression ou modification des entites non-fondees\n",
    "\n",
    "Dans l'exemple ci-dessus, les entites problematiques detectees seraient :\n",
    "- \"Milan\" (non mentionne dans le texte source, qui indique \"Geneve\")\n",
    "- \"200k euros\" (le texte source indique \"100k euros\")\n",
    "\n",
    "Cette technique est essentielle pour reduire les hallucinations des LLMs dans les systemes de production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Result\n",
    "\n",
    "OpenAI (ou Azure) peut renvoyer plusieurs complétions pour un même prompt.  \n",
    "On paramètre `number_of_responses=3` dans les settings.  \n",
    "Ensuite, `get_text_contents(...)` ou `get_chat_message_contents(...)` renvoie un *tableau* de résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================\n# Cellule : Multi-Result\n# ============================\n\nfrom semantic_kernel.connectors.ai.open_ai import OpenAITextPromptExecutionSettings\n\nsettings = OpenAITextPromptExecutionSettings(\n    extension_data={\n        \"max_tokens\": 60,\n        \"temperature\": 0.7,\n        \"number_of_responses\": 3\n    }\n)\n\nfrom semantic_kernel.connectors.ai.open_ai import OpenAITextCompletion\ntext_service = OpenAITextCompletion(service_id=\"multiResult\", ai_model_id=\"gpt-5-mini\")\n\nprompt = \"Donne-moi une brève blague sur les chats :\"\n\n# get_text_contents() => liste de réponses\nresponses = await text_service.get_text_contents(prompt, settings=settings)\n\nfor i, r in enumerate(responses):\n    print(f\"Réponse n°{i+1}:\\n{r}\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion et Resume\n",
    "\n",
    "Dans ce notebook, nous avons explore les fonctionnalites avancees de Semantic Kernel :\n",
    "\n",
    "| Concept | Description | Statut |\n",
    "|---------|-------------|--------|\n",
    "| **Chat avec KernelArguments** | Gestion de l'historique et du contexte | Fondamental |\n",
    "| **Function Calling** | Orchestration automatique via `FunctionChoiceBehavior.Auto()` | **Moderne** (remplace Planners) |\n",
    "| **Memoire & Embeddings** | Stockage vectoriel avec `InMemoryStore` | **API moderne** |\n",
    "| **Hugging Face** | Integration de modeles open-source | Optionnel |\n",
    "| **Groundedness Checking** | Reduction des hallucinations | Production |\n",
    "| **Multi-Result** | Generations multiples en un appel | Creatif |\n",
    "\n",
    "### Points cles a retenir\n",
    "\n",
    "1. **Function Calling remplace les Planners** : Plus simple, plus fiable, moins de tokens\n",
    "2. **API Memory modernisee** : `InMemoryStore` + `OpenAITextEmbedding` (voir notebook 05 pour Qdrant)\n",
    "3. **Groundedness** : Essentiel pour les systemes de production\n",
    "\n",
    "### Prochaines etapes\n",
    "\n",
    "- **[03-Agents](03-SemanticKernel-Agents.ipynb)** : Agents avec ChatCompletionAgent, AgentGroupChat\n",
    "- **[04-Filters](04-SemanticKernel-Filters-Observability.ipynb)** : Intercepter et logger les appels\n",
    "- **[05-VectorStores](05-SemanticKernel-VectorStores.ipynb)** : RAG complet avec Qdrant\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation** : [<< 01-Intro](01-SemanticKernel-Intro.ipynb) | [Index](README.md) | [03-Agents >>](03-SemanticKernel-Agents.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}