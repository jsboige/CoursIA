{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4665b29",
   "metadata": {
    "papermill": {
     "duration": 0.002708,
     "end_time": "2026-02-04T07:06:43.001480",
     "exception": false,
     "start_time": "2026-02-04T07:06:42.998772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SK-2-Functions : Function Calling, Memory et Fonctionnalites Avancees\n",
    "\n",
    "**Navigation** : [Index](README.md) | [<< 01-Intro](01-SemanticKernel-Intro.ipynb) | [03-Agents >>](03-SemanticKernel-Agents.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Utiliser **Function Calling** avec `FunctionChoiceBehavior` pour orchestrer automatiquement des fonctions\n",
    "2. Configurer la **memoire vectorielle** avec l'API moderne (InMemoryStore, embeddings)\n",
    "3. Integrer des modeles **Hugging Face** comme alternative a OpenAI\n",
    "4. Implementer un **Groundedness Checking** pour reduire les hallucinations\n",
    "5. Generer **plusieurs reponses** en un seul appel API\n",
    "\n",
    "### Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Notebook [01-SemanticKernel-Intro](01-SemanticKernel-Intro.ipynb) complete\n",
    "- Cle API OpenAI configuree dans `.env`\n",
    "\n",
    "### Duree estimee : 50 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Sommaire\n",
    "\n",
    "| Section | Contenu | Concepts cles |\n",
    "|---------|---------|---------------|\n",
    "| 1 | Installation et configuration | Kernel, services LLM |\n",
    "| 2 | Chat avec KernelArguments | Templates, historique |\n",
    "| 3 | **Function Calling moderne** | `FunctionChoiceBehavior.Auto()` |\n",
    "| 4 | Memoire et Embeddings | InMemoryStore, Vector Search |\n",
    "| 5 | Hugging Face Integration | Modeles open-source |\n",
    "| 6 | Groundedness Checking | Anti-hallucination |\n",
    "| 7 | Multi-Result | Generations multiples |\n",
    "\n",
    "> **Note importante** : Les anciens `SequentialPlanner` et `FunctionCallingStepwisePlanner` sont **deprecies** depuis SK 1.30. Ce notebook utilise l'approche moderne avec `FunctionChoiceBehavior`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9acba3",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-04T07:06:43.007494Z",
     "iopub.status.busy": "2026-02-04T07:06:43.007286Z",
     "iopub.status.idle": "2026-02-04T07:06:45.188778Z",
     "shell.execute_reply": "2026-02-04T07:06:45.187573Z"
    },
    "papermill": {
     "duration": 2.185303,
     "end_time": "2026-02-04T07:06:45.189426",
     "exception": false,
     "start_time": "2026-02-04T07:06:43.004123",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semantic-kernel in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.39.3)\n",
      "Requirement already satisfied: azure-ai-projects~=1.0.0b12 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (1.0.0)\n",
      "Requirement already satisfied: azure-ai-agents>=1.2.0b3 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (1.2.0b6)\n",
      "Requirement already satisfied: aiohttp~=3.8 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (3.13.3)\n",
      "Requirement already satisfied: cloudevents~=1.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (1.12.0)\n",
      "Requirement already satisfied: pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (2.11.10)\n",
      "Requirement already satisfied: pydantic-settings~=2.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (2.11.0)\n",
      "Requirement already satisfied: defusedxml~=0.7 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: azure-identity>=1.13 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (1.25.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (2.3.4)\n",
      "Requirement already satisfied: openai<2,>=1.98.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (1.109.1)\n",
      "Requirement already satisfied: openapi_core<0.20,>=0.18 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (0.19.5)\n",
      "Requirement already satisfied: websockets<16,>=13 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (15.0.1)\n",
      "Requirement already satisfied: aiortc>=1.9.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (1.14.0)\n",
      "Requirement already satisfied: opentelemetry-api~=1.24 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.24 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (1.39.1)\n",
      "Requirement already satisfied: prance<25.4.9,>=23.6.21 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (25.4.8.0)\n",
      "Requirement already satisfied: pybars4~=0.9 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (0.9.13)\n",
      "Requirement already satisfied: jinja2~=3.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (3.1.6)\n",
      "Requirement already satisfied: nest-asyncio~=1.6 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.15.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (1.17.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (6.33.0)\n",
      "Requirement already satisfied: typing-extensions>=4.13 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from semantic-kernel) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.22.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from azure-ai-projects~=1.0.0b12->semantic-kernel) (0.7.2)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from azure-ai-projects~=1.0.0b12->semantic-kernel) (1.36.0)\n",
      "Requirement already satisfied: azure-storage-blob>=12.15.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from azure-ai-projects~=1.0.0b12->semantic-kernel) (12.28.0)\n",
      "Requirement already satisfied: deprecation<3.0,>=2.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cloudevents~=1.0->semantic-kernel) (2.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from deprecation<3.0,>=2.0->cloudevents~=1.0->semantic-kernel) (25.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2~=3.1->semantic-kernel) (3.0.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (0.13.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2,>=1.98.0->semantic-kernel) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2,>=1.98.0->semantic-kernel) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2,>=1.98.0->semantic-kernel) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2,>=1.98.0->semantic-kernel) (0.16.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (4.25.1)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.3.4)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (10.8.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.6.3)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.7.2)\n",
      "Requirement already satisfied: parse in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (1.20.2)\n",
      "Requirement already satisfied: werkzeug<3.1.2 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (3.1.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.27.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (6.0.3)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (0.4.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (2.32.5)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.1.4)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel) (1.12.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from opentelemetry-api~=1.24->semantic-kernel) (8.7.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api~=1.24->semantic-kernel) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from opentelemetry-sdk~=1.24->semantic-kernel) (0.60b1)\n",
      "Requirement already satisfied: chardet>=5.2 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.18.10 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (0.19.1)\n",
      "Requirement already satisfied: PyMeta3>=0.5.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pybars4~=0.9->semantic-kernel) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic-settings~=2.0->semantic-kernel) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.31.0->jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.31.0->jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (2.5.0)\n",
      "Requirement already satisfied: aioice<1.0.0,>=0.10.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (0.10.2)\n",
      "Requirement already satisfied: av<17.0.0,>=14.0.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (16.1.0)\n",
      "Requirement already satisfied: cryptography>=44.0.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (46.0.3)\n",
      "Requirement already satisfied: google-crc32c>=1.1 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (1.8.0)\n",
      "Requirement already satisfied: pyee>=13.0.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (13.0.0)\n",
      "Requirement already satisfied: pylibsrtp>=0.10.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (1.0.0)\n",
      "Requirement already satisfied: pyopenssl>=25.0.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (25.3.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aioice<1.0.0,>=0.10.1->aiortc>=1.9.0->semantic-kernel) (2.8.0)\n",
      "Requirement already satisfied: ifaddr>=0.2.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aioice<1.0.0,>=0.10.1->aiortc>=1.9.0->semantic-kernel) (0.2.0)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from azure-identity>=1.13->semantic-kernel) (1.34.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from azure-identity>=1.13->semantic-kernel) (1.3.1)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cryptography>=44.0.0->aiortc>=1.9.0->semantic-kernel) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=2.0.0->cryptography>=44.0.0->aiortc>=1.9.0->semantic-kernel) (2.23)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity>=1.13->semantic-kernel) (2.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>4->openai<2,>=1.98.0->semantic-kernel) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\jsboi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rfc3339-validator->openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 26.0\n",
      "[notice] To update, run: C:\\Users\\jsboi\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports et installation OK.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Installation & Imports\n",
    "# ============================\n",
    "\n",
    "# N'installez qu'une seule fois si nécessaire\n",
    "%pip install -U semantic-kernel\n",
    "\n",
    "# Imports de base\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "print(\"Imports et installation OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b0543",
   "metadata": {
    "papermill": {
     "duration": 0.002378,
     "end_time": "2026-02-04T07:06:45.194511",
     "exception": false,
     "start_time": "2026-02-04T07:06:45.192133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Installation et configuration initiale\n",
    "\n",
    "Cette premiere cellule effectue deux operations essentielles :\n",
    "1. **Installation de semantic-kernel** : Le SDK Python pour orchestrer des modeles de langage\n",
    "2. **Imports de base** : Les modules fondamentaux pour interagir avec le kernel et gerer l'environnement\n",
    "\n",
    "Le fichier `.env` doit contenir vos cles API (voir la section suivante pour le format attendu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf64d5",
   "metadata": {
    "papermill": {
     "duration": 0.002374,
     "end_time": "2026-02-04T07:06:45.199360",
     "exception": false,
     "start_time": "2026-02-04T07:06:45.196986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3) Bloc Markdown – Configuration du Kernel & .env\n",
    "\n",
    "```markdown\n",
    "### Configuration du Kernel\n",
    "\n",
    "Pour exécuter les exemples, on suppose que vous avez un fichier `.env` comportant vos clés d'API OpenAI / Azure OpenAI / Hugging Face.  \n",
    "Exemple `.env` :\n",
    "\n",
    "```\n",
    "GLOBAL_LLM_SERVICE=\"OpenAI\"        # ou AzureOpenAI, HuggingFace\n",
    "OPENAI_API_KEY=\"sk-...\"\n",
    "OPENAI_CHAT_MODEL_ID=\"gpt-3.5-turbo\"\n",
    "...\n",
    "```\n",
    "\n",
    "Le `Kernel` lira ces informations pour décider quel connecteur LLM utiliser.  \n",
    "Ensuite, nous ajouterons nos services (`OpenAIChatCompletion`, `AzureChatCompletion`, etc.) selon la variable `GLOBAL_LLM_SERVICE`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a80384",
   "metadata": {
    "papermill": {
     "duration": 0.002586,
     "end_time": "2026-02-04T07:06:45.204332",
     "exception": false,
     "start_time": "2026-02-04T07:06:45.201746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Initialisation du Kernel avec service LLM dynamique\n",
    "\n",
    "Cette cellule configure le Kernel en fonction du service LLM specifie dans le fichier `.env`. Le code supporte trois backends :\n",
    "- **OpenAI** : API officielle d'OpenAI (GPT-4, GPT-3.5, etc.)\n",
    "- **Azure OpenAI** : Deploiement Azure avec endpoint personnalise\n",
    "- **Hugging Face** : Modeles open-source en local ou via le Hub\n",
    "\n",
    "Le pattern utilise ici est courant : on lit la variable `GLOBAL_LLM_SERVICE` pour determiner dynamiquement quel connecteur instancier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d7d73b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:06:45.210361Z",
     "iopub.status.busy": "2026-02-04T07:06:45.209974Z",
     "iopub.status.idle": "2026-02-04T07:06:45.927451Z",
     "shell.execute_reply": "2026-02-04T07:06:45.926768Z"
    },
    "papermill": {
     "duration": 0.721475,
     "end_time": "2026-02-04T07:06:45.928129",
     "exception": false,
     "start_time": "2026-02-04T07:06:45.206654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service OpenAI configuré.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Initialisation du Kernel\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    AzureChatCompletion\n",
    ")\n",
    "\n",
    "# On charge le .env\n",
    "load_dotenv()\n",
    "global_llm_service = os.getenv(\"GLOBAL_LLM_SERVICE\", \"AzureOpenAI\")\n",
    "\n",
    "# Initialisation du Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "service_id = \"default\"\n",
    "\n",
    "if global_llm_service.lower() == \"openai\":\n",
    "    # Ajout du service OpenAI\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service OpenAI configuré.\")\n",
    "elif global_llm_service.lower() == \"huggingface\":\n",
    "    # Ajout du service HuggingFace\n",
    "    from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "    kernel.add_service(\n",
    "        HuggingFaceTextCompletion(\n",
    "            service_id=service_id,\n",
    "            ai_model_id=\"distilgpt2\", # ex. pour text-generation\n",
    "            task=\"text-generation\"\n",
    "        ),\n",
    "    )\n",
    "    print(\"Service Hugging Face configuré.\")\n",
    "else:\n",
    "    # Par défaut : Azure OpenAI\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(service_id=service_id),\n",
    "    )\n",
    "    print(\"Service Azure OpenAI configuré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd340c",
   "metadata": {
    "papermill": {
     "duration": 0.002832,
     "end_time": "2026-02-04T07:06:45.933771",
     "exception": false,
     "start_time": "2026-02-04T07:06:45.930939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Choix du service LLM\n",
    "\n",
    "Le code ci-dessus illustre un pattern important : **la configuration dynamique du service LLM**.\n",
    "\n",
    "Selon la valeur de `GLOBAL_LLM_SERVICE` dans votre fichier `.env`, le Kernel se connectera a :\n",
    "- **OpenAI** : API officielle avec `OpenAIChatCompletion`\n",
    "- **Azure OpenAI** : Deploiement Azure avec `AzureChatCompletion`\n",
    "- **Hugging Face** : Modeles open-source avec `HuggingFaceTextCompletion`\n",
    "\n",
    "Cette flexibilite permet de switcher facilement entre fournisseurs sans modifier le code applicatif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c4225",
   "metadata": {
    "papermill": {
     "duration": 0.002225,
     "end_time": "2026-02-04T07:06:45.938327",
     "exception": false,
     "start_time": "2026-02-04T07:06:45.936102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Chat Basique avec KernelArguments\n",
    "\n",
    "L'idée : on crée une fonction de chat qui prend :\n",
    "- l’historique de conversation (objet `ChatHistory`)\n",
    "- un `user_input`\n",
    "et on stocke le tout dans un `KernelArguments`.  \n",
    "\n",
    "Cela permet d'alimenter un prompt (via un template) qui contient la variable `history` et `user_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7c06a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:06:45.944028Z",
     "iopub.status.busy": "2026-02-04T07:06:45.943755Z",
     "iopub.status.idle": "2026-02-04T07:07:01.611543Z",
     "shell.execute_reply": "2026-02-04T07:07:01.610381Z"
    },
    "papermill": {
     "duration": 15.671889,
     "end_time": "2026-02-04T07:07:01.612496",
     "exception": false,
     "start_time": "2026-02-04T07:06:45.940607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Utilisateur] : Salut, peux-tu me conseiller un livre sur la philosophie antique ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatBot] : Salut ! Bien sûr, je peux te recommander un livre sur la philosophie antique. L'un des ouvrages classiques est \"La République\" de Platon. Ce livre explore des thèmes tels que la justice, la politique, et la notion de la cité idéale. Si tu t'intéresses davantage à l'éthique, \"Éthique à Nicomaque\" d'Aristote est également un choix excellent. Ces deux livres te donneront une bonne compréhension des fondements de la philosophie occidentale. Bonne lecture !\n",
      "[Utilisateur] : Merci, tu peux détailler un peu plus la période concernée ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatBot] : Bien sûr ! La philosophie antique couvre généralement une large période de l'histoire qui va de l'émergence de la pensée philosophique en Grèce au VIe siècle avant notre ère jusqu'à la chute de l'Empire romain au Ve siècle de notre ère. Cette période se divise souvent en plusieurs phases distinctes :\n",
      "\n",
      "1. **Les Présocratiques** (VIe - Ve siècle av. J.-C.) : Ces premiers penseurs grecs, tels que Thalès, Héraclite et Parménide, se sont concentrés sur la nature et l'origine du cosmos, ainsi que sur la place de l'homme dans l'univers.\n",
      "\n",
      "2. **Le Siècle de Périclès ou de Socrate** (V-IVe siècle av. J.-C.) : Cette période voit l'émergence de figures centrales de la philosophie que sont Socrate, Platon et Aristote. Leurs travaux ont largement défini la tradition philosophique occidentale avec des discussions sur l'éthique, la politique, l'épistémologie et la métaphysique.\n",
      "\n",
      "3. **L'Hellénisme** (IVe - Ier siècle av. J.-C.) : Après la mort d'Alexandre le Grand, la philosophie devient plus diversifiée avec l'apparition de nouvelles écoles comme le stoïcisme (Zénon de Citium), l'épicurisme (Épicure) et le scepticisme.\n",
      "\n",
      "4. **La période romaine** (Ier siècle av. J.-C. - Ve siècle apr. J.-C.): Le stoïcisme et l'épicurisme restent influents, et des penseurs comme Sénèque, Épictète et Marc Aurèle contribuent à leur diffusion. De plus, Plotin développe le néoplatonisme, qui aura une influence majeure sur la pensée chrétienne.\n",
      "\n",
      "Chacune de ces phases a apporté des contributions uniques à la philosophie, et les œuvres des philosophes de ces époques continuent d'être étudiées et admirées aujourd'hui. Si tu as un intérêt pour une période ou un thème particulier, n'hésite pas à me le faire savoir pour des recommandations plus ciblées !\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Extrait d'un Chat Minimal\n",
    "# ============================\n",
    "\n",
    "# Exemple de prompt\n",
    "chat_prompt = \"\"\"\n",
    "{{$history}}\n",
    "User: {{$user_input}}\n",
    "ChatBot:\n",
    "\"\"\"\n",
    "\n",
    "# Création d'une fonction sémantique \"chat\"\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "\n",
    "pt_config = PromptTemplateConfig(\n",
    "    template=chat_prompt,\n",
    "    name=\"chatFunction\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"user_input\", description=\"User's message\"),\n",
    "        InputVariable(name=\"history\", description=\"Conversation history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    function_name=\"chat\",\n",
    "    plugin_name=\"myChatPlugin\",\n",
    "    prompt_template_config=pt_config\n",
    ")\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_system_message(\"Vous êtes un chatbot utile spécialisé en recommandations de livres.\")\n",
    "\n",
    "async def chat_kernel(input_text: str):\n",
    "    print(f\"[Utilisateur] : {input_text}\")\n",
    "    response = await kernel.invoke(\n",
    "        chat_function,\n",
    "        KernelArguments(user_input=input_text, history=str(chat_history))\n",
    "    )\n",
    "    print(f\"[ChatBot] : {response}\")\n",
    "    # Mise à jour de l'historique\n",
    "    chat_history.add_user_message(input_text)\n",
    "    chat_history.add_assistant_message(str(response))\n",
    "\n",
    "\n",
    "# Test\n",
    "await chat_kernel(\"Salut, peux-tu me conseiller un livre sur la philosophie antique ?\")\n",
    "await chat_kernel(\"Merci, tu peux détailler un peu plus la période concernée ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hh8z3xf564",
   "source": "### Interprétation : Chat Conversationnel avec Historique\n\n**Sortie obtenue** : Un chatbot conversationnel qui maintient le contexte entre les échanges.\n\n| Échange | Utilisateur | Assistant | Contexte maintenu |\n|---------|-------------|-----------|-------------------|\n| **1** | \"Peux-tu me conseiller un livre sur la philosophie antique ?\" | Recommande \"La République\" (Platon) et \"Éthique à Nicomaque\" (Aristote) | Message système (spécialisation livres) |\n| **2** | \"Merci, tu peux détailler un peu plus la période concernée ?\" | Détaille les 4 phases de la philosophie antique (Présocratiques → Romaine) | Se rappelle du sujet (philosophie antique) |\n\n**Points clés** :\n\n1. **Architecture du chat avec historique** :\n   ```python\n   ChatHistory → KernelArguments → PromptTemplate → LLM → Update ChatHistory\n        ↓              ↓                 ↓            ↓              ↓\n   Contexte      Variables        Prompt final   Réponse    Nouveau contexte\n   ```\n\n2. **Composants essentiels** :\n   | Composant | Rôle | Exemple |\n   |-----------|------|---------|\n   | `ChatHistory` | Stocke tous les messages (système, utilisateur, assistant) | Conserve le contexte |\n   | `PromptTemplateConfig` | Définit le template avec variables `{{$history}}`, `{{$user_input}}` | Structure du prompt |\n   | `KernelArguments` | Passe les valeurs dynamiques au template | `user_input`, `history` |\n   | `chat_history.add_*_message()` | Met à jour l'historique après chaque échange | Persistence mémoire |\n\n3. **Gestion du message système** :\n   ```python\n   chat_history.add_system_message(\"Vous êtes un chatbot utile spécialisé en recommandations de livres.\")\n   ```\n   - Définit le comportement et la personnalité du bot\n   - Reste présent dans tout l'historique\n   - Peut spécifier le format de réponse, le ton, les contraintes\n\n4. **Exemple de prompt généré (2ème échange)** :\n   ```\n   [SYSTEM] Vous êtes un chatbot utile spécialisé en recommandations de livres.\n   [USER] Salut, peux-tu me conseiller un livre sur la philosophie antique ?\n   [ASSISTANT] Salut ! Bien sûr, je peux te recommander \"La République\" de Platon...\n   [USER] Merci, tu peux détailler un peu plus la période concernée ?\n   [CHATBOT] <à générer>\n   ```\n\n**Limitations et améliorations** :\n\n1. **Limite de contexte** :\n   - Le `ChatHistory` peut devenir très long et dépasser le context window du LLM\n   - Solutions :\n     - Tronquer l'historique (garder N derniers messages)\n     - Résumer l'historique (summarization)\n     - Mémoire vectorielle (stocker l'historique dans un vector store)\n\n2. **Code amélioré avec gestion de contexte** :\n   ```python\n   MAX_HISTORY_LENGTH = 10  # Garder les 10 derniers échanges\n   \n   if len(chat_history.messages) > MAX_HISTORY_LENGTH:\n       # Garder le message système + N derniers messages\n       system_msg = chat_history.messages[0]\n       recent_msgs = chat_history.messages[-(MAX_HISTORY_LENGTH-1):]\n       chat_history = ChatHistory()\n       chat_history.add_message(system_msg)\n       for msg in recent_msgs:\n           chat_history.add_message(msg)\n   ```\n\n**Comparaison avec d'autres patterns** :\n| Pattern | Mémoire | Use case |\n|---------|---------|----------|\n| **Stateless** (prompt fixe) | Aucune | FAQ simple |\n| **ChatHistory** (ce notebook) | Court terme (session) | Chatbot conversationnel |\n| **Vector Memory** (notebook 05) | Long terme (permanent) | RAG, knowledge base |\n| **Agents** (notebook 03) | Multi-agents avec état partagé | Workflows complexes |\n\n**Cas d'usage typiques** :\n- **Support client** : Résoudre un problème en plusieurs étapes\n- **Assistant personnel** : Planification, organisation avec continuité\n- **Tuteur éducatif** : Adaptation progressive au niveau de l'apprenant",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "252128f4",
   "metadata": {
    "papermill": {
     "duration": 0.002927,
     "end_time": "2026-02-04T07:07:01.623466",
     "exception": false,
     "start_time": "2026-02-04T07:07:01.620539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Function Calling Moderne\n",
    "\n",
    "### Evolution des Planners vers Function Calling\n",
    "\n",
    "Les **Planners** (SequentialPlanner, FunctionCallingStepwisePlanner) ont ete **deprecies** dans Semantic Kernel 1.30+. Microsoft recommande maintenant d'utiliser :\n",
    "\n",
    "| Ancienne approche | Nouvelle approche | Avantages |\n",
    "|-------------------|-------------------|-----------|\n",
    "| `SequentialPlanner` | `FunctionChoiceBehavior.Auto()` | Moins de tokens, plus fiable |\n",
    "| `FunctionCallingStepwisePlanner` | Agent avec plugins | Pattern ReAct natif |\n",
    "| Planification XML | Function calling OpenAI | Standard API |\n",
    "\n",
    "### Qu'est-ce que Function Calling ?\n",
    "\n",
    "**Function Calling** permet au LLM de :\n",
    "1. Analyser la requete utilisateur\n",
    "2. Decider automatiquement quelle(s) fonction(s) appeler\n",
    "3. Extraire les parametres depuis le contexte\n",
    "4. Retourner le resultat au LLM pour formulation finale\n",
    "\n",
    "C'est le pattern **ReAct** (Reasoning + Acting) integre directement dans l'API OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aabd72",
   "metadata": {
    "papermill": {
     "duration": 0.002578,
     "end_time": "2026-02-04T07:07:01.628859",
     "exception": false,
     "start_time": "2026-02-04T07:07:01.626281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configuration de FunctionChoiceBehavior\n",
    "\n",
    "Semantic Kernel propose plusieurs modes de Function Calling :\n",
    "\n",
    "```python\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "\n",
    "# Mode Auto : le LLM decide quand appeler les fonctions\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Mode Required : force l'appel d'au moins une fonction\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Required()\n",
    "\n",
    "# Mode None : desactive le function calling\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.NoneInvoke()\n",
    "```\n",
    "\n",
    "### Parametres avances\n",
    "\n",
    "| Parametre | Description | Valeur par defaut |\n",
    "|-----------|-------------|-------------------|\n",
    "| `auto_invoke` | Executer automatiquement les fonctions | `True` |\n",
    "| `filters` | Filtrer les fonctions disponibles | `None` |\n",
    "| `maximum_auto_invoke_attempts` | Limite d'appels | `5` |\n",
    "\n",
    "L'exemple suivant montre comment configurer un kernel avec plusieurs plugins et laisser le LLM orchestrer automatiquement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a6354",
   "metadata": {
    "papermill": {
     "duration": 0.003062,
     "end_time": "2026-02-04T07:07:01.634465",
     "exception": false,
     "start_time": "2026-02-04T07:07:01.631403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### API de memoire vectorielle mise a jour\n",
    "\n",
    "**Note de version** : L'API de memoire a evolue dans Semantic Kernel. Les anciennes classes comme `VolatileMemoryStore` et `SemanticTextMemory` sont remplacees par :\n",
    "- `InMemoryStore` : Stockage en memoire volatile\n",
    "- Services d'embedding dedies avec `OpenAITextEmbedding`\n",
    "\n",
    "Cette cellule montre le pattern moderne pour configurer la memoire semantique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52141b6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:07:01.641181Z",
     "iopub.status.busy": "2026-02-04T07:07:01.640891Z",
     "iopub.status.idle": "2026-02-04T07:07:03.504906Z",
     "shell.execute_reply": "2026-02-04T07:07:03.504262Z"
    },
    "papermill": {
     "duration": 1.868017,
     "end_time": "2026-02-04T07:07:03.505368",
     "exception": false,
     "start_time": "2026-02-04T07:07:01.637351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonctions disponibles pour le LLM :\n",
      "--------------------------------------------------\n",
      "  myChatPlugin.chat: \n",
      "  WriterPlugin.Acronym: Generate an acronym for the given concept or phras...\n",
      "  WriterPlugin.AcronymGenerator: Given a request to generate an acronym from a stri...\n",
      "  WriterPlugin.AcronymReverse: Given a single word or acronym, generate the expan...\n",
      "  WriterPlugin.Brainstorm: Given a goal or topic description generate a list ...\n",
      "  WriterPlugin.EmailGen: Write an email from the given bullet points\n",
      "  WriterPlugin.EmailTo: Turn bullet points into an email to someone, using...\n",
      "  WriterPlugin.EnglishImprover: Translate text to English and improve it\n",
      "  WriterPlugin.NovelChapter: Write a chapter of a novel.\n",
      "  WriterPlugin.NovelChapterWithNotes: Write a chapter of a novel using notes about the c...\n",
      "  WriterPlugin.NovelOutline: Generate a list of chapter synopsis for a novel or...\n",
      "  WriterPlugin.Rewrite: Automatically generate compact notes for any text ...\n",
      "  WriterPlugin.ShortPoem: Turn a scenario into a short and entertaining poem...\n",
      "  WriterPlugin.StoryGen: Generate a list of synopsis for a novel or novella...\n",
      "  WriterPlugin.TellMeMore: Summarize given text or any text document\n",
      "  WriterPlugin.Translate: Translate the input into a language of your choice\n",
      "  WriterPlugin.TwoSentenceSummary: Summarize given text in two sentences or less\n",
      "  WriterPlugin.Shakespeare: Rewrite the input in the style of Shakespeare.\n",
      "  TextPlugin.lowercase: Convert a string to lowercase.\n",
      "  TextPlugin.trim: Trim whitespace from the start and end of a string...\n",
      "  TextPlugin.trim_end: Trim whitespace from the end of a string.\n",
      "  TextPlugin.trim_start: Trim whitespace from the start of a string.\n",
      "  TextPlugin.uppercase: Convert a string to uppercase.\n",
      "  MathPlugin.Add: Returns the addition result of the values provided...\n",
      "  MathPlugin.Subtract: Returns the difference of numbers provided (suppor...\n",
      "\n",
      "==================================================\n",
      "DEMO : Function Calling avec FunctionChoiceBehavior.Auto()\n",
      "==================================================\n",
      "\n",
      "Requete utilisateur : \n",
      "J'ai besoin d'aide pour deux choses :\n",
      "1. Convertir le texte \"hello world\" en majuscules\n",
      "2. Calculer 25 + 17\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reponse du LLM (avec function calling) :\n",
      "Voici les résultats :\n",
      "\n",
      "1. Le texte \"hello world\" en majuscules est : **HELLO WORLD**\n",
      "2. Le résultat de 25 + 17 est : **42**\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Function Calling Moderne avec FunctionChoiceBehavior\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.core_plugins.text_plugin import TextPlugin\n",
    "from semantic_kernel.core_plugins.math_plugin import MathPlugin\n",
    "from semantic_kernel.functions import KernelFunctionFromPrompt, kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "\n",
    "# Charger des plugins avec des fonctions utiles\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "writer_plugin = kernel.add_plugin(plugin_name=\"WriterPlugin\", parent_directory=plugins_directory)\n",
    "text_plugin = kernel.add_plugin(plugin=TextPlugin(), plugin_name=\"TextPlugin\")\n",
    "math_plugin = kernel.add_plugin(plugin=MathPlugin(), plugin_name=\"MathPlugin\")\n",
    "\n",
    "# Creer une fonction semantique pour la poesie\n",
    "shakespeare_func = KernelFunctionFromPrompt(\n",
    "    function_name=\"Shakespeare\",\n",
    "    plugin_name=\"WriterPlugin\",\n",
    "    prompt=\"\"\"\n",
    "{{$input}}\n",
    "\n",
    "Rewrite the above in the style of Shakespeare.\n",
    "\"\"\",\n",
    "    prompt_execution_settings=OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.8,\n",
    "    ),\n",
    "    description=\"Rewrite the input in the style of Shakespeare.\",\n",
    ")\n",
    "kernel.add_function(plugin_name=\"WriterPlugin\", function=shakespeare_func)\n",
    "\n",
    "# Lister les fonctions disponibles\n",
    "print(\"Fonctions disponibles pour le LLM :\")\n",
    "print(\"-\" * 50)\n",
    "for plugin_name, plugin in kernel.plugins.items():\n",
    "    for function_name, function in plugin.functions.items():\n",
    "        desc = function.description[:50] + \"...\" if function.description and len(function.description) > 50 else function.description\n",
    "        print(f\"  {plugin_name}.{function_name}: {desc}\")\n",
    "\n",
    "# ============================\n",
    "# Exemple : Function Calling Auto\n",
    "# ============================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DEMO : Function Calling avec FunctionChoiceBehavior.Auto()\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configurer les settings avec function calling auto\n",
    "execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "    service_id=service_id,\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    "    function_choice_behavior=FunctionChoiceBehavior.Auto(\n",
    "        auto_invoke=True,  # Execute automatiquement les fonctions\n",
    "        filters={\"included_plugins\": [\"TextPlugin\", \"MathPlugin\"]}  # Limiter aux plugins\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prompt qui necessite l'appel de fonctions\n",
    "user_request = \"\"\"\n",
    "J'ai besoin d'aide pour deux choses :\n",
    "1. Convertir le texte \"hello world\" en majuscules\n",
    "2. Calculer 25 + 17\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nRequete utilisateur : {user_request}\")\n",
    "\n",
    "# Creer une fonction de chat avec function calling\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_system_message(\"Tu es un assistant qui utilise les outils disponibles pour aider l'utilisateur.\")\n",
    "chat_history.add_user_message(user_request)\n",
    "\n",
    "# Invoquer avec function calling\n",
    "chat_service = kernel.get_service(service_id)\n",
    "result = await chat_service.get_chat_message_content(\n",
    "    chat_history=chat_history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel\n",
    ")\n",
    "\n",
    "print(f\"\\nReponse du LLM (avec function calling) :\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jycvmm3pkm",
   "source": "### Interprétation : Architecture Function Calling\n\n**Sortie obtenue** : Le LLM a automatiquement identifié et exécuté deux fonctions sans code manuel :\n- `TextPlugin.uppercase(\"hello world\")` → \"HELLO WORLD\"\n- `MathPlugin.Add(25, 17)` → 42\n\n| Aspect | Détail | Signification |\n|--------|--------|---------------|\n| **Plugins chargés** | WriterPlugin (18 fonctions), TextPlugin (5), MathPlugin (2) | Bibliothèque d'outils disponibles pour le LLM |\n| **Mode Auto** | `FunctionChoiceBehavior.Auto()` | Le LLM décide automatiquement quand et comment appeler les fonctions |\n| **Filtres** | `included_plugins: [\"TextPlugin\", \"MathPlugin\"]` | Limitation des fonctions accessibles pour sécurité/performance |\n| **Auto-invoke** | `auto_invoke=True` | Exécution automatique sans intervention manuelle |\n\n**Points clés** :\n\n1. **Pattern ReAct intégré** : Le LLM raisonne (analyse la requête) puis agit (appelle les fonctions)\n2. **Chaînage transparent** : Le LLM peut enchaîner plusieurs appels de fonctions si nécessaire\n3. **Extraction de paramètres** : Les valeurs sont extraites du contexte naturel (\"25 + 17\" → `Add(25, 17)`)\n4. **Formulation finale** : Le résultat brut des fonctions est reformulé dans une réponse naturelle\n\n**Note technique** : Cette approche remplace les anciens Planners (SequentialPlanner, FunctionCallingStepwisePlanner) qui généraient du XML intermédiaire. Function Calling utilise directement l'API OpenAI, ce qui réduit les tokens et améliore la fiabilité.\n\n**Comparaison avec Agents** :\n- **Function Calling** : Orchestration simple, un LLM qui appelle des fonctions\n- **Agents** (voir notebook 03) : Plusieurs agents qui collaborent, chacun avec ses propres plugins\n\n**Cas d'usage typiques** :\n- Assistants conversationnels avec accès à des APIs\n- Chatbots d'entreprise avec fonctions métier (recherche produits, calculs, requêtes DB)\n- Outils d'aide à la décision avec fonctions analytiques",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "e9e491da",
   "metadata": {
    "papermill": {
     "duration": 0.002533,
     "end_time": "2026-02-04T07:07:03.516360",
     "exception": false,
     "start_time": "2026-02-04T07:07:03.513827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Mémoire & Embeddings\n",
    "\n",
    "**SemanticTextMemory** permet de stocker des textes (avec un embedding) dans un store :\n",
    "- `VolatileMemoryStore` (en mémoire)\n",
    "- ou connecteurs vers Pinecone, Azure Cognitive Search, Qdrant, etc.\n",
    "\n",
    "On peut ensuite effectuer des requêtes sémantiques :  \n",
    "`await memory.search(\"MaCollection\", \"Quelle est mon budget pour 2024 ?\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9901756",
   "metadata": {
    "papermill": {
     "duration": 0.002794,
     "end_time": "2026-02-04T07:07:03.521583",
     "exception": false,
     "start_time": "2026-02-04T07:07:03.518789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Verification du GroundingPlugin\n",
    "\n",
    "Le plugin de Grounding (ancrage) permet de :\n",
    "1. **Extraire les entites** d'un texte resume\n",
    "2. **Verifier** chaque entite par rapport a un texte source de reference\n",
    "3. **Corriger** le resume en supprimant les informations non fondees\n",
    "\n",
    "Ce mecanisme est essentiel pour reduire les hallucinations des LLMs et garantir que les reponses sont ancrees dans des faits verifiables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cfbe543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:07:03.528341Z",
     "iopub.status.busy": "2026-02-04T07:07:03.528012Z",
     "iopub.status.idle": "2026-02-04T07:07:25.478984Z",
     "shell.execute_reply": "2026-02-04T07:07:25.478099Z"
    },
    "papermill": {
     "duration": 21.955489,
     "end_time": "2026-02-04T07:07:25.479646",
     "exception": false,
     "start_time": "2026-02-04T07:07:03.524157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: API de mémoire simplifiée pour cette version.\n",
      "Informations stockées :\n",
      "  - Budget 2024 = 100k€\n",
      "  - Budget 2023 = 70k€\n",
      "\n",
      "Requête : Quel est mon budget pour 2024 ?\n",
      "Réponse potentielle :  Budget 2024 = 100k€\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Extrait Mémoire\n",
    "# ============================\n",
    "\n",
    "# CORRECTION: Utilisation des nouvelles approches pour la mémoire vectorielle\n",
    "from semantic_kernel.connectors.in_memory import InMemoryStore\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n",
    "\n",
    "# Embedding service (ex : openai text-embedding-ada)\n",
    "embedding_service = OpenAITextEmbedding(\n",
    "    service_id=\"embeddingService\",\n",
    "    ai_model_id=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# Nouvelle approche avec InMemoryStore\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Ajout du service d'embedding au kernel\n",
    "kernel.add_service(embedding_service)\n",
    "\n",
    "# Pour la démonstration, on simule la mémoire et la recherche\n",
    "budget_2024 = \"Budget 2024 = 100k€\"\n",
    "budget_2023 = \"Budget 2023 = 70k€\"\n",
    "\n",
    "print(\"Note: API de mémoire simplifiée pour cette version.\")\n",
    "print(f\"Informations stockées :\")\n",
    "print(f\"  - {budget_2024}\")\n",
    "print(f\"  - {budget_2023}\")\n",
    "\n",
    "# Simulation de recherche\n",
    "query = \"Quel est mon budget pour 2024 ?\"\n",
    "print(f\"\\nRequête : {query}\")\n",
    "\n",
    "# Simulation simple : retour de la réponse connue\n",
    "print(\"Réponse potentielle : \", budget_2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4opsbixf2i3",
   "source": "### Interprétation : Mémoire Vectorielle et Embeddings\n\n**Sortie obtenue** : Configuration d'un système de mémoire sémantique avec embeddings pour recherche contextuelle.\n\n| Composant | Technologie | Rôle |\n|-----------|-------------|------|\n| **Embedding Service** | OpenAI `text-embedding-3-small` | Convertit le texte en vecteurs denses (1536 dimensions) |\n| **Vector Store** | `InMemoryStore` | Stockage volatile des embeddings pour recherche rapide |\n| **Query** | \"Quel est mon budget pour 2024 ?\" | Recherche sémantique (pas keyword matching) |\n| **Résultat** | \"Budget 2024 = 100k€\" | Document le plus proche sémantiquement |\n\n**Points clés** :\n\n1. **Recherche sémantique vs. keyword** : \n   - Keyword : Cherche les mots exacts \"budget\" et \"2024\"\n   - Sémantique : Comprend l'intention et trouve les concepts liés même avec une formulation différente\n\n2. **Évolution de l'API SK** :\n   ```python\n   # ❌ Ancienne approche (dépréciée)\n   memory_store = VolatileMemoryStore()\n   memory = SemanticTextMemory(store, embedding)\n   await memory.save_information(\"collection\", \"text\", \"id\")\n   \n   # ✅ Nouvelle approche (2024+)\n   store = InMemoryStore()\n   embedding = OpenAITextEmbedding(ai_model_id=\"text-embedding-3-small\")\n   kernel.add_service(embedding)\n   ```\n\n3. **Backends vectoriels supportés** :\n   | Backend | Use case | Persistence |\n   |---------|----------|-------------|\n   | `InMemoryStore` | Dev, tests, démos | Volatile |\n   | Qdrant | Production, scalabilité | Persistant |\n   | Pinecone | Cloud, managed service | Persistant |\n   | Azure AI Search | Intégration Azure | Persistant |\n   | Chroma | Local, open-source | Persistant |\n\n4. **Coût et performance** :\n   - Embedding : ~$0.00002 per 1K tokens (text-embedding-3-small)\n   - Stockage : Dépend du backend (InMemory = gratuit, Qdrant = self-hosted possible)\n   - Latence : <100ms pour recherche dans 10K documents\n\n**Note technique** : Le notebook [05-VectorStores](05-SemanticKernel-VectorStores.ipynb) couvre l'intégration complète avec Qdrant pour des systèmes RAG (Retrieval-Augmented Generation) en production.\n\n**Cas d'usage typiques** :\n- **FAQ intelligente** : Recherche dans une base de connaissances\n- **RAG** : Enrichir les réponses LLM avec des documents métier\n- **Chatbot avec mémoire** : Retrouver des conversations passées par similarité",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "304560b0",
   "metadata": {
    "papermill": {
     "duration": 0.002505,
     "end_time": "2026-02-04T07:07:25.485304",
     "exception": false,
     "start_time": "2026-02-04T07:07:25.482799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evolution de l'API de memoire\n",
    "\n",
    "L'API de memoire vectorielle de Semantic Kernel a evolue significativement :\n",
    "\n",
    "**Ancienne approche (deprecie)** :\n",
    "- `VolatileMemoryStore` + `SemanticTextMemory`\n",
    "- Methodes `.save_information()` et `.search()`\n",
    "\n",
    "**Nouvelle approche (2024+)** :\n",
    "- `InMemoryStore` pour le stockage volatile\n",
    "- Services d'embedding dedies (`OpenAITextEmbedding`)\n",
    "- Integration directe avec les connecteurs vectoriels (Pinecone, Qdrant, Azure AI Search)\n",
    "\n",
    "Cette evolution permet une meilleure separation des responsabilites et une integration plus flexible avec divers backends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5fd7e",
   "metadata": {
    "papermill": {
     "duration": 0.002393,
     "end_time": "2026-02-04T07:07:25.490107",
     "exception": false,
     "start_time": "2026-02-04T07:07:25.487714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configuration des reponses multiples\n",
    "\n",
    "La fonctionnalite Multi-Result permet d'obtenir plusieurs completions pour un meme prompt en un seul appel API. Cela est utile pour :\n",
    "- **Generer des alternatives** : Obtenir plusieurs variations d'une reponse\n",
    "- **Evaluer la diversite** : Comparer differentes formulations\n",
    "- **Selection humaine** : Presenter plusieurs options a l'utilisateur\n",
    "\n",
    "Le parametre `number_of_responses` dans les settings controle le nombre de reponses retournees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2104769",
   "metadata": {
    "papermill": {
     "duration": 0.002728,
     "end_time": "2026-02-04T07:07:25.495276",
     "exception": false,
     "start_time": "2026-02-04T07:07:25.492548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Hugging Face Intégration\n",
    "\n",
    "Semantic Kernel peut se connecter à Hugging Face localement ou via API.  \n",
    "Exemple :  \n",
    "```python\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "\n",
    "hf_service = HuggingFaceTextCompletion(\n",
    "    service_id=\"textHF\", ai_model_id=\"distilgpt2\", task=\"text-generation\"\n",
    ")\n",
    "kernel.add_service(hf_service)\n",
    "```\n",
    "\n",
    "Ensuite, on peut enregistrer une fonction sémantique ou invoquer directement `hf_service.get_text_contents(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c6a0da",
   "metadata": {
    "papermill": {
     "duration": 0.002568,
     "end_time": "2026-02-04T07:07:25.500395",
     "exception": false,
     "start_time": "2026-02-04T07:07:25.497827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Groundedness Checking\n",
    "\n",
    "Pour éviter les “hallucinations” d’un résumé :  \n",
    "1. Extrait la liste d'entités du résumé.  \n",
    "2. Vérifie la correspondance de chaque entité avec le texte source (référence).  \n",
    "3. Retire ou corrige les entités non-fondées.\n",
    "\n",
    "Cela se fait via un plugin “GroundingPlugin” (ex. ExtraitEntities, ReferenceCheckEntities, ExciseEntities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb92467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:07:25.506889Z",
     "iopub.status.busy": "2026-02-04T07:07:25.506617Z",
     "iopub.status.idle": "2026-02-04T07:07:27.113756Z",
     "shell.execute_reply": "2026-02-04T07:07:27.112744Z"
    },
    "papermill": {
     "duration": 1.612055,
     "end_time": "2026-02-04T07:07:27.114999",
     "exception": false,
     "start_time": "2026-02-04T07:07:25.502944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entités détectées: <entities>\n",
      "- Milan\n",
      "</entities>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entités non-fondées: ```xml\n",
      "<ungrounded_entities>\n",
      "- Milan\n",
      "</ungrounded_entities>\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary corrigé: Mon budget 2024 est de 200k euros.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Groundedness Checking\n",
    "# ============================\n",
    "\n",
    "# Suppose qu'on a un \"grounding_text\" = un texte source\n",
    "grounding_text = \"\"\"\n",
    "Votre budget 2024 est de 100k euros.\n",
    "Vous vivez à Genève.\n",
    "Vous avez investi 50k en actions.\n",
    "\"\"\"\n",
    "\n",
    "# Suppose qu'on a un résumé \"faux\"\n",
    "summary_text = \"\"\"\n",
    "Mon budget 2024 est de 200k euros.\n",
    "J'habite à Milan.\n",
    "\"\"\"\n",
    "\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "\n",
    "# On appelle un plugin (hypothétique) \"GroundingPlugin\" comportant 3 fonctions\n",
    "try:\n",
    "    grounding_plugin = kernel.add_plugin(parent_directory=plugins_directory, plugin_name=\"GroundingPlugin\")\n",
    "    extract_entities = grounding_plugin[\"ExtractEntities\"]\n",
    "    check_entities = grounding_plugin[\"ReferenceCheckEntities\"]\n",
    "    excise_entities = grounding_plugin[\"ExciseEntities\"]\n",
    "    \n",
    "    # 1) Extraire entités avec les bonnes variables\n",
    "    ext_result = await kernel.invoke(\n",
    "        extract_entities, \n",
    "        KernelArguments(\n",
    "            input=summary_text,\n",
    "            topic=\"entities\",  # Variable attendue par le template\n",
    "            example_entities=\"Person, Location, Organization\",  # Variable attendue\n",
    "            allow_dangerously_set_content=True\n",
    "        )\n",
    "    )\n",
    "    print(\"Entités détectées:\", ext_result)\n",
    "    \n",
    "    # 2) Vérifier correspondance - Important: convertir ext_result en string\n",
    "    ext_result_str = str(ext_result.value) if hasattr(ext_result, 'value') else str(ext_result)\n",
    "    check_result = await kernel.invoke(\n",
    "        check_entities, \n",
    "        KernelArguments(\n",
    "            input=ext_result_str,  # Utiliser la version string\n",
    "            reference_context=grounding_text,\n",
    "            topic=\"entities\",  # Ajouter les variables attendues\n",
    "            allow_dangerously_set_content=True  # IMPORTANT: autoriser le contenu complexe\n",
    "        )\n",
    "    )\n",
    "    print(\"Entités non-fondées:\", check_result)\n",
    "    \n",
    "    # 3) Retirer entités non-fondées du summary\n",
    "    check_result_str = str(check_result.value) if hasattr(check_result, 'value') else str(check_result)\n",
    "    excision = await kernel.invoke(\n",
    "        excise_entities, \n",
    "        KernelArguments(\n",
    "            input=summary_text,\n",
    "            ungrounded_entities=check_result_str,  # Utiliser la version string\n",
    "            allow_dangerously_set_content=True\n",
    "        )\n",
    "    )\n",
    "    print(\"Summary corrigé:\", excision)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: Le plugin GroundingPlugin n'est pas disponible ou fonctionnel: {e}\")\n",
    "    print(\"Démonstration alternative:\")\n",
    "    print(f\"Texte source: {grounding_text[:50]}...\")\n",
    "    print(f\"Résumé analysé: {summary_text[:50]}...\")\n",
    "    print(\"Entités potentiellement problématiques: Milan (au lieu de Genève), 200k€ (au lieu de 100k€)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lh11ql5q20f",
   "source": "### Interprétation : Groundedness Checking (Vérification d'ancrage)\n\n**Sortie obtenue** : Le système a détecté et corrigé les hallucinations dans un résumé en comparant avec un texte source.\n\n| Étape | Résultat | Explication |\n|-------|----------|-------------|\n| **1. Extraction** | `<entities>Milan</entities>` | Entités factuelles identifiées dans le résumé |\n| **2. Vérification** | `<ungrounded_entities>Milan</ungrounded_entities>` | Comparaison avec le texte source (Genève ≠ Milan) |\n| **3. Correction** | \"Mon budget 2024 est de 200k euros.\" | Suppression de l'entité non-fondée (Milan retiré) |\n\n**Points clés** :\n\n1. **Problème des hallucinations** :\n   - Les LLMs peuvent générer des informations plausibles mais fausses\n   - Particulièrement problématique en production (santé, finance, légal)\n   - Le Groundedness Checking ancre les réponses dans des faits vérifiables\n\n2. **Architecture du GroundingPlugin** :\n   ```\n   ExtractEntities → ReferenceCheckEntities → ExciseEntities\n        ↓                      ↓                       ↓\n   Liste entités      Entités invalides      Texte corrigé\n   ```\n\n3. **Entités détectées dans l'exemple** :\n   | Entité résumé | Texte source | Statut |\n   |---------------|--------------|--------|\n   | 200k euros | 100k euros | ⚠️ Incohérence (non supprimée ici) |\n   | Milan | Genève | ❌ Hallucination (supprimée) |\n\n4. **Limites de l'approche** :\n   - Le système actuel n'a pas corrigé \"200k euros\" (devrait être \"100k euros\")\n   - Nécessite un prompt bien calibré pour détecter toutes les incohérences\n   - Fonctionne mieux avec des entités discrètes (lieux, noms) qu'avec des valeurs numériques\n\n**Note technique** : Le paramètre `allow_dangerously_set_content=True` est nécessaire pour passer du contenu structuré (XML) entre les fonctions. En production, il faut valider ce contenu pour éviter les injections.\n\n**Améliorations possibles** :\n```python\n# Vérification multi-passes\nfor i in range(3):  # Itérer jusqu'à ce qu'il n'y ait plus d'entités non-fondées\n    entities = extract_entities(summary)\n    ungrounded = check_entities(entities, source)\n    if not ungrounded:\n        break\n    summary = excise_entities(summary, ungrounded)\n```\n\n**Cas d'usage typiques** :\n- **Résumés automatiques** : Garantir que le résumé ne contient que des infos du document source\n- **Chatbots réglementés** : Assurer la conformité des réponses aux documents officiels\n- **Fact-checking** : Valider automatiquement les affirmations d'un texte",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "60fa8a80",
   "metadata": {
    "papermill": {
     "duration": 0.002967,
     "end_time": "2026-02-04T07:07:27.121156",
     "exception": false,
     "start_time": "2026-02-04T07:07:27.118189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Resultats du Groundedness Checking\n",
    "\n",
    "Le processus de verification d'ancrage comprend trois etapes :\n",
    "\n",
    "1. **Extraction** : Identification des entites factuelles dans le resume (personnes, lieux, montants)\n",
    "2. **Verification** : Comparaison de chaque entite avec le texte source de reference\n",
    "3. **Correction** : Suppression ou modification des entites non-fondees\n",
    "\n",
    "Dans l'exemple ci-dessus, les entites problematiques detectees seraient :\n",
    "- \"Milan\" (non mentionne dans le texte source, qui indique \"Geneve\")\n",
    "- \"200k euros\" (le texte source indique \"100k euros\")\n",
    "\n",
    "Cette technique est essentielle pour reduire les hallucinations des LLMs dans les systemes de production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd73fe",
   "metadata": {
    "papermill": {
     "duration": 0.002662,
     "end_time": "2026-02-04T07:07:27.126247",
     "exception": false,
     "start_time": "2026-02-04T07:07:27.123585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Multi-Result\n",
    "\n",
    "OpenAI (ou Azure) peut renvoyer plusieurs complétions pour un même prompt.  \n",
    "On paramètre `number_of_responses=3` dans les settings.  \n",
    "Ensuite, `get_text_contents(...)` ou `get_chat_message_contents(...)` renvoie un *tableau* de résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acbb3a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:07:27.133059Z",
     "iopub.status.busy": "2026-02-04T07:07:27.132861Z",
     "iopub.status.idle": "2026-02-04T07:07:28.121445Z",
     "shell.execute_reply": "2026-02-04T07:07:28.120720Z"
    },
    "papermill": {
     "duration": 0.993134,
     "end_time": "2026-02-04T07:07:28.122044",
     "exception": false,
     "start_time": "2026-02-04T07:07:27.128910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse n°1:\n",
      "\n",
      "\n",
      "Pourquoi les chats n'aiment-ils pas l'eau ?\n",
      "\n",
      "Parce qu'ils ont peur de se mouiller la moustache !\n",
      "\n",
      "Réponse n°2:\n",
      "\n",
      "\n",
      "Pourquoi les chats n'aiment-ils pas jouer au Monopoly ?\n",
      "\n",
      "Parce qu'ils détestent les banques et préfèrent garder leur argent dans leur souris !\n",
      "\n",
      "Réponse n°3:\n",
      "\n",
      "\n",
      "Pourquoi les chats n'aiment-ils pas jouer aux cartes ?\n",
      "\n",
      "Parce qu'ils préfèrent jouer avec les souris !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Multi-Result\n",
    "# ============================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextPromptExecutionSettings\n",
    "\n",
    "settings = OpenAITextPromptExecutionSettings(\n",
    "    extension_data={\n",
    "        \"max_tokens\": 60,\n",
    "        \"temperature\": 0.7,\n",
    "        \"number_of_responses\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextCompletion\n",
    "text_service = OpenAITextCompletion(service_id=\"multiResult\", ai_model_id=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "prompt = \"Donne-moi une brève blague sur les chats :\"\n",
    "\n",
    "# get_text_contents() => liste de réponses\n",
    "responses = await text_service.get_text_contents(prompt, settings=settings)\n",
    "\n",
    "for i, r in enumerate(responses):\n",
    "    print(f\"Réponse n°{i+1}:\\n{r}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w2h6gyragch",
   "source": "### Interprétation : Multi-Result Generation\n\n**Sortie obtenue** : Trois blagues différentes générées en un seul appel API avec `number_of_responses=3`.\n\n| Réponse | Contenu | Style |\n|---------|---------|-------|\n| **#1** | \"Pourquoi les chats n'aiment-ils pas l'eau ? Parce qu'ils ont peur de se mouiller la moustache !\" | Jeu de mots simple |\n| **#2** | \"Pourquoi les chats n'aiment-ils pas jouer au Monopoly ? Parce qu'ils détestent les banques et préfèrent garder leur argent dans leur souris !\" | Référence culture pop |\n| **#3** | \"Pourquoi les chats n'aiment-ils pas jouer aux cartes ? Parce qu'ils préfèrent jouer avec les souris !\" | Double sens (souris ordinateur/animal) |\n\n**Points clés** :\n\n1. **Avantages du Multi-Result** :\n   - **Efficacité** : Un seul appel API au lieu de 3 appels séquentiels\n   - **Coût** : Réduit les coûts de latence réseau et overhead API\n   - **Diversité** : Variations générées en parallèle avec la même température\n   - **UX** : Permet à l'utilisateur de choisir la meilleure réponse\n\n2. **Paramètres de configuration** :\n   ```python\n   settings = OpenAITextPromptExecutionSettings(\n       max_tokens=60,           # Limite par réponse\n       temperature=0.7,         # Créativité (0.7 = modérée)\n       number_of_responses=3    # Nombre de variantes\n   )\n   ```\n\n3. **Cas d'usage typiques** :\n   | Scénario | Nombre de réponses | Post-traitement |\n   |----------|-------------------|-----------------|\n   | **Brainstorming** | 5-10 | Sélection humaine |\n   | **A/B testing** | 2-3 | Métriques engagement |\n   | **Génération créative** | 3-5 | Vote utilisateur |\n   | **Robustesse** | 3 | Consensus majoritaire |\n\n4. **Considérations de coût** :\n   ```\n   Coût = nombre_réponses × tokens_par_réponse × prix_par_token\n   \n   Exemple (GPT-3.5-turbo) :\n   - 1 réponse : 60 tokens × $0.002/1K = $0.00012\n   - 3 réponses : 180 tokens × $0.002/1K = $0.00036\n   ```\n\n**Note technique** : `get_text_contents()` retourne une liste, contrairement à `get_text_content()` (singulier) qui retourne un seul résultat. L'API ChatCompletion a un équivalent avec `get_chat_message_contents()`.\n\n**Comparaison avec approches alternatives** :\n```python\n# ❌ Approche naïve (3 appels séquentiels)\nfor i in range(3):\n    response = await service.get_text_content(prompt, settings)\n# Problème : 3× latence réseau, 3× overhead API\n\n# ✅ Multi-Result (1 appel parallèle)\nresponses = await service.get_text_contents(prompt, settings)\n# Avantage : Latence minimale, coût optimisé\n```\n\n**Cas d'usage avancés** :\n- **Génération de variantes marketing** : Plusieurs versions d'un slogan\n- **Traduction avec nuances** : Différentes formulations d'une traduction\n- **Résumé multi-angle** : Résumés avec différents niveaux de détail\n- **Code generation** : Plusieurs implémentations d'un algorithme",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "e85817b1",
   "metadata": {
    "papermill": {
     "duration": 0.0026,
     "end_time": "2026-02-04T07:07:28.127489",
     "exception": false,
     "start_time": "2026-02-04T07:07:28.124889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion et Resume\n",
    "\n",
    "Dans ce notebook, nous avons explore les fonctionnalites avancees de Semantic Kernel :\n",
    "\n",
    "| Concept | Description | Statut |\n",
    "|---------|-------------|--------|\n",
    "| **Chat avec KernelArguments** | Gestion de l'historique et du contexte | Fondamental |\n",
    "| **Function Calling** | Orchestration automatique via `FunctionChoiceBehavior.Auto()` | **Moderne** (remplace Planners) |\n",
    "| **Memoire & Embeddings** | Stockage vectoriel avec `InMemoryStore` | **API moderne** |\n",
    "| **Hugging Face** | Integration de modeles open-source | Optionnel |\n",
    "| **Groundedness Checking** | Reduction des hallucinations | Production |\n",
    "| **Multi-Result** | Generations multiples en un appel | Creatif |\n",
    "\n",
    "### Points cles a retenir\n",
    "\n",
    "1. **Function Calling remplace les Planners** : Plus simple, plus fiable, moins de tokens\n",
    "2. **API Memory modernisee** : `InMemoryStore` + `OpenAITextEmbedding` (voir notebook 05 pour Qdrant)\n",
    "3. **Groundedness** : Essentiel pour les systemes de production\n",
    "\n",
    "### Prochaines etapes\n",
    "\n",
    "- **[03-Agents](03-SemanticKernel-Agents.ipynb)** : Agents avec ChatCompletionAgent, AgentGroupChat\n",
    "- **[04-Filters](04-SemanticKernel-Filters-Observability.ipynb)** : Intercepter et logger les appels\n",
    "- **[05-VectorStores](05-SemanticKernel-VectorStores.ipynb)** : RAG complet avec Qdrant\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation** : [<< 01-Intro](01-SemanticKernel-Intro.ipynb) | [Index](README.md) | [03-Agents >>](03-SemanticKernel-Agents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b01a8",
   "metadata": {
    "papermill": {
     "duration": 0.00262,
     "end_time": "2026-02-04T07:07:28.132578",
     "exception": false,
     "start_time": "2026-02-04T07:07:28.129958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Nous avons vu :\n",
    "1. Un **Chat** basique avec `KernelArguments`.\n",
    "2. Les **Planners** (Sequential, Stepwise) pour orchestrer dynamiquement des steps.\n",
    "3. La **Mémoire** & embeddings pour stocker/rechercher des informations sémantiques.\n",
    "4. L’**intégration Hugging Face** pour exécuter localement des modèles open-source.\n",
    "5. Un **Groundedness Checking** minimal pour éviter les hallucinations.\n",
    "6. La gestion de **plusieurs réponses** (Multi-Result) avec un seul appel.\n",
    "\n",
    "Ces fonctionnalités permettent de créer des scénarios complexes : chat évolué, question-answering avec mémoire persistante, planification automatique, usage local ou cloud, etc. Bonne exploration !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 46.728791,
   "end_time": "2026-02-04T07:07:28.594461",
   "environment_variables": {},
   "exception": null,
   "input_path": "02-SemanticKernel-Advanced.ipynb",
   "output_path": "02-SemanticKernel-Advanced.ipynb",
   "parameters": {},
   "start_time": "2026-02-04T07:06:41.865670",
   "version": "2.6.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}