{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK-4-Filters : Filtres et Observabilite\n",
    "\n",
    "**Navigation** : [<< 03-Agents](03-SemanticKernel-Agents.ipynb) | [Index](README.md) | [05-VectorStores >>](05-SemanticKernel-VectorStores.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Intercepter les appels de **fonctions** avec des filtres\n",
    "2. Modifier les **prompts** avant envoi au LLM\n",
    "3. Controler le **Function Calling automatique**\n",
    "4. Configurer le **logging** pour le debugging\n",
    "5. Comprendre l'integration **OpenTelemetry** pour le monitoring\n",
    "\n",
    "### Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Notebooks 01-03 completes\n",
    "- Cle API OpenAI configuree (`.env`)\n",
    "\n",
    "### Duree estimee : 45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Sommaire\n",
    "\n",
    "| Section | Contenu | Concepts cles |\n",
    "|---------|---------|---------------|\n",
    "| 1 | Introduction | Pourquoi les filtres ? |\n",
    "| 2 | Function Filters | Avant/apres invocation |\n",
    "| 3 | Prompt Filters | Modification du prompt |\n",
    "| 4 | Auto-Invoke Filters | Controle du function calling |\n",
    "| 5 | Logging | Configuration, niveaux |\n",
    "| 6 | OpenTelemetry | Tracing, metriques |\n",
    "| 7 | Conclusion | Resume, exercices |\n",
    "\n",
    "> **Pourquoi les filtres ?** Les filtres permettent d'intercepter et modifier les appels a tous les niveaux de SK : avant/apres les fonctions, avant/apres les prompts, et lors du function calling automatique. C'est essentiel pour le logging, la securite, et le monitoring en production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation et imports\n",
    "%pip install semantic-kernel python-dotenv --quiet\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.functions import kernel_function, KernelArguments\n",
    "from semantic_kernel.filters import FilterTypes\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction aux Filtres\n",
    "\n",
    "Semantic Kernel propose un systeme de filtres inspire des middlewares web :\n",
    "\n",
    "| Type de Filtre | Point d'interception | Cas d'usage |\n",
    "|----------------|---------------------|-------------|\n",
    "| **Function Invocation** | Avant/apres chaque fonction | Logging, validation, timing |\n",
    "| **Prompt Rendering** | Avant envoi au LLM | Injection de regles, anonymisation |\n",
    "| **Auto Function Invocation** | Lors du function calling | Rate limiting, approbation |\n",
    "\n",
    "### Architecture des filtres\n",
    "\n",
    "```\n",
    "User Request\n",
    "    |\n",
    "    v\n",
    "[Prompt Filter] --> Modifie le prompt\n",
    "    |\n",
    "    v\n",
    "LLM Call\n",
    "    |\n",
    "    v\n",
    "[Auto-Invoke Filter] --> Controle les appels de fonction\n",
    "    |\n",
    "    v\n",
    "[Function Filter] --> Avant/apres chaque fonction\n",
    "    |\n",
    "    v\n",
    "Response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function Invocation Filters\n",
    "\n",
    "Les filtres de fonction permettent d'intercepter chaque appel de fonction du kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.filters.functions.function_invocation_context import FunctionInvocationContext\n",
    "from typing import Callable, Coroutine, Any\n",
    "import time\n",
    "\n",
    "# Creation du kernel\n",
    "kernel = Kernel()\n",
    "kernel.add_service(OpenAIChatCompletion(service_id=\"default\"))\n",
    "\n",
    "# Plugin de demonstration\n",
    "class MathPlugin:\n",
    "    @kernel_function(description=\"Additionne deux nombres\")\n",
    "    def add(self, a: int, b: int) -> int:\n",
    "        return a + b\n",
    "    \n",
    "    @kernel_function(description=\"Multiplie deux nombres\")\n",
    "    def multiply(self, a: int, b: int) -> int:\n",
    "        return a * b\n",
    "\n",
    "kernel.add_plugin(MathPlugin(), plugin_name=\"math\")\n",
    "\n",
    "# Filtre de logging avec timing\n",
    "@kernel.filter(FilterTypes.FUNCTION_INVOCATION)\n",
    "async def logging_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: Callable[[FunctionInvocationContext], Coroutine[Any, Any, None]]\n",
    "):\n",
    "    \"\"\"Filtre qui log les appels de fonction avec leur duree.\"\"\"\n",
    "    func_name = f\"{context.function.plugin_name}.{context.function.name}\"\n",
    "    print(f\"[AVANT] Appel de {func_name}\")\n",
    "    print(f\"  Arguments: {context.arguments}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Appel de la fonction (ou du filtre suivant)\n",
    "    await next(context)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"[APRES] {func_name} termine en {duration:.4f}s\")\n",
    "    print(f\"  Resultat: {context.result}\")\n",
    "\n",
    "# Test du filtre\n",
    "result = await kernel.invoke(kernel.get_function(\"math\", \"add\"), KernelArguments(a=5, b=3))\n",
    "print(f\"\\nResultat final: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Function Invocation Filter\n",
    "\n",
    "Le filtre ci-dessus illustre le pattern **middleware** :\n",
    "\n",
    "1. **Avant l'appel** : On log le nom de la fonction et ses arguments\n",
    "2. **`await next(context)`** : On execute la fonction (ou le filtre suivant)\n",
    "3. **Apres l'appel** : On log le resultat et la duree\n",
    "\n",
    "**Points cles** :\n",
    "- `context.function` : Metadata de la fonction (nom, plugin, description)\n",
    "- `context.arguments` : Arguments passes a la fonction\n",
    "- `context.result` : Resultat apres execution\n",
    "- `next(context)` : Appelle le filtre suivant ou la fonction elle-meme\n",
    "\n",
    "> **Attention** : Oublier `await next(context)` bloquera l'execution de la fonction !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de filtre de validation\n",
    "@kernel.filter(FilterTypes.FUNCTION_INVOCATION)\n",
    "async def validation_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: Callable[[FunctionInvocationContext], Coroutine[Any, Any, None]]\n",
    "):\n",
    "    \"\"\"Filtre qui valide les arguments avant execution.\"\"\"\n",
    "    # Validation specifique pour les fonctions math\n",
    "    if context.function.plugin_name == \"math\":\n",
    "        a = context.arguments.get(\"a\", 0)\n",
    "        b = context.arguments.get(\"b\", 0)\n",
    "        \n",
    "        # Exemple : bloquer les nombres negatifs\n",
    "        if a < 0 or b < 0:\n",
    "            raise ValueError(f\"Les nombres negatifs ne sont pas autorises: a={a}, b={b}\")\n",
    "    \n",
    "    await next(context)\n",
    "\n",
    "# Test avec nombres valides\n",
    "try:\n",
    "    result = await kernel.invoke(kernel.get_function(\"math\", \"multiply\"), KernelArguments(a=4, b=5))\n",
    "    print(f\"Resultat: {result}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Erreur de validation: {e}\")\n",
    "\n",
    "# Test avec nombre negatif (devrait echouer)\n",
    "try:\n",
    "    result = await kernel.invoke(kernel.get_function(\"math\", \"multiply\"), KernelArguments(a=-3, b=5))\n",
    "    print(f\"Resultat: {result}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Erreur de validation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt Rendering Filters\n",
    "\n",
    "Les filtres de prompt permettent de modifier le prompt avant son envoi au LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.filters.prompts.prompt_render_context import PromptRenderContext\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "\n",
    "# Nouveau kernel pour les filtres de prompt\n",
    "kernel_prompt = Kernel()\n",
    "kernel_prompt.add_service(OpenAIChatCompletion(service_id=\"default\"))\n",
    "\n",
    "# Filtre qui ajoute des instructions de securite\n",
    "@kernel_prompt.filter(FilterTypes.PROMPT_RENDERING)\n",
    "async def security_prompt_filter(\n",
    "    context: PromptRenderContext,\n",
    "    next: Callable[[PromptRenderContext], Coroutine[Any, Any, None]]\n",
    "):\n",
    "    \"\"\"Ajoute des regles de securite au prompt.\"\"\"\n",
    "    # Executer le rendu du template d'abord\n",
    "    await next(context)\n",
    "    \n",
    "    # Ajouter des instructions de securite apres le rendu\n",
    "    security_rules = \"\"\"\n",
    "\n",
    "REGLES DE SECURITE:\n",
    "- Ne jamais reveler d'informations personnelles\n",
    "- Ne pas generer de contenu offensant\n",
    "- Refuser les demandes de code malveillant\n",
    "\"\"\"\n",
    "    context.rendered_prompt = context.rendered_prompt + security_rules\n",
    "    print(f\"[Prompt Filter] Regles de securite ajoutees\")\n",
    "\n",
    "# Creation d'une fonction avec template\n",
    "prompt_config = PromptTemplateConfig(\n",
    "    template=\"Tu es un assistant. Reponds a: {{$input}}\",\n",
    "    name=\"secure_chat\",\n",
    "    template_format=\"semantic-kernel\"\n",
    ")\n",
    "\n",
    "chat_function = kernel_prompt.add_function(\n",
    "    function_name=\"chat\",\n",
    "    plugin_name=\"demo\",\n",
    "    prompt_template_config=prompt_config\n",
    ")\n",
    "\n",
    "# Test\n",
    "response = await kernel_prompt.invoke(chat_function, KernelArguments(input=\"Bonjour, comment ca va?\"))\n",
    "print(f\"\\nReponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Prompt Filters\n",
    "\n",
    "Les filtres de prompt sont cruciaux pour :\n",
    "\n",
    "| Cas d'usage | Implementation |\n",
    "|-------------|----------------|\n",
    "| **Securite** | Ajouter des regles (comme ci-dessus) |\n",
    "| **Anonymisation** | Masquer les donnees sensibles avant envoi |\n",
    "| **Contextualisation** | Injecter du contexte metier |\n",
    "| **A/B Testing** | Varier les prompts pour experimentation |\n",
    "| **Audit** | Logger tous les prompts envoyes |\n",
    "\n",
    "**Flux d'execution** :\n",
    "```\n",
    "Template: \"Tu es un assistant. Reponds a: {{$input}}\"\n",
    "    |\n",
    "    v  (rendu du template)\n",
    "Rendered: \"Tu es un assistant. Reponds a: Bonjour...\"\n",
    "    |\n",
    "    v  (filtre de securite)\n",
    "Final: \"Tu es un assistant. Reponds a: Bonjour... [REGLES DE SECURITE]\"\n",
    "    |\n",
    "    v\n",
    "LLM\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Auto Function Invocation Filters\n",
    "\n",
    "Ces filtres controlent le comportement du function calling automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.filters.auto_function_invocation.auto_function_invocation_context import AutoFunctionInvocationContext\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "\n",
    "# Nouveau kernel pour les filtres auto-invoke\n",
    "kernel_auto = Kernel()\n",
    "kernel_auto.add_service(OpenAIChatCompletion(service_id=\"default\"))\n",
    "\n",
    "# Plugin sensible\n",
    "class DatabasePlugin:\n",
    "    @kernel_function(description=\"Execute une requete SQL\")\n",
    "    def execute_sql(self, query: str) -> str:\n",
    "        print(f\"  [DB] Execution: {query}\")\n",
    "        return f\"Resultats pour: {query}\"\n",
    "    \n",
    "    @kernel_function(description=\"Supprime des donnees\")\n",
    "    def delete_data(self, table: str) -> str:\n",
    "        print(f\"  [DB] DELETE FROM {table}\")\n",
    "        return f\"Donnees supprimees de {table}\"\n",
    "\n",
    "kernel_auto.add_plugin(DatabasePlugin(), plugin_name=\"database\")\n",
    "\n",
    "# Compteur d'appels pour rate limiting\n",
    "call_count = {\"total\": 0}\n",
    "\n",
    "@kernel_auto.filter(FilterTypes.AUTO_FUNCTION_INVOCATION)\n",
    "async def rate_limit_filter(\n",
    "    context: AutoFunctionInvocationContext,\n",
    "    next: Callable[[AutoFunctionInvocationContext], Coroutine[Any, Any, None]]\n",
    "):\n",
    "    \"\"\"Limite le nombre d'appels de fonction automatiques.\"\"\"\n",
    "    MAX_CALLS = 3\n",
    "    \n",
    "    call_count[\"total\"] += 1\n",
    "    \n",
    "    if call_count[\"total\"] > MAX_CALLS:\n",
    "        print(f\"[Rate Limit] Limite atteinte ({MAX_CALLS} appels max)\")\n",
    "        context.terminate = True  # Stoppe le function calling\n",
    "        return\n",
    "    \n",
    "    func_name = context.function.name\n",
    "    print(f\"[Auto-Invoke] Appel #{call_count['total']}: {func_name}\")\n",
    "    \n",
    "    # Bloquer les operations dangereuses\n",
    "    if \"delete\" in func_name.lower():\n",
    "        print(f\"[Auto-Invoke] BLOQUE: Operation de suppression non autorisee\")\n",
    "        context.terminate = True\n",
    "        return\n",
    "    \n",
    "    await next(context)\n",
    "\n",
    "print(\"Filtre auto-invoke configure. Voir section 5 pour un exemple complet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Auto Function Invocation Filters\n",
    "\n",
    "Ces filtres sont essentiels pour controler le **function calling automatique** du LLM :\n",
    "\n",
    "| Propriete | Description |\n",
    "|-----------|-------------|\n",
    "| `context.function` | Fonction que le LLM veut appeler |\n",
    "| `context.arguments` | Arguments proposes par le LLM |\n",
    "| `context.terminate` | Met a `True` pour arreter le function calling |\n",
    "| `context.result` | Peut etre modifie pour retourner un resultat different |\n",
    "\n",
    "**Cas d'usage** :\n",
    "- **Rate limiting** : Limiter le nombre d'appels de fonction\n",
    "- **Approbation humaine** : Demander confirmation avant actions dangereuses\n",
    "- **Blocage selectif** : Interdire certaines fonctions (DELETE, etc.)\n",
    "- **Audit** : Logger toutes les decisions du LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logging et Debugging\n",
    "\n",
    "SK utilise le module `logging` standard de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configuration du logging pour SK\n",
    "def configure_sk_logging(level=logging.INFO):\n",
    "    \"\"\"Configure le logging pour Semantic Kernel.\"\"\"\n",
    "    # Handler pour la console\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(level)\n",
    "    \n",
    "    # Format detaille\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    # Configurer le logger SK\n",
    "    sk_logger = logging.getLogger(\"semantic_kernel\")\n",
    "    sk_logger.setLevel(level)\n",
    "    sk_logger.addHandler(handler)\n",
    "    \n",
    "    return sk_logger\n",
    "\n",
    "# Niveaux de logging disponibles\n",
    "print(\"Niveaux de logging disponibles:\")\n",
    "print(\"| Niveau   | Valeur | Description                              |\")\n",
    "print(\"|----------|--------|------------------------------------------|\")\n",
    "print(\"| DEBUG    | 10     | Details tres fins (prompts complets)     |\")\n",
    "print(\"| INFO     | 20     | Informations generales                   |\")\n",
    "print(\"| WARNING  | 30     | Avertissements                           |\")\n",
    "print(\"| ERROR    | 40     | Erreurs                                  |\")\n",
    "print(\"| CRITICAL | 50     | Erreurs critiques                        |\")\n",
    "\n",
    "# Activer le logging DEBUG pour voir les details\n",
    "logger = configure_sk_logging(logging.DEBUG)\n",
    "print(\"\\nLogging configure en mode DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple avec logging actif\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "\n",
    "kernel_log = Kernel()\n",
    "kernel_log.add_service(OpenAIChatCompletion(service_id=\"default\"))\n",
    "\n",
    "# Fonction simple\n",
    "simple_config = PromptTemplateConfig(\n",
    "    template=\"Dis bonjour a {{$name}} en francais.\",\n",
    "    name=\"greet\"\n",
    ")\n",
    "\n",
    "greet_function = kernel_log.add_function(\n",
    "    function_name=\"greet\",\n",
    "    plugin_name=\"demo\",\n",
    "    prompt_template_config=simple_config\n",
    ")\n",
    "\n",
    "# Execution avec logging\n",
    "print(\"=\" * 50)\n",
    "print(\"Execution avec logging actif:\")\n",
    "print(\"=\" * 50)\n",
    "response = await kernel_log.invoke(greet_function, KernelArguments(name=\"Alice\"))\n",
    "print(f\"\\nReponse finale: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. OpenTelemetry (Apercu)\n",
    "\n",
    "Pour le monitoring en production, SK supporte OpenTelemetry.\n",
    "\n",
    "### Architecture OpenTelemetry\n",
    "\n",
    "```\n",
    "Semantic Kernel\n",
    "    |\n",
    "    v\n",
    "OpenTelemetry SDK\n",
    "    |\n",
    "    +-- Traces (appels, latence)\n",
    "    |\n",
    "    +-- Metrics (compteurs, histogrammes)\n",
    "    |\n",
    "    +-- Logs (evenements)\n",
    "    |\n",
    "    v\n",
    "Exporters\n",
    "    |\n",
    "    +-- Azure Monitor\n",
    "    +-- Jaeger\n",
    "    +-- Prometheus\n",
    "    +-- Console (debug)\n",
    "```\n",
    "\n",
    "### Exemple conceptuel (non execute)\n",
    "\n",
    "```python\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n",
    "\n",
    "# Configuration OpenTelemetry\n",
    "provider = TracerProvider()\n",
    "processor = SimpleSpanProcessor(ConsoleSpanExporter())\n",
    "provider.add_span_processor(processor)\n",
    "trace.set_tracer_provider(provider)\n",
    "\n",
    "# SK detecte automatiquement OpenTelemetry\n",
    "kernel = Kernel()\n",
    "# Les traces sont emises automatiquement pour:\n",
    "# - Chaque invocation de fonction\n",
    "# - Chaque appel au LLM\n",
    "# - Les erreurs et exceptions\n",
    "```\n",
    "\n",
    "### Metriques disponibles\n",
    "\n",
    "| Metrique | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `semantic_kernel.function.invocations` | Counter | Nombre d'invocations |\n",
    "| `semantic_kernel.function.duration` | Histogram | Duree des appels |\n",
    "| `semantic_kernel.llm.tokens` | Counter | Tokens consommes |\n",
    "| `semantic_kernel.llm.latency` | Histogram | Latence LLM |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Resume des concepts\n",
    "\n",
    "| Concept | Description | Code cle |\n",
    "|---------|-------------|----------|\n",
    "| **Function Filter** | Intercepte avant/apres fonctions | `@kernel.filter(FilterTypes.FUNCTION_INVOCATION)` |\n",
    "| **Prompt Filter** | Modifie le prompt | `@kernel.filter(FilterTypes.PROMPT_RENDERING)` |\n",
    "| **Auto-Invoke Filter** | Controle function calling | `@kernel.filter(FilterTypes.AUTO_FUNCTION_INVOCATION)` |\n",
    "| **Logging** | Debug avec logging standard | `logging.getLogger(\"semantic_kernel\")` |\n",
    "| **OpenTelemetry** | Monitoring production | Traces, metriques, logs |\n",
    "\n",
    "## Points cles a retenir\n",
    "\n",
    "1. **Les filtres suivent le pattern middleware** - `await next(context)` passe au suivant\n",
    "2. **Plusieurs filtres peuvent etre chaines** - Ordre d'enregistrement = ordre d'execution\n",
    "3. **Le contexte est mutable** - On peut modifier arguments, resultats, prompts\n",
    "4. **`terminate = True`** - Stoppe l'execution (utile pour le rate limiting)\n",
    "5. **OpenTelemetry pour la production** - Traces et metriques automatiques\n",
    "\n",
    "## Exercices suggeres\n",
    "\n",
    "1. **Filtre de cache** : Creer un filtre qui cache les resultats de fonctions\n",
    "2. **Filtre d'anonymisation** : Masquer les emails/numeros dans les prompts\n",
    "3. **Filtre d'approbation** : Demander confirmation avant les operations sensibles\n",
    "\n",
    "## Pour aller plus loin\n",
    "\n",
    "| Notebook | Contenu |\n",
    "|----------|--------|\n",
    "| [05-VectorStores](05-SemanticKernel-VectorStores.ipynb) | RAG avec Qdrant |\n",
    "| [06-ProcessFramework](06-SemanticKernel-ProcessFramework.ipynb) | Workflows orchestres |\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation** : [<< 03-Agents](03-SemanticKernel-Agents.ipynb) | [Index](README.md) | [05-VectorStores >>](05-SemanticKernel-VectorStores.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
