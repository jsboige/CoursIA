{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK-5-VectorStores : RAG avec Qdrant\n",
    "\n",
    "**Navigation** : [<< 04-Filters](04-SemanticKernel-Filters-Observability.ipynb) | [Index](README.md) | [06-ProcessFramework >>](06-SemanticKernel-ProcessFramework.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Comprendre l'architecture **Vector Store** de SK\n",
    "2. Generer des **embeddings** avec OpenAI\n",
    "3. Utiliser **InMemoryVectorStore** pour le prototypage\n",
    "4. Connecter **Qdrant** pour la production\n",
    "5. Implementer un pipeline **RAG** complet\n",
    "\n",
    "### Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Notebooks 01-04 completes\n",
    "- Cle API OpenAI configuree (`.env`)\n",
    "- Acces Qdrant (optionnel, fourni)\n",
    "\n",
    "### Duree estimee : 50 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Sommaire\n",
    "\n",
    "| Section | Contenu | Concepts cles |\n",
    "|---------|---------|---------------|\n",
    "| 1 | Introduction | Pourquoi les Vector Stores ? |\n",
    "| 2 | Architecture SK | VectorStore, Collections, Records |\n",
    "| 3 | Embeddings | OpenAITextEmbedding |\n",
    "| 4 | InMemoryVectorStore | Prototypage rapide |\n",
    "| 5 | Qdrant | Production-ready |\n",
    "| 6 | RAG Pattern | Pipeline complet |\n",
    "| 7 | Conclusion | Resume, exercices |\n",
    "\n",
    "> **Qu'est-ce qu'un Vector Store ?** Une base de donnees optimisee pour stocker et rechercher des vecteurs (embeddings). C'est la fondation du RAG (Retrieval-Augmented Generation) qui permet aux LLMs d'acceder a vos donnees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "%pip install semantic-kernel qdrant-client python-dotenv --quiet\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Dependances installees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction aux Vector Stores\n",
    "\n",
    "### Pourquoi les Vector Stores ?\n",
    "\n",
    "Les LLMs ont une limite de contexte et pas d'acces a vos donnees privees. Les Vector Stores resolvent ce probleme :\n",
    "\n",
    "```\n",
    "Documents          Embeddings          Vector Store\n",
    "┌─────────┐       ┌─────────┐         ┌─────────────┐\n",
    "│ Doc 1   │──────>│ [0.1,   │────────>│             │\n",
    "│         │       │  0.3,   │         │   Qdrant    │\n",
    "└─────────┘       │  ...]   │         │             │\n",
    "                  └─────────┘         │  ou autre   │\n",
    "                                      └─────────────┘\n",
    "                                            |\n",
    "Query: \"Qu'est-ce que X ?\"                  |\n",
    "        |                                   |\n",
    "        v                                   v\n",
    "   Embedding Query ──────> Recherche similitude\n",
    "        |                                   |\n",
    "        v                                   v\n",
    "   [0.2, 0.4, ...]         Top-K documents pertinents\n",
    "                                    |\n",
    "                                    v\n",
    "                           Contexte pour le LLM\n",
    "```\n",
    "\n",
    "### Connecteurs SK disponibles\n",
    "\n",
    "| Connecteur | Type | Cas d'usage |\n",
    "|------------|------|-------------|\n",
    "| **InMemoryVectorStore** | Local | Prototypage, tests |\n",
    "| **QdrantVectorStore** | Cloud/Self-hosted | Production |\n",
    "| **AzureAISearchVectorStore** | Azure | Enterprise |\n",
    "| **PineconeVectorStore** | Cloud | Scalabilite |\n",
    "| **RedisVectorStore** | Cache distribue | Performance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture Vector Store SK\n",
    "\n",
    "SK utilise une abstraction a trois niveaux :\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────┐\n",
    "│              VectorStore                    │\n",
    "│  (InMemory, Qdrant, Azure, Pinecone, ...)   │\n",
    "│                                             │\n",
    "│   ┌─────────────────────────────────────┐  │\n",
    "│   │    VectorStoreRecordCollection      │  │\n",
    "│   │    (equivalent d'une \"table\")       │  │\n",
    "│   │                                     │  │\n",
    "│   │   ┌─────────────────────────────┐  │  │\n",
    "│   │   │  VectorStoreRecordDefinition│  │  │\n",
    "│   │   │  (schema des records)       │  │  │\n",
    "│   │   └─────────────────────────────┘  │  │\n",
    "│   └─────────────────────────────────────┘  │\n",
    "└─────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Concepts cles\n",
    "\n",
    "| Concept | Description | Analogie SQL |\n",
    "|---------|-------------|-------------|\n",
    "| **VectorStore** | Connexion a la base | Database connection |\n",
    "| **Collection** | Groupe de records | Table |\n",
    "| **Record** | Document + embedding | Row |\n",
    "| **Key** | Identifiant unique | Primary Key |\n",
    "| **Vector** | Embedding du contenu | Colonne indexee |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generation d'Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n",
    "\n",
    "# Configuration\n",
    "kernel = Kernel()\n",
    "\n",
    "# Service d'embedding\n",
    "embedding_service = OpenAITextEmbedding(\n",
    "    service_id=\"embedding\",\n",
    "    ai_model_id=\"text-embedding-3-small\"  # Modele recommande (peu couteux, performant)\n",
    ")\n",
    "kernel.add_service(embedding_service)\n",
    "\n",
    "# Test de generation d'embedding\n",
    "test_texts = [\n",
    "    \"Semantic Kernel est un SDK pour l'IA\",\n",
    "    \"Python est un langage de programmation\"\n",
    "]\n",
    "\n",
    "embeddings = await embedding_service.generate_embeddings(test_texts)\n",
    "\n",
    "print(f\"Nombre de textes: {len(test_texts)}\")\n",
    "print(f\"Dimension des embeddings: {len(embeddings[0])}\")\n",
    "print(f\"Premier embedding (debut): {embeddings[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Embeddings\n",
    "\n",
    "Les embeddings sont des representations vectorielles du sens :\n",
    "\n",
    "| Propriete | Valeur | Signification |\n",
    "|-----------|--------|---------------|\n",
    "| **Dimension** | 1536 (text-embedding-3-small) | Complexite de la representation |\n",
    "| **Plage** | [-1, 1] | Valeurs normalisees |\n",
    "| **Similarite** | Cosinus ou dot product | Plus proche = plus similaire |\n",
    "\n",
    "**Modeles OpenAI disponibles** :\n",
    "\n",
    "| Modele | Dimension | Prix | Usage |\n",
    "|--------|-----------|------|-------|\n",
    "| `text-embedding-3-small` | 1536 | $0.02/1M tokens | Recommande |\n",
    "| `text-embedding-3-large` | 3072 | $0.13/1M tokens | Haute precision |\n",
    "| `text-embedding-ada-002` | 1536 | $0.10/1M tokens | Legacy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. InMemoryVectorStore\n",
    "\n",
    "Pour le prototypage rapide sans infrastructure externe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Annotated\n",
    "from semantic_kernel.connectors.memory.in_memory import InMemoryVectorStore\n",
    "from semantic_kernel.data import (\n",
    "    VectorStoreRecordDataField,\n",
    "    VectorStoreRecordKeyField,\n",
    "    VectorStoreRecordVectorField,\n",
    "    VectorSearchOptions,\n",
    "    vectorstoremodel\n",
    ")\n",
    "\n",
    "# Definition du schema de record\n",
    "@vectorstoremodel\n",
    "@dataclass\n",
    "class DocumentRecord:\n",
    "    \"\"\"Schema d'un document dans le vector store.\"\"\"\n",
    "    id: Annotated[str, VectorStoreRecordKeyField()]\n",
    "    content: Annotated[str, VectorStoreRecordDataField()]\n",
    "    title: Annotated[str, VectorStoreRecordDataField()]\n",
    "    embedding: Annotated[\n",
    "        list[float] | None,\n",
    "        VectorStoreRecordVectorField(\n",
    "            dimensions=1536,\n",
    "            distance_function=\"cosine\"\n",
    "        )\n",
    "    ] = None\n",
    "\n",
    "# Creation du store en memoire\n",
    "memory_store = InMemoryVectorStore()\n",
    "\n",
    "# Obtenir ou creer une collection\n",
    "collection = memory_store.get_collection(\n",
    "    collection_name=\"documents\",\n",
    "    data_model_type=DocumentRecord\n",
    ")\n",
    "\n",
    "# Creer la collection (si elle n'existe pas)\n",
    "await collection.create_collection_if_not_exists()\n",
    "\n",
    "print(\"Collection 'documents' creee\")\n",
    "print(f\"Schema: id (key), content (data), title (data), embedding (vector)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents d'exemple\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"title\": \"Introduction a Semantic Kernel\",\n",
    "        \"content\": \"Semantic Kernel est un SDK open-source de Microsoft pour integrer des LLMs dans vos applications.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"title\": \"Plugins SK\",\n",
    "        \"content\": \"Les plugins dans Semantic Kernel sont des collections de fonctions que le kernel peut invoquer.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"title\": \"Agents SK\",\n",
    "        \"content\": \"L'Agent Framework permet de creer des agents autonomes qui utilisent des plugins et collaborent entre eux.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"title\": \"RAG avec SK\",\n",
    "        \"content\": \"RAG (Retrieval-Augmented Generation) combine la recherche vectorielle avec la generation de texte par LLM.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generer les embeddings\n",
    "contents = [doc[\"content\"] for doc in documents]\n",
    "embeddings = await embedding_service.generate_embeddings(contents)\n",
    "\n",
    "# Creer les records\n",
    "records = []\n",
    "for doc, emb in zip(documents, embeddings):\n",
    "    record = DocumentRecord(\n",
    "        id=doc[\"id\"],\n",
    "        title=doc[\"title\"],\n",
    "        content=doc[\"content\"],\n",
    "        embedding=list(emb)\n",
    "    )\n",
    "    records.append(record)\n",
    "\n",
    "# Inserer dans la collection\n",
    "keys = await collection.upsert_batch(records)\n",
    "print(f\"Documents inseres: {keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recherche vectorielle\n",
    "query = \"Comment creer des agents avec Semantic Kernel ?\"\n",
    "\n",
    "# Generer l'embedding de la requete\n",
    "query_embedding = (await embedding_service.generate_embeddings([query]))[0]\n",
    "\n",
    "# Rechercher les documents similaires\n",
    "search_options = VectorSearchOptions(\n",
    "    vector_field_name=\"embedding\",\n",
    "    top=3,\n",
    "    include_vectors=False\n",
    ")\n",
    "\n",
    "results = await collection.vectorized_search(\n",
    "    vector=list(query_embedding),\n",
    "    options=search_options\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"\\nResultats:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "async for result in results.results:\n",
    "    print(f\"Score: {result.score:.4f}\")\n",
    "    print(f\"Title: {result.record.title}\")\n",
    "    print(f\"Content: {result.record.content}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Recherche Vectorielle\n",
    "\n",
    "La recherche vectorielle retourne les documents les plus proches semantiquement :\n",
    "\n",
    "| Parametre | Description | Valeur typique |\n",
    "|-----------|-------------|----------------|\n",
    "| **top** | Nombre de resultats | 3-10 |\n",
    "| **score** | Similarite (0-1 pour cosinus) | > 0.7 = bon match |\n",
    "| **distance_function** | Mesure de distance | cosine, dotproduct, euclidean |\n",
    "\n",
    "**Points cles** :\n",
    "- Le document sur les \"Agents SK\" a le meilleur score car semantiquement lie a la question\n",
    "- Le score indique la pertinence (plus haut = plus pertinent)\n",
    "- Les resultats sont tries par score decroissant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Qdrant (Production)\n",
    "\n",
    "Qdrant est un vector store production-ready. Nous avons une instance disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.memory.qdrant import QdrantStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Configuration Qdrant (depuis .env ou valeurs fournies)\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"https://qdrant.myia.io\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\", \"4f89edd5-90f7-4ee0-ac25-9185e9835c44\")\n",
    "\n",
    "# Connexion au client Qdrant\n",
    "qdrant_client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY\n",
    ")\n",
    "\n",
    "# Verification de la connexion\n",
    "try:\n",
    "    collections = qdrant_client.get_collections()\n",
    "    print(f\"Connexion Qdrant reussie !\")\n",
    "    print(f\"Collections existantes: {[c.name for c in collections.collections]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur de connexion: {e}\")\n",
    "    print(\"Continuez avec InMemoryVectorStore pour les exemples suivants.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation du store Qdrant via SK\n",
    "try:\n",
    "    qdrant_store = QdrantStore(\n",
    "        url=QDRANT_URL,\n",
    "        api_key=QDRANT_API_KEY\n",
    "    )\n",
    "    \n",
    "    # Collection Qdrant\n",
    "    qdrant_collection = qdrant_store.get_collection(\n",
    "        collection_name=\"sk_demo\",\n",
    "        data_model_type=DocumentRecord\n",
    "    )\n",
    "    \n",
    "    await qdrant_collection.create_collection_if_not_exists()\n",
    "    \n",
    "    # Inserer les memes documents\n",
    "    keys = await qdrant_collection.upsert_batch(records)\n",
    "    print(f\"Documents inseres dans Qdrant: {keys}\")\n",
    "    \n",
    "    # Recherche\n",
    "    qdrant_results = await qdrant_collection.vectorized_search(\n",
    "        vector=list(query_embedding),\n",
    "        options=search_options\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRecherche Qdrant pour: '{query}'\")\n",
    "    async for result in qdrant_results.results:\n",
    "        print(f\"  {result.score:.4f} - {result.record.title}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Qdrant non disponible: {e}\")\n",
    "    print(\"Les exemples utilisent InMemoryVectorStore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : InMemory vs Qdrant\n",
    "\n",
    "| Caracteristique | InMemoryVectorStore | Qdrant |\n",
    "|-----------------|---------------------|--------|\n",
    "| **Persistance** | Non (RAM seulement) | Oui (disque/cloud) |\n",
    "| **Scalabilite** | ~10K documents | Millions de documents |\n",
    "| **Performance** | Rapide (petits datasets) | Optimise (HNSW index) |\n",
    "| **Infrastructure** | Aucune | Serveur/Cloud |\n",
    "| **Usage** | Dev/Test | Production |\n",
    "\n",
    "**Qdrant specifiques** :\n",
    "- Index HNSW pour recherche rapide\n",
    "- Filtres sur metadonnees\n",
    "- Sharding pour scalabilite horizontale\n",
    "- API REST et gRPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG Pattern Complet\n",
    "\n",
    "Assemblons tout pour un pipeline RAG fonctionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "# Ajouter le service de chat\n",
    "kernel.add_service(OpenAIChatCompletion(service_id=\"chat\"))\n",
    "\n",
    "async def rag_query(question: str, collection, embedding_service, kernel, top_k: int = 3):\n",
    "    \"\"\"Pipeline RAG complet.\"\"\"\n",
    "    \n",
    "    # 1. Generer l'embedding de la question\n",
    "    query_embedding = (await embedding_service.generate_embeddings([question]))[0]\n",
    "    \n",
    "    # 2. Rechercher les documents pertinents\n",
    "    search_options = VectorSearchOptions(\n",
    "        vector_field_name=\"embedding\",\n",
    "        top=top_k,\n",
    "        include_vectors=False\n",
    "    )\n",
    "    \n",
    "    results = await collection.vectorized_search(\n",
    "        vector=list(query_embedding),\n",
    "        options=search_options\n",
    "    )\n",
    "    \n",
    "    # 3. Construire le contexte\n",
    "    context_parts = []\n",
    "    async for result in results.results:\n",
    "        context_parts.append(f\"- {result.record.title}: {result.record.content}\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # 4. Construire le prompt augmente\n",
    "    augmented_prompt = f\"\"\"Tu es un assistant qui repond en utilisant uniquement le contexte fourni.\n",
    "    \n",
    "CONTEXTE:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "REPONSE (basee uniquement sur le contexte):\"\"\"\n",
    "    \n",
    "    # 5. Appeler le LLM\n",
    "    chat_service = kernel.get_service(service_id=\"chat\")\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(augmented_prompt)\n",
    "    \n",
    "    response = await chat_service.get_chat_message_contents(\n",
    "        chat_history=history\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"context\": context,\n",
    "        \"answer\": str(response[0])\n",
    "    }\n",
    "\n",
    "# Test du pipeline RAG\n",
    "result = await rag_query(\n",
    "    question=\"Comment les agents SK peuvent-ils utiliser des plugins ?\",\n",
    "    collection=collection,\n",
    "    embedding_service=embedding_service,\n",
    "    kernel=kernel\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nContexte utilise:\\n{result['context']}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nReponse:\\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Pipeline RAG\n",
    "\n",
    "Le pipeline RAG suit ces etapes :\n",
    "\n",
    "```\n",
    "1. EMBEDDING          2. SEARCH           3. AUGMENT         4. GENERATE\n",
    "┌─────────────┐     ┌─────────────┐     ┌─────────────┐    ┌─────────────┐\n",
    "│  Question   │────>│  Vector     │────>│  Prompt +   │───>│    LLM      │\n",
    "│  -> Vector  │     │  Search     │     │  Context    │    │  Response   │\n",
    "└─────────────┘     └─────────────┘     └─────────────┘    └─────────────┘\n",
    "```\n",
    "\n",
    "**Avantages du RAG** :\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **Donnees privees** | Le LLM peut acceder a vos documents |\n",
    "| **Actualite** | Pas besoin de re-entrainer le modele |\n",
    "| **Precision** | Reponses basees sur des sources |\n",
    "| **Tracabilite** | On sait d'ou vient l'information |\n",
    "| **Cout** | Moins cher que le fine-tuning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Resume des concepts\n",
    "\n",
    "| Concept | Description | Code cle |\n",
    "|---------|-------------|----------|\n",
    "| **VectorStore** | Abstraction de base | `InMemoryVectorStore()`, `QdrantStore()` |\n",
    "| **Collection** | Groupe de records | `store.get_collection(name, type)` |\n",
    "| **Record** | Document + embedding | `@vectorstoremodel @dataclass` |\n",
    "| **Embedding** | Vecteur semantique | `OpenAITextEmbedding.generate_embeddings()` |\n",
    "| **Search** | Recherche similitude | `collection.vectorized_search(vector, options)` |\n",
    "| **RAG** | Retrieval-Augmented Generation | Contexte + LLM |\n",
    "\n",
    "## Points cles a retenir\n",
    "\n",
    "1. **InMemory pour dev, Qdrant pour prod** - Meme API, backend different\n",
    "2. **Les embeddings capturent le sens** - Pas juste les mots-cles\n",
    "3. **RAG = Search + Generate** - Contexte pertinent pour le LLM\n",
    "4. **Le chunking est crucial** - Decouper les longs documents\n",
    "5. **Les metadonnees enrichissent** - Filtres et contexte additionnel\n",
    "\n",
    "## Exercices suggeres\n",
    "\n",
    "1. **RAG sur PDF** : Ingerer un PDF et poser des questions\n",
    "2. **Filtres** : Ajouter des filtres sur les metadonnees (date, auteur)\n",
    "3. **Evaluation** : Mesurer la qualite des reponses RAG\n",
    "\n",
    "## Pour aller plus loin\n",
    "\n",
    "| Notebook | Contenu |\n",
    "|----------|--------|\n",
    "| [06-ProcessFramework](06-SemanticKernel-ProcessFramework.ipynb) | Workflows orchestres |\n",
    "| [07-MultiModal](07-SemanticKernel-MultiModal.ipynb) | Images et audio |\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation** : [<< 04-Filters](04-SemanticKernel-Filters-Observability.ipynb) | [Index](README.md) | [06-ProcessFramework >>](06-SemanticKernel-ProcessFramework.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
