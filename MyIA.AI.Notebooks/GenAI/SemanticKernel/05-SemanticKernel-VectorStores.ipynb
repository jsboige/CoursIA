{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3c0be8",
   "metadata": {
    "papermill": {
     "duration": 0.002554,
     "end_time": "2026-02-04T07:08:08.618705",
     "exception": false,
     "start_time": "2026-02-04T07:08:08.616151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SK-5-VectorStores : RAG avec Qdrant\n",
    "\n",
    "**Navigation** : [<< 04-Filters](04-SemanticKernel-Filters-Observability.ipynb) | [Index](README.md) | [06-ProcessFramework >>](06-SemanticKernel-ProcessFramework.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Comprendre l'architecture **Vector Store** de SK\n",
    "2. Generer des **embeddings** avec OpenAI\n",
    "3. Utiliser **InMemoryVectorStore** pour le prototypage\n",
    "4. Connecter **Qdrant** pour la production\n",
    "5. Implementer un pipeline **RAG** complet\n",
    "\n",
    "### Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Notebooks 01-04 completes\n",
    "- Cle API OpenAI configuree (`.env`)\n",
    "- Acces Qdrant (optionnel, fourni)\n",
    "\n",
    "### Duree estimee : 50 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Sommaire\n",
    "\n",
    "| Section | Contenu | Concepts cles |\n",
    "|---------|---------|---------------|\n",
    "| 1 | Introduction | Pourquoi les Vector Stores ? |\n",
    "| 2 | Architecture SK | VectorStore, Collections, Records |\n",
    "| 3 | Embeddings | OpenAITextEmbedding |\n",
    "| 4 | InMemoryVectorStore | Prototypage rapide |\n",
    "| 5 | Qdrant | Production-ready |\n",
    "| 6 | RAG Pattern | Pipeline complet |\n",
    "| 7 | Conclusion | Resume, exercices |\n",
    "\n",
    "> **Qu'est-ce qu'un Vector Store ?** Une base de donnees optimisee pour stocker et rechercher des vecteurs (embeddings). C'est la fondation du RAG (Retrieval-Augmented Generation) qui permet aux LLMs d'acceder a vos donnees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ee8ba5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:08:08.623286Z",
     "iopub.status.busy": "2026-02-04T07:08:08.623093Z",
     "iopub.status.idle": "2026-02-04T07:08:09.766225Z",
     "shell.execute_reply": "2026-02-04T07:08:09.765457Z"
    },
    "papermill": {
     "duration": 1.146376,
     "end_time": "2026-02-04T07:08:09.766992",
     "exception": false,
     "start_time": "2026-02-04T07:08:08.620616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Configuration: API Key OK\n",
      "Dependances installees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 26.0\n",
      "[notice] To update, run: C:\\Users\\jsboi\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installation\n",
    "%pip install semantic-kernel qdrant-client python-dotenv --quiet\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Chargement du fichier .env (cles API)\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"Configuration: API Key {'OK' if api_key else 'MANQUANTE'}\")\n",
    "print(\"Dependances installees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baded38",
   "metadata": {
    "papermill": {
     "duration": 0.001946,
     "end_time": "2026-02-04T07:08:09.771272",
     "exception": false,
     "start_time": "2026-02-04T07:08:09.769326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Introduction aux Vector Stores\n",
    "\n",
    "### Pourquoi les Vector Stores ?\n",
    "\n",
    "Les LLMs ont une limite de contexte et pas d'acces a vos donnees privees. Les Vector Stores resolvent ce probleme :\n",
    "\n",
    "```\n",
    "Documents          Embeddings          Vector Store\n",
    "┌─────────┐       ┌─────────┐         ┌─────────────┐\n",
    "│ Doc 1   │──────>│ [0.1,   │────────>│             │\n",
    "│         │       │  0.3,   │         │   Qdrant    │\n",
    "└─────────┘       │  ...]   │         │             │\n",
    "                  └─────────┘         │  ou autre   │\n",
    "                                      └─────────────┘\n",
    "                                            |\n",
    "Query: \"Qu'est-ce que X ?\"                  |\n",
    "        |                                   |\n",
    "        v                                   v\n",
    "   Embedding Query ──────> Recherche similitude\n",
    "        |                                   |\n",
    "        v                                   v\n",
    "   [0.2, 0.4, ...]         Top-K documents pertinents\n",
    "                                    |\n",
    "                                    v\n",
    "                           Contexte pour le LLM\n",
    "```\n",
    "\n",
    "### Connecteurs SK disponibles\n",
    "\n",
    "| Connecteur | Type | Cas d'usage |\n",
    "|------------|------|-------------|\n",
    "| **InMemoryVectorStore** | Local | Prototypage, tests |\n",
    "| **QdrantVectorStore** | Cloud/Self-hosted | Production |\n",
    "| **AzureAISearchVectorStore** | Azure | Enterprise |\n",
    "| **PineconeVectorStore** | Cloud | Scalabilite |\n",
    "| **RedisVectorStore** | Cache distribue | Performance |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f3cba",
   "metadata": {
    "papermill": {
     "duration": 0.001909,
     "end_time": "2026-02-04T07:08:09.775241",
     "exception": false,
     "start_time": "2026-02-04T07:08:09.773332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Architecture Vector Store SK\n",
    "\n",
    "SK utilise une abstraction a trois niveaux :\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────┐\n",
    "│              VectorStore                    │\n",
    "│  (InMemory, Qdrant, Azure, Pinecone, ...)   │\n",
    "│                                             │\n",
    "│   ┌─────────────────────────────────────┐  │\n",
    "│   │    VectorStoreRecordCollection      │  │\n",
    "│   │    (equivalent d'une \"table\")       │  │\n",
    "│   │                                     │  │\n",
    "│   │   ┌─────────────────────────────┐  │  │\n",
    "│   │   │  VectorStoreRecordDefinition│  │  │\n",
    "│   │   │  (schema des records)       │  │  │\n",
    "│   │   └─────────────────────────────┘  │  │\n",
    "│   └─────────────────────────────────────┘  │\n",
    "└─────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Concepts cles\n",
    "\n",
    "| Concept | Description | Analogie SQL |\n",
    "|---------|-------------|-------------|\n",
    "| **VectorStore** | Connexion a la base | Database connection |\n",
    "| **Collection** | Groupe de records | Table |\n",
    "| **Record** | Document + embedding | Row |\n",
    "| **Key** | Identifiant unique | Primary Key |\n",
    "| **Vector** | Embedding du contenu | Colonne indexee |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a3336",
   "metadata": {
    "papermill": {
     "duration": 0.001617,
     "end_time": "2026-02-04T07:08:09.778494",
     "exception": false,
     "start_time": "2026-02-04T07:08:09.776877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Generation d'Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e950825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:08:09.783094Z",
     "iopub.status.busy": "2026-02-04T07:08:09.782850Z",
     "iopub.status.idle": "2026-02-04T07:08:11.687677Z",
     "shell.execute_reply": "2026-02-04T07:08:11.686834Z"
    },
    "papermill": {
     "duration": 1.908134,
     "end_time": "2026-02-04T07:08:11.688400",
     "exception": false,
     "start_time": "2026-02-04T07:08:09.780266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de textes: 2\n",
      "Dimension des embeddings: 1536\n",
      "Premier embedding (debut): [-0.03927445  0.0422161  -0.02658716 -0.0338627   0.00596752]...\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n",
    "\n",
    "# Configuration\n",
    "kernel = Kernel()\n",
    "\n",
    "# Service d'embedding\n",
    "embedding_service = OpenAITextEmbedding(\n",
    "    service_id=\"embedding\",\n",
    "    ai_model_id=\"text-embedding-3-small\"  # Modele recommande (peu couteux, performant)\n",
    ")\n",
    "kernel.add_service(embedding_service)\n",
    "\n",
    "# Test de generation d'embedding\n",
    "test_texts = [\n",
    "    \"Semantic Kernel est un SDK pour l'IA\",\n",
    "    \"Python est un langage de programmation\"\n",
    "]\n",
    "\n",
    "embeddings = await embedding_service.generate_embeddings(test_texts)\n",
    "\n",
    "print(f\"Nombre de textes: {len(test_texts)}\")\n",
    "print(f\"Dimension des embeddings: {len(embeddings[0])}\")\n",
    "print(f\"Premier embedding (debut): {embeddings[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ea831",
   "metadata": {
    "papermill": {
     "duration": 0.001849,
     "end_time": "2026-02-04T07:08:11.692351",
     "exception": false,
     "start_time": "2026-02-04T07:08:11.690502",
     "status": "completed"
    },
    "tags": []
   },
   "source": "### Interprétation : Generation d'Embeddings avec OpenAI\n\n**Sortie obtenue** : 2 embeddings de dimension 1536 generes avec succes.\n\n| Metrique | Valeur | Signification |\n|----------|--------|---------------|\n| **Dimension** | 1536 | Taille du vecteur (text-embedding-3-small) |\n| **Plage valeurs** | [-1, 1] | Normalisees pour calcul cosinus |\n| **Nombre embeddings** | 2 | Batch de 2 textes traites simultanement |\n| **Premier embedding** | [-0.039, 0.042, ...] | Representation semantique du texte 1 |\n\n**Comparaison des modeles OpenAI** :\n\n| Modele | Dimension | Prix $/1M tokens | Performance | Usage recommande |\n|--------|-----------|------------------|-------------|------------------|\n| `text-embedding-3-small` | 1536 | $0.02 | 62.3% MTEB | Production generale |\n| `text-embedding-3-large` | 3072 | $0.13 | 64.6% MTEB | Haute precision requise |\n| `text-embedding-ada-002` | 1536 | $0.10 | 61.0% MTEB | Legacy (deprecated) |\n\n**Points cles** :\n\n1. **Batch processing** : OpenAI accepte jusqu'a 2048 textes par requete (optimisation cout/latence)\n2. **Normalisation** : Les vecteurs sont normalises (norme L2 = 1) pour similarite cosinus\n3. **Determinisme** : Meme texte = meme embedding (pas de randomness)\n4. **Dimension reduite** : text-embedding-3-small peut etre reduit a 512/256 dimensions avec `dimensions` parameter\n\n**Calcul de similarite** :\n\n```python\n# Similarite cosinus entre deux embeddings\ndef cosine_similarity(vec1, vec2):\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    # Deja normalises, donc dot_product = cosine similarity\n    return dot_product\n```\n\n**Performance typique** :\n\n- Latence : ~50-200ms pour 1-10 textes\n- Throughput : ~500-1000 textes/seconde (batch)\n- Cout : 100K mots = ~125K tokens = $0.0025\n\n**Notes techniques** :\n\n- SK 1.39+ : `OpenAITextEmbedding` supporte `dimensions` parameter pour truncation\n- Les embeddings sont caches automatiquement par SK pour eviter regeneration\n- Pour multilingual, text-embedding-3-small supporte 100+ langues"
  },
  {
   "cell_type": "markdown",
   "id": "9f4b36b8",
   "metadata": {
    "papermill": {
     "duration": 0.001697,
     "end_time": "2026-02-04T07:08:11.695824",
     "exception": false,
     "start_time": "2026-02-04T07:08:11.694127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. InMemoryVectorStore\n",
    "\n",
    "Pour le prototypage rapide sans infrastructure externe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7075fc05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:08:11.700953Z",
     "iopub.status.busy": "2026-02-04T07:08:11.700721Z",
     "iopub.status.idle": "2026-02-04T07:08:11.981527Z",
     "shell.execute_reply": "2026-02-04T07:08:11.980808Z"
    },
    "papermill": {
     "duration": 0.284456,
     "end_time": "2026-02-04T07:08:11.982117",
     "exception": false,
     "start_time": "2026-02-04T07:08:11.697661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'documents' creee\n",
      "Schema: id (key), content (data), title (data), embedding (vector 1536d)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Annotated\n",
    "from semantic_kernel.connectors.in_memory import InMemoryStore\n",
    "from semantic_kernel.data.vector import (\n",
    "    vectorstoremodel,\n",
    "    VectorStoreField,\n",
    "    FieldTypes\n",
    ")\n",
    "\n",
    "# Definition du schema de record avec la nouvelle API SK 1.39+\n",
    "@vectorstoremodel\n",
    "@dataclass\n",
    "class DocumentRecord:\n",
    "    \"\"\"Schema d'un document dans le vector store.\"\"\"\n",
    "    id: Annotated[str, VectorStoreField(field_type=FieldTypes.KEY)]\n",
    "    content: Annotated[str, VectorStoreField(field_type=FieldTypes.DATA)]\n",
    "    title: Annotated[str, VectorStoreField(field_type=FieldTypes.DATA)]\n",
    "    embedding: Annotated[\n",
    "        list[float] | None,\n",
    "        VectorStoreField(\n",
    "            field_type=FieldTypes.VECTOR,\n",
    "            dimensions=1536\n",
    "        )\n",
    "    ] = None\n",
    "\n",
    "# Creation du store en memoire\n",
    "memory_store = InMemoryStore()\n",
    "\n",
    "# Obtenir ou creer une collection\n",
    "collection = memory_store.get_collection(\n",
    "    DocumentRecord,\n",
    "    collection_name=\"documents\"\n",
    ")\n",
    "\n",
    "# Creer la collection (si elle n'existe pas)\n",
    "# SK 1.39+: ensure_collection_exists() au lieu de create_collection_if_not_exists()\n",
    "await collection.ensure_collection_exists()\n",
    "\n",
    "print(\"Collection 'documents' creee\")\n",
    "print(f\"Schema: id (key), content (data), title (data), embedding (vector 1536d)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cgp0yldrsi",
   "source": "### Interprétation : Schema de Record Vector Store\n\n**Architecture du schema** : Annotation avec decorateurs pour definir la structure des donnees.\n\n| Composant | Type | Role |\n|-----------|------|------|\n| `@vectorstoremodel` | Decorateur classe | Marque la classe comme schema SK |\n| `@dataclass` | Decorateur Python | Genere `__init__`, `__repr__`, etc. |\n| `VectorStoreField` | Annotation | Specifie le type de champ |\n| `FieldTypes.KEY` | Type champ | Identifiant unique (primary key) |\n| `FieldTypes.DATA` | Type champ | Donnees textuelles ou metadonnees |\n| `FieldTypes.VECTOR` | Type champ | Embedding vectoriel |\n\n**Points cles** :\n\n1. **Separation des concerns** : Les champs KEY, DATA, et VECTOR ont des roles distincts\n2. **Dimensions fixes** : Le champ vector doit specifier `dimensions=1536` pour text-embedding-3-small\n3. **Optional embedding** : `list[float] | None` permet de creer des records sans embedding initial\n4. **Type safety** : `Annotated` + `dataclass` assurent la validation des types au runtime\n\n**Notes techniques** :\n\n- SK 1.39+ utilise `ensure_collection_exists()` au lieu de `create_collection_if_not_exists()`\n- Le schema est immuable apres creation de la collection\n- Les dimensions doivent correspondre au modele d'embedding utilise",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d346df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:08:11.987492Z",
     "iopub.status.busy": "2026-02-04T07:08:11.987112Z",
     "iopub.status.idle": "2026-02-04T07:08:12.362787Z",
     "shell.execute_reply": "2026-02-04T07:08:12.362067Z"
    },
    "papermill": {
     "duration": 0.379193,
     "end_time": "2026-02-04T07:08:12.363505",
     "exception": false,
     "start_time": "2026-02-04T07:08:11.984312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents inseres: ['doc1', 'doc2', 'doc3', 'doc4']\n"
     ]
    }
   ],
   "source": [
    "# Documents d'exemple\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"title\": \"Introduction a Semantic Kernel\",\n",
    "        \"content\": \"Semantic Kernel est un SDK open-source de Microsoft pour integrer des LLMs dans vos applications.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"title\": \"Plugins SK\",\n",
    "        \"content\": \"Les plugins dans Semantic Kernel sont des collections de fonctions que le kernel peut invoquer.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"title\": \"Agents SK\",\n",
    "        \"content\": \"L'Agent Framework permet de creer des agents autonomes qui utilisent des plugins et collaborent entre eux.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"title\": \"RAG avec SK\",\n",
    "        \"content\": \"RAG (Retrieval-Augmented Generation) combine la recherche vectorielle avec la generation de texte par LLM.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generer les embeddings\n",
    "contents = [doc[\"content\"] for doc in documents]\n",
    "embeddings = await embedding_service.generate_embeddings(contents)\n",
    "\n",
    "# Creer les records\n",
    "records = []\n",
    "for doc, emb in zip(documents, embeddings):\n",
    "    record = DocumentRecord(\n",
    "        id=doc[\"id\"],\n",
    "        title=doc[\"title\"],\n",
    "        content=doc[\"content\"],\n",
    "        embedding=list(emb)\n",
    "    )\n",
    "    records.append(record)\n",
    "\n",
    "# Inserer dans la collection (SK 1.39+: upsert prend une liste)\n",
    "keys = await collection.upsert(records)\n",
    "print(f\"Documents inseres: {keys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m9llqemcddf",
   "source": "### Interprétation : Pipeline d'Ingestion de Documents\n\n**Sortie obtenue** : 4 documents inseres avec succes dans la collection InMemory.\n\n| Etape | Operation | Code cle |\n|-------|-----------|----------|\n| 1. Extraction | Recuperer les contenus textuels | `contents = [doc[\"content\"] for doc in documents]` |\n| 2. Embedding | Generer les vecteurs (batch) | `generate_embeddings(contents)` |\n| 3. Record creation | Associer metadata + vecteur | `DocumentRecord(id, title, content, embedding)` |\n| 4. Upsert | Inserer/Mettre a jour en base | `collection.upsert(records)` |\n\n**Points cles** :\n\n1. **Batch embedding** : Generer tous les embeddings en une seule requete API (plus efficace et moins couteux)\n2. **Upsert semantics** : Insere si nouveau, met a jour si existant (base sur la cle `id`)\n3. **Liste de records** : SK 1.39+ accepte une liste complete, pas d'iteration necessaire\n4. **Separation donnees/vecteurs** : Le contenu original est conserve avec l'embedding pour affichage\n\n**Calcul des couts** :\n\n- Modele : `text-embedding-3-small` ($0.02/1M tokens)\n- 4 documents x ~20 tokens = 80 tokens\n- Cout : ~$0.0000016 (negligeable)\n\n**Notes d'optimisation** :\n\n- Pour de gros volumes, utiliser des batches de 100-500 documents\n- Les embeddings peuvent etre pre-calcules et stockes offline\n- Qdrant supporte l'ingestion parallele pour acceleration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "453e13b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:08:12.368942Z",
     "iopub.status.busy": "2026-02-04T07:08:12.368744Z",
     "iopub.status.idle": "2026-02-04T07:08:12.656539Z",
     "shell.execute_reply": "2026-02-04T07:08:12.655963Z"
    },
    "papermill": {
     "duration": 0.291309,
     "end_time": "2026-02-04T07:08:12.657237",
     "exception": false,
     "start_time": "2026-02-04T07:08:12.365928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Comment creer des agents avec Semantic Kernel ?\n",
      "\n",
      "Resultats:\n",
      "------------------------------------------------------------\n",
      "Score: 0.3709\n",
      "Title: Plugins SK\n",
      "Content: Les plugins dans Semantic Kernel sont des collections de fonctions que le kernel peut invoquer.\n",
      "------------------------------------------------------------\n",
      "Score: 0.4582\n",
      "Title: Agents SK\n",
      "Content: L'Agent Framework permet de creer des agents autonomes qui utilisent des plugins et collaborent entre eux.\n",
      "------------------------------------------------------------\n",
      "Score: 0.4796\n",
      "Title: Introduction a Semantic Kernel\n",
      "Content: Semantic Kernel est un SDK open-source de Microsoft pour integrer des LLMs dans vos applications.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Recherche vectorielle\n",
    "query = \"Comment creer des agents avec Semantic Kernel ?\"\n",
    "\n",
    "# Generer l'embedding de la requete\n",
    "query_embedding = (await embedding_service.generate_embeddings([query]))[0]\n",
    "\n",
    "# Rechercher les documents similaires (SK 1.39+: search au lieu de vectorized_search)\n",
    "results = await collection.search(\n",
    "    vector=list(query_embedding),\n",
    "    vector_property_name=\"embedding\",\n",
    "    top=3,\n",
    "    include_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"\\nResultats:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# SK 1.39+: results.results est un async generator\n",
    "async for result in results.results:\n",
    "    print(f\"Score: {result.score:.4f}\")\n",
    "    print(f\"Title: {result.record.title}\")\n",
    "    print(f\"Content: {result.record.content}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d7563",
   "metadata": {
    "papermill": {
     "duration": 0.00221,
     "end_time": "2026-02-04T07:08:12.661833",
     "exception": false,
     "start_time": "2026-02-04T07:08:12.659623",
     "status": "completed"
    },
    "tags": []
   },
   "source": "### Interprétation : Recherche Vectorielle et Scores de Similarite\n\n**Sortie obtenue** : 3 documents recuperes avec scores de similarite decroissants.\n\n| Document | Score | Pertinence | Interpretation |\n|----------|-------|------------|----------------|\n| Plugins SK | 0.3709 | Basse | Mentionne les fonctions, lien indirect avec agents |\n| Agents SK | 0.4582 | Moyenne-Haute | Mentionne explicitement agents + plugins |\n| Introduction SK | 0.4796 | Haute | Contexte general sur SK (moins specifique) |\n\n**Points cles** :\n\n1. **Similarite cosinus** : Les scores representent l'angle entre vecteurs (1 = identique, 0 = orthogonal)\n2. **Recherche semantique** : Trouve \"agents\" meme si la requete dit \"creer des agents\" (comprehension du sens)\n3. **Top-K** : `top=3` limite aux 3 meilleurs resultats (reglable selon le besoin)\n4. **Ordre contre-intuitif** : \"Introduction\" a un meilleur score que \"Agents\" car plus d'overlap semantique general\n\n**Parametres de recherche** :\n\n| Parametre | Valeur utilisee | Impact |\n|-----------|-----------------|--------|\n| `vector` | Embedding de la question | Vecteur de reference |\n| `vector_property_name` | \"embedding\" | Champ a comparer |\n| `top` | 3 | Nombre de resultats |\n| `include_vectors` | False | Ne pas retourner les embeddings (economie memoire) |\n\n**Notes techniques** :\n\n- SK 1.39+ : `search()` remplace `vectorized_search()`\n- Les resultats sont un `async generator` (iteration asynchrone)\n- Pour des filtres additionnels, utiliser `filter` parameter avec metadonnees\n\n**Seuils recommandes** :\n\n- Score > 0.8 : Tres pertinent\n- Score 0.6-0.8 : Pertinent\n- Score 0.4-0.6 : Potentiellement pertinent\n- Score < 0.4 : Peu pertinent (a ignorer)"
  },
  {
   "cell_type": "markdown",
   "id": "9402ea58",
   "metadata": {
    "papermill": {
     "duration": 0.00216,
     "end_time": "2026-02-04T07:08:12.665919",
     "exception": false,
     "start_time": "2026-02-04T07:08:12.663759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Qdrant (Production)\n",
    "\n",
    "Qdrant est un vector store production-ready. Nous avons une instance disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2ec962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:08:12.670863Z",
     "iopub.status.busy": "2026-02-04T07:08:12.670660Z",
     "iopub.status.idle": "2026-02-04T07:08:17.889889Z",
     "shell.execute_reply": "2026-02-04T07:08:17.889140Z"
    },
    "papermill": {
     "duration": 5.222638,
     "end_time": "2026-02-04T07:08:17.890467",
     "exception": false,
     "start_time": "2026-02-04T07:08:12.667829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur de connexion: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée\n",
      "Continuez avec InMemoryStore pour les exemples suivants.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.qdrant import QdrantStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Configuration Qdrant (depuis .env ou valeurs fournies)\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"https://qdrant.myia.io\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\", \"4f89edd5-90f7-4ee0-ac25-9185e9835c44\")\n",
    "\n",
    "# Connexion au client Qdrant\n",
    "qdrant_client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY\n",
    ")\n",
    "\n",
    "# Verification de la connexion\n",
    "try:\n",
    "    collections = qdrant_client.get_collections()\n",
    "    print(f\"Connexion Qdrant reussie !\")\n",
    "    print(f\"Collections existantes: {[c.name for c in collections.collections]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur de connexion: {e}\")\n",
    "    print(\"Continuez avec InMemoryStore pour les exemples suivants.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1043682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:08:17.895685Z",
     "iopub.status.busy": "2026-02-04T07:08:17.895371Z",
     "iopub.status.idle": "2026-02-04T07:08:22.445397Z",
     "shell.execute_reply": "2026-02-04T07:08:22.444564Z"
    },
    "papermill": {
     "duration": 4.553331,
     "end_time": "2026-02-04T07:08:22.446027",
     "exception": false,
     "start_time": "2026-02-04T07:08:17.892696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant non disponible: All connection attempts failed\n",
      "Les exemples utilisent InMemoryStore.\n"
     ]
    }
   ],
   "source": [
    "# Creation du store Qdrant via SK\n",
    "try:\n",
    "    qdrant_store = QdrantStore(\n",
    "        url=QDRANT_URL,\n",
    "        api_key=QDRANT_API_KEY\n",
    "    )\n",
    "    \n",
    "    # Collection Qdrant (SK 1.39+)\n",
    "    qdrant_collection = qdrant_store.get_collection(\n",
    "        DocumentRecord,\n",
    "        collection_name=\"sk_demo\"\n",
    "    )\n",
    "    \n",
    "    await qdrant_collection.ensure_collection_exists()\n",
    "    \n",
    "    # Inserer les memes documents\n",
    "    keys = await qdrant_collection.upsert(records)\n",
    "    print(f\"Documents inseres dans Qdrant: {keys}\")\n",
    "    \n",
    "    # Recherche\n",
    "    qdrant_results = await qdrant_collection.search(\n",
    "        vector=list(query_embedding),\n",
    "        vector_property_name=\"embedding\",\n",
    "        top=3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRecherche Qdrant pour: '{query}'\")\n",
    "    async for result in qdrant_results.results:\n",
    "        print(f\"  {result.score:.4f} - {result.record.title}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Qdrant non disponible: {e}\")\n",
    "    print(\"Les exemples utilisent InMemoryStore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb98266",
   "metadata": {
    "papermill": {
     "duration": 0.002026,
     "end_time": "2026-02-04T07:08:22.450278",
     "exception": false,
     "start_time": "2026-02-04T07:08:22.448252",
     "status": "completed"
    },
    "tags": []
   },
   "source": "### Interprétation : InMemory vs Qdrant - Choix Architectural\n\n**Comparaison detaillee des implementations** :\n\n| Caracteristique | InMemoryVectorStore | Qdrant |\n|-----------------|---------------------|--------|\n| **Persistance** | Non (RAM seulement) | Oui (disque/cloud) |\n| **Scalabilite** | ~10K documents | Millions de documents |\n| **Performance recherche** | O(n) lineaire | O(log n) avec HNSW |\n| **Infrastructure** | Aucune | Docker/Cloud requis |\n| **Latence typique** | <10ms (petits datasets) | 10-50ms (optimise) |\n| **Cout** | Gratuit (RAM locale) | $0.25-2/GB/mois (cloud) |\n| **Usage** | Dev/Test/Prototypage | Production/Scale |\n\n**Points cles** :\n\n1. **Courbe de performance** : InMemory devient lent au-dela de 10K documents (recherche lineaire)\n2. **HNSW advantage** : Qdrant utilise Hierarchical Navigable Small World graphs pour recherche sous-lineaire\n3. **Meme API SK** : Le code reste identique, seul le backend change (portabilite garantie)\n4. **Persistance critique** : InMemory perd tout au redemarrage, Qdrant survit aux crashes\n\n**Architecture Qdrant en production** :\n\n```\n┌──────────────────────────────────────────┐\n│         Application (SK Client)          │\n└──────────────┬───────────────────────────┘\n               │ REST/gRPC\n               v\n┌──────────────────────────────────────────┐\n│            Qdrant Cluster                │\n│  ┌────────┐  ┌────────┐  ┌────────┐     │\n│  │ Shard 1│  │ Shard 2│  │ Shard 3│     │\n│  │ 10M    │  │ 10M    │  │ 10M    │     │\n│  │ docs   │  │ docs   │  │ docs   │     │\n│  └────────┘  └────────┘  └────────┘     │\n└──────────────────────────────────────────┘\n```\n\n**Fonctionnalites Qdrant avancees** :\n\n| Feature | Description | Cas d'usage |\n|---------|-------------|-------------|\n| **Payload indexing** | Index sur metadonnees | Filtres rapides (date, categorie) |\n| **Quantization** | Compression vecteurs | Reduction memoire 4-8x |\n| **Snapshots** | Backup/restore | Disaster recovery |\n| **Replication** | Replicas pour HA | Zero-downtime |\n| **Sparse vectors** | Vecteurs creux | Hybrid search BM25+vector |\n\n**Migration InMemory -> Qdrant** :\n\n1. **Export** : Sauvegarder les records InMemory en JSON\n2. **Setup Qdrant** : Deployer via Docker ou cloud\n3. **Create collection** : Schema identique avec `ensure_collection_exists()`\n4. **Batch upsert** : Charger par batches de 100-1000 records\n5. **Validation** : Tester quelques requetes pour verifier coherence\n\n**Quand migrer vers Qdrant** :\n\n- Dataset > 5K documents\n- Besoin de persistance\n- Latence recherche > 100ms avec InMemory\n- Production deployments\n- Filtres complexes sur metadonnees\n\n**Alternatives a Qdrant** :\n\n| Vector DB | Avantage | Inconvenient |\n|-----------|----------|--------------|\n| **Pinecone** | Fully managed, simple | Cout eleve |\n| **Weaviate** | GraphQL, multimodal | Complexite setup |\n| **Milvus** | Open-source, scale | Infrastructure lourde |\n| **Chroma** | Leger, Python-native | Performance limitee |"
  },
  {
   "cell_type": "markdown",
   "id": "902f7b85",
   "metadata": {
    "papermill": {
     "duration": 0.002114,
     "end_time": "2026-02-04T07:08:22.454345",
     "exception": false,
     "start_time": "2026-02-04T07:08:22.452231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. RAG Pattern Complet\n",
    "\n",
    "Assemblons tout pour un pipeline RAG fonctionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc2caaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T07:08:22.459496Z",
     "iopub.status.busy": "2026-02-04T07:08:22.459299Z",
     "iopub.status.idle": "2026-02-04T07:08:24.115766Z",
     "shell.execute_reply": "2026-02-04T07:08:24.115180Z"
    },
    "papermill": {
     "duration": 1.660107,
     "end_time": "2026-02-04T07:08:24.116295",
     "exception": false,
     "start_time": "2026-02-04T07:08:22.456188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Question: Comment les agents SK peuvent-ils utiliser des plugins ?\n",
      "============================================================\n",
      "\n",
      "Contexte utilise:\n",
      "- Agents SK: L'Agent Framework permet de creer des agents autonomes qui utilisent des plugins et collaborent entre eux.\n",
      "- Plugins SK: Les plugins dans Semantic Kernel sont des collections de fonctions que le kernel peut invoquer.\n",
      "- Introduction a Semantic Kernel: Semantic Kernel est un SDK open-source de Microsoft pour integrer des LLMs dans vos applications.\n",
      "============================================================\n",
      "\n",
      "Reponse:\n",
      "Les agents SK utilisent des plugins en invoquant les collections de fonctions fournies par ces plugins. Les plugins sont intégrés dans le Semantic Kernel, ce qui permet aux agents autonomes de collaborer entre eux en s'appuyant sur ces fonctions.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "# Ajouter le service de chat\n",
    "kernel.add_service(OpenAIChatCompletion(service_id=\"chat\"))\n",
    "\n",
    "async def rag_query(question: str, collection, embedding_service, kernel, top_k: int = 3):\n",
    "    \"\"\"Pipeline RAG complet.\"\"\"\n",
    "    \n",
    "    # 1. Generer l'embedding de la question\n",
    "    query_embedding = (await embedding_service.generate_embeddings([question]))[0]\n",
    "    \n",
    "    # 2. Rechercher les documents pertinents (SK 1.39+)\n",
    "    results = await collection.search(\n",
    "        vector=list(query_embedding),\n",
    "        vector_property_name=\"embedding\",\n",
    "        top=top_k,\n",
    "        include_vectors=False\n",
    "    )\n",
    "    \n",
    "    # 3. Construire le contexte (SK 1.39+: async iteration)\n",
    "    context_parts = []\n",
    "    async for result in results.results:\n",
    "        context_parts.append(f\"- {result.record.title}: {result.record.content}\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # 4. Construire le prompt augmente\n",
    "    augmented_prompt = f\"\"\"Tu es un assistant qui repond en utilisant uniquement le contexte fourni.\n",
    "    \n",
    "CONTEXTE:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "REPONSE (basee uniquement sur le contexte):\"\"\"\n",
    "    \n",
    "    # 5. Appeler le LLM (SK 1.39+: settings requis)\n",
    "    chat_service = kernel.get_service(service_id=\"chat\")\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(augmented_prompt)\n",
    "    \n",
    "    settings = OpenAIChatPromptExecutionSettings()\n",
    "    response = await chat_service.get_chat_message_contents(\n",
    "        chat_history=history,\n",
    "        settings=settings\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"context\": context,\n",
    "        \"answer\": str(response[0])\n",
    "    }\n",
    "\n",
    "# Test du pipeline RAG\n",
    "result = await rag_query(\n",
    "    question=\"Comment les agents SK peuvent-ils utiliser des plugins ?\",\n",
    "    collection=collection,\n",
    "    embedding_service=embedding_service,\n",
    "    kernel=kernel\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nContexte utilise:\\n{result['context']}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nReponse:\\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a962d96",
   "metadata": {
    "papermill": {
     "duration": 0.002046,
     "end_time": "2026-02-04T07:08:24.120741",
     "exception": false,
     "start_time": "2026-02-04T07:08:24.118695",
     "status": "completed"
    },
    "tags": []
   },
   "source": "### Interprétation : Pipeline RAG - Retrieval-Augmented Generation\n\n**Sortie obtenue** : Reponse LLM basee sur le contexte recupere via recherche vectorielle.\n\n| Etape | Temps typique | Cout | Optimisation possible |\n|-------|---------------|------|----------------------|\n| 1. Embedding requete | ~50ms | $0.000002 | Cache pour requetes frequentes |\n| 2. Recherche vectorielle | ~10ms (InMemory) | Gratuit | Index HNSW pour gros volumes |\n| 3. Construction prompt | <1ms | Gratuit | Templates pre-compiles |\n| 4. Generation LLM | ~2000ms | $0.0001-0.001 | Streaming pour UX |\n\n**Architecture du prompt augmente** :\n\n```\n┌─────────────────────────────────────────┐\n│  System: \"Tu es un assistant...\"       │ <- Instructions comportement\n├─────────────────────────────────────────┤\n│  CONTEXTE: [Top-K documents]           │ <- Knowledge base dynamique\n├─────────────────────────────────────────┤\n│  QUESTION: [Question utilisateur]      │ <- Input utilisateur\n├─────────────────────────────────────────┤\n│  REPONSE: [Generee par LLM]           │ <- Output\n└─────────────────────────────────────────┘\n```\n\n**Points cles** :\n\n1. **Grounding** : Le contexte limite les hallucinations en fournissant des faits verifiables\n2. **Contexte limite** : Avec GPT-4, on peut inclure ~3-10 documents (selon taille)\n3. **Instruction stricte** : \"uniquement sur le contexte\" force le LLM a ne pas inventer\n4. **Tracabilite** : On peut logger les documents utilises pour audit\n\n**Comparaison RAG vs Fine-tuning** :\n\n| Aspect | RAG | Fine-tuning |\n|--------|-----|-------------|\n| **Cout initial** | Faible (~$10-100) | Eleve (~$1000-10000) |\n| **Mise a jour** | Immediate (re-index) | Lente (re-entrainement) |\n| **Donnees privees** | Reste externe | Integre au modele |\n| **Explicabilite** | Haute (sources visibles) | Basse (boite noire) |\n| **Precision** | Haute (sources exactes) | Variable (apprentissage) |\n\n**Limites du RAG** :\n\n- **Chunking critique** : Decouper mal les documents = contexte incomplet\n- **Limite de tokens** : GPT-4 Turbo = 128K tokens, mais cout augmente\n- **Qualite embeddings** : Un mauvais embedding = mauvaise recherche\n- **Ordre des documents** : Le LLM peut privileger les premiers documents\n\n**Ameliorations avancees** :\n\n1. **Reranking** : Utiliser un modele de reranking apres la recherche vectorielle\n2. **Hybrid search** : Combiner recherche vectorielle + BM25 (keywords)\n3. **Metadata filtering** : Filtrer par date, auteur, type avant recherche\n4. **Chain-of-thought** : Demander au LLM d'expliquer son raisonnement"
  },
  {
   "cell_type": "markdown",
   "id": "082d4692",
   "metadata": {
    "papermill": {
     "duration": 0.002008,
     "end_time": "2026-02-04T07:08:24.124744",
     "exception": false,
     "start_time": "2026-02-04T07:08:24.122736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "## Resume des concepts\n",
    "\n",
    "| Concept | Description | Code cle |\n",
    "|---------|-------------|----------|\n",
    "| **VectorStore** | Abstraction de base | `InMemoryVectorStore()`, `QdrantStore()` |\n",
    "| **Collection** | Groupe de records | `store.get_collection(name, type)` |\n",
    "| **Record** | Document + embedding | `@vectorstoremodel @dataclass` |\n",
    "| **Embedding** | Vecteur semantique | `OpenAITextEmbedding.generate_embeddings()` |\n",
    "| **Search** | Recherche similitude | `collection.vectorized_search(vector, options)` |\n",
    "| **RAG** | Retrieval-Augmented Generation | Contexte + LLM |\n",
    "\n",
    "## Points cles a retenir\n",
    "\n",
    "1. **InMemory pour dev, Qdrant pour prod** - Meme API, backend different\n",
    "2. **Les embeddings capturent le sens** - Pas juste les mots-cles\n",
    "3. **RAG = Search + Generate** - Contexte pertinent pour le LLM\n",
    "4. **Le chunking est crucial** - Decouper les longs documents\n",
    "5. **Les metadonnees enrichissent** - Filtres et contexte additionnel\n",
    "\n",
    "## Exercices suggeres\n",
    "\n",
    "1. **RAG sur PDF** : Ingerer un PDF et poser des questions\n",
    "2. **Filtres** : Ajouter des filtres sur les metadonnees (date, auteur)\n",
    "3. **Evaluation** : Mesurer la qualite des reponses RAG\n",
    "\n",
    "## Pour aller plus loin\n",
    "\n",
    "| Notebook | Contenu |\n",
    "|----------|--------|\n",
    "| [06-ProcessFramework](06-SemanticKernel-ProcessFramework.ipynb) | Workflows orchestres |\n",
    "| [07-MultiModal](07-SemanticKernel-MultiModal.ipynb) | Images et audio |\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation** : [<< 04-Filters](04-SemanticKernel-Filters-Observability.ipynb) | [Index](README.md) | [06-ProcessFramework >>](06-SemanticKernel-ProcessFramework.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.152168,
   "end_time": "2026-02-04T07:08:24.689680",
   "environment_variables": {},
   "exception": null,
   "input_path": "05-SemanticKernel-VectorStores.ipynb",
   "output_path": "05-SemanticKernel-VectorStores.ipynb",
   "parameters": {},
   "start_time": "2026-02-04T07:08:07.537512",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}