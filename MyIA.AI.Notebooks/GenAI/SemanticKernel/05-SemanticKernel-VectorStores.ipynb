{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK-5-VectorStores : RAG avec Qdrant\n",
    "\n",
    "**Navigation** : [<< 04-Filters](04-SemanticKernel-Filters-Observability.ipynb) | [Index](README.md) | [06-ProcessFramework >>](06-SemanticKernel-ProcessFramework.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Comprendre l'architecture **Vector Store** de SK\n",
    "2. Generer des **embeddings** avec OpenAI\n",
    "3. Utiliser **InMemoryVectorStore** pour le prototypage\n",
    "4. Connecter **Qdrant** pour la production\n",
    "5. Implementer un pipeline **RAG** complet\n",
    "\n",
    "### Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Notebooks 01-04 completes\n",
    "- Cle API OpenAI configuree (`.env`)\n",
    "- Acces Qdrant (optionnel, fourni)\n",
    "\n",
    "### Duree estimee : 50 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Sommaire\n",
    "\n",
    "| Section | Contenu | Concepts cles |\n",
    "|---------|---------|---------------|\n",
    "| 1 | Introduction | Pourquoi les Vector Stores ? |\n",
    "| 2 | Architecture SK | VectorStore, Collections, Records |\n",
    "| 3 | Embeddings | OpenAITextEmbedding |\n",
    "| 4 | InMemoryVectorStore | Prototypage rapide |\n",
    "| 5 | Qdrant | Production-ready |\n",
    "| 6 | RAG Pattern | Pipeline complet |\n",
    "| 7 | Conclusion | Resume, exercices |\n",
    "\n",
    "> **Qu'est-ce qu'un Vector Store ?** Une base de donnees optimisee pour stocker et rechercher des vecteurs (embeddings). C'est la fondation du RAG (Retrieval-Augmented Generation) qui permet aux LLMs d'acceder a vos donnees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Configuration: API Key OK\n",
      "Dependances installees\n"
     ]
    }
   ],
   "source": [
    "# Installation\n",
    "%pip install semantic-kernel qdrant-client python-dotenv --quiet\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Chargement du fichier .env (cles API)\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"Configuration: API Key {'OK' if api_key else 'MANQUANTE'}\")\n",
    "print(\"Dependances installees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction aux Vector Stores\n",
    "\n",
    "### Pourquoi les Vector Stores ?\n",
    "\n",
    "Les LLMs ont une limite de contexte et pas d'acces a vos donnees privees. Les Vector Stores resolvent ce probleme :\n",
    "\n",
    "```\n",
    "Documents          Embeddings          Vector Store\n",
    "┌─────────┐       ┌─────────┐         ┌─────────────┐\n",
    "│ Doc 1   │──────>│ [0.1,   │────────>│             │\n",
    "│         │       │  0.3,   │         │   Qdrant    │\n",
    "└─────────┘       │  ...]   │         │             │\n",
    "                  └─────────┘         │  ou autre   │\n",
    "                                      └─────────────┘\n",
    "                                            |\n",
    "Query: \"Qu'est-ce que X ?\"                  |\n",
    "        |                                   |\n",
    "        v                                   v\n",
    "   Embedding Query ──────> Recherche similitude\n",
    "        |                                   |\n",
    "        v                                   v\n",
    "   [0.2, 0.4, ...]         Top-K documents pertinents\n",
    "                                    |\n",
    "                                    v\n",
    "                           Contexte pour le LLM\n",
    "```\n",
    "\n",
    "### Connecteurs SK disponibles\n",
    "\n",
    "| Connecteur | Type | Cas d'usage |\n",
    "|------------|------|-------------|\n",
    "| **InMemoryVectorStore** | Local | Prototypage, tests |\n",
    "| **QdrantVectorStore** | Cloud/Self-hosted | Production |\n",
    "| **AzureAISearchVectorStore** | Azure | Enterprise |\n",
    "| **PineconeVectorStore** | Cloud | Scalabilite |\n",
    "| **RedisVectorStore** | Cache distribue | Performance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture Vector Store SK\n",
    "\n",
    "SK utilise une abstraction a trois niveaux :\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────┐\n",
    "│              VectorStore                    │\n",
    "│  (InMemory, Qdrant, Azure, Pinecone, ...)   │\n",
    "│                                             │\n",
    "│   ┌─────────────────────────────────────┐  │\n",
    "│   │    VectorStoreRecordCollection      │  │\n",
    "│   │    (equivalent d'une \"table\")       │  │\n",
    "│   │                                     │  │\n",
    "│   │   ┌─────────────────────────────┐  │  │\n",
    "│   │   │  VectorStoreRecordDefinition│  │  │\n",
    "│   │   │  (schema des records)       │  │  │\n",
    "│   │   └─────────────────────────────┘  │  │\n",
    "│   └─────────────────────────────────────┘  │\n",
    "└─────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Concepts cles\n",
    "\n",
    "| Concept | Description | Analogie SQL |\n",
    "|---------|-------------|-------------|\n",
    "| **VectorStore** | Connexion a la base | Database connection |\n",
    "| **Collection** | Groupe de records | Table |\n",
    "| **Record** | Document + embedding | Row |\n",
    "| **Key** | Identifiant unique | Primary Key |\n",
    "| **Vector** | Embedding du contenu | Colonne indexee |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generation d'Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de textes: 2\n",
      "Dimension des embeddings: 1536\n",
      "Premier embedding (debut): [-0.03926323  0.04215915 -0.02655712 -0.03383058  0.00597704]...\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n",
    "\n",
    "# Configuration\n",
    "kernel = Kernel()\n",
    "\n",
    "# Service d'embedding\n",
    "embedding_service = OpenAITextEmbedding(\n",
    "    service_id=\"embedding\",\n",
    "    ai_model_id=\"text-embedding-3-small\"  # Modele recommande (peu couteux, performant)\n",
    ")\n",
    "kernel.add_service(embedding_service)\n",
    "\n",
    "# Test de generation d'embedding\n",
    "test_texts = [\n",
    "    \"Semantic Kernel est un SDK pour l'IA\",\n",
    "    \"Python est un langage de programmation\"\n",
    "]\n",
    "\n",
    "embeddings = await embedding_service.generate_embeddings(test_texts)\n",
    "\n",
    "print(f\"Nombre de textes: {len(test_texts)}\")\n",
    "print(f\"Dimension des embeddings: {len(embeddings[0])}\")\n",
    "print(f\"Premier embedding (debut): {embeddings[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Embeddings\n",
    "\n",
    "Les embeddings sont des representations vectorielles du sens :\n",
    "\n",
    "| Propriete | Valeur | Signification |\n",
    "|-----------|--------|---------------|\n",
    "| **Dimension** | 1536 (text-embedding-3-small) | Complexite de la representation |\n",
    "| **Plage** | [-1, 1] | Valeurs normalisees |\n",
    "| **Similarite** | Cosinus ou dot product | Plus proche = plus similaire |\n",
    "\n",
    "**Modeles OpenAI disponibles** :\n",
    "\n",
    "| Modele | Dimension | Prix | Usage |\n",
    "|--------|-----------|------|-------|\n",
    "| `text-embedding-3-small` | 1536 | $0.02/1M tokens | Recommande |\n",
    "| `text-embedding-3-large` | 3072 | $0.13/1M tokens | Haute precision |\n",
    "| `text-embedding-ada-002` | 1536 | $0.10/1M tokens | Legacy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. InMemoryVectorStore\n",
    "\n",
    "Pour le prototypage rapide sans infrastructure externe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'documents' creee\n",
      "Schema: id (key), content (data), title (data), embedding (vector 1536d)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Annotated\n",
    "from semantic_kernel.connectors.in_memory import InMemoryStore\n",
    "from semantic_kernel.data.vector import (\n",
    "    vectorstoremodel,\n",
    "    VectorStoreField,\n",
    "    FieldTypes\n",
    ")\n",
    "\n",
    "# Definition du schema de record avec la nouvelle API SK 1.39+\n",
    "@vectorstoremodel\n",
    "@dataclass\n",
    "class DocumentRecord:\n",
    "    \"\"\"Schema d'un document dans le vector store.\"\"\"\n",
    "    id: Annotated[str, VectorStoreField(field_type=FieldTypes.KEY)]\n",
    "    content: Annotated[str, VectorStoreField(field_type=FieldTypes.DATA)]\n",
    "    title: Annotated[str, VectorStoreField(field_type=FieldTypes.DATA)]\n",
    "    embedding: Annotated[\n",
    "        list[float] | None,\n",
    "        VectorStoreField(\n",
    "            field_type=FieldTypes.VECTOR,\n",
    "            dimensions=1536\n",
    "        )\n",
    "    ] = None\n",
    "\n",
    "# Creation du store en memoire\n",
    "memory_store = InMemoryStore()\n",
    "\n",
    "# Obtenir ou creer une collection\n",
    "collection = memory_store.get_collection(\n",
    "    DocumentRecord,\n",
    "    collection_name=\"documents\"\n",
    ")\n",
    "\n",
    "# Creer la collection (si elle n'existe pas)\n",
    "# SK 1.39+: ensure_collection_exists() au lieu de create_collection_if_not_exists()\n",
    "await collection.ensure_collection_exists()\n",
    "\n",
    "print(\"Collection 'documents' creee\")\n",
    "print(f\"Schema: id (key), content (data), title (data), embedding (vector 1536d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents inseres: ['doc1', 'doc2', 'doc3', 'doc4']\n"
     ]
    }
   ],
   "source": [
    "# Documents d'exemple\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"title\": \"Introduction a Semantic Kernel\",\n",
    "        \"content\": \"Semantic Kernel est un SDK open-source de Microsoft pour integrer des LLMs dans vos applications.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"title\": \"Plugins SK\",\n",
    "        \"content\": \"Les plugins dans Semantic Kernel sont des collections de fonctions que le kernel peut invoquer.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"title\": \"Agents SK\",\n",
    "        \"content\": \"L'Agent Framework permet de creer des agents autonomes qui utilisent des plugins et collaborent entre eux.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"title\": \"RAG avec SK\",\n",
    "        \"content\": \"RAG (Retrieval-Augmented Generation) combine la recherche vectorielle avec la generation de texte par LLM.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generer les embeddings\n",
    "contents = [doc[\"content\"] for doc in documents]\n",
    "embeddings = await embedding_service.generate_embeddings(contents)\n",
    "\n",
    "# Creer les records\n",
    "records = []\n",
    "for doc, emb in zip(documents, embeddings):\n",
    "    record = DocumentRecord(\n",
    "        id=doc[\"id\"],\n",
    "        title=doc[\"title\"],\n",
    "        content=doc[\"content\"],\n",
    "        embedding=list(emb)\n",
    "    )\n",
    "    records.append(record)\n",
    "\n",
    "# Inserer dans la collection (SK 1.39+: upsert prend une liste)\n",
    "keys = await collection.upsert(records)\n",
    "print(f\"Documents inseres: {keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Comment creer des agents avec Semantic Kernel ?\n",
      "\n",
      "Resultats:\n",
      "------------------------------------------------------------\n",
      "Score: 0.3710\n",
      "Title: Plugins SK\n",
      "Content: Les plugins dans Semantic Kernel sont des collections de fonctions que le kernel peut invoquer.\n",
      "------------------------------------------------------------\n",
      "Score: 0.4583\n",
      "Title: Agents SK\n",
      "Content: L'Agent Framework permet de creer des agents autonomes qui utilisent des plugins et collaborent entre eux.\n",
      "------------------------------------------------------------\n",
      "Score: 0.4795\n",
      "Title: Introduction a Semantic Kernel\n",
      "Content: Semantic Kernel est un SDK open-source de Microsoft pour integrer des LLMs dans vos applications.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Recherche vectorielle\n",
    "query = \"Comment creer des agents avec Semantic Kernel ?\"\n",
    "\n",
    "# Generer l'embedding de la requete\n",
    "query_embedding = (await embedding_service.generate_embeddings([query]))[0]\n",
    "\n",
    "# Rechercher les documents similaires (SK 1.39+: search au lieu de vectorized_search)\n",
    "results = await collection.search(\n",
    "    vector=list(query_embedding),\n",
    "    vector_property_name=\"embedding\",\n",
    "    top=3,\n",
    "    include_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"\\nResultats:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# SK 1.39+: results.results est un async generator\n",
    "async for result in results.results:\n",
    "    print(f\"Score: {result.score:.4f}\")\n",
    "    print(f\"Title: {result.record.title}\")\n",
    "    print(f\"Content: {result.record.content}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Recherche Vectorielle\n",
    "\n",
    "La recherche vectorielle retourne les documents les plus proches semantiquement :\n",
    "\n",
    "| Parametre | Description | Valeur typique |\n",
    "|-----------|-------------|----------------|\n",
    "| **top** | Nombre de resultats | 3-10 |\n",
    "| **score** | Similarite (0-1 pour cosinus) | > 0.7 = bon match |\n",
    "| **distance_function** | Mesure de distance | cosine, dotproduct, euclidean |\n",
    "\n",
    "**Points cles** :\n",
    "- Le document sur les \"Agents SK\" a le meilleur score car semantiquement lie a la question\n",
    "- Le score indique la pertinence (plus haut = plus pertinent)\n",
    "- Les resultats sont tries par score decroissant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Qdrant (Production)\n",
    "\n",
    "Qdrant est un vector store production-ready. Nous avons une instance disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from semantic_kernel.connectors.qdrant import QdrantStore\nfrom qdrant_client import QdrantClient\n\n# Configuration Qdrant depuis .env\nQDRANT_URL = os.getenv(\"QDRANT_URL\")\nQDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n\n# Verification de la configuration\nif not QDRANT_URL or not QDRANT_API_KEY:\n    print(\"Configuration Qdrant manquante dans .env\")\n    print(\"Ajoutez QDRANT_URL et QDRANT_API_KEY a votre fichier .env\")\n    print(\"Les exemples Qdrant seront ignores (InMemoryStore utilise a la place).\")\n    qdrant_available = False\nelse:\n    # Connexion au client Qdrant\n    try:\n        qdrant_client = QdrantClient(\n            url=QDRANT_URL,\n            api_key=QDRANT_API_KEY\n        )\n        \n        # Verification de la connexion\n        collections = qdrant_client.get_collections()\n        print(f\"Connexion Qdrant reussie !\")\n        print(f\"URL: {QDRANT_URL}\")\n        print(f\"Collections existantes: {[c.name for c in collections.collections]}\")\n        qdrant_available = True\n    except Exception as e:\n        print(f\"Erreur de connexion Qdrant: {e}\")\n        print(\"Continuez avec InMemoryStore pour les exemples suivants.\")\n        qdrant_available = False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Creation du store Qdrant via SK\nif qdrant_available:\n    try:\n        qdrant_store = QdrantStore(\n            url=QDRANT_URL,\n            api_key=QDRANT_API_KEY\n        )\n        \n        # Collection Qdrant (SK 1.39+)\n        qdrant_collection = qdrant_store.get_collection(\n            DocumentRecord,\n            collection_name=\"sk_demo\"\n        )\n        \n        await qdrant_collection.ensure_collection_exists()\n        \n        # Inserer les memes documents\n        keys = await qdrant_collection.upsert(records)\n        print(f\"Documents inseres dans Qdrant: {keys}\")\n        \n        # Recherche\n        qdrant_results = await qdrant_collection.search(\n            vector=list(query_embedding),\n            vector_property_name=\"embedding\",\n            top=3\n        )\n        \n        print(f\"\\nRecherche Qdrant pour: '{query}'\")\n        async for result in qdrant_results.results:\n            print(f\"  {result.score:.4f} - {result.record.title}\")\n            \n    except Exception as e:\n        print(f\"Erreur Qdrant: {e}\")\n        print(\"Les exemples utilisent InMemoryStore.\")\nelse:\n    print(\"Qdrant non configure - exemples ignores.\")\n    print(\"Les resultats ci-dessus proviennent d'InMemoryStore.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : InMemory vs Qdrant\n",
    "\n",
    "| Caracteristique | InMemoryVectorStore | Qdrant |\n",
    "|-----------------|---------------------|--------|\n",
    "| **Persistance** | Non (RAM seulement) | Oui (disque/cloud) |\n",
    "| **Scalabilite** | ~10K documents | Millions de documents |\n",
    "| **Performance** | Rapide (petits datasets) | Optimise (HNSW index) |\n",
    "| **Infrastructure** | Aucune | Serveur/Cloud |\n",
    "| **Usage** | Dev/Test | Production |\n",
    "\n",
    "**Qdrant specifiques** :\n",
    "- Index HNSW pour recherche rapide\n",
    "- Filtres sur metadonnees\n",
    "- Sharding pour scalabilite horizontale\n",
    "- API REST et gRPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG Pattern Complet\n",
    "\n",
    "Assemblons tout pour un pipeline RAG fonctionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Question: Comment les agents SK peuvent-ils utiliser des plugins ?\n",
      "============================================================\n",
      "\n",
      "Contexte utilise:\n",
      "- Agents SK: L'Agent Framework permet de creer des agents autonomes qui utilisent des plugins et collaborent entre eux.\n",
      "- Plugins SK: Les plugins dans Semantic Kernel sont des collections de fonctions que le kernel peut invoquer.\n",
      "- Introduction a Semantic Kernel: Semantic Kernel est un SDK open-source de Microsoft pour integrer des LLMs dans vos applications.\n",
      "============================================================\n",
      "\n",
      "Reponse:\n",
      "Dans Semantic Kernel, les agents SK (créés avec l'Agent Framework) utilisent des plugins en invoquant les fonctions que ces plugins exposent. Les plugins SK sont des collections de fonctions que le kernel peut appeler, et les agents autonomes s'appuient sur ces fonctions pour accomplir leurs tâches et collaborer entre eux. Le tout s'inscrit dans Semantic Kernel, le SDK open-source de Microsoft pour intégrer des LLMs dans vos applications.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "# Ajouter le service de chat\n",
    "kernel.add_service(OpenAIChatCompletion(service_id=\"chat\"))\n",
    "\n",
    "async def rag_query(question: str, collection, embedding_service, kernel, top_k: int = 3):\n",
    "    \"\"\"Pipeline RAG complet.\"\"\"\n",
    "    \n",
    "    # 1. Generer l'embedding de la question\n",
    "    query_embedding = (await embedding_service.generate_embeddings([question]))[0]\n",
    "    \n",
    "    # 2. Rechercher les documents pertinents (SK 1.39+)\n",
    "    results = await collection.search(\n",
    "        vector=list(query_embedding),\n",
    "        vector_property_name=\"embedding\",\n",
    "        top=top_k,\n",
    "        include_vectors=False\n",
    "    )\n",
    "    \n",
    "    # 3. Construire le contexte (SK 1.39+: async iteration)\n",
    "    context_parts = []\n",
    "    async for result in results.results:\n",
    "        context_parts.append(f\"- {result.record.title}: {result.record.content}\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # 4. Construire le prompt augmente\n",
    "    augmented_prompt = f\"\"\"Tu es un assistant qui repond en utilisant uniquement le contexte fourni.\n",
    "    \n",
    "CONTEXTE:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "REPONSE (basee uniquement sur le contexte):\"\"\"\n",
    "    \n",
    "    # 5. Appeler le LLM (SK 1.39+: settings requis)\n",
    "    chat_service = kernel.get_service(service_id=\"chat\")\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(augmented_prompt)\n",
    "    \n",
    "    settings = OpenAIChatPromptExecutionSettings()\n",
    "    response = await chat_service.get_chat_message_contents(\n",
    "        chat_history=history,\n",
    "        settings=settings\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"context\": context,\n",
    "        \"answer\": str(response[0])\n",
    "    }\n",
    "\n",
    "# Test du pipeline RAG\n",
    "result = await rag_query(\n",
    "    question=\"Comment les agents SK peuvent-ils utiliser des plugins ?\",\n",
    "    collection=collection,\n",
    "    embedding_service=embedding_service,\n",
    "    kernel=kernel\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nContexte utilise:\\n{result['context']}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nReponse:\\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Pipeline RAG\n",
    "\n",
    "Le pipeline RAG suit ces etapes :\n",
    "\n",
    "```\n",
    "1. EMBEDDING          2. SEARCH           3. AUGMENT         4. GENERATE\n",
    "┌─────────────┐     ┌─────────────┐     ┌─────────────┐    ┌─────────────┐\n",
    "│  Question   │────>│  Vector     │────>│  Prompt +   │───>│    LLM      │\n",
    "│  -> Vector  │     │  Search     │     │  Context    │    │  Response   │\n",
    "└─────────────┘     └─────────────┘     └─────────────┘    └─────────────┘\n",
    "```\n",
    "\n",
    "**Avantages du RAG** :\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **Donnees privees** | Le LLM peut acceder a vos documents |\n",
    "| **Actualite** | Pas besoin de re-entrainer le modele |\n",
    "| **Precision** | Reponses basees sur des sources |\n",
    "| **Tracabilite** | On sait d'ou vient l'information |\n",
    "| **Cout** | Moins cher que le fine-tuning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Resume des concepts\n",
    "\n",
    "| Concept | Description | Code cle |\n",
    "|---------|-------------|----------|\n",
    "| **VectorStore** | Abstraction de base | `InMemoryVectorStore()`, `QdrantStore()` |\n",
    "| **Collection** | Groupe de records | `store.get_collection(name, type)` |\n",
    "| **Record** | Document + embedding | `@vectorstoremodel @dataclass` |\n",
    "| **Embedding** | Vecteur semantique | `OpenAITextEmbedding.generate_embeddings()` |\n",
    "| **Search** | Recherche similitude | `collection.vectorized_search(vector, options)` |\n",
    "| **RAG** | Retrieval-Augmented Generation | Contexte + LLM |\n",
    "\n",
    "## Points cles a retenir\n",
    "\n",
    "1. **InMemory pour dev, Qdrant pour prod** - Meme API, backend different\n",
    "2. **Les embeddings capturent le sens** - Pas juste les mots-cles\n",
    "3. **RAG = Search + Generate** - Contexte pertinent pour le LLM\n",
    "4. **Le chunking est crucial** - Decouper les longs documents\n",
    "5. **Les metadonnees enrichissent** - Filtres et contexte additionnel\n",
    "\n",
    "## Exercices suggeres\n",
    "\n",
    "1. **RAG sur PDF** : Ingerer un PDF et poser des questions\n",
    "2. **Filtres** : Ajouter des filtres sur les metadonnees (date, auteur)\n",
    "3. **Evaluation** : Mesurer la qualite des reponses RAG\n",
    "\n",
    "## Pour aller plus loin\n",
    "\n",
    "| Notebook | Contenu |\n",
    "|----------|--------|\n",
    "| [06-ProcessFramework](06-SemanticKernel-ProcessFramework.ipynb) | Workflows orchestres |\n",
    "| [07-MultiModal](07-SemanticKernel-MultiModal.ipynb) | Images et audio |\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation** : [<< 04-Filters](04-SemanticKernel-Filters-Observability.ipynb) | [Index](README.md) | [06-ProcessFramework >>](06-SemanticKernel-ProcessFramework.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}