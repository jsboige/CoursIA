{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK-7-MultiModal : Images, Audio et Vision\n",
    "\n",
    "**Navigation** : [<< 06-ProcessFramework](06-SemanticKernel-ProcessFramework.ipynb) | [Index](README.md) | [08-MCP >>](08-SemanticKernel-MCP.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Generer des **images** avec DALL-E 3\n",
    "2. Analyser des **images** avec GPT-4 Vision\n",
    "3. Transcrire de l'**audio** avec Whisper\n",
    "4. Generer de l'**audio** avec Text-to-Speech\n",
    "5. Combiner les modalites dans un **pipeline**\n",
    "\n",
    "### Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Notebooks 01-06 completes\n",
    "- Cle API OpenAI configuree (`.env`)\n",
    "- Acces aux modeles DALL-E et Whisper (OpenAI payant)\n",
    "\n",
    "### Duree estimee : 45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Sommaire\n",
    "\n",
    "| Section | Contenu | Concepts cles |\n",
    "|---------|---------|---------------|\n",
    "| 1 | Introduction | Capacites multi-modales |\n",
    "| 2 | Text-to-Image | DALL-E 3 |\n",
    "| 3 | Image-to-Text | GPT-4 Vision |\n",
    "| 4 | Speech-to-Text | Whisper |\n",
    "| 5 | Text-to-Speech | OpenAI TTS |\n",
    "| 6 | Pipeline combine | Audio -> Texte -> Image |\n",
    "| 7 | Conclusion | Resume, exercices |\n",
    "\n",
    "> **Multi-Modal AI** : Les modeles modernes peuvent traiter et generer du contenu dans plusieurs modalites (texte, image, audio, video). SK fournit des connecteurs uniformes pour ces capacites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "%pip install semantic-kernel python-dotenv openai Pillow --quiet\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from semantic_kernel import Kernel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Verification de la cle API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"API Key configuree: {'Oui' if api_key else 'Non'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction aux capacites Multi-Modales\n",
    "\n",
    "SK supporte plusieurs modalites via des services specialises :\n",
    "\n",
    "| Modalite | Direction | Service SK | Modele OpenAI |\n",
    "|----------|-----------|------------|---------------|\n",
    "| **Texte -> Image** | Generation | `OpenAITextToImage` | DALL-E 3 |\n",
    "| **Image -> Texte** | Analyse | `OpenAIChatCompletion` | GPT-4 Vision |\n",
    "| **Audio -> Texte** | Transcription | `OpenAIAudioToText` | Whisper |\n",
    "| **Texte -> Audio** | Synthese | `OpenAITextToAudio` | TTS-1, TTS-1-HD |\n",
    "\n",
    "### Architecture multi-modale\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│                    KERNEL                           │\n",
    "│  ┌─────────────┐  ┌─────────────┐  ┌────────────┐  │\n",
    "│  │    Chat     │  │   Image     │  │   Audio    │  │\n",
    "│  │  Completion │  │ Generation  │  │ Services   │  │\n",
    "│  │   (GPT-4V)  │  │  (DALL-E)   │  │ (Whisper)  │  │\n",
    "│  └─────────────┘  └─────────────┘  └────────────┘  │\n",
    "│         ↑               ↑               ↑          │\n",
    "│         └───────────────┼───────────────┘          │\n",
    "│                         │                          │\n",
    "│              Unified Service Interface             │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text-to-Image avec DALL-E 3\n",
    "\n",
    "Generation d'images a partir de descriptions textuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextToImage\n",
    "from IPython.display import Image, display\n",
    "import urllib.request\n",
    "\n",
    "# Configuration du service DALL-E\n",
    "kernel = Kernel()\n",
    "\n",
    "dalle_service = OpenAITextToImage(\n",
    "    service_id=\"dalle\",\n",
    "    ai_model_id=\"dall-e-3\"  # ou \"dall-e-2\" pour moins cher\n",
    ")\n",
    "kernel.add_service(dalle_service)\n",
    "\n",
    "print(\"Service DALL-E configure\")\n",
    "print(\"\\nModeles disponibles:\")\n",
    "print(\"| Modele    | Resolution | Prix/image |\")\n",
    "print(\"|-----------|------------|------------|\")\n",
    "print(\"| dall-e-3  | 1024x1024  | $0.04      |\")\n",
    "print(\"| dall-e-3  | 1792x1024  | $0.08      |\")\n",
    "print(\"| dall-e-2  | 1024x1024  | $0.02      |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation d'une image\n",
    "prompt = \"A futuristic city with flying cars and neon lights, cyberpunk style, highly detailed\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"Generation en cours...\")\n",
    "\n",
    "try:\n",
    "    # Generer l'image\n",
    "    image_result = await dalle_service.generate_image(\n",
    "        description=prompt,\n",
    "        width=1024,\n",
    "        height=1024\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nImage generee avec succes!\")\n",
    "    print(f\"URL: {image_result[:80]}...\")\n",
    "    \n",
    "    # Afficher l'image\n",
    "    display(Image(url=image_result))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur: {e}\")\n",
    "    print(\"Assurez-vous d'avoir acces a l'API DALL-E.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : DALL-E 3\n",
    "\n",
    "DALL-E 3 offre des ameliorations significatives :\n",
    "\n",
    "| Caracteristique | DALL-E 2 | DALL-E 3 |\n",
    "|-----------------|----------|----------|\n",
    "| **Qualite** | Bonne | Excellente |\n",
    "| **Texte dans images** | Difficile | Supporte |\n",
    "| **Suivi du prompt** | Moyen | Tres precis |\n",
    "| **Styles** | Limites | Variés |\n",
    "| **Prix** | $0.02 | $0.04-0.08 |\n",
    "\n",
    "**Bonnes pratiques pour les prompts** :\n",
    "- Etre specifique sur le style (\"cyberpunk\", \"watercolor\", \"photorealistic\")\n",
    "- Inclure des details de composition\n",
    "- Specifier l'eclairage et l'ambiance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image-to-Text avec GPT-4 Vision\n",
    "\n",
    "Analyse et description d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.contents import ChatHistory, ImageContent, TextContent\n",
    "\n",
    "# Configuration du service GPT-4 Vision\n",
    "vision_service = OpenAIChatCompletion(\n",
    "    service_id=\"vision\",\n",
    "    ai_model_id=\"gpt-4o\"  # ou \"gpt-4o-mini\" pour moins cher\n",
    ")\n",
    "kernel.add_service(vision_service)\n",
    "\n",
    "print(\"Service GPT-4 Vision configure\")\n",
    "print(\"\\nModeles avec vision:\")\n",
    "print(\"| Modele       | Vision | Prix (input/1M) |\")\n",
    "print(\"|--------------|--------|-----------------|\")\n",
    "print(\"| gpt-4o       | Oui    | $2.50           |\")\n",
    "print(\"| gpt-4o-mini  | Oui    | $0.15           |\")\n",
    "print(\"| gpt-4-turbo  | Oui    | $10.00          |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse d'une image\n",
    "# Utilisons une image publique pour l'exemple\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n",
    "\n",
    "print(f\"Image a analyser: {image_url[:50]}...\")\n",
    "display(Image(url=image_url, width=300))\n",
    "\n",
    "# Creer l'historique avec image\n",
    "history = ChatHistory()\n",
    "history.add_user_message([\n",
    "    TextContent(text=\"Decris cette image en detail. Que vois-tu?\"),\n",
    "    ImageContent(uri=image_url)\n",
    "])\n",
    "\n",
    "try:\n",
    "    response = await vision_service.get_chat_message_contents(\n",
    "        chat_history=history\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDescription de l'image:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(str(response[0]))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : GPT-4 Vision\n",
    "\n",
    "Les modeles avec vision peuvent :\n",
    "\n",
    "| Capacite | Description | Exemple |\n",
    "|----------|-------------|--------|\n",
    "| **Description** | Decrire le contenu | \"Un chat roux assis...\" |\n",
    "| **OCR** | Lire du texte | Extraire du texte d'une photo |\n",
    "| **Analyse** | Comprendre le contexte | \"Cette image montre une scene de bureau\" |\n",
    "| **Comparaison** | Comparer des images | Differences entre 2 versions |\n",
    "| **Code** | Lire du code/diagrammes | Comprendre un schema UML |\n",
    "\n",
    "**Limitations** :\n",
    "- Pas de generation d'images (utiliser DALL-E)\n",
    "- Cout par token d'image (~85 tokens pour 512x512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Speech-to-Text avec Whisper\n",
    "\n",
    "Transcription audio en texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAIAudioToText\n",
    "\n",
    "# Configuration du service Whisper\n",
    "whisper_service = OpenAIAudioToText(\n",
    "    service_id=\"whisper\",\n",
    "    ai_model_id=\"whisper-1\"\n",
    ")\n",
    "kernel.add_service(whisper_service)\n",
    "\n",
    "print(\"Service Whisper configure\")\n",
    "print(\"\\nCaracteristiques Whisper:\")\n",
    "print(\"| Caracteristique | Valeur           |\")\n",
    "print(\"|-----------------|------------------|\")\n",
    "print(\"| Langues         | 99+              |\")\n",
    "print(\"| Formats audio   | mp3, wav, m4a... |\")\n",
    "print(\"| Taille max      | 25 MB            |\")\n",
    "print(\"| Prix            | $0.006/minute    |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de transcription (necessite un fichier audio)\n",
    "# Creeons un fichier audio de test avec TTS d'abord\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "async def create_test_audio():\n",
    "    \"\"\"Cree un fichier audio de test avec TTS.\"\"\"\n",
    "    client = AsyncOpenAI()\n",
    "    \n",
    "    text = \"Bonjour, ceci est un test de transcription audio avec Whisper.\"\n",
    "    \n",
    "    response = await client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=text\n",
    "    )\n",
    "    \n",
    "    # Sauvegarder temporairement\n",
    "    temp_path = os.path.join(tempfile.gettempdir(), \"test_audio.mp3\")\n",
    "    with open(temp_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    return temp_path, text\n",
    "\n",
    "try:\n",
    "    audio_path, original_text = await create_test_audio()\n",
    "    print(f\"Audio cree: {audio_path}\")\n",
    "    print(f\"Texte original: {original_text}\")\n",
    "    \n",
    "    # Transcription\n",
    "    with open(audio_path, \"rb\") as audio_file:\n",
    "        transcription = await whisper_service.get_text_content(\n",
    "            audio_content=audio_file.read(),\n",
    "            settings=None\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nTranscription: {transcription}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur: {e}\")\n",
    "    print(\"\\nExemple conceptuel (fichier audio requis):\")\n",
    "    print(\"\"\"```python\n",
    "with open('audio.mp3', 'rb') as f:\n",
    "    text = await whisper_service.get_text_content(audio_content=f.read())\n",
    "print(text)\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text-to-Speech\n",
    "\n",
    "Generation audio a partir de texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextToAudio\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Configuration du service TTS\n",
    "tts_service = OpenAITextToAudio(\n",
    "    service_id=\"tts\",\n",
    "    ai_model_id=\"tts-1\"  # ou \"tts-1-hd\" pour haute qualite\n",
    ")\n",
    "kernel.add_service(tts_service)\n",
    "\n",
    "print(\"Service TTS configure\")\n",
    "print(\"\\nVoix disponibles:\")\n",
    "print(\"| Voix   | Description        |\")\n",
    "print(\"|--------|-------------------|\")\n",
    "print(\"| alloy  | Neutre, versatile |\")\n",
    "print(\"| echo   | Grave, masculin   |\")\n",
    "print(\"| fable  | Narratif, chaleureux |\")\n",
    "print(\"| onyx   | Profond, autoritaire |\")\n",
    "print(\"| nova   | Feminin, dynamique |\")\n",
    "print(\"| shimmer| Doux, expressif   |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation audio\n",
    "text_to_speak = \"Semantic Kernel est un SDK puissant pour creer des applications d'intelligence artificielle.\"\n",
    "\n",
    "try:\n",
    "    audio_content = await tts_service.get_audio_content(\n",
    "        text=text_to_speak,\n",
    "        settings=None\n",
    "    )\n",
    "    \n",
    "    # Sauvegarder et jouer\n",
    "    output_path = os.path.join(tempfile.gettempdir(), \"output_speech.mp3\")\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(audio_content.data)\n",
    "    \n",
    "    print(f\"Audio genere: {output_path}\")\n",
    "    print(f\"Taille: {len(audio_content.data)} bytes\")\n",
    "    \n",
    "    # Afficher le lecteur audio\n",
    "    display(Audio(output_path))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur: {e}\")\n",
    "    print(\"\\nExemple conceptuel:\")\n",
    "    print(\"\"\"```python\n",
    "audio = await tts_service.get_audio_content(text=\"Bonjour!\")\n",
    "with open('speech.mp3', 'wb') as f:\n",
    "    f.write(audio.data)\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pipeline Multi-Modal\n",
    "\n",
    "Combinons les modalites dans un flux complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def multimodal_pipeline(audio_description: str):\n",
    "    \"\"\"\n",
    "    Pipeline multi-modal:\n",
    "    1. Texte -> Description enrichie (LLM)\n",
    "    2. Description -> Image (DALL-E)\n",
    "    3. Image -> Analyse (Vision)\n",
    "    4. Analyse -> Audio (TTS)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PIPELINE MULTI-MODAL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Etape 1: Enrichir la description\n",
    "    print(\"\\n[1/4] Enrichissement de la description...\")\n",
    "    chat_service = kernel.get_service(service_id=\"vision\")\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\n",
    "        f\"Transforme cette description en prompt detaille pour DALL-E: '{audio_description}'\"\n",
    "    )\n",
    "    response = await chat_service.get_chat_message_contents(chat_history=history)\n",
    "    enriched_prompt = str(response[0])\n",
    "    print(f\"Prompt enrichi: {enriched_prompt[:100]}...\")\n",
    "    \n",
    "    # Etape 2: Generer l'image\n",
    "    print(\"\\n[2/4] Generation de l'image...\")\n",
    "    dalle = kernel.get_service(service_id=\"dalle\")\n",
    "    image_url = await dalle.generate_image(\n",
    "        description=enriched_prompt,\n",
    "        width=1024,\n",
    "        height=1024\n",
    "    )\n",
    "    print(f\"Image generee: {image_url[:50]}...\")\n",
    "    display(Image(url=image_url, width=400))\n",
    "    \n",
    "    # Etape 3: Analyser l'image\n",
    "    print(\"\\n[3/4] Analyse de l'image...\")\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message([\n",
    "        TextContent(text=\"Decris cette image de facon poetique en 2 phrases.\"),\n",
    "        ImageContent(uri=image_url)\n",
    "    ])\n",
    "    response = await chat_service.get_chat_message_contents(chat_history=history)\n",
    "    analysis = str(response[0])\n",
    "    print(f\"Analyse: {analysis}\")\n",
    "    \n",
    "    # Etape 4: Convertir en audio\n",
    "    print(\"\\n[4/4] Generation audio...\")\n",
    "    tts = kernel.get_service(service_id=\"tts\")\n",
    "    audio = await tts.get_audio_content(text=analysis)\n",
    "    \n",
    "    output_path = os.path.join(tempfile.gettempdir(), \"pipeline_output.mp3\")\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(audio.data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE TERMINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    display(Audio(output_path))\n",
    "    \n",
    "    return {\n",
    "        \"original\": audio_description,\n",
    "        \"enriched_prompt\": enriched_prompt,\n",
    "        \"image_url\": image_url,\n",
    "        \"analysis\": analysis,\n",
    "        \"audio_path\": output_path\n",
    "    }\n",
    "\n",
    "# Test du pipeline\n",
    "try:\n",
    "    result = await multimodal_pipeline(\"Un paysage de montagne au coucher du soleil\")\n",
    "except Exception as e:\n",
    "    print(f\"Pipeline interrompu: {e}\")\n",
    "    print(\"\\nCe pipeline necessite l'acces a DALL-E, GPT-4V, et TTS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Pipeline Multi-Modal\n",
    "\n",
    "Ce pipeline illustre la puissance de combiner les modalites :\n",
    "\n",
    "```\n",
    "Texte simple          LLM           DALL-E         Vision          TTS\n",
    "\"paysage de    -->  Prompt    -->  Image     -->  Analyse   -->  Audio\n",
    " montagne\"          enrichi        generee       poetique       narration\n",
    "```\n",
    "\n",
    "**Applications concretes** :\n",
    "\n",
    "| Application | Pipeline |\n",
    "|-------------|----------|\n",
    "| **Podcast automatique** | Texte -> Resume -> TTS |\n",
    "| **Accessibilite** | Image -> Description -> TTS |\n",
    "| **Creation contenu** | Idee -> Image -> Post social |\n",
    "| **Transcription meetings** | Audio -> Texte -> Resume -> Actions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Resume des concepts\n",
    "\n",
    "| Service | Direction | Code cle |\n",
    "|---------|-----------|----------|\n",
    "| **OpenAITextToImage** | Texte -> Image | `generate_image(description)` |\n",
    "| **GPT-4V** | Image -> Texte | `ImageContent(uri=...)` |\n",
    "| **OpenAIAudioToText** | Audio -> Texte | `get_text_content(audio)` |\n",
    "| **OpenAITextToAudio** | Texte -> Audio | `get_audio_content(text)` |\n",
    "\n",
    "## Points cles a retenir\n",
    "\n",
    "1. **Interface unifiee** - Tous les services partagent le meme pattern\n",
    "2. **Combiner les modalites** - Pipelines riches possibles\n",
    "3. **Couts variables** - DALL-E 3 plus cher que Whisper\n",
    "4. **GPT-4V pour l'analyse** - Pas de modele \"vision\" separe\n",
    "5. **Qualite vs prix** - TTS-1-HD vs TTS-1, DALL-E 3 vs 2\n",
    "\n",
    "## Exercices suggeres\n",
    "\n",
    "1. **Assistant vocal** : Audio -> Texte -> LLM -> Audio\n",
    "2. **Generateur de storyboard** : Script -> Serie d'images\n",
    "3. **Traducteur visuel** : Image -> Description -> Traduction -> Image\n",
    "\n",
    "## Pour aller plus loin\n",
    "\n",
    "| Notebook | Contenu |\n",
    "|----------|--------|\n",
    "| [08-MCP](08-SemanticKernel-MCP.ipynb) | Model Context Protocol |\n",
    "| [05-NotebookMaker](05-NotebookMaker.ipynb) | Agents multi-modaux |\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation** : [<< 06-ProcessFramework](06-SemanticKernel-ProcessFramework.ipynb) | [Index](README.md) | [08-MCP >>](08-SemanticKernel-MCP.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
