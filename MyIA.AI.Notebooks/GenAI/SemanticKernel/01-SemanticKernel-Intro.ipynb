{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction √† Semantic Kernel\n",
    "\n",
    "Ce notebook vous permettra de d√©couvrir **Semantic Kernel**, un SDK permettant d'int√©grer et d'orchestrer des mod√®les de langage dans vos applications Python.  \n",
    "Nous verrons notamment :\n",
    "- L'installation et la configuration du SDK.\n",
    "- La configuration du service LLM (OpenAI ou Azure OpenAI) via un fichier `.env`.\n",
    "- Le chargement et l'utilisation du Kernel.\n",
    "- L'ex√©cution de fonctions s√©mantiques d√©finies depuis des fichiers prompt et directement en code.\n",
    "- La gestion d'une conversation (chat) en utilisant des arguments de Kernel pour garder l'historique.\n",
    "\n",
    "Ce notebook s'adresse √† des √©tudiants de niveau licence ayant quelques bases en Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Installation de Semantic Kernel (si n√©cessaire)\n",
    "%pip install semantic-kernel\n",
    "\n",
    "# V√©rification de la version install√©e\n",
    "from semantic_kernel import __version__\n",
    "print(f\"Semantic Kernel version : {__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### üìå **Importation des biblioth√®ques n√©cessaires**\n",
    "Dans cette cellule, nous allons importer les modules principaux.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.functions import KernelArguments  # Correction de l'import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìÅ 2. Chargement des param√®tres de configuration**\n",
    "### üìå **Lecture des param√®tres depuis un fichier `.env` ou JSON**\n",
    "Le fichier de configuration `.env` doit contenir les cl√©s n√©cessaires pour acc√©der aux services OpenAI/Azure OpenAI.\n",
    "\n",
    "üí° **V√©rifiez que vous avez bien cr√©√© un fichier `.env`** dans le m√™me dossier que ce notebook avec ces valeurs :\n",
    "\n",
    "```plaintext\n",
    "GLOBAL_LLM_SERVICE=\"OpenAI\"\n",
    "OPENAI_API_KEY=\"sk-...\"\n",
    "OPENAI_CHAT_MODEL_ID=\"gpt-4o-mini\"\n",
    "```\n",
    "\n",
    "üëâ Nous allons maintenant **charger ces param√®tres en Python** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Chargement du fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# R√©cup√©ration des cl√©s API et du mod√®le\n",
    "llm_service = os.getenv(\"GLOBAL_LLM_SERVICE\", \"OpenAI\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_id = os.getenv(\"OPENAI_CHAT_MODEL_ID\", \"gpt-4o-mini\")\n",
    "\n",
    "# V√©rification\n",
    "print(f\"Service s√©lectionn√© : {llm_service}\")\n",
    "print(f\"Mod√®le utilis√© : {model_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Importer le Kernel depuis Semantic Kernel\n",
    "from semantic_kernel import Kernel\n",
    "\n",
    "# Cr√©er une instance du Kernel\n",
    "kernel = Kernel()\n",
    "print(\"Kernel initialis√© avec succ√®s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# R√©cup√®re la valeur du .env et nettoie les espaces\n",
    "base_url_from_env = os.getenv(\"OPENAI_BASE_URL\", \"\").strip()\n",
    "\n",
    "# D√©termine explicitement l'URL finale\n",
    "if base_url_from_env:\n",
    "    final_base_url = base_url_from_env\n",
    "    print(f\"Utilisation de l'URL du .env : {final_base_url}\")\n",
    "else:\n",
    "    final_base_url = \"https://api.openai.com/v1\" # <<< URL par d√©faut explicite\n",
    "    print(f\"Utilisation de l'URL par d√©faut : {final_base_url}\")\n",
    "\n",
    "model_id = os.getenv(\"OPENAI_CHAT_MODEL_ID\", \"gpt-4o-mini\")\n",
    "print(f\"Utilisation du mod√®le : {model_id}\")\n",
    "print(f\"Cl√© API utilis√©e : {'Oui' if api_key else 'Non'}\")\n",
    "\n",
    "async def test_connection():\n",
    "    try:\n",
    "        if not api_key:\n",
    "            print(\"ERREUR : Cl√© API non d√©finie.\")\n",
    "            return\n",
    "\n",
    "        # Utilise final_base_url d√©termin√© ci-dessus\n",
    "        client = AsyncOpenAI(api_key=api_key, base_url=final_base_url)\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say hello!\"}],\n",
    "            max_tokens=10,\n",
    "            timeout=20\n",
    "        )\n",
    "        print(\"Connexion r√©ussie !\")\n",
    "        print(\"R√©ponse :\", response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"ERREUR lors du test de connexion : {type(e).__name__} - {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Ex√©cuter le test\n",
    "await test_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration du service LLM\n",
    "\n",
    "Nous devons connecter notre Kernel √† un service de Chat Completion.  \n",
    "Pour cet exemple, nous allons utiliser OpenAI. Si vous pr√©f√©rez Azure OpenAI, adaptez le code en cons√©quence (voir la documentation).\n",
    "\n",
    "La configuration se fait via l'ajout d'un service au Kernel.  \n",
    "Assurez-vous que votre fichier `.env` contient votre cl√© API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Importation du service OpenAI pour le Chat Completion\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "# Ajout du service \"default\" au Kernel\n",
    "kernel.add_service(OpenAIChatCompletion(service_id=\"default\"))\n",
    "print(\"Service OpenAI ajout√© au Kernel.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser un Plugin de Prompt\n",
    "\n",
    "Semantic Kernel permet de charger des **prompt plugins** stock√©s sur disque.  \n",
    "Dans cet exemple, nous chargerons le plugin \"FunPlugin\" qui contient, par exemple, une fonction pour g√©n√©rer une blague.\n",
    "\n",
    "Les fichiers du plugin (le prompt et sa configuration) sont stock√©s dans le r√©pertoire `prompt_template_samples/`.  \n",
    "Nous allons charger ce plugin et invoquer la fonction \"Joke\" pour g√©n√©rer une blague sur un sujet donn√©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Chemin correct vers les plugins\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "\n",
    "# V√©rifier si le dossier du plugin existe avant de charger\n",
    "if os.path.exists(os.path.join(plugins_directory, \"FunPlugin\")):\n",
    "    fun_plugin = kernel.add_plugin(parent_directory=plugins_directory, plugin_name=\"FunPlugin\")\n",
    "    joke_function = fun_plugin[\"Joke\"]\n",
    "    print(\"Plugin charg√© avec succ√®s.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Le plugin FunPlugin est introuvable. V√©rifiez le chemin et assurez-vous qu'il est bien pr√©sent.\")\n",
    "\n",
    "\n",
    "# Invoquer la fonction pour g√©n√©rer une blague sur un th√®me donn√©\n",
    "# Pour ce faire, nous utilisons des KernelArguments (ici, seul l'input est n√©cessaire)\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "print(joke_function)\n",
    "\n",
    "# Exemple : g√©n√©rer une blague sur \"time travel to dinosaur age\" avec un style \"super silly\"\n",
    "joke_response = await kernel.invoke(joke_function, KernelArguments(input=\"time travel to dinosaur age\", style=\"super silly\"))\n",
    "print(\"Blague g√©n√©r√©e :\", joke_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D√©finir une fonction s√©mantique en ligne\n",
    "\n",
    "Outre l'utilisation de plugins stock√©s sur disque, il est possible de d√©finir des fonctions s√©mantiques directement dans votre code Python.  \n",
    "Cette approche est particuli√®rement utile pour :\n",
    "- G√©n√©rer dynamiquement des prompts en fonction du contexte\n",
    "- Prototyper rapidement des id√©es sans cr√©er de fichiers s√©par√©s\n",
    "\n",
    "Dans cet exemple, nous allons cr√©er une fonction qui r√©sume un texte donn√© en quelques mots (TL;DR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "\n",
    "# D√©finition du prompt\n",
    "tldr_prompt = \"\"\"\n",
    "{{$input}}\n",
    "\n",
    "Donne-moi un r√©sum√© en 5 mots ou moins.\n",
    "\"\"\"\n",
    "\n",
    "# Configuration de l'ex√©cution\n",
    "execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "    service_id=\"default\",\n",
    "    ai_model_id=model_id,\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Configuration du prompt template\n",
    "tldr_template_config = PromptTemplateConfig(\n",
    "    template=tldr_prompt,\n",
    "    name=\"tldr\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[InputVariable(name=\"input\", description=\"Texte √† r√©sumer\", is_required=True)],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "# Ajout de la fonction au Kernel\n",
    "tldr_function = kernel.add_function(function_name=\"tldrFunction\", plugin_name=\"tldrPlugin\", prompt_template_config=tldr_template_config)\n",
    "\n",
    "# Ex√©cution de la fonction\n",
    "async def run_tldr():\n",
    "    input_text = \"Demo √©tait une po√©tesse grecque ancienne connue pour un unique po√®me grav√© sur le Colosse de Memnon.\"\n",
    "    tldr_summary = await kernel.invoke(tldr_function, KernelArguments(input=input_text))\n",
    "    print(\"R√©sum√© (TL;DR) :\", tldr_summary)\n",
    "\n",
    "# Lancer la fonction\n",
    "await run_tldr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat interactif avec le Kernel\n",
    "\n",
    "Semantic Kernel offre √©galement la possibilit√© de cr√©er des interactions de type chatbot.  \n",
    "Nous allons configurer une fonction de chat qui utilise des **Kernel Arguments** pour conserver l'historique de la conversation.\n",
    "\n",
    "L'objectif est de simuler une conversation o√π l'utilisateur envoie un message, le bot y r√©pond, et l'historique est mis √† jour √† chaque √©change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "# Initialiser l'historique\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_system_message(\"Vous √™tes un chatbot utile et vous fournissez des recommandations de livres.\")\n",
    "\n",
    "# D√©finition du prompt de chat\n",
    "chat_prompt = \"\"\"\n",
    "{{$history}}\n",
    "User: {{$user_input}}\n",
    "ChatBot:\n",
    "\"\"\"\n",
    "\n",
    "# Configuration de l'ex√©cution\n",
    "chat_exec_settings = OpenAIChatPromptExecutionSettings(\n",
    "    service_id=\"default\",\n",
    "    ai_model_id=model_id,\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Configuration du chat template\n",
    "chat_template_config = PromptTemplateConfig(\n",
    "    template=chat_prompt,\n",
    "    name=\"chat\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"user_input\", description=\"Message de l'utilisateur\", is_required=True),\n",
    "        InputVariable(name=\"history\", description=\"Historique de la conversation\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=chat_exec_settings,\n",
    ")\n",
    "\n",
    "# Ajout au Kernel\n",
    "chat_function = kernel.add_function(function_name=\"chat\", plugin_name=\"chatPlugin\", prompt_template_config=chat_template_config)\n",
    "\n",
    "# Fonction asynchrone pour le chat\n",
    "async def chat(input_text: str):\n",
    "    print(f\"Utilisateur : {input_text}\")\n",
    "    response = await kernel.invoke(chat_function, KernelArguments(\n",
    "        user_input=input_text, \n",
    "        history=str(chat_history),\n",
    "        allow_dangerously_set_content=True\n",
    "    ))\n",
    "    print(f\"ChatBot : {response}\")\n",
    "    chat_history.add_user_message(input_text)\n",
    "    chat_history.add_assistant_message(str(response))\n",
    "\n",
    "# Ex√©cution des exemples\n",
    "await chat(\"Salut, je cherche des suggestions de livres sur la philosophie antique.\")\n",
    "await chat(\"Peux-tu m'en recommander quelques-uns ?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons :\n",
    "- Install√© et configur√© **Semantic Kernel**.\n",
    "- Charg√© et configur√© le service LLM (ici, OpenAI).\n",
    "- Utilis√© un plugin de prompt pour g√©n√©rer une blague.\n",
    "- D√©fini une fonction s√©mantique inline pour obtenir un r√©sum√© (TL;DR) d‚Äôun texte.\n",
    "- Cr√©√© une interaction de type chatbot en g√©rant l'historique via **Kernel Arguments**.\n",
    "\n",
    "Ces exemples illustrent comment Semantic Kernel peut vous aider √† int√©grer des capacit√©s avanc√©es de traitement du langage naturel dans vos applications Python de mani√®re modulaire et flexible.\n",
    "\n",
    "Pour aller plus loin, vous pouvez explorer :\n",
    "- La cr√©ation de plugins plus complexes.\n",
    "- L'int√©gration de fonctions suppl√©mentaires et la gestion d'historique persistant.\n",
    "- L'utilisation de Semantic Kernel avec diff√©rents services LLM (Azure OpenAI, HuggingFace, etc.).\n",
    "\n",
    "Bonne exploration et n'h√©sitez pas √† poser des questions !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
