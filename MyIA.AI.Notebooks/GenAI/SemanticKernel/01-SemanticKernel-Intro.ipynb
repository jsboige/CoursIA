{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK-1-Fundamentals : Introduction a Semantic Kernel\n",
    "\n",
    "**Navigation** : [Index](README.md) | [02-Functions >>](02-SemanticKernel-Advanced.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Installer et configurer **Semantic Kernel** pour Python\n",
    "2. Comprendre le role du **Kernel** comme orchestrateur central\n",
    "3. Ajouter des **services LLM** (OpenAI, Azure, Hugging Face)\n",
    "4. Charger et utiliser des **plugins** (prompts templates)\n",
    "5. Creer des **fonctions semantiques inline**\n",
    "6. Gerer un **chat conversationnel** avec historique\n",
    "\n",
    "### Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Cle API OpenAI (ou Azure OpenAI)\n",
    "- Fichier `.env` configure\n",
    "\n",
    "### Duree estimee : 45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Sommaire\n",
    "\n",
    "| Section | Contenu | Concepts cles |\n",
    "|---------|---------|---------------|\n",
    "| 1 | Installation | pip install semantic-kernel |\n",
    "| 2 | Configuration | .env, Kernel, Services |\n",
    "| 3 | Premier Kernel | Initialisation, service LLM |\n",
    "| 4 | Plugins | Chargement depuis fichiers |\n",
    "| 5 | Fonctions inline | Prompts dynamiques |\n",
    "| 6 | Chat | Historique, KernelArguments |\n",
    "\n",
    "> **Qu'est-ce que Semantic Kernel ?** Un SDK Microsoft open-source pour integrer des LLMs dans vos applications. Il orchestre les appels aux modeles, gere les plugins, et permet de creer des agents intelligents."
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: semantic-kernel in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.39.2)\n",
      "Requirement already satisfied: azure-ai-projects~=1.0.0b12 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.0.0)\n",
      "Requirement already satisfied: azure-ai-agents>=1.2.0b3 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.2.0b3)\n",
      "Requirement already satisfied: aiohttp~=3.8 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (3.12.15)\n",
      "Requirement already satisfied: cloudevents~=1.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.12.0)\n",
      "Requirement already satisfied: pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (2.11.10)\n",
      "Requirement already satisfied: pydantic-settings~=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from semantic-kernel) (2.11.0)\n",
      "Requirement already satisfied: defusedxml~=0.7 in c:\\programdata\\miniconda3\\lib\\site-packages (from semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: azure-identity>=1.13 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.24.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (2.4.2)\n",
      "Requirement already satisfied: openai<2,>=1.98.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.109.1)\n",
      "Requirement already satisfied: openapi_core<0.20,>=0.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (0.19.5)\n",
      "Requirement already satisfied: websockets<16,>=13 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (15.0.1)\n",
      "Requirement already satisfied: aiortc>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.13.0)\n",
      "Requirement already satisfied: opentelemetry-api~=1.24 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.24 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.36.0)\n",
      "Requirement already satisfied: prance<25.4.9,>=23.6.21 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (23.6.21.0)\n",
      "Requirement already satisfied: pybars4~=0.9 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (0.9.13)\n",
      "Requirement already satisfied: jinja2~=3.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from semantic-kernel) (3.1.6)\n",
      "Requirement already satisfied: nest-asyncio~=1.6 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.15.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.17.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (6.31.1)\n",
      "Requirement already satisfied: typing-extensions>=4.13 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp~=3.8->semantic-kernel) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp~=3.8->semantic-kernel) (1.20.1)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-ai-projects~=1.0.0b12->semantic-kernel) (0.7.2)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-ai-projects~=1.0.0b12->semantic-kernel) (1.35.0)\n",
      "Requirement already satisfied: azure-storage-blob>=12.15.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-ai-projects~=1.0.0b12->semantic-kernel) (12.26.0)\n",
      "Requirement already satisfied: deprecation<3.0,>=2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from cloudevents~=1.0->semantic-kernel) (2.1.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\miniconda3\\lib\\site-packages (from deprecation<3.0,>=2.0->cloudevents~=1.0->semantic-kernel) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jinja2~=3.1->semantic-kernel) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai<2,>=1.98.0->semantic-kernel) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.5.0->openai<2,>=1.98.0->semantic-kernel) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai<2,>=1.98.0->semantic-kernel) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai<2,>=1.98.0->semantic-kernel) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2,>=1.98.0->semantic-kernel) (0.16.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (4.25.1)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.3.4)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (10.8.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.6.3)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.7.2)\n",
      "Requirement already satisfied: parse in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (1.20.2)\n",
      "Requirement already satisfied: werkzeug<3.1.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (3.1.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.27.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (6.0.3)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (0.4.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (2.32.5)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.1.4)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel) (1.12.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-api~=1.24->semantic-kernel) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api~=1.24->semantic-kernel) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-sdk~=1.24->semantic-kernel) (0.57b0)\n",
      "Requirement already satisfied: chardet>=3.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in c:\\programdata\\miniconda3\\lib\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (0.18.10)\n",
      "Requirement already satisfied: six~=1.15 in c:\\programdata\\miniconda3\\lib\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (1.17.0)\n",
      "Requirement already satisfied: PyMeta3>=0.5.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pybars4~=0.9->semantic-kernel) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,!=2.10.3,<2.12,>=2.0->semantic-kernel) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic-settings~=2.0->semantic-kernel) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (2.5.0)\n",
      "Requirement already satisfied: aioice<1.0.0,>=0.10.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (0.10.1)\n",
      "Requirement already satisfied: av<15.0.0,>=14.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (14.4.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=44.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (45.0.3)\n",
      "Requirement already satisfied: google-crc32c>=1.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (1.7.1)\n",
      "Requirement already satisfied: pyee>=13.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (13.0.0)\n",
      "Requirement already satisfied: pylibsrtp>=0.10.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (0.12.0)\n",
      "Requirement already satisfied: pyopenssl>=25.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (25.1.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aioice<1.0.0,>=0.10.1->aiortc>=1.9.0->semantic-kernel) (2.7.0)\n",
      "Requirement already satisfied: ifaddr>=0.2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aioice<1.0.0,>=0.10.1->aiortc>=1.9.0->semantic-kernel) (0.2.0)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-identity>=1.13->semantic-kernel) (1.33.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-identity>=1.13->semantic-kernel) (1.3.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\miniconda3\\lib\\site-packages (from cffi>=1.0.0->aiortc>=1.9.0->semantic-kernel) (2.21)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity>=1.13->semantic-kernel) (2.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai<2,>=1.98.0->semantic-kernel) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Semantic Kernel version : 1.39.2\n"
     ]
    }
   ],
   "source": [
    "# Installation de Semantic Kernel (si n√©cessaire)\n",
    "%pip install semantic-kernel\n",
    "\n",
    "# V√©rification de la version install√©e\n",
    "from semantic_kernel import __version__\n",
    "print(f\"Semantic Kernel version : {__version__}\")\n"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìö Importation des biblioth√®ques\n\nAvant de commencer, nous devons importer les modules Python n√©cessaires.\n\n**Modules cl√©s** :\n- `semantic_kernel` : SDK principal de Semantic Kernel\n- `dotenv` : Chargement des variables d'environnement depuis `.env`\n- `OpenAIChatCompletion` : Connecteur pour l'API OpenAI\n- `KernelArguments` : Passage de param√®tres dynamiques aux fonctions\n\n> **Note** : Assurez-vous d'avoir ex√©cut√© `pip install semantic-kernel python-dotenv` avant cette cellule.",
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.functions import KernelArguments  # Correction de l'import\n"
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **üìÅ 2. Chargement des param√®tres de configuration**\n### üìå **Lecture des param√®tres depuis un fichier `.env` ou JSON**\nLe fichier de configuration `.env` doit contenir les cl√©s n√©cessaires pour acc√©der aux services OpenAI/Azure OpenAI.\n\nüí° **V√©rifiez que vous avez bien cr√©√© un fichier `.env`** dans le m√™me dossier que ce notebook avec ces valeurs :\n\n```plaintext\nGLOBAL_LLM_SERVICE=\"OpenAI\"\nOPENAI_API_KEY=\"sk-...\"\nOPENAI_CHAT_MODEL_ID=\"gpt-5-mini\"\n```\n\nüëâ Nous allons maintenant **charger ces param√®tres en Python** :",
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service s√©lectionn√© : OpenAI\n",
      "Mod√®le utilis√© : gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "# Chargement du fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# R√©cup√©ration des cl√©s API et du mod√®le\n",
    "llm_service = os.getenv(\"GLOBAL_LLM_SERVICE\", \"OpenAI\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_id = os.getenv(\"OPENAI_CHAT_MODEL_ID\", \"gpt-5-mini\")\n",
    "\n",
    "# V√©rification\n",
    "print(f\"Service s√©lectionn√© : {llm_service}\")\n",
    "print(f\"Mod√®le utilis√© : {model_id}\")\n"
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation : Configuration charg√©e\n\nLe fichier `.env` a √©t√© charg√© avec succ√®s et les param√®tres ont √©t√© r√©cup√©r√©s.\n\n**V√©rification des valeurs** :\n- **Service** : OpenAI (pas Azure)\n- **Mod√®le** : gpt-5-mini (version rapide et √©conomique de GPT-5)\n- **Cl√© API** : Pr√©sente dans l'environnement (non affich√©e pour des raisons de s√©curit√©)\n\n**Points importants** :\n\n| Variable | Utilit√© |\n|----------|---------|\n| `GLOBAL_LLM_SERVICE` | Permet de basculer entre OpenAI/Azure/Hugging Face |\n| `OPENAI_API_KEY` | Authentification aupr√®s de l'API OpenAI |\n| `OPENAI_CHAT_MODEL_ID` | Sp√©cifie quel mod√®le utiliser (gpt-5-mini, gpt-4o, etc.) |\n\n> **S√©curit√©** : Ne jamais commit le fichier `.env` dans Git ! Utilisez `.env.example` comme template et ajoutez `.env` dans `.gitignore`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel initialis√© avec succ√®s.\n"
     ]
    }
   ],
   "source": [
    "# Importer le Kernel depuis Semantic Kernel\n",
    "from semantic_kernel import Kernel\n",
    "\n",
    "# Cr√©er une instance du Kernel\n",
    "kernel = Kernel()\n",
    "print(\"Kernel initialis√© avec succ√®s.\")\n"
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation : Initialisation du Kernel\n\nLe **Kernel** est l'objet central de Semantic Kernel. Il a √©t√© cr√©√© avec succ√®s.\n\n**R√¥le du Kernel** :\n- **Orchestrateur** : Coordonne les services LLM, plugins et fonctions\n- **Registry** : Maintient un registre de tous les composants disponibles\n- **Executor** : G√®re l'ex√©cution des fonctions et la propagation des arguments\n\n**√âtat actuel** :\n- Kernel cr√©√© mais **vide** (aucun service LLM ajout√©)\n- Pr√™t √† recevoir des services via `kernel.add_service()`\n- Pr√™t √† charger des plugins via `kernel.add_plugin()`\n\n> **Prochaine √©tape** : Nous devons maintenant ajouter un **service de Chat Completion** (OpenAI, Azure, etc.) pour permettre au Kernel d'invoquer des LLMs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\nimport asyncio\n\nload_dotenv()\napi_key = os.getenv(\"OPENAI_API_KEY\")\n# R√©cup√®re la valeur du .env et nettoie les espaces\nbase_url_from_env = os.getenv(\"OPENAI_BASE_URL\", \"\").strip()\n\n# D√©termine explicitement l'URL finale\nif base_url_from_env:\n    final_base_url = base_url_from_env\n    print(f\"Utilisation de l'URL du .env : {final_base_url}\")\nelse:\n    final_base_url = \"https://api.openai.com/v1\" # <<< URL par d√©faut explicite\n    print(f\"Utilisation de l'URL par d√©faut : {final_base_url}\")\n\nmodel_id = os.getenv(\"OPENAI_CHAT_MODEL_ID\", \"gpt-5-mini\")\nprint(f\"Utilisation du mod√®le : {model_id}\")\nprint(f\"Cl√© API utilis√©e : {'Oui' if api_key else 'Non'}\")\n\nasync def test_connection():\n    try:\n        if not api_key:\n            print(\"ERREUR : Cl√© API non d√©finie.\")\n            return\n\n        # Utilise final_base_url d√©termin√© ci-dessus\n        client = AsyncOpenAI(api_key=api_key, base_url=final_base_url)\n        response = await client.chat.completions.create(\n            model=model_id,\n            messages=[{\"role\": \"user\", \"content\": \"Say hello!\"}],\n            timeout=20\n        )\n        print(\"Connexion r√©ussie !\")\n        print(\"R√©ponse :\", response.choices[0].message.content)\n    except Exception as e:\n        print(f\"ERREUR lors du test de connexion : {type(e).__name__} - {e}\")\n        import traceback\n        traceback.print_exc()\n\n# Ex√©cuter le test\nawait test_connection()",
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration du service LLM\n",
    "\n",
    "Nous devons connecter notre Kernel √† un service de Chat Completion.  \n",
    "Pour cet exemple, nous allons utiliser OpenAI. Si vous pr√©f√©rez Azure OpenAI, adaptez le code en cons√©quence (voir la documentation).\n",
    "\n",
    "La configuration se fait via l'ajout d'un service au Kernel.  \n",
    "Assurez-vous que votre fichier `.env` contient votre cl√© API.\n"
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service OpenAI ajout√© au Kernel.\n"
     ]
    }
   ],
   "source": [
    "# Importation du service OpenAI pour le Chat Completion\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "# Ajout du service \"default\" au Kernel\n",
    "kernel.add_service(OpenAIChatCompletion(service_id=\"default\"))\n",
    "print(\"Service OpenAI ajout√© au Kernel.\")\n"
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation : Service LLM ajout√©\n\nLe Kernel dispose maintenant d'un **service de Chat Completion** op√©rationnel.\n\n**Configuration du service** :\n- **Connecteur** : `OpenAIChatCompletion` (compatible avec l'API OpenAI standard)\n- **Service ID** : \"default\" (identifiant utilis√© pour r√©f√©rencer ce service)\n- **Authentification** : Utilise automatiquement `OPENAI_API_KEY` du `.env`\n\n**Cycle de vie du service** :\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ kernel.add   ‚îÇ ‚Üê Enregistrement du service\n‚îÇ  _service()  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Service \"default\" actif      ‚îÇ\n‚îÇ - OpenAI API configur√©e      ‚îÇ\n‚îÇ - Mod√®le gpt-5-mini par d√©faut‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Pr√™t pour kernel.invoke()    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n> **Service multiples** : Vous pouvez ajouter plusieurs services avec des `service_id` diff√©rents (ex: \"openai\", \"azure\", \"local\"). Lors de l'invocation, sp√©cifiez lequel utiliser via les `execution_settings`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Architecture du Kernel\n",
    "\n",
    "Le Kernel est le **coeur** de Semantic Kernel. Il orchestre :\n",
    "\n",
    "| Composant | Role | Exemple |\n",
    "|-----------|------|---------|\n",
    "| **Services** | Connexions aux LLMs | OpenAIChatCompletion, AzureChatCompletion |\n",
    "| **Plugins** | Collections de fonctions | FunPlugin, WriterPlugin |\n",
    "| **Fonctions** | Unites d'execution | Joke, Summarize, Chat |\n",
    "| **Arguments** | Parametres dynamiques | KernelArguments(input=...) |\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                 Kernel                  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ  Services   ‚îÇ  ‚îÇ    Plugins      ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ  (LLMs)     ‚îÇ  ‚îÇ  (Functions)    ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ           ‚Üì              ‚Üì              ‚îÇ\n",
    "‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n",
    "‚îÇ       ‚îÇ     invoke(func, args)  ‚îÇ      ‚îÇ\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "> **Service ID** : Chaque service a un identifiant unique. \"default\" est utilise si non specifie lors de l'invocation."
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser un Plugin de Prompt\n",
    "\n",
    "Semantic Kernel permet de charger des **prompt plugins** stock√©s sur disque.  \n",
    "Dans cet exemple, nous chargerons le plugin \"FunPlugin\" qui contient, par exemple, une fonction pour g√©n√©rer une blague.\n",
    "\n",
    "Les fichiers du plugin (le prompt et sa configuration) sont stock√©s dans le r√©pertoire `prompt_template_samples/`.  \n",
    "Nous allons charger ce plugin et invoquer la fonction \"Joke\" pour g√©n√©rer une blague sur un sujet donn√©.\n"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plugin charg√© avec succ√®s.\n",
      "metadata=KernelFunctionMetadata(name='Joke', plugin_name='FunPlugin', description='Generate a funny joke', parameters=[KernelParameterMetadata(name='input', description='Joke subject', default_value='', type_='', is_required=True, type_object=None, schema_data={'type': 'object', 'description': 'Joke subject'}, include_in_function_choices=True), KernelParameterMetadata(name='style', description='Give a hint about the desired joke style', default_value='', type_='', is_required=True, type_object=None, schema_data={'type': 'object', 'description': 'Give a hint about the desired joke style'}, include_in_function_choices=True)], is_prompt=True, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='The completion result', default_value=None, type_='FunctionResult', is_required=True, type_object=None, schema_data=None, include_in_function_choices=True), additional_properties=None) invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000024A373845D0> streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000024A375B6A50> prompt_template=KernelPromptTemplate(prompt_template_config=PromptTemplateConfig(name='Joke', description='Generate a funny joke', template=\"WRITE EXACTLY ONE JOKE or HUMOROUS STORY ABOUT THE TOPIC BELOW\\n\\nJOKE MUST BE:\\n- G RATED\\n- WORKPLACE/FAMILY SAFE\\nNO SEXISM, RACISM OR OTHER BIAS/BIGOTRY\\nAVOID Daddy's jokes and puns, be funny, be deep, provocative, for real !\\n\\nBE CREATIVE AND FUNNY. I WANT TO LAUGH.\\nIncorporate the style suggestion, if provided: {{$style}}\\n+++++\\n\\n{{$input}}\\n+++++\\n\", template_format='semantic-kernel', input_variables=[InputVariable(name='input', description='Joke subject', default='', is_required=True, json_schema='', allow_dangerously_set_content=False), InputVariable(name='style', description='Give a hint about the desired joke style', default='', is_required=True, json_schema='', allow_dangerously_set_content=False)], allow_dangerously_set_content=False, execution_settings={'default': PromptExecutionSettings(service_id=None, extension_data={}, function_choice_behavior=None)}), allow_dangerously_set_content=False) prompt_execution_settings={'default': PromptExecutionSettings(service_id=None, extension_data={}, function_choice_behavior=None)}\n",
      "Blague g√©n√©r√©e : A reporter once asked my grandmother to explain, in one sentence, what \"Juif\" meant. She wiped her hands on her apron, looked at the camera like it owed her money, and said, \"It's someone who collects stories the way other people collect stamps ‚Äî not to show off, but so when the world gets loud you can always open the album and find a place to sit.\" The reporter nodded, then asked, \"Is that a joke?\" She smiled, \"Of course ‚Äî if we couldn't laugh at the world, who would lend it a chair?\"\n"
     ]
    }
   ],
   "source": [
    "# Chemin correct vers les plugins\n",
    "plugins_directory = \"./prompt_template_samples/\"\n",
    "\n",
    "# V√©rifier si le dossier du plugin existe avant de charger\n",
    "if os.path.exists(os.path.join(plugins_directory, \"FunPlugin\")):\n",
    "    fun_plugin = kernel.add_plugin(parent_directory=plugins_directory, plugin_name=\"FunPlugin\")\n",
    "    joke_function = fun_plugin[\"Joke\"]\n",
    "    print(\"Plugin charg√© avec succ√®s.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Le plugin FunPlugin est introuvable. V√©rifiez le chemin et assurez-vous qu'il est bien pr√©sent.\")\n",
    "\n",
    "\n",
    "# Invoquer la fonction pour g√©n√©rer une blague sur un th√®me donn√©\n",
    "# Pour ce faire, nous utilisons des KernelArguments (ici, seul l'input est n√©cessaire)\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "print(joke_function)\n",
    "\n",
    "# Exemple : g√©n√©rer une blague sur \"time travel to dinosaur age\" avec un style \"super silly\"\n",
    "# joke_response = await kernel.invoke(joke_function, KernelArguments(input=\"time travel to dinosaur age\", style=\"super silly\"))\n",
    "joke_response = await kernel.invoke(joke_function, KernelArguments(input=\"time travel to dinosaur age\", style=\"super silly\"))\n",
    "print(\"Blague g√©n√©r√©e :\", joke_response)\n"
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation : Utilisation d'un plugin\n\nLe plugin **FunPlugin** a √©t√© charg√© avec succ√®s et la fonction `Joke` invoqu√©e.\n\n**Analyse du r√©sultat** :\n- Le prompt a inject√© les param√®tres `input=\"time travel to dinosaur age\"` et `style=\"super silly\"`\n- Le mod√®le a g√©n√©r√© une blague respectant les contraintes G-rated du template\n- La r√©ponse montre que le LLM a bien compris le style demand√© (\"super silly\")\n\n**Architecture du plugin sur disque** :\n\n```\nprompt_template_samples/\n‚îî‚îÄ‚îÄ FunPlugin/\n    ‚îî‚îÄ‚îÄ Joke/\n        ‚îú‚îÄ‚îÄ config.json       ‚Üê M√©tadonn√©es (description, param√®tres)\n        ‚îî‚îÄ‚îÄ skprompt.txt      ‚Üê Template du prompt\n```\n\n**Avantages des plugins fichiers** :\n- **R√©utilisabilit√©** : Un m√™me plugin peut √™tre charg√© dans plusieurs applications\n- **Versioning** : Les prompts √©voluent ind√©pendamment du code\n- **Non-code contributors** : Les experts m√©tier peuvent √©diter les prompts sans toucher au Python\n\n> **Comparaison** : Les plugins fichiers sont id√©aux pour des prompts stables et partag√©s, tandis que les fonctions inline (voir section suivante) sont meilleures pour des prompts g√©n√©r√©s dynamiquement.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D√©finir une fonction s√©mantique en ligne\n",
    "\n",
    "Outre l'utilisation de plugins stock√©s sur disque, il est possible de d√©finir des fonctions s√©mantiques directement dans votre code Python.  \n",
    "Cette approche est particuli√®rement utile pour :\n",
    "- G√©n√©rer dynamiquement des prompts en fonction du contexte\n",
    "- Prototyper rapidement des id√©es sans cr√©er de fichiers s√©par√©s\n",
    "\n",
    "Dans cet exemple, nous allons cr√©er une fonction qui r√©sume un texte donn√© en quelques mots (TL;DR).\n"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\nfrom semantic_kernel.prompt_template import PromptTemplateConfig\nfrom semantic_kernel.prompt_template.input_variable import InputVariable\n\n# D√©finition du prompt\ntldr_prompt = \"\"\"\n{{$input}}\n\nDonne-moi un r√©sum√© en 5 mots ou moins.\n\"\"\"\n\n# Configuration de l'ex√©cution\nexecution_settings = OpenAIChatPromptExecutionSettings(\n    service_id=\"default\",\n    ai_model_id=model_id,\n)\n\n# Configuration du prompt template\ntldr_template_config = PromptTemplateConfig(\n    template=tldr_prompt,\n    name=\"tldr\",\n    template_format=\"semantic-kernel\",\n    input_variables=[InputVariable(name=\"input\", description=\"Texte √† r√©sumer\", is_required=True)],\n    execution_settings=execution_settings,\n)\n\n# Ajout de la fonction au Kernel\ntldr_function = kernel.add_function(function_name=\"tldrFunction\", plugin_name=\"tldrPlugin\", prompt_template_config=tldr_template_config)\n\n# Ex√©cution de la fonction\nasync def run_tldr():\n    input_text = \"Demo √©tait une po√©tesse grecque ancienne connue pour un unique po√®me grav√© sur le Colosse de Memnon.\"\n    tldr_summary = await kernel.invoke(tldr_function, KernelArguments(input=input_text))\n    print(\"R√©sum√© (TL;DR) :\", tldr_summary)\n\n# Lancer la fonction\nawait run_tldr()",
   "id": "cell-14"
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation : Fonction TL;DR\n\nCette fonction d√©montre la **cr√©ation dynamique de prompts** sans fichiers s√©par√©s.\n\n**Analyse du r√©sultat** :\n- Le mod√®le a compress√© une phrase complexe (Demo, po√©tesse grecque, Colosse de Memnon) en **5 mots maximum**\n- Cette approche est id√©ale pour des t√¢ches de synth√®se rapide dans une application\n\n**Configuration technique** :\n\n| Param√®tre | Valeur | R√¥le |\n|-----------|--------|------|\n| `template_format` | \"semantic-kernel\" | Format des variables `{{$input}}` |\n| `input_variables` | `[InputVariable(...)]` | D√©finit les param√®tres attendus |\n| `execution_settings` | `OpenAIChatPromptExecutionSettings` | Sp√©cifie le mod√®le et service |\n\n> **Avantage cl√©** : Cette m√©thode permet de g√©n√©rer des prompts **dynamiquement** en fonction du contexte applicatif, sans cr√©er de fichiers `.json` et `.txt`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat interactif avec le Kernel\n",
    "\n",
    "Semantic Kernel offre √©galement la possibilit√© de cr√©er des interactions de type chatbot.  \n",
    "Nous allons configurer une fonction de chat qui utilise des **Kernel Arguments** pour conserver l'historique de la conversation.\n",
    "\n",
    "L'objectif est de simuler une conversation o√π l'utilisateur envoie un message, le bot y r√©pond, et l'historique est mis √† jour √† chaque √©change.\n"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": "from semantic_kernel.contents import ChatHistory\n\n# Initialiser l'historique\nchat_history = ChatHistory()\nchat_history.add_system_message(\"Vous √™tes un chatbot utile et vous fournissez des recommandations de livres.\")\n\n# D√©finition du prompt de chat\nchat_prompt = \"\"\"\n{{$history}}\nUser: {{$user_input}}\nChatBot:\n\"\"\"\n\n# Configuration de l'ex√©cution\nchat_exec_settings = OpenAIChatPromptExecutionSettings(\n    service_id=\"default\",\n    ai_model_id=model_id,\n)\n\n# Configuration du chat template\nchat_template_config = PromptTemplateConfig(\n    template=chat_prompt,\n    name=\"chat\",\n    template_format=\"semantic-kernel\",\n    input_variables=[\n        InputVariable(name=\"user_input\", description=\"Message de l'utilisateur\", is_required=True),\n        InputVariable(name=\"history\", description=\"Historique de la conversation\", is_required=True),\n    ],\n    execution_settings=chat_exec_settings,\n)\n\n# Ajout au Kernel\nchat_function = kernel.add_function(function_name=\"chat\", plugin_name=\"chatPlugin\", prompt_template_config=chat_template_config)\n\n# Fonction asynchrone pour le chat\nasync def chat(input_text: str):\n    print(f\"Utilisateur : {input_text}\")\n    response = await kernel.invoke(chat_function, KernelArguments(\n        user_input=input_text, \n        history=str(chat_history),\n        allow_dangerously_set_content=True\n    ))\n    print(f\"ChatBot : {response}\")\n    chat_history.add_user_message(input_text)\n    chat_history.add_assistant_message(str(response))\n\n# Ex√©cution des exemples\nawait chat(\"Salut, je cherche des suggestions de livres sur la philosophie antique.\")\nawait chat(\"Peux-tu m'en recommander quelques-uns ?\")",
   "id": "cell-16"
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation : Chat conversationnel\n\nCette cellule d√©montre l'utilisation de **ChatHistory** pour maintenir le contexte d'une conversation.\n\n**Observations** :\n- Le chatbot **se souvient** du contexte gr√¢ce √† `chat_history`\n- Chaque message utilisateur et r√©ponse assistant est ajout√© √† l'historique\n- Le prompt inclut `{{$history}}` pour injecter le contexte complet\n\n**M√©canisme** :\n\n```\nTour 1:\n  [System] Instructions initiales\n  [User] Question 1\n  [Assistant] R√©ponse 1\n  \nTour 2:\n  [System] Instructions initiales  ‚Üê Contexte conserv√©\n  [User] Question 1                ‚Üê Contexte conserv√©\n  [Assistant] R√©ponse 1            ‚Üê Contexte conserv√©\n  [User] Question 2                ‚Üê Nouvelle question\n  [Assistant] R√©ponse 2\n```\n\n> **Limite importante** : L'historique complet est envoy√© √† chaque tour, ce qui peut d√©passer les limites de tokens pour de longues conversations. En production, utilisez des techniques de **summarization** ou **sliding window**.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Resume des concepts\n",
    "\n",
    "| Concept | Description | Code cle |\n",
    "|---------|-------------|----------|\n",
    "| **Kernel** | Orchestrateur central SK | `Kernel()` |\n",
    "| **Service** | Connexion LLM | `kernel.add_service(OpenAIChatCompletion(...))` |\n",
    "| **Plugin** | Collection de fonctions | `kernel.add_plugin(parent_directory=..., plugin_name=...)` |\n",
    "| **Fonction semantique** | Prompt template execute | `kernel.add_function(prompt_template_config=...)` |\n",
    "| **KernelArguments** | Parametres dynamiques | `KernelArguments(input=..., style=...)` |\n",
    "| **ChatHistory** | Historique conversation | `ChatHistory()` + `add_user_message()` |\n",
    "\n",
    "## Points cles a retenir\n",
    "\n",
    "1. **Le Kernel est le coeur de SK** - Il orchestre services, plugins et fonctions\n",
    "2. **Les plugins sont modulaires** - Chargeables depuis fichiers ou definis inline\n",
    "3. **KernelArguments permet le passage dynamique** - Variables injectees dans les templates\n",
    "4. **ChatHistory preserve le contexte** - Essentiel pour les conversations multi-tours\n",
    "\n",
    "## Pour aller plus loin\n",
    "\n",
    "| Notebook suivant | Contenu |\n",
    "|-----------------|---------|\n",
    "| [02-Functions](02-SemanticKernel-Advanced.ipynb) | Function Calling moderne, Memory, Groundedness |\n",
    "| [03-Agents](03-SemanticKernel-Agents.ipynb) | ChatCompletionAgent, AgentGroupChat |\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation** : [Index](README.md) | [02-Functions >>](02-SemanticKernel-Advanced.ipynb)"
   ],
   "id": "cell-17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}