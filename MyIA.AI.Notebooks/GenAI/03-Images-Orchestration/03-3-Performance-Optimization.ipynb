{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Performance Optimization pour la GÃ©nÃ©ration d'Images\n",
    "\n",
    "**Module :** 03-Images-Orchestration  \n",
    "**Niveau :** IntermÃ©diaire  \n",
    "**DurÃ©e estimÃ©e :** 45 minutes  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] MaÃ®triser les techniques de profilage GPU et mÃ©moire\n",
    "- [ ] ImplÃ©menter la quantification pour rÃ©duire l'empreinte mÃ©moire\n",
    "- [ ] Optimiser les pipelines avec attention mechanisms avancÃ©s\n",
    "- [ ] Concevoir des stratÃ©gies de batch processing efficaces\n",
    "- [ ] Utiliser le caching pour accÃ©lÃ©rer les workloads rÃ©pÃ©titifs\n",
    "\n",
    "## PrÃ©requis\n",
    "\n",
    "- Module 00 (Environment Setup) complÃ©tÃ©\n",
    "- Module 03-1 et 03-2 (Comparaison, Orchestration) complÃ©tÃ©s\n",
    "- GPU CUDA disponible (RTX 3060+ recommandÃ©)\n",
    "\n",
    "## Architecture du Notebook\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Performance Optimization                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Profilage                                                       â”‚\n",
    "â”‚     â”œâ”€â”€ GPU Memory Tracking                                         â”‚\n",
    "â”‚     â”œâ”€â”€ Inference Time Measurement                                  â”‚\n",
    "â”‚     â””â”€â”€ Bottleneck Identification                                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  2. Memory Optimization                                             â”‚\n",
    "â”‚     â”œâ”€â”€ FP16/BF16 Precision                                         â”‚\n",
    "â”‚     â”œâ”€â”€ Model Quantization (INT8, FP8)                             â”‚\n",
    "â”‚     â””â”€â”€ CPU Offloading                                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  3. Speed Optimization                                              â”‚\n",
    "â”‚     â”œâ”€â”€ xFormers / Flash Attention                                  â”‚\n",
    "â”‚     â”œâ”€â”€ Torch Compile                                               â”‚\n",
    "â”‚     â””â”€â”€ VAE Tiling/Slicing                                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  4. Batch & Caching                                                 â”‚\n",
    "â”‚     â”œâ”€â”€ Optimal Batch Sizing                                        â”‚\n",
    "â”‚     â”œâ”€â”€ Prompt Embedding Cache                                      â”‚\n",
    "â”‚     â””â”€â”€ Result Caching Strategy                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParamÃ¨tres Papermill - Configuration globale\n",
    "\n",
    "notebook_mode = \"interactive\"\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Configuration benchmarks\n",
    "benchmark_iterations = 3  # Nombre d'itÃ©rations par test\n",
    "warmup_runs = 1  # Runs de warmup avant mesure\n",
    "save_benchmark_results = True\n",
    "\n",
    "# Configuration mÃ©moire\n",
    "target_memory_reduction = 0.5  # Objectif: 50% de rÃ©duction\n",
    "enable_quantization = True\n",
    "enable_cpu_offload = False  # Activer si GPU < 8GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup et Utilitaires de Profilage\n",
    "\n",
    "CommenÃ§ons par crÃ©er les outils de mesure de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Callable, Any, Tuple\n",
    "from contextlib import contextmanager\n",
    "from functools import wraps\n",
    "import logging\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('perf_optimization')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ Performance Optimization - GenAI Image Generation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode: {notebook_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VÃ©rification GPU et imports conditionnels\n",
    "import torch\n",
    "\n",
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_COUNT = torch.cuda.device_count() if CUDA_AVAILABLE else 0\n",
    "\n",
    "if CUDA_AVAILABLE:\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    GPU_MEMORY_TOTAL = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"\\nâœ… GPU DÃ©tectÃ©: {GPU_NAME}\")\n",
    "    print(f\"   MÃ©moire totale: {GPU_MEMORY_TOTAL:.1f} GB\")\n",
    "    print(f\"   GPUs disponibles: {GPU_COUNT}\")\n",
    "    \n",
    "    # Multi-GPU info\n",
    "    if GPU_COUNT > 1:\n",
    "        print(f\"\\nğŸ“Š Configuration Multi-GPU:\")\n",
    "        for i in range(GPU_COUNT):\n",
    "            name = torch.cuda.get_device_name(i)\n",
    "            mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "            print(f\"   GPU {i}: {name} ({mem:.1f} GB)\")\n",
    "else:\n",
    "    GPU_NAME = \"CPU Only\"\n",
    "    GPU_MEMORY_TOTAL = 0\n",
    "    print(\"\\nâš ï¸ Pas de GPU CUDA dÃ©tectÃ© - Mode CPU uniquement\")\n",
    "    print(\"   Les optimisations GPU seront simulÃ©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Conteneur pour les mÃ©triques de performance.\"\"\"\n",
    "    name: str\n",
    "    execution_time_ms: float\n",
    "    gpu_memory_peak_mb: float\n",
    "    gpu_memory_allocated_mb: float\n",
    "    throughput_images_per_sec: float = 0.0\n",
    "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "    extra_info: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'execution_time_ms': self.execution_time_ms,\n",
    "            'gpu_memory_peak_mb': self.gpu_memory_peak_mb,\n",
    "            'gpu_memory_allocated_mb': self.gpu_memory_allocated_mb,\n",
    "            'throughput_images_per_sec': self.throughput_images_per_sec,\n",
    "            'timestamp': self.timestamp,\n",
    "            **self.extra_info\n",
    "        }\n",
    "\n",
    "\n",
    "class GPUProfiler:\n",
    "    \"\"\"Profiler GPU pour mesurer mÃ©moire et temps d'exÃ©cution.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics_history: List[PerformanceMetrics] = []\n",
    "        self.cuda_available = CUDA_AVAILABLE\n",
    "    \n",
    "    def get_memory_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Retourne les stats mÃ©moire GPU actuelles.\"\"\"\n",
    "        if not self.cuda_available:\n",
    "            return {'allocated_mb': 0, 'reserved_mb': 0, 'peak_mb': 0}\n",
    "        \n",
    "        return {\n",
    "            'allocated_mb': torch.cuda.memory_allocated() / (1024**2),\n",
    "            'reserved_mb': torch.cuda.memory_reserved() / (1024**2),\n",
    "            'peak_mb': torch.cuda.max_memory_allocated() / (1024**2)\n",
    "        }\n",
    "    \n",
    "    def reset_peak_memory(self):\n",
    "        \"\"\"Reset le compteur de mÃ©moire peak.\"\"\"\n",
    "        if self.cuda_available:\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"LibÃ¨re le cache GPU.\"\"\"\n",
    "        if self.cuda_available:\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    @contextmanager\n",
    "    def profile(self, name: str, num_images: int = 1):\n",
    "        \"\"\"Context manager pour profiler une opÃ©ration.\"\"\"\n",
    "        self.clear_cache()\n",
    "        self.reset_peak_memory()\n",
    "        \n",
    "        start_memory = self.get_memory_stats()\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        if self.cuda_available:\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            if self.cuda_available:\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            end_memory = self.get_memory_stats()\n",
    "            \n",
    "            execution_time_ms = (end_time - start_time) * 1000\n",
    "            throughput = num_images / (execution_time_ms / 1000) if execution_time_ms > 0 else 0\n",
    "            \n",
    "            metrics = PerformanceMetrics(\n",
    "                name=name,\n",
    "                execution_time_ms=execution_time_ms,\n",
    "                gpu_memory_peak_mb=end_memory['peak_mb'],\n",
    "                gpu_memory_allocated_mb=end_memory['allocated_mb'],\n",
    "                throughput_images_per_sec=throughput\n",
    "            )\n",
    "            self.metrics_history.append(metrics)\n",
    "    \n",
    "    def get_comparison_table(self) -> str:\n",
    "        \"\"\"GÃ©nÃ¨re un tableau de comparaison des mÃ©triques.\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            return \"Aucune mÃ©trique enregistrÃ©e\"\n",
    "        \n",
    "        lines = [\n",
    "            \"\\n\" + \"=\"*80,\n",
    "            f\"{'Test':<30} {'Temps (ms)':<15} {'MÃ©m Peak (MB)':<15} {'Throughput':<15}\",\n",
    "            \"=\"*80\n",
    "        ]\n",
    "        \n",
    "        for m in self.metrics_history:\n",
    "            throughput_str = f\"{m.throughput_images_per_sec:.2f} img/s\" if m.throughput_images_per_sec > 0 else \"N/A\"\n",
    "            lines.append(\n",
    "                f\"{m.name:<30} {m.execution_time_ms:<15.1f} {m.gpu_memory_peak_mb:<15.1f} {throughput_str:<15}\"\n",
    "            )\n",
    "        \n",
    "        lines.append(\"=\"*80)\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Instance globale du profiler\n",
    "profiler = GPUProfiler()\n",
    "print(\"\\nâœ… GPUProfiler initialisÃ©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse de la Baseline (Sans Optimisations)\n",
    "\n",
    "Avant d'optimiser, mesurons les performances de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage Ã©tat mÃ©moire initial\n",
    "print(\"ğŸ“Š Ã‰tat MÃ©moire Initial\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "initial_stats = profiler.get_memory_stats()\n",
    "print(f\"MÃ©moire allouÃ©e: {initial_stats['allocated_mb']:.1f} MB\")\n",
    "print(f\"MÃ©moire rÃ©servÃ©e: {initial_stats['reserved_mb']:.1f} MB\")\n",
    "\n",
    "if CUDA_AVAILABLE:\n",
    "    free_memory = GPU_MEMORY_TOTAL * 1024 - initial_stats['reserved_mb']\n",
    "    print(f\"MÃ©moire libre estimÃ©e: {free_memory:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_model_inference(size_mb: int = 1000, compute_ms: int = 500):\n",
    "    \"\"\"\n",
    "    Simule une infÃ©rence de modÃ¨le pour dÃ©monstration.\n",
    "    \n",
    "    En production, remplacer par le vrai pipeline de gÃ©nÃ©ration.\n",
    "    \"\"\"\n",
    "    if CUDA_AVAILABLE:\n",
    "        # Alloue de la mÃ©moire GPU pour simuler un modÃ¨le\n",
    "        elements = (size_mb * 1024 * 1024) // 4  # float32 = 4 bytes\n",
    "        tensor = torch.randn(elements, device='cuda')\n",
    "        \n",
    "        # Simule du calcul\n",
    "        for _ in range(10):\n",
    "            tensor = tensor * 1.001 + 0.001\n",
    "        \n",
    "        time.sleep(compute_ms / 1000)\n",
    "        del tensor\n",
    "    else:\n",
    "        time.sleep(compute_ms / 1000)\n",
    "\n",
    "\n",
    "# Test baseline\n",
    "print(\"\\nğŸ”¬ Test Baseline (Sans Optimisations)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "with profiler.profile(\"Baseline - FP32\"):\n",
    "    simulate_model_inference(size_mb=500, compute_ms=200)\n",
    "\n",
    "baseline_metrics = profiler.metrics_history[-1]\n",
    "print(f\"Temps d'exÃ©cution: {baseline_metrics.execution_time_ms:.1f} ms\")\n",
    "print(f\"MÃ©moire peak: {baseline_metrics.gpu_memory_peak_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimisations MÃ©moire\n",
    "\n",
    "### 3.1 PrÃ©cision RÃ©duite (FP16/BF16)\n",
    "\n",
    "La rÃ©duction de prÃ©cision est l'optimisation la plus simple et efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionManager:\n",
    "    \"\"\"\n",
    "    Gestionnaire de prÃ©cision pour les modÃ¨les.\n",
    "    \n",
    "    PrÃ©cisions supportÃ©es:\n",
    "    - FP32: Full precision (baseline)\n",
    "    - FP16: Half precision (2x moins de mÃ©moire)\n",
    "    - BF16: Brain Float 16 (meilleur range que FP16)\n",
    "    \"\"\"\n",
    "    \n",
    "    PRECISION_MAP = {\n",
    "        'fp32': torch.float32,\n",
    "        'fp16': torch.float16,\n",
    "        'bf16': torch.bfloat16\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_precision = 'fp32'\n",
    "        self._check_bf16_support()\n",
    "    \n",
    "    def _check_bf16_support(self):\n",
    "        \"\"\"VÃ©rifie le support BF16 (Ampere+).\"\"\"\n",
    "        self.bf16_supported = False\n",
    "        if CUDA_AVAILABLE:\n",
    "            capability = torch.cuda.get_device_capability()\n",
    "            self.bf16_supported = capability[0] >= 8  # Ampere = 8.0+\n",
    "        print(f\"Support BF16: {'âœ… Oui' if self.bf16_supported else 'âŒ Non (GPU < Ampere)'}\")\n",
    "    \n",
    "    def get_recommended_precision(self) -> str:\n",
    "        \"\"\"Retourne la prÃ©cision recommandÃ©e pour ce GPU.\"\"\"\n",
    "        if self.bf16_supported:\n",
    "            return 'bf16'\n",
    "        elif CUDA_AVAILABLE:\n",
    "            return 'fp16'\n",
    "        return 'fp32'\n",
    "    \n",
    "    def get_dtype(self, precision: str = None) -> torch.dtype:\n",
    "        \"\"\"Retourne le dtype torch correspondant.\"\"\"\n",
    "        precision = precision or self.current_precision\n",
    "        return self.PRECISION_MAP.get(precision, torch.float32)\n",
    "    \n",
    "    def estimate_memory_savings(self, base_size_mb: float, precision: str) -> Dict:\n",
    "        \"\"\"Estime les Ã©conomies de mÃ©moire.\"\"\"\n",
    "        factors = {'fp32': 1.0, 'fp16': 0.5, 'bf16': 0.5}\n",
    "        factor = factors.get(precision, 1.0)\n",
    "        \n",
    "        return {\n",
    "            'original_mb': base_size_mb,\n",
    "            'optimized_mb': base_size_mb * factor,\n",
    "            'savings_mb': base_size_mb * (1 - factor),\n",
    "            'savings_percent': (1 - factor) * 100\n",
    "        }\n",
    "\n",
    "\n",
    "precision_mgr = PrecisionManager()\n",
    "recommended = precision_mgr.get_recommended_precision()\n",
    "print(f\"\\nğŸ’¡ PrÃ©cision recommandÃ©e: {recommended.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des prÃ©cisions\n",
    "print(\"\\nğŸ“Š Comparaison des PrÃ©cisions\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Estimation thÃ©orique pour un modÃ¨le SD de 4GB\n",
    "base_model_size = 4000  # MB\n",
    "\n",
    "for precision in ['fp32', 'fp16', 'bf16']:\n",
    "    savings = precision_mgr.estimate_memory_savings(base_model_size, precision)\n",
    "    print(f\"\\n{precision.upper()}:\")\n",
    "    print(f\"  Taille modÃ¨le: {savings['optimized_mb']:.0f} MB\")\n",
    "    print(f\"  Ã‰conomie: {savings['savings_percent']:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pratique FP16\n",
    "print(\"\\nğŸ”¬ Test FP16 vs FP32\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def simulate_with_precision(precision: str, size_mb: int = 500):\n",
    "    \"\"\"Simule un modÃ¨le avec prÃ©cision spÃ©cifiÃ©e.\"\"\"\n",
    "    dtype = precision_mgr.get_dtype(precision)\n",
    "    bytes_per_element = 2 if dtype in [torch.float16, torch.bfloat16] else 4\n",
    "    \n",
    "    if CUDA_AVAILABLE:\n",
    "        elements = (size_mb * 1024 * 1024) // bytes_per_element\n",
    "        tensor = torch.randn(elements, device='cuda', dtype=dtype)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            tensor = tensor * 1.001 + 0.001\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        del tensor\n",
    "    else:\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# FP32\n",
    "with profiler.profile(\"Test - FP32\"):\n",
    "    simulate_with_precision('fp32', 500)\n",
    "\n",
    "# FP16\n",
    "with profiler.profile(\"Test - FP16\"):\n",
    "    simulate_with_precision('fp16', 500)\n",
    "\n",
    "# BF16 si supportÃ©\n",
    "if precision_mgr.bf16_supported:\n",
    "    with profiler.profile(\"Test - BF16\"):\n",
    "        simulate_with_precision('bf16', 500)\n",
    "\n",
    "print(profiler.get_comparison_table())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Quantification de ModÃ¨les\n",
    "\n",
    "La quantification va plus loin que FP16 en utilisant INT8 ou mÃªme INT4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizationConfig:\n",
    "    \"\"\"\n",
    "    Configuration de quantification pour les modÃ¨les GenAI.\n",
    "    \n",
    "    Types de quantification:\n",
    "    - INT8: 4x rÃ©duction, lÃ©gÃ¨re perte qualitÃ©\n",
    "    - INT4: 8x rÃ©duction, perte qualitÃ© notable\n",
    "    - NF4: 4-bit normalfloat (QLoRA), bon compromis\n",
    "    \"\"\"\n",
    "    \n",
    "    QUANT_TYPES = {\n",
    "        'none': {'bits': 32, 'factor': 1.0, 'quality_impact': 'Aucun'},\n",
    "        'fp16': {'bits': 16, 'factor': 0.5, 'quality_impact': 'NÃ©gligeable'},\n",
    "        'int8': {'bits': 8, 'factor': 0.25, 'quality_impact': 'Minime'},\n",
    "        'int4': {'bits': 4, 'factor': 0.125, 'quality_impact': 'LÃ©ger'},\n",
    "        'nf4': {'bits': 4, 'factor': 0.125, 'quality_impact': 'Minimal'}\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_config_for_vram(cls, vram_gb: float, model_size_gb: float = 4.0) -> Dict:\n",
    "        \"\"\"\n",
    "        Recommande une config de quantification basÃ©e sur la VRAM disponible.\n",
    "        \n",
    "        Args:\n",
    "            vram_gb: VRAM disponible en GB\n",
    "            model_size_gb: Taille du modÃ¨le en FP32\n",
    "        \n",
    "        Returns:\n",
    "            Configuration recommandÃ©e\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        for quant_type, config in cls.QUANT_TYPES.items():\n",
    "            required_vram = model_size_gb * config['factor'] * 1.3  # 30% overhead\n",
    "            if required_vram <= vram_gb:\n",
    "                recommendations.append({\n",
    "                    'type': quant_type,\n",
    "                    'required_vram_gb': required_vram,\n",
    "                    **config\n",
    "                })\n",
    "        \n",
    "        # Retourne la config avec le moins de compression possible\n",
    "        return recommendations[0] if recommendations else cls.QUANT_TYPES['int4']\n",
    "    \n",
    "    @classmethod\n",
    "    def print_comparison_table(cls, model_size_gb: float = 4.0):\n",
    "        \"\"\"Affiche un tableau comparatif des options de quantification.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"{'Type':<10} {'Bits':<8} {'Taille (GB)':<15} {'RÃ©duction':<12} {'Impact QualitÃ©':<15}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for quant_type, config in cls.QUANT_TYPES.items():\n",
    "            size = model_size_gb * config['factor']\n",
    "            reduction = (1 - config['factor']) * 100\n",
    "            print(f\"{quant_type:<10} {config['bits']:<8} {size:<15.2f} {reduction:<12.0f}% {config['quality_impact']:<15}\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "\n",
    "\n",
    "# Afficher les options de quantification\n",
    "print(\"\\nğŸ“Š Options de Quantification pour ModÃ¨le 4GB\")\n",
    "QuantizationConfig.print_comparison_table(4.0)\n",
    "\n",
    "# Recommandation pour notre GPU\n",
    "if CUDA_AVAILABLE:\n",
    "    recommended_quant = QuantizationConfig.get_config_for_vram(GPU_MEMORY_TOTAL, 4.0)\n",
    "    print(f\"\\nğŸ’¡ Recommandation pour {GPU_NAME} ({GPU_MEMORY_TOTAL:.0f}GB):\")\n",
    "    print(f\"   Type: {recommended_quant['type'].upper()}\")\n",
    "    print(f\"   VRAM requise: {recommended_quant.get('required_vram_gb', 'N/A'):.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de configuration BitsAndBytes pour quantification\n",
    "def get_bnb_config(quant_type: str = 'int8') -> Dict:\n",
    "    \"\"\"\n",
    "    GÃ©nÃ¨re une configuration BitsAndBytes pour le chargement quantifiÃ©.\n",
    "    \n",
    "    Exemple d'utilisation avec diffusers:\n",
    "    ```python\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    config = get_bnb_config('int8')\n",
    "    bnb_config = BitsAndBytesConfig(**config)\n",
    "    \n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "    ```\n",
    "    \"\"\"\n",
    "    configs = {\n",
    "        'int8': {\n",
    "            'load_in_8bit': True,\n",
    "            'llm_int8_threshold': 6.0,\n",
    "        },\n",
    "        'int4': {\n",
    "            'load_in_4bit': True,\n",
    "            'bnb_4bit_compute_dtype': 'float16',\n",
    "            'bnb_4bit_quant_type': 'fp4',\n",
    "        },\n",
    "        'nf4': {\n",
    "            'load_in_4bit': True,\n",
    "            'bnb_4bit_compute_dtype': 'float16',\n",
    "            'bnb_4bit_quant_type': 'nf4',\n",
    "            'bnb_4bit_use_double_quant': True,  # Double quantification\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return configs.get(quant_type, configs['int8'])\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“‹ Configurations BitsAndBytes GÃ©nÃ©rÃ©es:\")\n",
    "for qt in ['int8', 'int4', 'nf4']:\n",
    "    config = get_bnb_config(qt)\n",
    "    print(f\"\\n{qt.upper()}:\")\n",
    "    for k, v in config.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 CPU Offloading\n",
    "\n",
    "Pour les GPUs avec peu de VRAM, le CPU offloading permet d'exÃ©cuter des modÃ¨les plus grands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffloadingStrategy:\n",
    "    \"\"\"\n",
    "    StratÃ©gies d'offloading pour modÃ¨les gÃ©nÃ©ratifs.\n",
    "    \n",
    "    StratÃ©gies:\n",
    "    - none: Tout sur GPU\n",
    "    - model_cpu_offload: Modules dÃ©placÃ©s au besoin\n",
    "    - sequential_cpu_offload: Un module Ã  la fois sur GPU\n",
    "    - disk_offload: Offload vers disque (lent mais minimal VRAM)\n",
    "    \"\"\"\n",
    "    \n",
    "    STRATEGIES = {\n",
    "        'none': {\n",
    "            'description': 'Tout le modÃ¨le sur GPU',\n",
    "            'vram_required': 'Ã‰levÃ©e',\n",
    "            'speed': 'Maximale',\n",
    "            'method': None\n",
    "        },\n",
    "        'model_cpu_offload': {\n",
    "            'description': 'Modules dÃ©placÃ©s CPUâ†”GPU au besoin',\n",
    "            'vram_required': 'Moyenne',\n",
    "            'speed': '~1.2x plus lent',\n",
    "            'method': 'pipe.enable_model_cpu_offload()'\n",
    "        },\n",
    "        'sequential_cpu_offload': {\n",
    "            'description': 'Un seul module sur GPU Ã  la fois',\n",
    "            'vram_required': 'Faible',\n",
    "            'speed': '~2x plus lent',\n",
    "            'method': 'pipe.enable_sequential_cpu_offload()'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_strategy_for_vram(cls, vram_gb: float, model_size_gb: float = 4.0) -> str:\n",
    "        \"\"\"Recommande une stratÃ©gie basÃ©e sur la VRAM.\"\"\"\n",
    "        ratio = vram_gb / model_size_gb\n",
    "        \n",
    "        if ratio >= 2.0:\n",
    "            return 'none'\n",
    "        elif ratio >= 1.2:\n",
    "            return 'model_cpu_offload'\n",
    "        else:\n",
    "            return 'sequential_cpu_offload'\n",
    "    \n",
    "    @classmethod\n",
    "    def print_strategies(cls):\n",
    "        \"\"\"Affiche les stratÃ©gies disponibles.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"StratÃ©gies d'Offloading CPU\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for name, info in cls.STRATEGIES.items():\n",
    "            print(f\"\\nğŸ“Œ {name}\")\n",
    "            print(f\"   {info['description']}\")\n",
    "            print(f\"   VRAM: {info['vram_required']} | Vitesse: {info['speed']}\")\n",
    "            if info['method']:\n",
    "                print(f\"   Code: {info['method']}\")\n",
    "\n",
    "\n",
    "OffloadingStrategy.print_strategies()\n",
    "\n",
    "if CUDA_AVAILABLE:\n",
    "    recommended_strategy = OffloadingStrategy.get_strategy_for_vram(GPU_MEMORY_TOTAL, 4.0)\n",
    "    print(f\"\\nğŸ’¡ StratÃ©gie recommandÃ©e pour {GPU_MEMORY_TOTAL:.0f}GB VRAM: {recommended_strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimisations de Vitesse\n",
    "\n",
    "### 4.1 Attention Optimizations (xFormers, Flash Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionOptimizer:\n",
    "    \"\"\"\n",
    "    Gestionnaire des optimisations d'attention.\n",
    "    \n",
    "    Options:\n",
    "    - xFormers: Memory-efficient attention (NVIDIA)\n",
    "    - Flash Attention 2: Faster on Ampere+ GPUs\n",
    "    - SDPA: Scaled Dot Product Attention (PyTorch natif)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.xformers_available = self._check_xformers()\n",
    "        self.flash_attention_available = self._check_flash_attention()\n",
    "        self.sdpa_available = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "    \n",
    "    def _check_xformers(self) -> bool:\n",
    "        \"\"\"VÃ©rifie si xFormers est installÃ©.\"\"\"\n",
    "        try:\n",
    "            import xformers\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "    \n",
    "    def _check_flash_attention(self) -> bool:\n",
    "        \"\"\"VÃ©rifie si Flash Attention est disponible.\"\"\"\n",
    "        if not CUDA_AVAILABLE:\n",
    "            return False\n",
    "        try:\n",
    "            capability = torch.cuda.get_device_capability()\n",
    "            return capability[0] >= 8  # Ampere+\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def get_status(self) -> Dict[str, bool]:\n",
    "        \"\"\"Retourne le statut de chaque optimisation.\"\"\"\n",
    "        return {\n",
    "            'xformers': self.xformers_available,\n",
    "            'flash_attention_2': self.flash_attention_available,\n",
    "            'sdpa': self.sdpa_available\n",
    "        }\n",
    "    \n",
    "    def get_recommended(self) -> str:\n",
    "        \"\"\"Retourne l'optimisation recommandÃ©e.\"\"\"\n",
    "        if self.flash_attention_available:\n",
    "            return 'flash_attention_2'\n",
    "        elif self.xformers_available:\n",
    "            return 'xformers'\n",
    "        elif self.sdpa_available:\n",
    "            return 'sdpa'\n",
    "        return 'none'\n",
    "    \n",
    "    def get_diffusers_code(self, optimization: str) -> str:\n",
    "        \"\"\"GÃ©nÃ¨re le code pour activer l'optimisation dans diffusers.\"\"\"\n",
    "        codes = {\n",
    "            'xformers': 'pipe.enable_xformers_memory_efficient_attention()',\n",
    "            'flash_attention_2': '''pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")''',\n",
    "            'sdpa': '# SDPA activÃ© par dÃ©faut dans PyTorch 2.0+',\n",
    "            'none': '# Pas d\\'optimisation d\\'attention'\n",
    "        }\n",
    "        return codes.get(optimization, codes['none'])\n",
    "\n",
    "\n",
    "attention_opt = AttentionOptimizer()\n",
    "status = attention_opt.get_status()\n",
    "\n",
    "print(\"\\nğŸ“Š Statut des Optimisations d'Attention\")\n",
    "print(\"=\"*50)\n",
    "for opt, available in status.items():\n",
    "    icon = 'âœ…' if available else 'âŒ'\n",
    "    print(f\"{icon} {opt}: {'Disponible' if available else 'Non disponible'}\")\n",
    "\n",
    "recommended_attention = attention_opt.get_recommended()\n",
    "print(f\"\\nğŸ’¡ Recommandation: {recommended_attention}\")\n",
    "print(f\"\\nCode:\")\n",
    "print(attention_opt.get_diffusers_code(recommended_attention))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Torch Compile (PyTorch 2.0+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCompileConfig:\n",
    "    \"\"\"\n",
    "    Configuration torch.compile pour accÃ©lÃ©rer l'infÃ©rence.\n",
    "    \n",
    "    Modes:\n",
    "    - default: Bon Ã©quilibre compilation/performance\n",
    "    - reduce-overhead: Minimise l'overhead, bon pour petits batches\n",
    "    - max-autotune: Performance maximale, compilation longue\n",
    "    \"\"\"\n",
    "    \n",
    "    MODES = {\n",
    "        'default': {\n",
    "            'description': 'Ã‰quilibre compilation/performance',\n",
    "            'compile_time': 'ModÃ©rÃ©',\n",
    "            'speedup': '10-20%',\n",
    "            'recommended_for': 'Usage gÃ©nÃ©ral'\n",
    "        },\n",
    "        'reduce-overhead': {\n",
    "            'description': 'Minimise l\\'overhead GPU',\n",
    "            'compile_time': 'Court',\n",
    "            'speedup': '5-15%',\n",
    "            'recommended_for': 'Petits batches, latence critique'\n",
    "        },\n",
    "        'max-autotune': {\n",
    "            'description': 'Performance maximale via autotuning',\n",
    "            'compile_time': 'Long (minutes)',\n",
    "            'speedup': '20-40%',\n",
    "            'recommended_for': 'Production, grands batches'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def is_available(cls) -> bool:\n",
    "        \"\"\"VÃ©rifie si torch.compile est disponible.\"\"\"\n",
    "        return hasattr(torch, 'compile') and torch.__version__ >= '2.0'\n",
    "    \n",
    "    @classmethod\n",
    "    def get_compile_code(cls, mode: str = 'default') -> str:\n",
    "        \"\"\"GÃ©nÃ¨re le code de compilation.\"\"\"\n",
    "        return f'''# Compiler le modÃ¨le UNet pour accÃ©lÃ©rer l'infÃ©rence\n",
    "pipe.unet = torch.compile(\n",
    "    pipe.unet,\n",
    "    mode=\"{mode}\",\n",
    "    fullgraph=True\n",
    ")\n",
    "\n",
    "# Note: La premiÃ¨re infÃ©rence sera lente (compilation)\n",
    "# Les suivantes seront accÃ©lÃ©rÃ©es'''\n",
    "\n",
    "\n",
    "print(\"ğŸ“Š Options torch.compile\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Disponible: {'âœ… Oui' if TorchCompileConfig.is_available() else 'âŒ Non (PyTorch < 2.0)'}\")\n",
    "\n",
    "for mode, info in TorchCompileConfig.MODES.items():\n",
    "    print(f\"\\nğŸ“Œ {mode}\")\n",
    "    print(f\"   {info['description']}\")\n",
    "    print(f\"   Speedup: {info['speedup']} | Compilation: {info['compile_time']}\")\n",
    "    print(f\"   Pour: {info['recommended_for']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 VAE Optimizations (Tiling & Slicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEOptimizer:\n",
    "    \"\"\"\n",
    "    Optimisations pour le VAE (encodeur/dÃ©codeur d'images).\n",
    "    \n",
    "    Le VAE est souvent le goulot d'Ã©tranglement mÃ©moire pour les grandes images.\n",
    "    \n",
    "    Techniques:\n",
    "    - Tiling: DÃ©coupe l'image en tuiles pour le dÃ©codage\n",
    "    - Slicing: Traite les channels par tranches\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_vae_memory(width: int, height: int, batch_size: int = 1) -> float:\n",
    "        \"\"\"\n",
    "        Estime la mÃ©moire VAE requise (en MB).\n",
    "        \n",
    "        Formule approximative pour SD/SDXL VAE.\n",
    "        \"\"\"\n",
    "        # Latent size = image_size / 8\n",
    "        latent_h, latent_w = height // 8, width // 8\n",
    "        \n",
    "        # VAE channels = 4 (latent) ou 3 (RGB)\n",
    "        # Estimation: ~0.5MB par 64x64 pixels en FP16\n",
    "        pixels = width * height\n",
    "        mb_per_mpixel = 500  # ~500MB par megapixel\n",
    "        \n",
    "        return (pixels / 1_000_000) * mb_per_mpixel * batch_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_optimization_code(enable_tiling: bool = True, enable_slicing: bool = True) -> str:\n",
    "        \"\"\"GÃ©nÃ¨re le code d'optimisation VAE.\"\"\"\n",
    "        lines = ['# Optimisations VAE pour grandes images']\n",
    "        \n",
    "        if enable_tiling:\n",
    "            lines.append('pipe.vae.enable_tiling()  # DÃ©coupe en tuiles')\n",
    "        \n",
    "        if enable_slicing:\n",
    "            lines.append('pipe.vae.enable_slicing()  # Traite par tranches')\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Estimation mÃ©moire VAE pour diffÃ©rentes rÃ©solutions\n",
    "print(\"\\nğŸ“Š Estimation MÃ©moire VAE par RÃ©solution\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "resolutions = [\n",
    "    (512, 512, \"SD 1.5 standard\"),\n",
    "    (768, 768, \"SD 1.5 high-res\"),\n",
    "    (1024, 1024, \"SDXL standard\"),\n",
    "    (1536, 1536, \"SDXL high-res\"),\n",
    "    (2048, 2048, \"Ultra high-res\")\n",
    "]\n",
    "\n",
    "for w, h, desc in resolutions:\n",
    "    mem = VAEOptimizer.estimate_vae_memory(w, h)\n",
    "    print(f\"{w}x{h} ({desc}): ~{mem:.0f} MB\")\n",
    "\n",
    "print(\"\\n\" + VAEOptimizer.get_optimization_code())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing OptimisÃ©\n",
    "\n",
    "Le traitement par lots peut significativement amÃ©liorer le throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchOptimizer:\n",
    "    \"\"\"\n",
    "    Optimiseur de taille de batch pour maximiser le throughput.\n",
    "    \n",
    "    StratÃ©gie:\n",
    "    1. Commencer avec batch_size=1\n",
    "    2. Doubler jusqu'Ã  OOM ou performance plateau\n",
    "    3. Retourner la taille optimale\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vram_gb: float, model_vram_gb: float = 4.0):\n",
    "        self.vram_gb = vram_gb\n",
    "        self.model_vram_gb = model_vram_gb\n",
    "        self.available_vram = vram_gb - model_vram_gb\n",
    "    \n",
    "    def estimate_optimal_batch_size(self, \n",
    "                                     width: int = 1024, \n",
    "                                     height: int = 1024,\n",
    "                                     precision: str = 'fp16') -> int:\n",
    "        \"\"\"\n",
    "        Estime la taille de batch optimale.\n",
    "        \n",
    "        Returns:\n",
    "            Taille de batch recommandÃ©e\n",
    "        \"\"\"\n",
    "        # MÃ©moire par image (estimation)\n",
    "        precision_factor = 0.5 if precision == 'fp16' else 1.0\n",
    "        mem_per_image_gb = (width * height * 4 * precision_factor) / (1024**3) * 50  # Facteur empirique\n",
    "        \n",
    "        # Garder 20% de marge\n",
    "        usable_vram = self.available_vram * 0.8\n",
    "        \n",
    "        optimal = max(1, int(usable_vram / mem_per_image_gb))\n",
    "        return min(optimal, 8)  # Cap Ã  8 pour Ã©viter OOM\n",
    "    \n",
    "    def get_throughput_comparison(self) -> Dict[int, Dict]:\n",
    "        \"\"\"\n",
    "        Compare le throughput thÃ©orique pour diffÃ©rentes tailles de batch.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Temps d'infÃ©rence typique (secondes)\n",
    "        base_time = 2.0  # Pour batch=1\n",
    "        \n",
    "        for batch_size in [1, 2, 4, 8]:\n",
    "            # Le temps n'augmente pas linÃ©airement avec le batch\n",
    "            scaling_factor = 1 + (batch_size - 1) * 0.3  # ~30% overhead par image\n",
    "            total_time = base_time * scaling_factor\n",
    "            throughput = batch_size / total_time\n",
    "            \n",
    "            results[batch_size] = {\n",
    "                'time_seconds': total_time,\n",
    "                'throughput_img_per_sec': throughput,\n",
    "                'speedup_vs_batch1': throughput / (1 / base_time)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "if CUDA_AVAILABLE:\n",
    "    batch_opt = BatchOptimizer(GPU_MEMORY_TOTAL, model_vram_gb=4.0)\n",
    "    \n",
    "    print(\"\\nğŸ“Š Analyse Batch Size\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"VRAM totale: {GPU_MEMORY_TOTAL:.1f} GB\")\n",
    "    print(f\"VRAM pour modÃ¨le: ~4.0 GB\")\n",
    "    print(f\"VRAM disponible pour batching: ~{batch_opt.available_vram:.1f} GB\")\n",
    "    \n",
    "    optimal = batch_opt.estimate_optimal_batch_size(1024, 1024, 'fp16')\n",
    "    print(f\"\\nğŸ’¡ Batch size optimal estimÃ© (1024x1024, FP16): {optimal}\")\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Comparaison Throughput ThÃ©orique:\")\n",
    "    print(\"-\"*60)\n",
    "    comparison = batch_opt.get_throughput_comparison()\n",
    "    for bs, metrics in comparison.items():\n",
    "        print(f\"Batch {bs}: {metrics['throughput_img_per_sec']:.2f} img/s (speedup: {metrics['speedup_vs_batch1']:.1f}x)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ GPU requis pour l'analyse de batch sizing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. StratÃ©gies de Caching\n",
    "\n",
    "Le caching intelligent peut Ã©liminer les calculs redondants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from functools import lru_cache\n",
    "\n",
    "class EmbeddingCache:\n",
    "    \"\"\"\n",
    "    Cache pour les embeddings de prompts.\n",
    "    \n",
    "    Les text encoders sont coÃ»teux mais dÃ©terministes.\n",
    "    Cacher les embeddings Ã©vite de recalculer pour les mÃªmes prompts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.cache: Dict[str, Any] = {}\n",
    "        self.max_size = max_size\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _hash_prompt(self, prompt: str, negative_prompt: str = \"\") -> str:\n",
    "        \"\"\"GÃ©nÃ¨re un hash unique pour la paire de prompts.\"\"\"\n",
    "        combined = f\"{prompt}|||{negative_prompt}\"\n",
    "        return hashlib.md5(combined.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, prompt: str, negative_prompt: str = \"\") -> Optional[Any]:\n",
    "        \"\"\"RÃ©cupÃ¨re un embedding du cache.\"\"\"\n",
    "        key = self._hash_prompt(prompt, negative_prompt)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            self.hits += 1\n",
    "            return self.cache[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def put(self, prompt: str, negative_prompt: str, embedding: Any):\n",
    "        \"\"\"Stocke un embedding dans le cache.\"\"\"\n",
    "        # Ã‰viction LRU simple si cache plein\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            oldest_key = next(iter(self.cache))\n",
    "            del self.cache[oldest_key]\n",
    "        \n",
    "        key = self._hash_prompt(prompt, negative_prompt)\n",
    "        self.cache[key] = embedding\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Retourne les statistiques du cache.\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = (self.hits / total * 100) if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'size': len(self.cache),\n",
    "            'max_size': self.max_size,\n",
    "            'hits': self.hits,\n",
    "            'misses': self.misses,\n",
    "            'hit_rate_percent': hit_rate\n",
    "        }\n",
    "\n",
    "\n",
    "# DÃ©monstration du cache\n",
    "embedding_cache = EmbeddingCache(max_size=50)\n",
    "\n",
    "# Simuler des requÃªtes\n",
    "test_prompts = [\n",
    "    \"a beautiful sunset over mountains\",\n",
    "    \"a cat sitting on a couch\",\n",
    "    \"a beautiful sunset over mountains\",  # RÃ©pÃ©tition\n",
    "    \"abstract art with vibrant colors\",\n",
    "    \"a cat sitting on a couch\",  # RÃ©pÃ©tition\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ—„ï¸ DÃ©monstration Cache Embeddings\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    cached = embedding_cache.get(prompt)\n",
    "    if cached is None:\n",
    "        # Simuler calcul d'embedding\n",
    "        fake_embedding = f\"embedding_{len(prompt)}\"\n",
    "        embedding_cache.put(prompt, \"\", fake_embedding)\n",
    "        print(f\"âŒ MISS: '{prompt[:40]}...'\")\n",
    "    else:\n",
    "        print(f\"âœ… HIT:  '{prompt[:40]}...'\")\n",
    "\n",
    "stats = embedding_cache.get_stats()\n",
    "print(f\"\\nğŸ“Š Statistiques Cache:\")\n",
    "print(f\"   Taille: {stats['size']}/{stats['max_size']}\")\n",
    "print(f\"   Hit Rate: {stats['hit_rate_percent']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultCache:\n",
    "    \"\"\"\n",
    "    Cache pour les images gÃ©nÃ©rÃ©es (dÃ©duplication).\n",
    "    \n",
    "    Utile pour:\n",
    "    - Ã‰viter de regÃ©nÃ©rer des images identiques\n",
    "    - Servir rapidement des rÃ©sultats rÃ©cents\n",
    "    - Debugging et comparaisons A/B\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"./cache/results\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.index: Dict[str, Path] = {}\n",
    "        self._load_index()\n",
    "    \n",
    "    def _load_index(self):\n",
    "        \"\"\"Charge l'index du cache depuis le disque.\"\"\"\n",
    "        index_file = self.cache_dir / \"index.json\"\n",
    "        if index_file.exists():\n",
    "            with open(index_file) as f:\n",
    "                self.index = json.load(f)\n",
    "    \n",
    "    def _save_index(self):\n",
    "        \"\"\"Sauvegarde l'index sur disque.\"\"\"\n",
    "        index_file = self.cache_dir / \"index.json\"\n",
    "        with open(index_file, 'w') as f:\n",
    "            json.dump(self.index, f)\n",
    "    \n",
    "    def _generate_key(self, prompt: str, params: Dict) -> str:\n",
    "        \"\"\"GÃ©nÃ¨re une clÃ© unique basÃ©e sur prompt + paramÃ¨tres.\"\"\"\n",
    "        # Inclure les paramÃ¨tres qui affectent le rÃ©sultat\n",
    "        key_data = {\n",
    "            'prompt': prompt,\n",
    "            'seed': params.get('seed'),\n",
    "            'width': params.get('width'),\n",
    "            'height': params.get('height'),\n",
    "            'steps': params.get('num_inference_steps'),\n",
    "            'guidance': params.get('guidance_scale')\n",
    "        }\n",
    "        key_str = json.dumps(key_data, sort_keys=True)\n",
    "        return hashlib.sha256(key_str.encode()).hexdigest()[:16]\n",
    "    \n",
    "    def get(self, prompt: str, params: Dict) -> Optional[Path]:\n",
    "        \"\"\"RÃ©cupÃ¨re un rÃ©sultat du cache.\"\"\"\n",
    "        key = self._generate_key(prompt, params)\n",
    "        if key in self.index:\n",
    "            path = Path(self.index[key])\n",
    "            if path.exists():\n",
    "                return path\n",
    "        return None\n",
    "    \n",
    "    def put(self, prompt: str, params: Dict, image_path: Path) -> str:\n",
    "        \"\"\"Ajoute un rÃ©sultat au cache.\"\"\"\n",
    "        key = self._generate_key(prompt, params)\n",
    "        self.index[key] = str(image_path)\n",
    "        self._save_index()\n",
    "        return key\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“¦ Configuration ResultCache\")\n",
    "print(f\"   RÃ©pertoire: ./cache/results\")\n",
    "print(f\"   Usage: DÃ©dupliquer les gÃ©nÃ©rations identiques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipeline OptimisÃ© Complet\n",
    "\n",
    "Combinons toutes les optimisations dans un pipeline unifiÃ©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OptimizationProfile:\n",
    "    \"\"\"Profil d'optimisation prÃ©dÃ©fini.\"\"\"\n",
    "    name: str\n",
    "    precision: str\n",
    "    quantization: Optional[str]\n",
    "    attention: str\n",
    "    cpu_offload: str\n",
    "    vae_tiling: bool\n",
    "    vae_slicing: bool\n",
    "    torch_compile: Optional[str]\n",
    "    batch_size: int\n",
    "    description: str\n",
    "\n",
    "\n",
    "class OptimizedPipelineFactory:\n",
    "    \"\"\"\n",
    "    Factory pour crÃ©er des pipelines optimisÃ©s selon le matÃ©riel.\n",
    "    \n",
    "    Profils prÃ©dÃ©finis:\n",
    "    - low_vram: Pour GPUs 4-6GB (GTX 1060, RTX 3050)\n",
    "    - medium_vram: Pour GPUs 8-12GB (RTX 3060/3070)\n",
    "    - high_vram: Pour GPUs 16GB+ (RTX 3080/3090/4090)\n",
    "    - production: Maximum performance pour serving\n",
    "    \"\"\"\n",
    "    \n",
    "    PROFILES = {\n",
    "        'low_vram': OptimizationProfile(\n",
    "            name='Low VRAM (4-6GB)',\n",
    "            precision='fp16',\n",
    "            quantization='int8',\n",
    "            attention='xformers',\n",
    "            cpu_offload='sequential_cpu_offload',\n",
    "            vae_tiling=True,\n",
    "            vae_slicing=True,\n",
    "            torch_compile=None,\n",
    "            batch_size=1,\n",
    "            description='OptimisÃ© pour GPUs limitÃ©s, priorise la compatibilitÃ©'\n",
    "        ),\n",
    "        'medium_vram': OptimizationProfile(\n",
    "            name='Medium VRAM (8-12GB)',\n",
    "            precision='fp16',\n",
    "            quantization=None,\n",
    "            attention='sdpa',\n",
    "            cpu_offload='model_cpu_offload',\n",
    "            vae_tiling=True,\n",
    "            vae_slicing=False,\n",
    "            torch_compile='default',\n",
    "            batch_size=2,\n",
    "            description='Bon Ã©quilibre performance/mÃ©moire'\n",
    "        ),\n",
    "        'high_vram': OptimizationProfile(\n",
    "            name='High VRAM (16GB+)',\n",
    "            precision='bf16',\n",
    "            quantization=None,\n",
    "            attention='flash_attention_2',\n",
    "            cpu_offload='none',\n",
    "            vae_tiling=False,\n",
    "            vae_slicing=False,\n",
    "            torch_compile='reduce-overhead',\n",
    "            batch_size=4,\n",
    "            description='Performance maximale pour GPUs haut de gamme'\n",
    "        ),\n",
    "        'production': OptimizationProfile(\n",
    "            name='Production Server',\n",
    "            precision='fp16',\n",
    "            quantization=None,\n",
    "            attention='flash_attention_2',\n",
    "            cpu_offload='none',\n",
    "            vae_tiling=False,\n",
    "            vae_slicing=False,\n",
    "            torch_compile='max-autotune',\n",
    "            batch_size=8,\n",
    "            description='Throughput maximum, temps de warmup long'\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_profile_for_vram(cls, vram_gb: float) -> OptimizationProfile:\n",
    "        \"\"\"SÃ©lectionne automatiquement le profil optimal.\"\"\"\n",
    "        if vram_gb < 8:\n",
    "            return cls.PROFILES['low_vram']\n",
    "        elif vram_gb < 16:\n",
    "            return cls.PROFILES['medium_vram']\n",
    "        else:\n",
    "            return cls.PROFILES['high_vram']\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_pipeline_code(cls, profile: OptimizationProfile) -> str:\n",
    "        \"\"\"GÃ©nÃ¨re le code Python pour crÃ©er un pipeline optimisÃ©.\"\"\"\n",
    "        lines = [\n",
    "            f'# Pipeline OptimisÃ©: {profile.name}',\n",
    "            f'# {profile.description}',\n",
    "            '',\n",
    "            'import torch',\n",
    "            'from diffusers import StableDiffusionXLPipeline',\n",
    "            ''\n",
    "        ]\n",
    "        \n",
    "        # Dtype\n",
    "        dtype_map = {'fp32': 'torch.float32', 'fp16': 'torch.float16', 'bf16': 'torch.bfloat16'}\n",
    "        dtype = dtype_map.get(profile.precision, 'torch.float16')\n",
    "        \n",
    "        # Load pipeline\n",
    "        load_args = [f'torch_dtype={dtype}']\n",
    "        if profile.attention == 'flash_attention_2':\n",
    "            load_args.append('attn_implementation=\"flash_attention_2\"')\n",
    "        \n",
    "        lines.append('pipe = StableDiffusionXLPipeline.from_pretrained(')\n",
    "        lines.append('    \"stabilityai/stable-diffusion-xl-base-1.0\",')\n",
    "        for arg in load_args:\n",
    "            lines.append(f'    {arg},')\n",
    "        lines.append(').to(\"cuda\")')\n",
    "        lines.append('')\n",
    "        \n",
    "        # Optimizations\n",
    "        lines.append('# Optimisations')\n",
    "        \n",
    "        if profile.attention == 'xformers':\n",
    "            lines.append('pipe.enable_xformers_memory_efficient_attention()')\n",
    "        \n",
    "        if profile.cpu_offload == 'model_cpu_offload':\n",
    "            lines.append('pipe.enable_model_cpu_offload()')\n",
    "        elif profile.cpu_offload == 'sequential_cpu_offload':\n",
    "            lines.append('pipe.enable_sequential_cpu_offload()')\n",
    "        \n",
    "        if profile.vae_tiling:\n",
    "            lines.append('pipe.vae.enable_tiling()')\n",
    "        if profile.vae_slicing:\n",
    "            lines.append('pipe.vae.enable_slicing()')\n",
    "        \n",
    "        if profile.torch_compile:\n",
    "            lines.append('')\n",
    "            lines.append(f'# torch.compile (mode={profile.torch_compile})')\n",
    "            lines.append(f'pipe.unet = torch.compile(pipe.unet, mode=\"{profile.torch_compile}\")')\n",
    "        \n",
    "        lines.append('')\n",
    "        lines.append(f'# Batch size recommandÃ©: {profile.batch_size}')\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Afficher tous les profils\n",
    "print(\"\\nğŸ“‹ Profils d'Optimisation Disponibles\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, profile in OptimizedPipelineFactory.PROFILES.items():\n",
    "    print(f\"\\nğŸ”§ {profile.name}\")\n",
    "    print(f\"   PrÃ©cision: {profile.precision} | Attention: {profile.attention}\")\n",
    "    print(f\"   Offload: {profile.cpu_offload} | Batch: {profile.batch_size}\")\n",
    "    print(f\"   {profile.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GÃ©nÃ©rer le code pour notre GPU\n",
    "if CUDA_AVAILABLE:\n",
    "    optimal_profile = OptimizedPipelineFactory.get_profile_for_vram(GPU_MEMORY_TOTAL)\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ Profil RecommandÃ© pour {GPU_NAME} ({GPU_MEMORY_TOTAL:.0f}GB):\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    code = OptimizedPipelineFactory.generate_pipeline_code(optimal_profile)\n",
    "    print(code)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ GPU requis pour gÃ©nÃ©rer le profil optimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Benchmarking et Comparaison\n",
    "\n",
    "Mesurons l'impact des diffÃ©rentes optimisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkSuite:\n",
    "    \"\"\"\n",
    "    Suite de benchmarks pour comparer les optimisations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, profiler: GPUProfiler):\n",
    "        self.profiler = profiler\n",
    "        self.results: List[Dict] = []\n",
    "    \n",
    "    def run_synthetic_benchmark(self, name: str, \n",
    "                                 memory_mb: int = 500,\n",
    "                                 compute_ms: int = 100,\n",
    "                                 iterations: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        ExÃ©cute un benchmark synthÃ©tique.\n",
    "        \n",
    "        Pour un benchmark rÃ©el, remplacer par l'infÃ©rence du modÃ¨le.\n",
    "        \"\"\"\n",
    "        times = []\n",
    "        memories = []\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            with self.profiler.profile(f\"{name}_iter{i}\"):\n",
    "                simulate_model_inference(memory_mb, compute_ms)\n",
    "            \n",
    "            metrics = self.profiler.metrics_history[-1]\n",
    "            times.append(metrics.execution_time_ms)\n",
    "            memories.append(metrics.gpu_memory_peak_mb)\n",
    "        \n",
    "        result = {\n",
    "            'name': name,\n",
    "            'avg_time_ms': sum(times) / len(times),\n",
    "            'min_time_ms': min(times),\n",
    "            'max_time_ms': max(times),\n",
    "            'avg_memory_mb': sum(memories) / len(memories),\n",
    "            'iterations': iterations\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"Affiche les rÃ©sultats de benchmark.\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"Aucun rÃ©sultat de benchmark\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RÃ‰SULTATS BENCHMARK\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Configuration':<25} {'Temps Moy (ms)':<18} {'MÃ©moire (MB)':<15} {'Speedup':<10}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        baseline_time = self.results[0]['avg_time_ms'] if self.results else 1\n",
    "        \n",
    "        for r in self.results:\n",
    "            speedup = baseline_time / r['avg_time_ms']\n",
    "            print(f\"{r['name']:<25} {r['avg_time_ms']:<18.1f} {r['avg_memory_mb']:<15.1f} {speedup:<10.2f}x\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def export_results(self, filepath: str = \"benchmark_results.json\"):\n",
    "        \"\"\"Exporte les rÃ©sultats en JSON.\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'gpu': GPU_NAME if CUDA_AVAILABLE else 'CPU',\n",
    "                'vram_gb': GPU_MEMORY_TOTAL if CUDA_AVAILABLE else 0,\n",
    "                'results': self.results\n",
    "            }, f, indent=2)\n",
    "        print(f\"\\nğŸ“ RÃ©sultats exportÃ©s vers {filepath}\")\n",
    "\n",
    "\n",
    "# ExÃ©cuter les benchmarks\n",
    "print(\"\\nğŸƒ ExÃ©cution des Benchmarks...\")\n",
    "benchmark = BenchmarkSuite(profiler)\n",
    "\n",
    "# Benchmark FP32 (baseline)\n",
    "benchmark.run_synthetic_benchmark(\"Baseline (FP32)\", memory_mb=500, compute_ms=150)\n",
    "\n",
    "# Benchmark FP16\n",
    "benchmark.run_synthetic_benchmark(\"FP16\", memory_mb=250, compute_ms=120)\n",
    "\n",
    "# Benchmark avec optimisations\n",
    "benchmark.run_synthetic_benchmark(\"FP16 + Optimisations\", memory_mb=250, compute_ms=80)\n",
    "\n",
    "benchmark.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RÃ©sumÃ© et Recommandations\n",
    "\n",
    "Voici les points clÃ©s pour optimiser vos pipelines de gÃ©nÃ©ration d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimization_report() -> str:\n",
    "    \"\"\"GÃ©nÃ¨re un rapport de recommandations personnalisÃ©es.\"\"\"\n",
    "    report = []\n",
    "    report.append(\"\\n\" + \"=\"*70)\n",
    "    report.append(\"ğŸ“Š RAPPORT D'OPTIMISATION PERSONNALISÃ‰\")\n",
    "    report.append(\"=\"*70)\n",
    "    \n",
    "    if CUDA_AVAILABLE:\n",
    "        report.append(f\"\\nğŸ–¥ï¸ Configuration DÃ©tectÃ©e:\")\n",
    "        report.append(f\"   GPU: {GPU_NAME}\")\n",
    "        report.append(f\"   VRAM: {GPU_MEMORY_TOTAL:.1f} GB\")\n",
    "        report.append(f\"   GPUs: {GPU_COUNT}\")\n",
    "        \n",
    "        # Profil recommandÃ©\n",
    "        profile = OptimizedPipelineFactory.get_profile_for_vram(GPU_MEMORY_TOTAL)\n",
    "        report.append(f\"\\nğŸ’¡ Profil RecommandÃ©: {profile.name}\")\n",
    "        \n",
    "        report.append(f\"\\nğŸ“‹ Recommandations:\")\n",
    "        report.append(f\"   âœ“ PrÃ©cision: {profile.precision.upper()}\")\n",
    "        report.append(f\"   âœ“ Attention: {profile.attention}\")\n",
    "        report.append(f\"   âœ“ Offloading: {profile.cpu_offload}\")\n",
    "        report.append(f\"   âœ“ Batch size: {profile.batch_size}\")\n",
    "        \n",
    "        if profile.vae_tiling:\n",
    "            report.append(f\"   âœ“ VAE Tiling: ActivÃ© (images haute rÃ©solution)\")\n",
    "        if profile.torch_compile:\n",
    "            report.append(f\"   âœ“ torch.compile: {profile.torch_compile}\")\n",
    "        \n",
    "        # Ã‰conomies estimÃ©es\n",
    "        savings = precision_mgr.estimate_memory_savings(4000, profile.precision)\n",
    "        report.append(f\"\\nğŸ“ˆ Impact EstimÃ©:\")\n",
    "        report.append(f\"   Ã‰conomie mÃ©moire: {savings['savings_percent']:.0f}%\")\n",
    "        report.append(f\"   VRAM modÃ¨le: ~{savings['optimized_mb']/1000:.1f} GB\")\n",
    "        \n",
    "    else:\n",
    "        report.append(\"\\nâš ï¸ Mode CPU uniquement dÃ©tectÃ©\")\n",
    "        report.append(\"   Les optimisations GPU ne sont pas applicables\")\n",
    "        report.append(\"   ConsidÃ©rez l'utilisation d'APIs cloud (fal.ai, Replicate)\")\n",
    "    \n",
    "    report.append(\"\\n\" + \"=\"*70)\n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "\n",
    "print(generate_optimization_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau rÃ©capitulatif des techniques\n",
    "print(\"\\nğŸ“š RÃ‰CAPITULATIF DES TECHNIQUES D'OPTIMISATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "techniques = [\n",
    "    (\"PrÃ©cision FP16/BF16\", \"50%\", \"Aucun\", \"â˜…â˜…â˜…â˜…â˜…\"),\n",
    "    (\"Quantification INT8\", \"75%\", \"Minime\", \"â˜…â˜…â˜…â˜…â˜†\"),\n",
    "    (\"xFormers Attention\", \"30%\", \"Aucun\", \"â˜…â˜…â˜…â˜…â˜…\"),\n",
    "    (\"Flash Attention 2\", \"40%\", \"Aucun\", \"â˜…â˜…â˜…â˜…â˜…\"),\n",
    "    (\"torch.compile\", \"Variable\", \"Aucun\", \"â˜…â˜…â˜…â˜†â˜†\"),\n",
    "    (\"VAE Tiling\", \"50%+\", \"Aucun\", \"â˜…â˜…â˜…â˜…â˜†\"),\n",
    "    (\"CPU Offload\", \"70%+\", \"Latence\", \"â˜…â˜…â˜…â˜†â˜†\"),\n",
    "    (\"Embedding Cache\", \"N/A\", \"Aucun\", \"â˜…â˜…â˜…â˜…â˜…\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Technique':<25} {'Ã‰con. MÃ©moire':<15} {'Impact QualitÃ©':<15} {'RecommandÃ©'}\")\n",
    "print(\"-\"*70)\n",
    "for tech, mem, quality, rec in techniques:\n",
    "    print(f\"{tech:<25} {mem:<15} {quality:<15} {rec}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Conseil: Combinez plusieurs techniques pour des gains cumulatifs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercices Pratiques\n",
    "\n",
    "### Exercice 1: Profilage de votre pipeline\n",
    "Utilisez le `GPUProfiler` pour mesurer les performances de votre propre pipeline de gÃ©nÃ©ration.\n",
    "\n",
    "### Exercice 2: Comparaison A/B\n",
    "Comparez la qualitÃ© d'image entre FP32 et FP16 sur 10 prompts identiques.\n",
    "\n",
    "### Exercice 3: Optimisation de batch\n",
    "Trouvez le batch size optimal pour votre GPU en mesurant le throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalisation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ NOTEBOOK TERMINÃ‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Statistiques du profiler\n",
    "print(f\"\\nğŸ“Š Statistiques de la session:\")\n",
    "print(f\"   Tests exÃ©cutÃ©s: {len(profiler.metrics_history)}\")\n",
    "\n",
    "if profiler.metrics_history:\n",
    "    avg_time = sum(m.execution_time_ms for m in profiler.metrics_history) / len(profiler.metrics_history)\n",
    "    print(f\"   Temps moyen: {avg_time:.1f} ms\")\n",
    "\n",
    "print(\"\\nğŸ“š Prochaines Ã©tapes:\")\n",
    "print(\"   â†’ Appliquer les optimisations Ã  vos pipelines de production\")\n",
    "print(\"   â†’ Benchmarker avec vos modÃ¨les rÃ©els\")\n",
    "print(\"   â†’ Explorer les techniques de multi-GPU si disponible\")\n",
    "\n",
    "print(\"\\nâœ… Module 03-Images-Orchestration complÃ©tÃ©!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
