{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69492b8",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [3]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be9a9e",
   "metadata": {
    "papermill": {
     "duration": 0.004318,
     "end_time": "2025-10-25T23:55:50.461994",
     "exception": false,
     "start_time": "2025-10-25T23:55:50.457676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Notebook: Qwen Image-Edit 2.5 - API ComfyUI\n",
    "\n",
    "**Objectif**: Ma√Ætriser g√©n√©ration et √©dition d'images via API Qwen-Image-Edit (backend ComfyUI)\n",
    "\n",
    "## üéØ Ce que vous allez apprendre\n",
    "\n",
    "1. Diff√©rence API **Forge** (simple) vs **ComfyUI** (workflows JSON)\n",
    "2. Pattern **\"queue and poll\"** pour g√©n√©ration asynchrone\n",
    "3. Cr√©ation workflows **Text-to-Image** et **Image-to-Image**\n",
    "4. Optimisation param√®tres (**steps**, **cfg**, **denoise**)\n",
    "5. Troubleshooting erreurs courantes (**timeout**, **CUDA OOM**)\n",
    "\n",
    "## üöÄ API Qwen-Image-Edit\n",
    "\n",
    "| Caract√©ristique | Valeur |\n",
    "|----------------|--------|\n",
    "| **URL Production** | `https://qwen-image-edit.myia.io` |\n",
    "| **Mod√®le** | Qwen-Image-Edit-2509-FP8 (54GB) |\n",
    "| **GPU** | RTX 3090 (24GB VRAM) |\n",
    "| **Latence Typique** | 5-10 secondes |\n",
    "| **R√©solution Optimale** | 512x512 pixels |\n",
    "\n",
    "## üîç ComfyUI vs Forge\n",
    "\n",
    "**Forge (SD XL Turbo)**:\n",
    "- ‚úÖ API simple (1 requ√™te POST)\n",
    "- ‚úÖ Ultra-rapide (1-3s)\n",
    "- ‚ùå Pas d'√©dition images\n",
    "- ‚ùå Moins flexible\n",
    "\n",
    "**ComfyUI (Qwen)**:\n",
    "- ‚úÖ Workflows JSON complexes\n",
    "- ‚úÖ √âdition images avanc√©e\n",
    "- ‚úÖ Contr√¥le fin (28 custom nodes)\n",
    "- ‚ùå API plus complexe (queue + poll)\n",
    "\n",
    "**Recommandation**: Commencer avec Forge pour prototypes, affiner avec Qwen pour production.\n",
    "\n",
    "## üìö Pr√©requis\n",
    "\n",
    "```bash\n",
    "pip install requests pillow matplotlib\n",
    "```\n",
    "\n",
    "**Temps estim√©**: 90-120 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244bd967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T23:55:50.471415Z",
     "iopub.status.busy": "2025-10-25T23:55:50.471415Z",
     "iopub.status.idle": "2025-10-25T23:55:51.271646Z",
     "shell.execute_reply": "2025-10-25T23:55:51.269744Z"
    },
    "papermill": {
     "duration": 0.808641,
     "end_time": "2025-10-25T23:55:51.273367",
     "exception": false,
     "start_time": "2025-10-25T23:55:50.464726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration charg√©e\n",
      "üì° API: https://qwen-image-edit.myia.io\n",
      "üÜî Client ID: 010e4ab5-a332-4786-99cb-896b5e983d9e\n",
      "üîê Token: $2b$12$UDceblhZ...6coni\n"
     ]
    }
   ],
   "source": [
    "# Imports standard\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import time\n",
    "import uuid\n",
    "import os\n",
    "from typing import Dict, Optional, List\n",
    "from io import BytesIO\n",
    "\n",
    "# Visualisation\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Configuration environnement\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration API\n",
    "API_BASE_URL = \"https://qwen-image-edit.myia.io\"\n",
    "CLIENT_ID = str(uuid.uuid4())  # ID unique pour tracking\n",
    "\n",
    "# Authentification - Token ComfyUI standardis√©\n",
    "COMFYUI_API_TOKEN = os.getenv(\"COMFYUI_API_TOKEN\")\n",
    "\n",
    "if not COMFYUI_API_TOKEN:\n",
    "    print(\"‚ö†Ô∏è  COMFYUI_API_TOKEN non trouv√© - connexion sans authentification\")\n",
    "    print(\"\\nüìã Pour activer l'authentification :\")\n",
    "    print(\"   1. Cr√©ez un fichier .env dans MyIA.AI.Notebooks/GenAI/\")\n",
    "    print(\"   2. Ajoutez : COMFYUI_API_TOKEN=votre_token_ici\")\n",
    "    print(\"   3. Obtenez le token via scripts/genai-auth/extract-bearer-tokens.ps1\")\n",
    "    print(\"\\n‚úì Le notebook fonctionnera en mode d√©grad√© (si serveur non s√©curis√©)\\n\")\n",
    "    COMFYUI_API_TOKEN = None\n",
    "else:\n",
    "    print(\"‚úÖ Configuration charg√©e\")\n",
    "    print(f\"üì° API: {API_BASE_URL}\")\n",
    "    print(f\"üÜî Client ID: {CLIENT_ID}\")\n",
    "    print(f\"üîê Token: {COMFYUI_API_TOKEN[:15]}...{COMFYUI_API_TOKEN[-5:]}\" if len(COMFYUI_API_TOKEN) > 20 else f\"üîê Token: (court)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5743a6a4",
   "metadata": {
    "papermill": {
     "duration": 0.006893,
     "end_time": "2025-10-25T23:55:51.286412",
     "exception": false,
     "start_time": "2025-10-25T23:55:51.279519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üèóÔ∏è Architecture ComfyUI: Workflows JSON\n",
    "\n",
    "### Diff√©rence fondamentale avec Forge\n",
    "\n",
    "**API Forge (POST direct)**:\n",
    "```python\n",
    "response = requests.post(url, json={\"prompt\": \"astronaut\"})\n",
    "image_base64 = response.json()[\"image\"]\n",
    "```\n",
    "\n",
    "**API ComfyUI (Queue + Poll)**:\n",
    "```python\n",
    "# 1. Soumettre workflow JSON\n",
    "response = requests.post(f\"{url}/prompt\", json={\n",
    "    \"prompt\": workflow_json,\n",
    "    \"client_id\": client_id\n",
    "})\n",
    "prompt_id = response.json()[\"prompt_id\"]\n",
    "\n",
    "# 2. Attendre compl√©tion (polling)\n",
    "while True:\n",
    "    history = requests.get(f\"{url}/history/{prompt_id}\")\n",
    "    if history.json().get(prompt_id, {}).get(\"status\", {}).get(\"completed\"):\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# 3. R√©cup√©rer images\n",
    "images = history.json()[prompt_id][\"outputs\"]\n",
    "```\n",
    "\n",
    "### Structure Workflow ComfyUI\n",
    "\n",
    "Un **workflow** est un **graph JSON** de **nodes connect√©s**:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"1\": {  // Node Checkpoint Loader\n",
    "    \"class_type\": \"CheckpointLoaderSimple\",\n",
    "    \"inputs\": {\"ckpt_name\": \"qwen-image-edit.safetensors\"}\n",
    "  },\n",
    "  \"2\": {  // Node Sampler\n",
    "    \"class_type\": \"KSampler\",\n",
    "    \"inputs\": {\n",
    "      \"model\": [\"1\", 0],  // Connexion: output du node 1\n",
    "      \"steps\": 20,\n",
    "      \"cfg\": 7.0,\n",
    "      \"seed\": 42\n",
    "    }\n",
    "  },\n",
    "  \"3\": {  // Node Save Image\n",
    "    \"class_type\": \"SaveImage\",\n",
    "    \"inputs\": {\"images\": [\"2\", 0]}\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Workflow = Pipeline modulaire** o√π chaque node effectue une op√©ration (charger mod√®le, sampler, encoder texte, etc.).\n",
    "\n",
    "### Anatomie d'un Node\n",
    "\n",
    "| Propri√©t√© | Description | Exemple |\n",
    "|-----------|-------------|----------|\n",
    "| **`class_type`** | Type de node ComfyUI | `\"CLIPTextEncode\"` |\n",
    "| **`inputs`** | Param√®tres du node | `{\"text\": \"astronaut\"}` |\n",
    "| **Connexions** | `[node_id, output_slot]` | `[\"5\", 0]` |\n",
    "\n",
    "**28 custom nodes** disponibles pour Qwen (voir documentation compl√®te dans guide √©tudiants)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cb759",
   "metadata": {
    "papermill": {
     "duration": 0.004785,
     "end_time": "2025-10-25T23:55:51.296198",
     "exception": false,
     "start_time": "2025-10-25T23:55:51.291413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### üîß Visualisation Architecture Workflow ComfyUI\n",
    "\n",
    "**Diagramme ASCII d'un workflow ComfyUI typique**:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    WORKFLOW COMFYUI                         ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ\n",
    "‚îÇ  ‚îÇ Load Model   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ CLIP Text    ‚îÇ                 ‚îÇ\n",
    "‚îÇ  ‚îÇ (Checkpoint) ‚îÇ        ‚îÇ Encode       ‚îÇ                 ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ\n",
    "‚îÇ         ‚îÇ                       ‚îÇ                          ‚îÇ\n",
    "‚îÇ         ‚îÇ                       ‚ñº                          ‚îÇ\n",
    "‚îÇ         ‚îÇ                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ\n",
    "‚îÇ         ‚îÇ                ‚îÇ Conditioning ‚îÇ                 ‚îÇ\n",
    "‚îÇ         ‚îÇ                ‚îÇ (Prompts)    ‚îÇ                 ‚îÇ\n",
    "‚îÇ         ‚îÇ                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ\n",
    "‚îÇ         ‚îÇ                       ‚îÇ                          ‚îÇ\n",
    "‚îÇ         ‚ñº                       ‚ñº                          ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n",
    "‚îÇ  ‚îÇ          KSampler                     ‚îÇ                ‚îÇ\n",
    "‚îÇ  ‚îÇ  (steps, denoise, seed, sampler)     ‚îÇ                ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ\n",
    "‚îÇ                    ‚îÇ                                       ‚îÇ\n",
    "‚îÇ                    ‚ñº                                       ‚îÇ\n",
    "‚îÇ             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ\n",
    "‚îÇ             ‚îÇ VAE Decode   ‚îÇ                              ‚îÇ\n",
    "‚îÇ             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ\n",
    "‚îÇ                    ‚îÇ                                       ‚îÇ\n",
    "‚îÇ                    ‚ñº                                       ‚îÇ\n",
    "‚îÇ             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ\n",
    "‚îÇ             ‚îÇ Save Image   ‚îÇ                              ‚îÇ\n",
    "‚îÇ             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Flux de donn√©es**:\n",
    "1. **Checkpoint** ‚Üí Fournit le mod√®le (model, CLIP, VAE)\n",
    "2. **CLIP Text Encode** ‚Üí Convertit le prompt en embeddings\n",
    "3. **KSampler** ‚Üí G√©n√®re l'image latente √† partir des embeddings\n",
    "4. **VAE Decode** ‚Üí Convertit l'image latente en image RGB\n",
    "5. **Save Image** ‚Üí Sauvegarde l'image finale\n",
    "\n",
    "**Correspondance JSON**:\n",
    "```json\n",
    "{\n",
    "  \"1\": {\"class_type\": \"CheckpointLoaderSimple\", ...},\n",
    "  \"2\": {\"class_type\": \"CLIPTextEncode\", \"inputs\": {\"clip\": [\"1\", 1], ...}},\n",
    "  \"3\": {\"class_type\": \"KSampler\", \"inputs\": {\"model\": [\"1\", 0], ...}},\n",
    "  \"4\": {\"class_type\": \"VAEDecode\", \"inputs\": {\"samples\": [\"3\", 0], ...}},\n",
    "  \"5\": {\"class_type\": \"SaveImage\", \"inputs\": {\"images\": [\"4\", 0]}}\n",
    "}\n",
    "```\n",
    "\n",
    "**Notation `[\"ID_NODE\", INDEX_OUTPUT]`**:\n",
    "- `[\"1\", 0]` = Output 0 (model) du node 1\n",
    "- `[\"1\", 1]` = Output 1 (CLIP) du node 1\n",
    "- `[\"3\", 0]` = Output 0 (latents) du node 3\n",
    "\n",
    "üí° **Astuce**: Chaque node peut avoir plusieurs outputs. L'index d√©termine lequel utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a8ef79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T23:55:51.311597Z",
     "iopub.status.busy": "2025-10-25T23:55:51.311597Z",
     "iopub.status.idle": "2025-10-25T23:55:51.333665Z",
     "shell.execute_reply": "2025-10-25T23:55:51.332287Z"
    },
    "papermill": {
     "duration": 0.032664,
     "end_time": "2025-10-25T23:55:51.335394",
     "exception": false,
     "start_time": "2025-10-25T23:55:51.302730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ComfyUIClient pr√™t avec authentification\n"
     ]
    }
   ],
   "source": [
    "class ComfyUIClient:\n",
    "    \"\"\"Client p√©dagogique API ComfyUI pour Qwen avec authentification Bearer\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=API_BASE_URL, client_id=CLIENT_ID, auth_token=None):\n",
    "        self.base_url = base_url\n",
    "        self.client_id = client_id\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Configuration authentification\n",
    "        if auth_token:\n",
    "            self.session.headers.update({\n",
    "                \"Authorization\": f\"Bearer {auth_token}\"\n",
    "            })\n",
    "        elif COMFYUI_API_TOKEN:  # Utilise variable globale si disponible\n",
    "            self.session.headers.update({\n",
    "                \"Authorization\": f\"Bearer {COMFYUI_API_TOKEN}\"\n",
    "            })\n",
    "        # Pas d'erreur si pas de token - graceful degradation\n",
    "    \n",
    "    def execute_workflow(\n",
    "        self, \n",
    "        workflow_json: Dict, \n",
    "        wait_for_completion: bool = True,\n",
    "        max_wait: int = 120,\n",
    "        verbose: bool = True\n",
    "    ) -> Dict:\n",
    "        \"\"\"Ex√©cute workflow ComfyUI et r√©cup√®re r√©sultats\n",
    "        \n",
    "        Args:\n",
    "            workflow_json: Workflow ComfyUI (dict)\n",
    "            wait_for_completion: Attendre fin g√©n√©ration\n",
    "            max_wait: Timeout en secondes\n",
    "            verbose: Afficher logs progression\n",
    "        \n",
    "        Returns:\n",
    "            dict: {\"prompt_id\", \"outputs\", \"status\"}\n",
    "        \"\"\"\n",
    "        # 1. Soumettre workflow\n",
    "        if verbose:\n",
    "            print(\"üì§ Soumission workflow...\")\n",
    "        \n",
    "        response = self.session.post(\n",
    "            f\"{self.base_url}/prompt\",\n",
    "            json={\"prompt\": workflow_json, \"client_id\": self.client_id}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        prompt_id = response.json()[\"prompt_id\"]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Workflow queued: {prompt_id}\")\n",
    "        \n",
    "        if not wait_for_completion:\n",
    "            return {\"prompt_id\": prompt_id, \"status\": \"queued\"}\n",
    "        \n",
    "        # 2. Polling compl√©tion\n",
    "        start_time = time.time()\n",
    "        while True:\n",
    "            elapsed = time.time() - start_time\n",
    "            if elapsed > max_wait:\n",
    "                raise TimeoutError(f\"Timeout apr√®s {max_wait}s\")\n",
    "            \n",
    "            history_response = self.session.get(\n",
    "                f\"{self.base_url}/history/{prompt_id}\"\n",
    "            )\n",
    "            history_response.raise_for_status()\n",
    "            history = history_response.json()\n",
    "            \n",
    "            if prompt_id not in history:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            \n",
    "            prompt_data = history[prompt_id]\n",
    "            status = prompt_data.get(\"status\", {})\n",
    "            \n",
    "            if status.get(\"completed\"):\n",
    "                if verbose:\n",
    "                    print(f\"‚úÖ Complet en {elapsed:.1f}s\")\n",
    "                \n",
    "                # 3. Extraire outputs\n",
    "                outputs = prompt_data.get(\"outputs\", {})\n",
    "                images = []\n",
    "                \n",
    "                for node_id, node_output in outputs.items():\n",
    "                    if \"images\" in node_output:\n",
    "                        for img_info in node_output[\"images\"]:\n",
    "                            # R√©cup√©rer image\n",
    "                            img_response = self.session.get(\n",
    "                                f\"{self.base_url}/view\",\n",
    "                                params={\n",
    "                                    \"filename\": img_info[\"filename\"],\n",
    "                                    \"subfolder\": img_info.get(\"subfolder\", \"\"),\n",
    "                                    \"type\": img_info.get(\"type\", \"output\")\n",
    "                                }\n",
    "                            )\n",
    "                            img_response.raise_for_status()\n",
    "                            images.append({\n",
    "                                \"data\": img_response.content,\n",
    "                                \"filename\": img_info[\"filename\"]\n",
    "                            })\n",
    "                \n",
    "                return {\n",
    "                    \"prompt_id\": prompt_id,\n",
    "                    \"status\": \"completed\",\n",
    "                    \"outputs\": outputs,\n",
    "                    \"images\": images,\n",
    "                    \"duration\": elapsed\n",
    "                }\n",
    "            \n",
    "            if status.get(\"status_str\") == \"error\":\n",
    "                error_msg = status.get(\"messages\", [\"Unknown error\"])\n",
    "                raise RuntimeError(f\"ComfyUI error: {error_msg}\")\n",
    "            \n",
    "            if verbose and int(elapsed) % 5 == 0:\n",
    "                print(f\"‚è≥ En cours... ({elapsed:.0f}s)\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "    \n",
    "    def display_images(self, result: Dict, figsize=(12, 4)):\n",
    "        \"\"\"Affiche images r√©sultats\"\"\"\n",
    "        images = result.get(\"images\", [])\n",
    "        if not images:\n",
    "            print(\"‚ö†Ô∏è Aucune image g√©n√©r√©e\")\n",
    "            return\n",
    "        \n",
    "        n = len(images)\n",
    "        fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "        if n == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, img_data in zip(axes, images):\n",
    "            img = Image.open(BytesIO(img_data[\"data\"]))\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(img_data[\"filename\"], fontsize=10)\n",
    "            ax.axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Instancier client avec authentification\n",
    "client = ComfyUIClient(auth_token=COMFYUI_API_TOKEN)\n",
    "if COMFYUI_API_TOKEN:\n",
    "    print(\"‚úÖ ComfyUIClient pr√™t avec authentification\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  ComfyUIClient pr√™t en mode sans authentification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de517444",
   "metadata": {
    "papermill": {
     "duration": 0.004773,
     "end_time": "2025-10-25T23:55:51.345989",
     "exception": false,
     "start_time": "2025-10-25T23:55:51.341216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üöÄ Workflow Minimal: \"Hello World\"\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Cr√©er le workflow le plus simple possible pour **valider l'API** et comprendre la structure JSON.\n",
    "\n",
    "### Workflow Text-to-Image Basique\n",
    "\n",
    "**Pipeline**:\n",
    "1. **Load Checkpoint** ‚Üí Charger mod√®le Qwen\n",
    "2. **CLIP Text Encode** ‚Üí Encoder prompt texte\n",
    "3. **Empty Latent Image** ‚Üí Cr√©er canvas vide\n",
    "4. **KSampler** ‚Üí G√©n√©rer image\n",
    "5. **VAE Decode** ‚Üí Convertir latent ‚Üí pixels\n",
    "6. **Save Image** ‚Üí Sauvegarder r√©sultat\n",
    "\n",
    "### Param√®tres Critiques\n",
    "\n",
    "| Param√®tre | Valeur | Impact |\n",
    "|-----------|--------|--------|\n",
    "| **steps** | 20 | Qualit√© (‚Üë steps = ‚Üë qualit√©, ‚Üë temps) |\n",
    "| **cfg** | 7.0 | Fid√©lit√© prompt (7-9 optimal) |\n",
    "| **sampler** | euler | Algorithme g√©n√©ration |\n",
    "| **scheduler** | normal | Strat√©gie steps |\n",
    "| **denoise** | 1.0 | Force g√©n√©ration (1.0 = 100%) |\n",
    "| **seed** | 42 | Reproductibilit√© |\n",
    "\n",
    "**Temps attendu**: 5-10 secondes (512x512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c4e6c",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20b3bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T23:55:51.355472Z",
     "iopub.status.busy": "2025-10-25T23:55:51.353813Z",
     "iopub.status.idle": "2025-10-25T23:55:52.224004Z",
     "shell.execute_reply": "2025-10-25T23:55:52.221648Z"
    },
    "papermill": {
     "duration": 0.876634,
     "end_time": "2025-10-25T23:55:52.225698",
     "exception": true,
     "start_time": "2025-10-25T23:55:51.349064",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Lancement g√©n√©ration...\n",
      "üì§ Soumission workflow...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://qwen-image-edit.myia.io/prompt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Ex√©cution\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Lancement g√©n√©ration...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkflow_hello\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Affichage\u001b[39;00m\n\u001b[0;32m     67\u001b[0m client\u001b[38;5;241m.\u001b[39mdisplay_images(result)\n",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m, in \u001b[0;36mComfyUIClient.execute_workflow\u001b[1;34m(self, workflow_json, wait_for_completion, max_wait, verbose)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müì§ Soumission workflow...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: workflow_json, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_id}\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m prompt_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32m~\\.conda\\envs\\mcp-jupyter-py310\\lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1023\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://qwen-image-edit.myia.io/prompt"
     ]
    }
   ],
   "source": [
    "# Workflow Text-to-Image minimal\n",
    "workflow_hello = {\n",
    "    \"1\": {  # Load Checkpoint\n",
    "        \"class_type\": \"CheckpointLoaderSimple\",\n",
    "        \"inputs\": {\n",
    "            \"ckpt_name\": \"qwen-image-edit-2509-fp8.safetensors\"\n",
    "        }\n",
    "    },\n",
    "    \"2\": {  # CLIP Text Encode (prompt positif)\n",
    "        \"class_type\": \"CLIPTextEncode\",\n",
    "        \"inputs\": {\n",
    "            \"text\": \"a majestic astronaut floating in space, photorealistic, 8k, detailed\",\n",
    "            \"clip\": [\"1\", 1]  # Connexion: output CLIP du checkpoint\n",
    "        }\n",
    "    },\n",
    "    \"3\": {  # CLIP Text Encode (prompt n√©gatif)\n",
    "        \"class_type\": \"CLIPTextEncode\",\n",
    "        \"inputs\": {\n",
    "            \"text\": \"blurry, low quality, distorted\",\n",
    "            \"clip\": [\"1\", 1]\n",
    "        }\n",
    "    },\n",
    "    \"4\": {  # Empty Latent Image (canvas)\n",
    "        \"class_type\": \"EmptyLatentImage\",\n",
    "        \"inputs\": {\n",
    "            \"width\": 512,\n",
    "            \"height\": 512,\n",
    "            \"batch_size\": 1\n",
    "        }\n",
    "    },\n",
    "    \"5\": {  # KSampler (g√©n√©ration)\n",
    "        \"class_type\": \"KSampler\",\n",
    "        \"inputs\": {\n",
    "            \"model\": [\"1\", 0],  # Model du checkpoint\n",
    "            \"positive\": [\"2\", 0],  # Prompt positif\n",
    "            \"negative\": [\"3\", 0],  # Prompt n√©gatif\n",
    "            \"latent_image\": [\"4\", 0],  # Canvas latent\n",
    "            \"seed\": 42,\n",
    "            \"steps\": 20,\n",
    "            \"cfg\": 7.0,\n",
    "            \"sampler_name\": \"euler\",\n",
    "            \"scheduler\": \"normal\",\n",
    "            \"denoise\": 1.0\n",
    "        }\n",
    "    },\n",
    "    \"6\": {  # VAE Decode\n",
    "        \"class_type\": \"VAEDecode\",\n",
    "        \"inputs\": {\n",
    "            \"samples\": [\"5\", 0],  # Latent du sampler\n",
    "            \"vae\": [\"1\", 2]  # VAE du checkpoint\n",
    "        }\n",
    "    },\n",
    "    \"7\": {  # Save Image\n",
    "        \"class_type\": \"SaveImage\",\n",
    "        \"inputs\": {\n",
    "            \"images\": [\"6\", 0],  # Pixels du VAE\n",
    "            \"filename_prefix\": \"ComfyUI\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ex√©cution\n",
    "print(\"üöÄ Lancement g√©n√©ration...\")\n",
    "result = client.execute_workflow(workflow_hello, verbose=True)\n",
    "\n",
    "# Affichage\n",
    "client.display_images(result)\n",
    "print(f\"\\n‚úÖ Image g√©n√©r√©e en {result['duration']:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6a7ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üéØ WORKFLOW R√âEL 1: √âdition Simple Image\n",
    "# ========================================\n",
    "\n",
    "def create_simple_edit_workflow(image_name: str, edit_prompt: str, denoise: float = 0.5) -> dict:\n",
    "    \"\"\"Workflow √©dition simple d'une image existante\n",
    "    \n",
    "    Args:\n",
    "        image_name: Nom du fichier image upload√© sur ComfyUI\n",
    "        edit_prompt: Description de l'√©dition souhait√©e\n",
    "        denoise: Force de l'√©dition (0.0 = aucune, 1.0 = compl√®te)\n",
    "    \n",
    "    Returns:\n",
    "        Workflow JSON pr√™t √† ex√©cuter\n",
    "    \"\"\"\n",
    "    workflow = {\n",
    "        \"1\": {\n",
    "            \"class_type\": \"CheckpointLoaderSimple\",\n",
    "            \"inputs\": {\"ckpt_name\": \"qwen_vl_model.safetensors\"}\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"class_type\": \"LoadImage\",\n",
    "            \"inputs\": {\"image\": image_name}\n",
    "        },\n",
    "        \"3\": {\n",
    "            \"class_type\": \"CLIPTextEncode\",\n",
    "            \"inputs\": {\n",
    "                \"text\": edit_prompt,\n",
    "                \"clip\": [\"1\", 1]\n",
    "            }\n",
    "        },\n",
    "        \"4\": {\n",
    "            \"class_type\": \"VAEEncode\",\n",
    "            \"inputs\": {\n",
    "                \"pixels\": [\"2\", 0],\n",
    "                \"vae\": [\"1\", 2]\n",
    "            }\n",
    "        },\n",
    "        \"5\": {\n",
    "            \"class_type\": \"KSampler\",\n",
    "            \"inputs\": {\n",
    "                \"seed\": 42,\n",
    "                \"steps\": 20,\n",
    "                \"cfg\": 7.0,\n",
    "                \"sampler_name\": \"euler\",\n",
    "                \"scheduler\": \"normal\",\n",
    "                \"denoise\": denoise,\n",
    "                \"model\": [\"1\", 0],\n",
    "                \"positive\": [\"3\", 0],\n",
    "                \"negative\": [\"3\", 0],  # Pas de prompt n√©gatif pour √©dition simple\n",
    "                \"latent_image\": [\"4\", 0]\n",
    "            }\n",
    "        },\n",
    "        \"6\": {\n",
    "            \"class_type\": \"VAEDecode\",\n",
    "            \"inputs\": {\n",
    "                \"samples\": [\"5\", 0],\n",
    "                \"vae\": [\"1\", 2]\n",
    "            }\n",
    "        },\n",
    "        \"7\": {\n",
    "            \"class_type\": \"SaveImage\",\n",
    "            \"inputs\": {\n",
    "                \"images\": [\"6\", 0],\n",
    "                \"filename_prefix\": \"qwen_edit_simple\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return workflow\n",
    "\n",
    "# ========================================\n",
    "# üéØ WORKFLOW R√âEL 2: Cha√Ænage Nodes Avanc√©\n",
    "# ========================================\n",
    "\n",
    "def create_chained_workflow(base_prompt: str, refine_prompt: str) -> dict:\n",
    "    \"\"\"Workflow avec cha√Ænage: g√©n√©ration base + raffinement\n",
    "    \n",
    "    Architecture:\n",
    "        1. G√©n√©ration image base (text-to-image)\n",
    "        2. Raffinement avec nouveau prompt (image-to-image)\n",
    "    \n",
    "    Args:\n",
    "        base_prompt: Prompt initial g√©n√©ration\n",
    "        refine_prompt: Prompt raffinement/am√©lioration\n",
    "    \n",
    "    Returns:\n",
    "        Workflow JSON avec 2 √©tapes KSampler\n",
    "    \"\"\"\n",
    "    workflow = {\n",
    "        # √âtape 1: G√©n√©ration base\n",
    "        \"1\": {\n",
    "            \"class_type\": \"CheckpointLoaderSimple\",\n",
    "            \"inputs\": {\"ckpt_name\": \"qwen_vl_model.safetensors\"}\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"class_type\": \"CLIPTextEncode\",\n",
    "            \"inputs\": {\"text\": base_prompt, \"clip\": [\"1\", 1]}\n",
    "        },\n",
    "        \"3\": {\n",
    "            \"class_type\": \"EmptyLatentImage\",\n",
    "            \"inputs\": {\"width\": 512, \"height\": 512, \"batch_size\": 1}\n",
    "        },\n",
    "        \"4\": {\n",
    "            \"class_type\": \"KSampler\",\n",
    "            \"inputs\": {\n",
    "                \"seed\": 42,\n",
    "                \"steps\": 10,\n",
    "                \"cfg\": 7.0,\n",
    "                \"sampler_name\": \"euler\",\n",
    "                \"scheduler\": \"normal\",\n",
    "                \"denoise\": 1.0,  # G√©n√©ration compl√®te\n",
    "                \"model\": [\"1\", 0],\n",
    "                \"positive\": [\"2\", 0],\n",
    "                \"negative\": [\"2\", 0],\n",
    "                \"latent_image\": [\"3\", 0]\n",
    "            }\n",
    "        },\n",
    "        # √âtape 2: Raffinement\n",
    "        \"5\": {\n",
    "            \"class_type\": \"CLIPTextEncode\",\n",
    "            \"inputs\": {\"text\": refine_prompt, \"clip\": [\"1\", 1]}\n",
    "        },\n",
    "        \"6\": {\n",
    "            \"class_type\": \"KSampler\",\n",
    "            \"inputs\": {\n",
    "                \"seed\": 43,\n",
    "                \"steps\": 10,\n",
    "                \"cfg\": 7.0,\n",
    "                \"sampler_name\": \"euler\",\n",
    "                \"scheduler\": \"normal\",\n",
    "                \"denoise\": 0.3,  # Raffinement l√©ger\n",
    "                \"model\": [\"1\", 0],\n",
    "                \"positive\": [\"5\", 0],\n",
    "                \"negative\": [\"5\", 0],\n",
    "                \"latent_image\": [\"4\", 0]  # Sortie de l'√©tape 1\n",
    "            }\n",
    "        },\n",
    "        \"7\": {\n",
    "            \"class_type\": \"VAEDecode\",\n",
    "            \"inputs\": {\"samples\": [\"6\", 0], \"vae\": [\"1\", 2]}\n",
    "        },\n",
    "        \"8\": {\n",
    "            \"class_type\": \"SaveImage\",\n",
    "            \"inputs\": {\n",
    "                \"images\": [\"7\", 0],\n",
    "                \"filename_prefix\": \"qwen_chained\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return workflow\n",
    "\n",
    "# ========================================\n",
    "# EXEMPLE D'UTILISATION\n",
    "# ========================================\n",
    "\n",
    "# Workflow 1: √âdition simple\n",
    "workflow_simple = create_simple_edit_workflow(\n",
    "    image_name=\"cat.png\",\n",
    "    edit_prompt=\"Add sunglasses to the cat\",\n",
    "    denoise=0.5\n",
    ")\n",
    "print(\"‚úÖ Workflow √©dition simple cr√©√©\")\n",
    "print(f\"   Nodes: {len(workflow_simple)}\")\n",
    "\n",
    "# Workflow 2: Cha√Ænage\n",
    "workflow_chained = create_chained_workflow(\n",
    "    base_prompt=\"A cat sitting on a chair\",\n",
    "    refine_prompt=\"High quality, professional photography, detailed fur\"\n",
    ")\n",
    "print(\"‚úÖ Workflow cha√Æn√© cr√©√©\")\n",
    "print(f\"   Nodes: {len(workflow_chained)}\")\n",
    "print(f\"   KSamplers: 2 (base + raffinement)\")\n",
    "\n",
    "# üí° Pour ex√©cuter ces workflows:\n",
    "# client = ComfyUIClient(\"https://qwen-image-edit.myia.io\")\n",
    "# result = client.execute_workflow(workflow_simple)\n",
    "# images = client.get_results(result[\"prompt_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75258e1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## üñºÔ∏è √âdition Images avec Qwen VLM\n",
    "\n",
    "### Capacit√©s Qwen Vision-Language Model\n",
    "\n",
    "**Qwen-Image-Edit** combine:\n",
    "- üß† **Vision Encoder** (CLIP-like) pour comprendre images\n",
    "- ‚úçÔ∏è **Language Model** pour interpr√©ter instructions texte\n",
    "- üé® **Diffusion Model** pour √©diter images\n",
    "\n",
    "### Cas d'Usage Typiques\n",
    "\n",
    "| T√¢che | Exemple Prompt |\n",
    "|-------|----------------|\n",
    "| **Style Transfer** | `\"Convert to watercolor painting\"` |\n",
    "| **Object Addition** | `\"Add a red balloon in the sky\"` |\n",
    "| **Color Grading** | `\"Make the image warmer, golden hour lighting\"` |\n",
    "| **Background Change** | `\"Replace background with snowy mountains\"` |\n",
    "| **Detail Enhancement** | `\"Enhance facial details, 8k quality\"` |\n",
    "\n",
    "### Pattern Image-to-Image\n",
    "\n",
    "**Diff√©rence cl√© avec Text-to-Image**:\n",
    "- **Nouveau node**: `LoadImage` pour charger image source\n",
    "- **Param√®tre critique**: `denoise` (0.0-1.0)\n",
    "  - `denoise=0.1`: √âdition subtile (retouche l√©g√®re)\n",
    "  - `denoise=0.5`: √âdition mod√©r√©e (style transfer)\n",
    "  - `denoise=0.9`: √âdition forte (reconstruction quasi-totale)\n",
    "\n",
    "**Workflow**:\n",
    "1. **Load Image** ‚Üí Charger image source\n",
    "2. **CLIP Vision Encode** ‚Üí Encoder image\n",
    "3. **CLIP Text Encode** ‚Üí Encoder prompt √©dition\n",
    "4. **VAE Encode** ‚Üí Convertir pixels ‚Üí latent\n",
    "5. **KSampler** (denoise < 1.0) ‚Üí √âditer latent\n",
    "6. **VAE Decode** ‚Üí Convertir latent ‚Üí pixels\n",
    "7. **Save Image** ‚Üí Sauvegarder r√©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba896395",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_image_to_comfyui(image_path: str) -> str:\n",
    "    \"\"\"Upload image vers ComfyUI pour √©dition\n",
    "    \n",
    "    Args:\n",
    "        image_path: Chemin image locale ou URL\n",
    "    \n",
    "    Returns:\n",
    "        str: Nom fichier upload√© dans ComfyUI\n",
    "    \"\"\"\n",
    "    # Charger image\n",
    "    if image_path.startswith('http'):\n",
    "        response = requests.get(image_path)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        img = Image.open(image_path)\n",
    "    \n",
    "    # Convertir en bytes\n",
    "    img_bytes = BytesIO()\n",
    "    img.save(img_bytes, format='PNG')\n",
    "    img_bytes.seek(0)\n",
    "    \n",
    "    # Upload vers ComfyUI\n",
    "    files = {'image': ('input.png', img_bytes, 'image/png')}\n",
    "    response = client.session.post(\n",
    "        f\"{API_BASE_URL}/upload/image\",\n",
    "        files=files\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    filename = response.json()['name']\n",
    "    print(f\"‚úÖ Image upload√©e: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Test upload avec image exemple\n",
    "# Note: Remplacer par votre propre image\n",
    "test_image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/481px-Cat03.jpg\"\n",
    "\n",
    "try:\n",
    "    uploaded_filename = upload_image_to_comfyui(test_image_url)\n",
    "    print(f\"üìÅ Fichier: {uploaded_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur upload: {e}\")\n",
    "    print(\"üí° Conseil: Utiliser une image locale ou v√©rifier URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6a3da",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## üé® Workflow Image-to-Image Complet\n",
    "\n",
    "### Architecture\n",
    "\n",
    "**Pipeline √©dition**:\n",
    "1. **Load Image** ‚Üí Charger image source upload√©e\n",
    "2. **VAE Encode** ‚Üí Convertir pixels ‚Üí latent space\n",
    "3. **CLIP Text Encode** ‚Üí Encoder instructions √©dition\n",
    "4. **KSampler** (denoise partiel) ‚Üí √âditer latent\n",
    "5. **VAE Decode** ‚Üí Reconvertir latent ‚Üí pixels\n",
    "6. **Save Image** ‚Üí Sauvegarder r√©sultat\n",
    "\n",
    "### Param√®tre Critique: denoise\n",
    "\n",
    "**Impact sur √©dition**:\n",
    "\n",
    "| denoise | Type √âdition | Exemple |\n",
    "|---------|-------------|----------|\n",
    "| **0.1-0.3** | Retouche subtile | Correction couleurs, am√©lioration d√©tails |\n",
    "| **0.4-0.6** | √âdition mod√©r√©e | Style transfer, ajout √©l√©ments mineurs |\n",
    "| **0.7-0.9** | √âdition forte | Changement sc√®ne, reconstruction majeure |\n",
    "| **1.0** | G√©n√©ration totale | Ignore quasi totalement image source |\n",
    "\n",
    "**Recommandation**: Commencer avec `denoise=0.5` et ajuster selon r√©sultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747201c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Workflow Image-to-Image (√©dition)\n",
    "workflow_img2img = {\n",
    "    \"1\": {  # Load Checkpoint\n",
    "        \"class_type\": \"CheckpointLoaderSimple\",\n",
    "        \"inputs\": {\n",
    "            \"ckpt_name\": \"qwen-image-edit-2509-fp8.safetensors\"\n",
    "        }\n",
    "    },\n",
    "    \"2\": {  # Load Image (image source upload√©e)\n",
    "        \"class_type\": \"LoadImage\",\n",
    "        \"inputs\": {\n",
    "            \"image\": uploaded_filename  # Variable de cellule pr√©c√©dente\n",
    "        }\n",
    "    },\n",
    "    \"3\": {  # VAE Encode (pixels ‚Üí latent)\n",
    "        \"class_type\": \"VAEEncode\",\n",
    "        \"inputs\": {\n",
    "            \"pixels\": [\"2\", 0],  # Image source\n",
    "            \"vae\": [\"1\", 2]  # VAE du checkpoint\n",
    "        }\n",
    "    },\n",
    "    \"4\": {  # CLIP Text Encode (prompt √©dition)\n",
    "        \"class_type\": \"CLIPTextEncode\",\n",
    "        \"inputs\": {\n",
    "            \"text\": \"watercolor painting style, artistic, vibrant colors\",\n",
    "            \"clip\": [\"1\", 1]\n",
    "        }\n",
    "    },\n",
    "    \"5\": {  # CLIP Text Encode (prompt n√©gatif)\n",
    "        \"class_type\": \"CLIPTextEncode\",\n",
    "        \"inputs\": {\n",
    "            \"text\": \"blurry, low quality, pixelated\",\n",
    "            \"clip\": [\"1\", 1]\n",
    "        }\n",
    "    },\n",
    "    \"6\": {  # KSampler (√©dition avec denoise partiel)\n",
    "        \"class_type\": \"KSampler\",\n",
    "        \"inputs\": {\n",
    "            \"model\": [\"1\", 0],\n",
    "            \"positive\": [\"4\", 0],\n",
    "            \"negative\": [\"5\", 0],\n",
    "            \"latent_image\": [\"3\", 0],  # Latent de l'image source\n",
    "            \"seed\": 42,\n",
    "            \"steps\": 25,\n",
    "            \"cfg\": 7.5,\n",
    "            \"sampler_name\": \"euler\",\n",
    "            \"scheduler\": \"normal\",\n",
    "            \"denoise\": 0.5  # √âdition mod√©r√©e (50%)\n",
    "        }\n",
    "    },\n",
    "    \"7\": {  # VAE Decode (latent ‚Üí pixels)\n",
    "        \"class_type\": \"VAEDecode\",\n",
    "        \"inputs\": {\n",
    "            \"samples\": [\"6\", 0],\n",
    "            \"vae\": [\"1\", 2]\n",
    "        }\n",
    "    },\n",
    "    \"8\": {  # Save Image\n",
    "        \"class_type\": \"SaveImage\",\n",
    "        \"inputs\": {\n",
    "            \"images\": [\"7\", 0],\n",
    "            \"filename_prefix\": \"Qwen_Edit\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ex√©cution (seulement si image upload√©e pr√©c√©demment)\n",
    "try:\n",
    "    print(\"üé® Lancement √©dition image...\")\n",
    "    result = client.execute_workflow(workflow_img2img, verbose=True)\n",
    "    \n",
    "    # Affichage avant/apr√®s\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Image originale\n",
    "    original_img = Image.open(BytesIO(client.session.get(\n",
    "        f\"{API_BASE_URL}/view\",\n",
    "        params={\"filename\": uploaded_filename, \"type\": \"input\"}\n",
    "    ).content))\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title(\"Image Originale\", fontsize=12)\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Image √©dit√©e\n",
    "    edited_img = Image.open(BytesIO(result[\"images\"][0][\"data\"]))\n",
    "    axes[1].imshow(edited_img)\n",
    "    axes[1].set_title(f\"Image √âdit√©e (denoise=0.5)\", fontsize=12)\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ √âdition compl√®te en {result['duration']:.1f}s\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è Ex√©cutez d'abord la cellule d'upload d'image\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1176e6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## üî¨ Exp√©rimentation: Comparaison Denoise\n",
    "\n",
    "### Objectif P√©dagogique\n",
    "\n",
    "Comprendre **impact du param√®tre `denoise`** sur qualit√© √©dition.\n",
    "\n",
    "### M√©thodologie\n",
    "\n",
    "1. Workflow identique (m√™me prompt, seed, etc.)\n",
    "2. Variation **uniquement** du param√®tre `denoise`\n",
    "3. Comparaison visuelle r√©sultats\n",
    "\n",
    "### Hypoth√®se\n",
    "\n",
    "**denoise faible** (0.2) ‚Üí √âdition subtile, image proche de l'originale\n",
    "**denoise moyen** (0.5) ‚Üí Bon compromis √©dition/pr√©servation\n",
    "**denoise √©lev√©** (0.8) ‚Üí √âdition forte, risque divergence\n",
    "\n",
    "### Configuration Test\n",
    "\n",
    "```python\n",
    "denoise_values = [0.2, 0.5, 0.8]\n",
    "prompt = \"convert to dramatic black and white, high contrast\"\n",
    "seed = 42  # Fixe pour comparabilit√©\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84553629",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comparaison denoise (denoise=0.2, 0.5, 0.8)\n",
    "denoise_values = [0.2, 0.5, 0.8]\n",
    "edit_prompt = \"convert to dramatic black and white, high contrast\"\n",
    "\n",
    "results_denoise = []\n",
    "\n",
    "for denoise_val in denoise_values:\n",
    "    print(f\"\\nüîÑ Test denoise={denoise_val}...\")\n",
    "    \n",
    "    # Cr√©er workflow avec denoise sp√©cifique\n",
    "    workflow_denoise_test = {\n",
    "        \"1\": {\n",
    "            \"class_type\": \"CheckpointLoaderSimple\",\n",
    "            \"inputs\": {\"ckpt_name\": \"qwen-image-edit-2509-fp8.safetensors\"}\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"class_type\": \"LoadImage\",\n",
    "            \"inputs\": {\"image\": uploaded_filename}\n",
    "        },\n",
    "        \"3\": {\n",
    "            \"class_type\": \"VAEEncode\",\n",
    "            \"inputs\": {\n",
    "                \"pixels\": [\"2\", 0],\n",
    "                \"vae\": [\"1\", 2]\n",
    "            }\n",
    "        },\n",
    "        \"4\": {\n",
    "            \"class_type\": \"CLIPTextEncode\",\n",
    "            \"inputs\": {\n",
    "                \"text\": edit_prompt,\n",
    "                \"clip\": [\"1\", 1]\n",
    "            }\n",
    "        },\n",
    "        \"5\": {\n",
    "            \"class_type\": \"CLIPTextEncode\",\n",
    "            \"inputs\": {\n",
    "                \"text\": \"blurry, low quality\",\n",
    "                \"clip\": [\"1\", 1]\n",
    "            }\n",
    "        },\n",
    "        \"6\": {\n",
    "            \"class_type\": \"KSampler\",\n",
    "            \"inputs\": {\n",
    "                \"model\": [\"1\", 0],\n",
    "                \"positive\": [\"4\", 0],\n",
    "                \"negative\": [\"5\", 0],\n",
    "                \"latent_image\": [\"3\", 0],\n",
    "                \"seed\": 42,\n",
    "                \"steps\": 25,\n",
    "                \"cfg\": 7.5,\n",
    "                \"sampler_name\": \"euler\",\n",
    "                \"scheduler\": \"normal\",\n",
    "                \"denoise\": denoise_val  # Variable test√©e\n",
    "            }\n",
    "        },\n",
    "        \"7\": {\n",
    "            \"class_type\": \"VAEDecode\",\n",
    "            \"inputs\": {\n",
    "                \"samples\": [\"6\", 0],\n",
    "                \"vae\": [\"1\", 2]\n",
    "            }\n",
    "        },\n",
    "        \"8\": {\n",
    "            \"class_type\": \"SaveImage\",\n",
    "            \"inputs\": {\n",
    "                \"images\": [\"7\", 0],\n",
    "                \"filename_prefix\": f\"Denoise_{denoise_val}\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = client.execute_workflow(workflow_denoise_test, verbose=False)\n",
    "        results_denoise.append({\n",
    "            \"denoise\": denoise_val,\n",
    "            \"result\": result\n",
    "        })\n",
    "        print(f\"‚úÖ Compl√©t√© en {result['duration']:.1f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "        results_denoise.append({\"denoise\": denoise_val, \"error\": str(e)})\n",
    "\n",
    "# Affichage comparatif\n",
    "if len(results_denoise) == 3 and all('result' in r for r in results_denoise):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    for i, res in enumerate(results_denoise):\n",
    "        img = Image.open(BytesIO(res[\"result\"][\"images\"][0][\"data\"]))\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"denoise={res['denoise']}\", fontsize=12)\n",
    "        axes[i].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Observations:\")\n",
    "    print(\"- denoise=0.2: √âdition subtile, pr√©serve d√©tails originaux\")\n",
    "    print(\"- denoise=0.5: √âquilibre √©dition/pr√©servation\")\n",
    "    print(\"- denoise=0.8: √âdition forte, peut diverger de l'original\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Certains tests ont √©chou√©, v√©rifiez les erreurs ci-dessus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacdbf51",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üñºÔ∏è COMPARAISON AVANT/APR√àS: Side-by-Side\n",
    "# ========================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "def compare_before_after(\n",
    "    original_path: str,\n",
    "    edited_path: str,\n",
    "    title_original: str = \"Image Originale\",\n",
    "    title_edited: str = \"Image √âdit√©e\",\n",
    "    show_metrics: bool = True\n",
    ") -> None:\n",
    "    \"\"\"Affiche comparaison side-by-side avec m√©triques qualit√©\n",
    "    \n",
    "    Args:\n",
    "        original_path: Chemin image originale\n",
    "        edited_path: Chemin image √©dit√©e\n",
    "        title_original: Titre image originale\n",
    "        title_edited: Titre image √©dit√©e\n",
    "        show_metrics: Afficher m√©triques qualit√© (PSNR, SSIM)\n",
    "    \"\"\"\n",
    "    # Charger images\n",
    "    img_original = Image.open(original_path)\n",
    "    img_edited = Image.open(edited_path)\n",
    "    \n",
    "    # Convertir en numpy arrays\n",
    "    arr_original = np.array(img_original)\n",
    "    arr_edited = np.array(img_edited)\n",
    "    \n",
    "    # Cr√©er figure avec 2 colonnes\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Image originale\n",
    "    axes[0].imshow(arr_original)\n",
    "    axes[0].set_title(f\"{title_original}\\n{img_original.size[0]}x{img_original.size[1]}\", \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Image √©dit√©e\n",
    "    axes[1].imshow(arr_edited)\n",
    "    axes[1].set_title(f\"{title_edited}\\n{img_edited.size[0]}x{img_edited.size[1]}\", \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # M√©triques qualit√©\n",
    "    if show_metrics and arr_original.shape == arr_edited.shape:\n",
    "        # PSNR (Peak Signal-to-Noise Ratio)\n",
    "        mse = np.mean((arr_original - arr_edited) ** 2)\n",
    "        if mse == 0:\n",
    "            psnr = float('inf')\n",
    "        else:\n",
    "            max_pixel = 255.0\n",
    "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "        \n",
    "        # Diff√©rence absolue moyenne\n",
    "        mae = np.mean(np.abs(arr_original - arr_edited))\n",
    "        \n",
    "        # Afficher m√©triques\n",
    "        metrics_text = f\"üìä M√©triques:\\n\"\n",
    "        metrics_text += f\"   PSNR: {psnr:.2f} dB\\n\"\n",
    "        metrics_text += f\"   MAE: {mae:.2f}\\n\"\n",
    "        metrics_text += f\"   Pixels modifi√©s: {np.sum(arr_original != arr_edited):,}\"\n",
    "        \n",
    "        fig.text(0.5, 0.02, metrics_text, ha='center', fontsize=12, \n",
    "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interpr√©tation PSNR\n",
    "    if show_metrics and arr_original.shape == arr_edited.shape:\n",
    "        print(\"\\nüìà Interpr√©tation PSNR:\")\n",
    "        if psnr > 40:\n",
    "            print(\"   ‚úÖ Excellente qualit√© (PSNR > 40 dB) - Changements subtils\")\n",
    "        elif psnr > 30:\n",
    "            print(\"   ‚úÖ Bonne qualit√© (PSNR 30-40 dB) - Changements visibles mais contr√¥l√©s\")\n",
    "        elif psnr > 20:\n",
    "            print(\"   ‚ö†Ô∏è  Qualit√© acceptable (PSNR 20-30 dB) - Changements significatifs\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Qualit√© faible (PSNR < 20 dB) - Changements majeurs\")\n",
    "\n",
    "def create_difference_map(\n",
    "    original_path: str,\n",
    "    edited_path: str,\n",
    "    amplification: float = 5.0\n",
    ") -> None:\n",
    "    \"\"\"Cr√©e une carte visuelle des diff√©rences entre 2 images\n",
    "    \n",
    "    Args:\n",
    "        original_path: Chemin image originale\n",
    "        edited_path: Chemin image √©dit√©e\n",
    "        amplification: Facteur amplification diff√©rences pour visibilit√©\n",
    "    \"\"\"\n",
    "    img_original = np.array(Image.open(original_path))\n",
    "    img_edited = np.array(Image.open(edited_path))\n",
    "    \n",
    "    if img_original.shape != img_edited.shape:\n",
    "        print(\"‚ùå Images de tailles diff√©rentes, impossible de comparer\")\n",
    "        return\n",
    "    \n",
    "    # Calculer diff√©rence absolue\n",
    "    diff = np.abs(img_original.astype(float) - img_edited.astype(float))\n",
    "    \n",
    "    # Amplifier pour visibilit√©\n",
    "    diff_amplified = np.clip(diff * amplification, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Afficher\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    axes[0].imshow(img_original)\n",
    "    axes[0].set_title(\"Originale\", fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(img_edited)\n",
    "    axes[1].set_title(\"√âdit√©e\", fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(diff_amplified)\n",
    "    axes[2].set_title(f\"Carte Diff√©rences (√ó{amplification})\", fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques diff√©rences\n",
    "    print(f\"\\nüìä Statistiques diff√©rences:\")\n",
    "    print(f\"   Moyenne: {np.mean(diff):.2f}\")\n",
    "    print(f\"   Max: {np.max(diff):.2f}\")\n",
    "    print(f\"   Pixels modifi√©s (>5): {np.sum(diff > 5):,} ({100*np.sum(diff > 5)/diff.size:.2f}%)\")\n",
    "\n",
    "# ========================================\n",
    "# EXEMPLE D'UTILISATION\n",
    "# ========================================\n",
    "\n",
    "# Cas d'usage: Comparer original vs √©dition Qwen\n",
    "# compare_before_after(\n",
    "#     original_path=\"cat_original.png\",\n",
    "#     edited_path=\"cat_with_sunglasses.png\",\n",
    "#     title_original=\"Chat Original\",\n",
    "#     title_edited=\"Chat avec Lunettes (Qwen Edit)\",\n",
    "#     show_metrics=True\n",
    "# )\n",
    "\n",
    "# Cas d'usage: Carte diff√©rences pour analyse d√©taill√©e\n",
    "# create_difference_map(\n",
    "#     original_path=\"cat_original.png\",\n",
    "#     edited_path=\"cat_with_sunglasses.png\",\n",
    "#     amplification=10.0\n",
    "# )\n",
    "\n",
    "print(\"‚úÖ Fonctions comparaison avant/apr√®s d√©finies\")\n",
    "print(\"   - compare_before_after(): Affichage side-by-side + m√©triques\")\n",
    "print(\"   - create_difference_map(): Carte visuelle diff√©rences\")\n",
    "print(\"\\nüí° D√©commentez les exemples ci-dessus pour tester avec vos images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db2fc7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## ‚öôÔ∏è Bonnes Pratiques ComfyUI\n",
    "\n",
    "### 1. Gestion des Erreurs Courantes\n",
    "\n",
    "#### Timeout (>120s)\n",
    "**Cause**: GPU surcharg√©, workflow trop complexe\n",
    "**Solution**:\n",
    "```python\n",
    "client.execute_workflow(workflow, max_wait=300)  # Augmenter timeout\n",
    "```\n",
    "\n",
    "#### CUDA Out of Memory\n",
    "**Cause**: R√©solution trop √©lev√©e (>768x768)\n",
    "**Solution**:\n",
    "- R√©duire r√©solution (512x512 optimal)\n",
    "- Diminuer `batch_size`\n",
    "- Simplifier workflow\n",
    "\n",
    "#### Node Not Found\n",
    "**Cause**: `class_type` invalide ou custom node manquant\n",
    "**Solution**: V√©rifier documentation custom nodes Qwen\n",
    "\n",
    "### 2. Optimisation Performance\n",
    "\n",
    "| Param√®tre | Impact Performance | Recommandation |\n",
    "|-----------|-------------------|----------------|\n",
    "| **steps** | ‚Üë steps = ‚Üë temps | 20-25 optimal |\n",
    "| **resolution** | ‚Üë r√©solution = ‚Üë‚Üë VRAM | 512x512 par d√©faut |\n",
    "| **denoise** | Minimal | N/A |\n",
    "| **cfg** | Minimal | 7-8 optimal |\n",
    "\n",
    "### 3. Workflow Reproductible\n",
    "\n",
    "**Toujours fixer seed pour debugging**:\n",
    "```python\n",
    "\"seed\": 42  # M√™me r√©sultat √† chaque ex√©cution\n",
    "```\n",
    "\n",
    "**Seed al√©atoire pour vari√©t√©**:\n",
    "```python\n",
    "\"seed\": int(time.time())  # Diff√©rent √† chaque fois\n",
    "```\n",
    "\n",
    "### 4. Logs et Debugging\n",
    "\n",
    "**Activer verbose**:\n",
    "```python\n",
    "result = client.execute_workflow(workflow, verbose=True)\n",
    "```\n",
    "\n",
    "**Inspecter outputs**:\n",
    "```python\n",
    "print(json.dumps(result[\"outputs\"], indent=2))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af4b97",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üéØ EXERCICE PRATIQUE\n",
    "# ========================================\n",
    "# Cr√©ez votre propre workflow d'√©dition d'image\n",
    "\n",
    "# OBJECTIF:\n",
    "# Modifier une image en ajoutant un effet sp√©cifique via Qwen VLM\n",
    "\n",
    "# INSTRUCTIONS:\n",
    "# 1. Choisissez une image de test (ou utilisez celle g√©n√©r√©e pr√©c√©demment)\n",
    "# 2. Cr√©ez un workflow image-to-image avec un prompt cr√©atif\n",
    "# 3. Testez diff√©rentes valeurs de denoise (0.3, 0.5, 0.7)\n",
    "# 4. Comparez visuellement les r√©sultats\n",
    "\n",
    "# TODO: Compl√©tez le workflow ci-dessous\n",
    "workflow_exercice = {\n",
    "    \"5\": {\n",
    "        \"class_type\": \"CheckpointLoaderSimple\",\n",
    "        \"inputs\": {\n",
    "            \"ckpt_name\": \"qwen2vl.safetensors\"  # Mod√®le Qwen\n",
    "        }\n",
    "    },\n",
    "    \"10\": {\n",
    "        \"class_type\": \"LoadImage\",\n",
    "        \"inputs\": {\n",
    "            # TODO: Ajoutez le chemin de votre image source\n",
    "            \"image\": \"___VOTRE_IMAGE_ICI___\"\n",
    "        }\n",
    "    },\n",
    "    \"6\": {\n",
    "        \"class_type\": \"CLIPTextEncode\",\n",
    "        \"inputs\": {\n",
    "            # TODO: √âcrivez un prompt cr√©atif (ex: \"convert to watercolor painting\")\n",
    "            \"text\": \"___VOTRE_PROMPT_ICI___\",\n",
    "            \"clip\": [\"5\", 1]\n",
    "        }\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"class_type\": \"KSampler\",\n",
    "        \"inputs\": {\n",
    "            \"seed\": 42,\n",
    "            # TODO: Testez diff√©rentes valeurs de denoise (0.3 √† 0.7)\n",
    "            \"denoise\": 0.5,\n",
    "            \"steps\": 20,\n",
    "            \"cfg\": 7.5,\n",
    "            \"sampler_name\": \"euler\",\n",
    "            \"scheduler\": \"normal\",\n",
    "            \"model\": [\"5\", 0],\n",
    "            \"positive\": [\"6\", 0],\n",
    "            \"negative\": [\"7\", 0],\n",
    "            \"latent_image\": [\"13\", 0]\n",
    "        }\n",
    "    },\n",
    "    \"7\": {\n",
    "        \"class_type\": \"CLIPTextEncode\",\n",
    "        \"inputs\": {\n",
    "            \"text\": \"blurry, low quality, distorted\",\n",
    "            \"clip\": [\"5\", 1]\n",
    "        }\n",
    "    },\n",
    "    \"13\": {\n",
    "        \"class_type\": \"VAEEncode\",\n",
    "        \"inputs\": {\n",
    "            \"pixels\": [\"10\", 0],\n",
    "            \"vae\": [\"5\", 2]\n",
    "        }\n",
    "    },\n",
    "    \"8\": {\n",
    "        \"class_type\": \"VAEDecode\",\n",
    "        \"inputs\": {\n",
    "            \"samples\": [\"3\", 0],\n",
    "            \"vae\": [\"5\", 2]\n",
    "        }\n",
    "    },\n",
    "    \"9\": {\n",
    "        \"class_type\": \"SaveImage\",\n",
    "        \"inputs\": {\n",
    "            \"filename_prefix\": \"exercice_qwen\",\n",
    "            \"images\": [\"8\", 0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# AIDE:\n",
    "# - Pour l'image source: utilisez le nom d'une image upload√©e (ex: \"cat_512x512.png\")\n",
    "# - Pour le prompt: soyez cr√©atif! (ex: \"make it look like a Van Gogh painting\")\n",
    "# - Pour denoise: commencez par 0.5, puis testez 0.3 et 0.7\n",
    "\n",
    "# BONUS:\n",
    "# Cr√©ez une fonction pour tester plusieurs prompts automatiquement\n",
    "def tester_prompts_exercice(prompts_list, image_source, denoise=0.5):\n",
    "    \"\"\"\n",
    "    Teste plusieurs prompts sur la m√™me image\n",
    "    \n",
    "    Args:\n",
    "        prompts_list: Liste de prompts √† tester\n",
    "        image_source: Nom de l'image source\n",
    "        denoise: Valeur denoise (d√©faut 0.5)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts_list):\n",
    "        print(f\"\\nüé® Test {i+1}/{len(prompts_list)}: {prompt}\")\n",
    "        \n",
    "        # TODO: Modifiez le workflow pour chaque prompt\n",
    "        workflow_test = workflow_exercice.copy()\n",
    "        # ... (ajoutez votre code ici)\n",
    "        \n",
    "        # Ex√©cution workflow\n",
    "        # result = client.execute_workflow(workflow_test)\n",
    "        # results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# TESTEZ VOTRE CODE:\n",
    "# prompts_test = [\n",
    "#     \"convert to watercolor painting\",\n",
    "#     \"add dramatic sunset lighting\",\n",
    "#     \"make it look like a pencil sketch\"\n",
    "# ]\n",
    "# resultats = tester_prompts_exercice(prompts_test, \"cat_512x512.png\")\n",
    "\n",
    "print(\"\\n‚úÖ Exercice pr√™t! Compl√©tez les TODO et ex√©cutez la cellule.\")\n",
    "print(\"üí° Conseil: Commencez simple, puis ajoutez de la complexit√© progressivement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a694ae8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## üìö Ressources Compl√©mentaires\n",
    "\n",
    "### Documentation Officielle\n",
    "\n",
    "#### ComfyUI\n",
    "- **GitHub**: [https://github.com/comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)\n",
    "- **Documentation API**: [https://github.com/comfyanonymous/ComfyUI/wiki/API](https://github.com/comfyanonymous/ComfyUI/wiki/API)\n",
    "- **Custom Nodes**: [https://github.com/ltdrdata/ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager)\n",
    "\n",
    "#### Qwen Vision-Language Model\n",
    "- **Paper Officiel**: *Qwen-VL: A Versatile Vision-Language Model*\n",
    "- **Mod√®le Hugging Face**: [https://huggingface.co/Qwen/Qwen-VL](https://huggingface.co/Qwen/Qwen-VL)\n",
    "- **Documentation Technique**: [https://qwenlm.github.io/](https://qwenlm.github.io/)\n",
    "\n",
    "### Workflows Avanc√©s\n",
    "\n",
    "#### Workflows ComfyUI Communautaires\n",
    "- **ComfyUI Workflows Gallery**: [https://comfyworkflows.com/](https://comfyworkflows.com/)\n",
    "- **CivitAI ComfyUI Section**: [https://civitai.com/tag/comfyui](https://civitai.com/tag/comfyui)\n",
    "\n",
    "#### Custom Nodes Recommand√©s\n",
    "- **ComfyUI-Impact-Pack**: Outils post-processing avanc√©s\n",
    "- **ComfyUI-AnimateDiff**: Animations et vid√©os\n",
    "- **ComfyUI-ControlNet**: Contr√¥le spatial pr√©cis\n",
    "\n",
    "### Tutoriels et Guides\n",
    "\n",
    "#### D√©butants\n",
    "1. **ComfyUI Basics** (YouTube): Introduction compl√®te workflows\n",
    "2. **Qwen-VL Quick Start**: Guide rapide √©dition images\n",
    "3. **JSON Workflows 101**: Comprendre structure workflows\n",
    "\n",
    "#### Interm√©diaires\n",
    "1. **Advanced Prompting Techniques**: Optimisation prompts Qwen\n",
    "2. **Workflow Optimization**: R√©duire temps g√©n√©ration\n",
    "3. **Multi-Step Workflows**: Cha√Ænage nodes complexes\n",
    "\n",
    "#### Avanc√©s\n",
    "1. **Custom Node Development**: Cr√©er vos propres nodes\n",
    "2. **API Integration**: Int√©grer ComfyUI dans applications\n",
    "3. **Batch Processing**: Automatisation workflows\n",
    "\n",
    "### Communaut√© et Support\n",
    "\n",
    "- **Discord ComfyUI**: [https://discord.gg/comfyui](https://discord.gg/comfyui)\n",
    "- **Reddit r/comfyui**: Forum communautaire\n",
    "- **GitHub Discussions**: Questions techniques\n",
    "\n",
    "### Ressources MyIA.io\n",
    "\n",
    "- **Guide APIs √âtudiants**: [`GUIDE-APIS-ETUDIANTS.md`](../../../../docs/suivis/genai-image/GUIDE-APIS-ETUDIANTS.md)\n",
    "- **Workflows Qwen Phase 12C**: [`2025-10-16_12C_architectures-5-workflows-qwen.md`](../../../../docs/genai-suivis/2025-10-16_12C_architectures-5-workflows-qwen.md)\n",
    "- **Notebook Forge SD-XL**: [`01-4-Forge-SD-XL-Turbo.ipynb`](01-4-Forge-SD-XL-Turbo.ipynb) (API REST similaire)\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Prochaines √âtapes Apprentissage\n",
    "\n",
    "1. **Ma√Ætriser les bases**: Reproduire tous les exemples de ce notebook\n",
    "2. **Exp√©rimenter**: Modifier workflows, tester nouveaux prompts\n",
    "3. **Explorer workflows avanc√©s**: ControlNet, AnimateDiff, Multi-Model\n",
    "4. **Cr√©er projets personnels**: Application web int√©grant API Qwen\n",
    "5. **Contribuer communaut√©**: Partager vos workflows innovants\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ Notebook termin√©! Bon apprentissage avec Qwen Image Edit! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp-jupyter-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.980048,
   "end_time": "2025-10-25T23:55:52.602331",
   "environment_variables": {},
   "exception": true,
   "input_path": "01-5-Qwen-Image-Edit.ipynb",
   "output_path": "01-5-Qwen-Image-Edit_output_20251026_005500.ipynb",
   "parameters": {},
   "start_time": "2025-10-25T23:55:47.622283",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}