{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "79be9a9e",
      "metadata": {},
      "source": [
        "# Notebook: Qwen Image-Edit 2.5 - API ComfyUI\n",
        "\n",
        "**Objectif**: Maîtriser génération et édition d'images via API Qwen-Image-Edit (backend ComfyUI)\n",
        "\n",
        "## 🎯 Ce que vous allez apprendre\n",
        "\n",
        "1. Différence API **Forge** (simple) vs **ComfyUI** (workflows JSON)\n",
        "2. Pattern **\"queue and poll\"** pour génération asynchrone\n",
        "3. Création workflows **Text-to-Image** et **Image-to-Image**\n",
        "4. Optimisation paramètres (**steps**, **cfg**, **denoise**)\n",
        "5. Troubleshooting erreurs courantes (**timeout**, **CUDA OOM**)\n",
        "\n",
        "## 🚀 API Qwen-Image-Edit\n",
        "\n",
        "| Caractéristique | Valeur |\n",
        "|----------------|--------|\n",
        "| **URL Production** | `https://qwen-image-edit.myia.io` |\n",
        "| **Modèle** | Qwen-Image-Edit-2509-FP8 (54GB) |\n",
        "| **GPU** | RTX 3090 (24GB VRAM) |\n",
        "| **Latence Typique** | 5-10 secondes |\n",
        "| **Résolution Optimale** | 512x512 pixels |\n",
        "\n",
        "## 🔍 ComfyUI vs Forge\n",
        "\n",
        "**Forge (SD XL Turbo)**:\n",
        "- ✅ API simple (1 requête POST)\n",
        "- ✅ Ultra-rapide (1-3s)\n",
        "- ❌ Pas d'édition images\n",
        "- ❌ Moins flexible\n",
        "\n",
        "**ComfyUI (Qwen)**:\n",
        "- ✅ Workflows JSON complexes\n",
        "- ✅ Édition images avancée\n",
        "- ✅ Contrôle fin (28 custom nodes)\n",
        "- ❌ API plus complexe (queue + poll)\n",
        "\n",
        "**Recommandation**: Commencer avec Forge pour prototypes, affiner avec Qwen pour production.\n",
        "\n",
        "## 📚 Prérequis\n",
        "\n",
        "```bash\n",
        "pip install requests pillow matplotlib\n",
        "```\n",
        "\n",
        "**Temps estimé**: 90-120 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "244bd967",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports standard\n",
        "import requests\n",
        "import json\n",
        "import base64\n",
        "import time\n",
        "import uuid\n",
        "from typing import Dict, Optional, List\n",
        "from io import BytesIO\n",
        "\n",
        "# Visualisation\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Configuration API\n",
        "API_BASE_URL = \"https://qwen-image-edit.myia.io\"\n",
        "CLIENT_ID = str(uuid.uuid4())  # ID unique pour tracking\n",
        "\n",
        "print(f\"✅ Configuration chargée\")\n",
        "print(f\"📡 API: {API_BASE_URL}\")\n",
        "print(f\"🆔 Client ID: {CLIENT_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5743a6a4",
      "metadata": {},
      "source": [
        "## 🏗️ Architecture ComfyUI: Workflows JSON\n",
        "\n",
        "### Différence fondamentale avec Forge\n",
        "\n",
        "**API Forge (POST direct)**:\n",
        "```python\n",
        "response = requests.post(url, json={\"prompt\": \"astronaut\"})\n",
        "image_base64 = response.json()[\"image\"]\n",
        "```\n",
        "\n",
        "**API ComfyUI (Queue + Poll)**:\n",
        "```python\n",
        "# 1. Soumettre workflow JSON\n",
        "response = requests.post(f\"{url}/prompt\", json={\n",
        "    \"prompt\": workflow_json,\n",
        "    \"client_id\": client_id\n",
        "})\n",
        "prompt_id = response.json()[\"prompt_id\"]\n",
        "\n",
        "# 2. Attendre complétion (polling)\n",
        "while True:\n",
        "    history = requests.get(f\"{url}/history/{prompt_id}\")\n",
        "    if history.json().get(prompt_id, {}).get(\"status\", {}).get(\"completed\"):\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# 3. Récupérer images\n",
        "images = history.json()[prompt_id][\"outputs\"]\n",
        "```\n",
        "\n",
        "### Structure Workflow ComfyUI\n",
        "\n",
        "Un **workflow** est un **graph JSON** de **nodes connectés**:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"1\": {  // Node Checkpoint Loader\n",
        "    \"class_type\": \"CheckpointLoaderSimple\",\n",
        "    \"inputs\": {\"ckpt_name\": \"qwen-image-edit.safetensors\"}\n",
        "  },\n",
        "  \"2\": {  // Node Sampler\n",
        "    \"class_type\": \"KSampler\",\n",
        "    \"inputs\": {\n",
        "      \"model\": [\"1\", 0],  // Connexion: output du node 1\n",
        "      \"steps\": 20,\n",
        "      \"cfg\": 7.0,\n",
        "      \"seed\": 42\n",
        "    }\n",
        "  },\n",
        "  \"3\": {  // Node Save Image\n",
        "    \"class_type\": \"SaveImage\",\n",
        "    \"inputs\": {\"images\": [\"2\", 0]}\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Workflow = Pipeline modulaire** où chaque node effectue une opération (charger modèle, sampler, encoder texte, etc.).\n",
        "\n",
        "### Anatomie d'un Node\n",
        "\n",
        "| Propriété | Description | Exemple |\n",
        "|-----------|-------------|----------|\n",
        "| **`class_type`** | Type de node ComfyUI | `\"CLIPTextEncode\"` |\n",
        "| **`inputs`** | Paramètres du node | `{\"text\": \"astronaut\"}` |\n",
        "| **Connexions** | `[node_id, output_slot]` | `[\"5\", 0]` |\n",
        "\n",
        "**28 custom nodes** disponibles pour Qwen (voir documentation complète dans guide étudiants)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔧 Visualisation Architecture Workflow ComfyUI\n",
        "\n",
        "**Diagramme ASCII d'un workflow ComfyUI typique**:\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│                    WORKFLOW COMFYUI                         │\n",
        "│                                                             │\n",
        "│  ┌──────────────┐        ┌──────────────┐                 │\n",
        "│  │ Load Model   │───────▶│ CLIP Text    │                 │\n",
        "│  │ (Checkpoint) │        │ Encode       │                 │\n",
        "│  └──────────────┘        └──────────────┘                 │\n",
        "│         │                       │                          │\n",
        "│         │                       ▼                          │\n",
        "│         │                ┌──────────────┐                 │\n",
        "│         │                │ Conditioning │                 │\n",
        "│         │                │ (Prompts)    │                 │\n",
        "│         │                └──────────────┘                 │\n",
        "│         │                       │                          │\n",
        "│         ▼                       ▼                          │\n",
        "│  ┌──────────────────────────────────────┐                │\n",
        "│  │          KSampler                     │                │\n",
        "│  │  (steps, denoise, seed, sampler)     │                │\n",
        "│  └──────────────────────────────────────┘                │\n",
        "│                    │                                       │\n",
        "│                    ▼                                       │\n",
        "│             ┌──────────────┐                              │\n",
        "│             │ VAE Decode   │                              │\n",
        "│             └──────────────┘                              │\n",
        "│                    │                                       │\n",
        "│                    ▼                                       │\n",
        "│             ┌──────────────┐                              │\n",
        "│             │ Save Image   │                              │\n",
        "│             └──────────────┘                              │\n",
        "└─────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "**Flux de données**:\n",
        "1. **Checkpoint** → Fournit le modèle (model, CLIP, VAE)\n",
        "2. **CLIP Text Encode** → Convertit le prompt en embeddings\n",
        "3. **KSampler** → Génère l'image latente à partir des embeddings\n",
        "4. **VAE Decode** → Convertit l'image latente en image RGB\n",
        "5. **Save Image** → Sauvegarde l'image finale\n",
        "\n",
        "**Correspondance JSON**:\n",
        "```json\n",
        "{\n",
        "  \"1\": {\"class_type\": \"CheckpointLoaderSimple\", ...},\n",
        "  \"2\": {\"class_type\": \"CLIPTextEncode\", \"inputs\": {\"clip\": [\"1\", 1], ...}},\n",
        "  \"3\": {\"class_type\": \"KSampler\", \"inputs\": {\"model\": [\"1\", 0], ...}},\n",
        "  \"4\": {\"class_type\": \"VAEDecode\", \"inputs\": {\"samples\": [\"3\", 0], ...}},\n",
        "  \"5\": {\"class_type\": \"SaveImage\", \"inputs\": {\"images\": [\"4\", 0]}}\n",
        "}\n",
        "```\n",
        "\n",
        "**Notation `[\"ID_NODE\", INDEX_OUTPUT]`**:\n",
        "- `[\"1\", 0]` = Output 0 (model) du node 1\n",
        "- `[\"1\", 1]` = Output 1 (CLIP) du node 1\n",
        "- `[\"3\", 0]` = Output 0 (latents) du node 3\n",
        "\n",
        "💡 **Astuce**: Chaque node peut avoir plusieurs outputs. L'index détermine lequel utiliser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a8ef79",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ComfyUIClient:\n",
        "    \"\"\"Client pédagogique API ComfyUI pour Qwen\"\"\"\n",
        "    \n",
        "    def __init__(self, base_url=API_BASE_URL, client_id=CLIENT_ID):\n",
        "        self.base_url = base_url\n",
        "        self.client_id = client_id\n",
        "        self.session = requests.Session()\n",
        "    \n",
        "    def execute_workflow(\n",
        "        self, \n",
        "        workflow_json: Dict, \n",
        "        wait_for_completion: bool = True,\n",
        "        max_wait: int = 120,\n",
        "        verbose: bool = True\n",
        "    ) -> Dict:\n",
        "        \"\"\"Exécute workflow ComfyUI et récupère résultats\n",
        "        \n",
        "        Args:\n",
        "            workflow_json: Workflow ComfyUI (dict)\n",
        "            wait_for_completion: Attendre fin génération\n",
        "            max_wait: Timeout en secondes\n",
        "            verbose: Afficher logs progression\n",
        "        \n",
        "        Returns:\n",
        "            dict: {\"prompt_id\", \"outputs\", \"status\"}\n",
        "        \"\"\"\n",
        "        # 1. Soumettre workflow\n",
        "        if verbose:\n",
        "            print(\"📤 Soumission workflow...\")\n",
        "        \n",
        "        response = self.session.post(\n",
        "            f\"{self.base_url}/prompt\",\n",
        "            json={\"prompt\": workflow_json, \"client_id\": self.client_id}\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        prompt_id = response.json()[\"prompt_id\"]\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"✅ Workflow queued: {prompt_id}\")\n",
        "        \n",
        "        if not wait_for_completion:\n",
        "            return {\"prompt_id\": prompt_id, \"status\": \"queued\"}\n",
        "        \n",
        "        # 2. Polling complétion\n",
        "        start_time = time.time()\n",
        "        while True:\n",
        "            elapsed = time.time() - start_time\n",
        "            if elapsed > max_wait:\n",
        "                raise TimeoutError(f\"Timeout après {max_wait}s\")\n",
        "            \n",
        "            history_response = self.session.get(\n",
        "                f\"{self.base_url}/history/{prompt_id}\"\n",
        "            )\n",
        "            history_response.raise_for_status()\n",
        "            history = history_response.json()\n",
        "            \n",
        "            if prompt_id not in history:\n",
        "                time.sleep(1)\n",
        "                continue\n",
        "            \n",
        "            prompt_data = history[prompt_id]\n",
        "            status = prompt_data.get(\"status\", {})\n",
        "            \n",
        "            if status.get(\"completed\"):\n",
        "                if verbose:\n",
        "                    print(f\"✅ Complet en {elapsed:.1f}s\")\n",
        "                \n",
        "                # 3. Extraire outputs\n",
        "                outputs = prompt_data.get(\"outputs\", {})\n",
        "                images = []\n",
        "                \n",
        "                for node_id, node_output in outputs.items():\n",
        "                    if \"images\" in node_output:\n",
        "                        for img_info in node_output[\"images\"]:\n",
        "                            # Récupérer image\n",
        "                            img_response = self.session.get(\n",
        "                                f\"{self.base_url}/view\",\n",
        "                                params={\n",
        "                                    \"filename\": img_info[\"filename\"],\n",
        "                                    \"subfolder\": img_info.get(\"subfolder\", \"\"),\n",
        "                                    \"type\": img_info.get(\"type\", \"output\")\n",
        "                                }\n",
        "                            )\n",
        "                            img_response.raise_for_status()\n",
        "                            images.append({\n",
        "                                \"data\": img_response.content,\n",
        "                                \"filename\": img_info[\"filename\"]\n",
        "                            })\n",
        "                \n",
        "                return {\n",
        "                    \"prompt_id\": prompt_id,\n",
        "                    \"status\": \"completed\",\n",
        "                    \"outputs\": outputs,\n",
        "                    \"images\": images,\n",
        "                    \"duration\": elapsed\n",
        "                }\n",
        "            \n",
        "            if status.get(\"status_str\") == \"error\":\n",
        "                error_msg = status.get(\"messages\", [\"Unknown error\"])\n",
        "                raise RuntimeError(f\"ComfyUI error: {error_msg}\")\n",
        "            \n",
        "            if verbose and int(elapsed) % 5 == 0:\n",
        "                print(f\"⏳ En cours... ({elapsed:.0f}s)\")\n",
        "            \n",
        "            time.sleep(1)\n",
        "    \n",
        "    def display_images(self, result: Dict, figsize=(12, 4)):\n",
        "        \"\"\"Affiche images résultats\"\"\"\n",
        "        images = result.get(\"images\", [])\n",
        "        if not images:\n",
        "            print(\"⚠️ Aucune image générée\")\n",
        "            return\n",
        "        \n",
        "        n = len(images)\n",
        "        fig, axes = plt.subplots(1, n, figsize=figsize)\n",
        "        if n == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for ax, img_data in zip(axes, images):\n",
        "            img = Image.open(BytesIO(img_data[\"data\"]))\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(img_data[\"filename\"], fontsize=10)\n",
        "            ax.axis(\"off\")\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Instancier client\n",
        "client = ComfyUIClient()\n",
        "print(\"✅ ComfyUIClient prêt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de517444",
      "metadata": {},
      "source": [
        "## 🚀 Workflow Minimal: \"Hello World\"\n",
        "\n",
        "### Objectif\n",
        "\n",
        "Créer le workflow le plus simple possible pour **valider l'API** et comprendre la structure JSON.\n",
        "\n",
        "### Workflow Text-to-Image Basique\n",
        "\n",
        "**Pipeline**:\n",
        "1. **Load Checkpoint** → Charger modèle Qwen\n",
        "2. **CLIP Text Encode** → Encoder prompt texte\n",
        "3. **Empty Latent Image** → Créer canvas vide\n",
        "4. **KSampler** → Générer image\n",
        "5. **VAE Decode** → Convertir latent → pixels\n",
        "6. **Save Image** → Sauvegarder résultat\n",
        "\n",
        "### Paramètres Critiques\n",
        "\n",
        "| Paramètre | Valeur | Impact |\n",
        "|-----------|--------|--------|\n",
        "| **steps** | 20 | Qualité (↑ steps = ↑ qualité, ↑ temps) |\n",
        "| **cfg** | 7.0 | Fidélité prompt (7-9 optimal) |\n",
        "| **sampler** | euler | Algorithme génération |\n",
        "| **scheduler** | normal | Stratégie steps |\n",
        "| **denoise** | 1.0 | Force génération (1.0 = 100%) |\n",
        "| **seed** | 42 | Reproductibilité |\n",
        "\n",
        "**Temps attendu**: 5-10 secondes (512x512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20b3bf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Workflow Text-to-Image minimal\n",
        "workflow_hello = {\n",
        "    \"1\": {  # Load Checkpoint\n",
        "        \"class_type\": \"CheckpointLoaderSimple\",\n",
        "        \"inputs\": {\n",
        "            \"ckpt_name\": \"qwen-image-edit-2509-fp8.safetensors\"\n",
        "        }\n",
        "    },\n",
        "    \"2\": {  # CLIP Text Encode (prompt positif)\n",
        "        \"class_type\": \"CLIPTextEncode\",\n",
        "        \"inputs\": {\n",
        "            \"text\": \"a majestic astronaut floating in space, photorealistic, 8k, detailed\",\n",
        "            \"clip\": [\"1\", 1]  # Connexion: output CLIP du checkpoint\n",
        "        }\n",
        "    },\n",
        "    \"3\": {  # CLIP Text Encode (prompt négatif)\n",
        "        \"class_type\": \"CLIPTextEncode\",\n",
        "        \"inputs\": {\n",
        "            \"text\": \"blurry, low quality, distorted\",\n",
        "            \"clip\": [\"1\", 1]\n",
        "        }\n",
        "    },\n",
        "    \"4\": {  # Empty Latent Image (canvas)\n",
        "        \"class_type\": \"EmptyLatentImage\",\n",
        "        \"inputs\": {\n",
        "            \"width\": 512,\n",
        "            \"height\": 512,\n",
        "            \"batch_size\": 1\n",
        "        }\n",
        "    },\n",
        "    \"5\": {  # KSampler (génération)\n",
        "        \"class_type\": \"KSampler\",\n",
        "        \"inputs\": {\n",
        "            \"model\": [\"1\", 0],  # Model du checkpoint\n",
        "            \"positive\": [\"2\", 0],  # Prompt positif\n",
        "            \"negative\": [\"3\", 0],  # Prompt négatif\n",
        "            \"latent_image\": [\"4\", 0],  # Canvas latent\n",
        "            \"seed\": 42,\n",
        "            \"steps\": 20,\n",
        "            \"cfg\": 7.0,\n",
        "            \"sampler_name\": \"euler\",\n",
        "            \"scheduler\": \"normal\",\n",
        "            \"denoise\": 1.0\n",
        "        }\n",
        "    },\n",
        "    \"6\": {  # VAE Decode\n",
        "        \"class_type\": \"VAEDecode\",\n",
        "        \"inputs\": {\n",
        "            \"samples\": [\"5\", 0],  # Latent du sampler\n",
        "            \"vae\": [\"1\", 2]  # VAE du checkpoint\n",
        "        }\n",
        "    },\n",
        "    \"7\": {  # Save Image\n",
        "        \"class_type\": \"SaveImage\",\n",
        "        \"inputs\": {\n",
        "            \"images\": [\"6\", 0],  # Pixels du VAE\n",
        "            \"filename_prefix\": \"ComfyUI\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Exécution\n",
        "print(\"🚀 Lancement génération...\")\n",
        "result = client.execute_workflow(workflow_hello, verbose=True)\n",
        "\n",
        "# Affichage\n",
        "client.display_images(result)\n",
        "print(f\"\\n✅ Image générée en {result['duration']:.1f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 🎯 WORKFLOW RÉEL 1: Édition Simple Image\n",
        "# ========================================\n",
        "\n",
        "def create_simple_edit_workflow(image_name: str, edit_prompt: str, denoise: float = 0.5) -> dict:\n",
        "    \"\"\"Workflow édition simple d'une image existante\n",
        "    \n",
        "    Args:\n",
        "        image_name: Nom du fichier image uploadé sur ComfyUI\n",
        "        edit_prompt: Description de l'édition souhaitée\n",
        "        denoise: Force de l'édition (0.0 = aucune, 1.0 = complète)\n",
        "    \n",
        "    Returns:\n",
        "        Workflow JSON prêt à exécuter\n",
        "    \"\"\"\n",
        "    workflow = {\n",
        "        \"1\": {\n",
        "            \"class_type\": \"CheckpointLoaderSimple\",\n",
        "            \"inputs\": {\"ckpt_name\": \"qwen_vl_model.safetensors\"}\n",
        "        },\n",
        "        \"2\": {\n",
        "            \"class_type\": \"LoadImage\",\n",
        "            \"inputs\": {\"image\": image_name}\n",
        "        },\n",
        "        \"3\": {\n",
        "            \"class_type\": \"CLIPTextEncode\",\n",
        "            \"inputs\": {\n",
        "                \"text\": edit_prompt,\n",
        "                \"clip\": [\"1\", 1]\n",
        "            }\n",
        "        },\n",
        "        \"4\": {\n",
        "            \"class_type\": \"VAEEncode\",\n",
        "            \"inputs\": {\n",
        "                \"pixels\": [\"2\", 0],\n",
        "                \"vae\": [\"1\", 2]\n",
        "            }\n",
        "        },\n",
        "        \"5\": {\n",
        "            \"class_type\": \"KSampler\",\n",
        "            \"inputs\": {\n",
        "                \"seed\": 42,\n",
        "                \"steps\": 20,\n",
        "                \"cfg\": 7.0,\n",
        "                \"sampler_name\": \"euler\",\n",
        "                \"scheduler\": \"normal\",\n",
        "                \"denoise\": denoise,\n",
        "                \"model\": [\"1\", 0],\n",
        "                \"positive\": [\"3\", 0],\n",
        "                \"negative\": [\"3\", 0],  # Pas de prompt négatif pour édition simple\n",
        "                \"latent_image\": [\"4\", 0]\n",
        "            }\n",
        "        },\n",
        "        \"6\": {\n",
        "            \"class_type\": \"VAEDecode\",\n",
        "            \"inputs\": {\n",
        "                \"samples\": [\"5\", 0],\n",
        "                \"vae\": [\"1\", 2]\n",
        "            }\n",
        "        },\n",
        "        \"7\": {\n",
        "            \"class_type\": \"SaveImage\",\n",
        "            \"inputs\": {\n",
        "                \"images\": [\"6\", 0],\n",
        "                \"filename_prefix\": \"qwen_edit_simple\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return workflow\n",
        "\n",
        "# ========================================\n",
        "# 🎯 WORKFLOW RÉEL 2: Chaînage Nodes Avancé\n",
        "# ========================================\n",
        "\n",
        "def create_chained_workflow(base_prompt: str, refine_prompt: str) -> dict:\n",
        "    \"\"\"Workflow avec chaînage: génération base + raffinement\n",
        "    \n",
        "    Architecture:\n",
        "        1. Génération image base (text-to-image)\n",
        "        2. Raffinement avec nouveau prompt (image-to-image)\n",
        "    \n",
        "    Args:\n",
        "        base_prompt: Prompt initial génération\n",
        "        refine_prompt: Prompt raffinement/amélioration\n",
        "    \n",
        "    Returns:\n",
        "        Workflow JSON avec 2 étapes KSampler\n",
        "    \"\"\"\n",
        "    workflow = {\n",
        "        # Étape 1: Génération base\n",
        "        \"1\": {\n",
        "            \"class_type\": \"CheckpointLoaderSimple\",\n",
        "            \"inputs\": {\"ckpt_name\": \"qwen_vl_model.safetensors\"}\n",
        "        },\n",
        "        \"2\": {\n",
        "            \"class_type\": \"CLIPTextEncode\",\n",
        "            \"inputs\": {\"text\": base_prompt, \"clip\": [\"1\", 1]}\n",
        "        },\n",
        "        \"3\": {\n",
        "            \"class_type\": \"EmptyLatentImage\",\n",
        "            \"inputs\": {\"width\": 512, \"height\": 512, \"batch_size\": 1}\n",
        "        },\n",
        "        \"4\": {\n",
        "            \"class_type\": \"KSampler\",\n",
        "            \"inputs\": {\n",
        "                \"seed\": 42,\n",
        "                \"steps\": 10,\n",
        "                \"cfg\": 7.0,\n",
        "                \"sampler_name\": \"euler\",\n",
        "                \"scheduler\": \"normal\",\n",
        "                \"denoise\": 1.0,  # Génération complète\n",
        "                \"model\": [\"1\", 0],\n",
        "                \"positive\": [\"2\", 0],\n",
        "                \"negative\": [\"2\", 0],\n",
        "                \"latent_image\": [\"3\", 0]\n",
        "            }\n",
        "        },\n",
        "        # Étape 2: Raffinement\n",
        "        \"5\": {\n",
        "            \"class_type\": \"CLIPTextEncode\",\n",
        "            \"inputs\": {\"text\": refine_prompt, \"clip\": [\"1\", 1]}\n",
        "        },\n",
        "        \"6\": {\n",
        "            \"class_type\": \"KSampler\",\n",
        "            \"inputs\": {\n",
        "                \"seed\": 43,\n",
        "                \"steps\": 10,\n",
        "                \"cfg\": 7.0,\n",
        "                \"sampler_name\": \"euler\",\n",
        "                \"scheduler\": \"normal\",\n",
        "                \"denoise\": 0.3,  # Raffinement léger\n",
        "                \"model\": [\"1\", 0],\n",
        "                \"positive\": [\"5\", 0],\n",
        "                \"negative\": [\"5\", 0],\n",
        "                \"latent_image\": [\"4\", 0]  # Sortie de l'étape 1\n",
        "            }\n",
        "        },\n",
        "        \"7\": {\n",
        "            \"class_type\": \"VAEDecode\",\n",
        "            \"inputs\": {\"samples\": [\"6\", 0], \"vae\": [\"1\", 2]}\n",
        "        },\n",
        "        \"8\": {\n",
        "            \"class_type\": \"SaveImage\",\n",
        "            \"inputs\": {\n",
        "                \"images\": [\"7\", 0],\n",
        "                \"filename_prefix\": \"qwen_chained\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return workflow\n",
        "\n",
        "# ========================================\n",
        "# EXEMPLE D'UTILISATION\n",
        "# ========================================\n",
        "\n",
        "# Workflow 1: Édition simple\n",
        "workflow_simple = create_simple_edit_workflow(\n",
        "    image_name=\"cat.png\",\n",
        "    edit_prompt=\"Add sunglasses to the cat\",\n",
        "    denoise=0.5\n",
        ")\n",
        "print(\"✅ Workflow édition simple créé\")\n",
        "print(f\"   Nodes: {len(workflow_simple)}\")\n",
        "\n",
        "# Workflow 2: Chaînage\n",
        "workflow_chained = create_chained_workflow(\n",
        "    base_prompt=\"A cat sitting on a chair\",\n",
        "    refine_prompt=\"High quality, professional photography, detailed fur\"\n",
        ")\n",
        "print(\"✅ Workflow chaîné créé\")\n",
        "print(f\"   Nodes: {len(workflow_chained)}\")\n",
        "print(f\"   KSamplers: 2 (base + raffinement)\")\n",
        "\n",
        "# 💡 Pour exécuter ces workflows:\n",
        "# client = ComfyUIClient(\"https://qwen-image-edit.myia.io\")\n",
        "# result = client.execute_workflow(workflow_simple)\n",
        "# images = client.get_results(result[\"prompt_id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75258e1",
      "metadata": {},
      "source": [
        "## 🖼️ Édition Images avec Qwen VLM\n",
        "\n",
        "### Capacités Qwen Vision-Language Model\n",
        "\n",
        "**Qwen-Image-Edit** combine:\n",
        "- 🧠 **Vision Encoder** (CLIP-like) pour comprendre images\n",
        "- ✍️ **Language Model** pour interpréter instructions texte\n",
        "- 🎨 **Diffusion Model** pour éditer images\n",
        "\n",
        "### Cas d'Usage Typiques\n",
        "\n",
        "| Tâche | Exemple Prompt |\n",
        "|-------|----------------|\n",
        "| **Style Transfer** | `\"Convert to watercolor painting\"` |\n",
        "| **Object Addition** | `\"Add a red balloon in the sky\"` |\n",
        "| **Color Grading** | `\"Make the image warmer, golden hour lighting\"` |\n",
        "| **Background Change** | `\"Replace background with snowy mountains\"` |\n",
        "| **Detail Enhancement** | `\"Enhance facial details, 8k quality\"` |\n",
        "\n",
        "### Pattern Image-to-Image\n",
        "\n",
        "**Différence clé avec Text-to-Image**:\n",
        "- **Nouveau node**: `LoadImage` pour charger image source\n",
        "- **Paramètre critique**: `denoise` (0.0-1.0)\n",
        "  - `denoise=0.1`: Édition subtile (retouche légère)\n",
        "  - `denoise=0.5`: Édition modérée (style transfer)\n",
        "  - `denoise=0.9`: Édition forte (reconstruction quasi-totale)\n",
        "\n",
        "**Workflow**:\n",
        "1. **Load Image** → Charger image source\n",
        "2. **CLIP Vision Encode** → Encoder image\n",
        "3. **CLIP Text Encode** → Encoder prompt édition\n",
        "4. **VAE Encode** → Convertir pixels → latent\n",
        "5. **KSampler** (denoise < 1.0) → Éditer latent\n",
        "6. **VAE Decode** → Convertir latent → pixels\n",
        "7. **Save Image** → Sauvegarder résultat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba896395",
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_image_to_comfyui(image_path: str) -> str:\n",
        "    \"\"\"Upload image vers ComfyUI pour édition\n",
        "    \n",
        "    Args:\n",
        "        image_path: Chemin image locale ou URL\n",
        "    \n",
        "    Returns:\n",
        "        str: Nom fichier uploadé dans ComfyUI\n",
        "    \"\"\"\n",
        "    # Charger image\n",
        "    if image_path.startswith('http'):\n",
        "        response = requests.get(image_path)\n",
        "        response.raise_for_status()\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "    else:\n",
        "        img = Image.open(image_path)\n",
        "    \n",
        "    # Convertir en bytes\n",
        "    img_bytes = BytesIO()\n",
        "    img.save(img_bytes, format='PNG')\n",
        "    img_bytes.seek(0)\n",
        "    \n",
        "    # Upload vers ComfyUI\n",
        "    files = {'image': ('input.png', img_bytes, 'image/png')}\n",
        "    response = client.session.post(\n",
        "        f\"{API_BASE_URL}/upload/image\",\n",
        "        files=files\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    filename = response.json()['name']\n",
        "    print(f\"✅ Image uploadée: {filename}\")\n",
        "    return filename\n",
        "\n",
        "# Test upload avec image exemple\n",
        "# Note: Remplacer par votre propre image\n",
        "test_image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/481px-Cat03.jpg\"\n",
        "\n",
        "try:\n",
        "    uploaded_filename = upload_image_to_comfyui(test_image_url)\n",
        "    print(f\"📁 Fichier: {uploaded_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Erreur upload: {e}\")\n",
        "    print(\"💡 Conseil: Utiliser une image locale ou vérifier URL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a6a3da",
      "metadata": {},
      "source": [
        "## 🎨 Workflow Image-to-Image Complet\n",
        "\n",
        "### Architecture\n",
        "\n",
        "**Pipeline édition**:\n",
        "1. **Load Image** → Charger image source uploadée\n",
        "2. **VAE Encode** → Convertir pixels → latent space\n",
        "3. **CLIP Text Encode** → Encoder instructions édition\n",
        "4. **KSampler** (denoise partiel) → Éditer latent\n",
        "5. **VAE Decode** → Reconvertir latent → pixels\n",
        "6. **Save Image** → Sauvegarder résultat\n",
        "\n",
        "### Paramètre Critique: denoise\n",
        "\n",
        "**Impact sur édition**:\n",
        "\n",
        "| denoise | Type Édition | Exemple |\n",
        "|---------|-------------|----------|\n",
        "| **0.1-0.3** | Retouche subtile | Correction couleurs, amélioration détails |\n",
        "| **0.4-0.6** | Édition modérée | Style transfer, ajout éléments mineurs |\n",
        "| **0.7-0.9** | Édition forte | Changement scène, reconstruction majeure |\n",
        "| **1.0** | Génération totale | Ignore quasi totalement image source |\n",
        "\n",
        "**Recommandation**: Commencer avec `denoise=0.5` et ajuster selon résultat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a747201c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Workflow Image-to-Image (édition)\n",
        "workflow_img2img = {\n",
        "    \"1\": {  # Load Checkpoint\n",
        "        \"class_type\": \"CheckpointLoaderSimple\",\n",
        "        \"inputs\": {\n",
        "            \"ckpt_name\": \"qwen-image-edit-2509-fp8.safetensors\"\n",
        "        }\n",
        "    },\n",
        "    \"2\": {  # Load Image (image source uploadée)\n",
        "        \"class_type\": \"LoadImage\",\n",
        "        \"inputs\": {\n",
        "            \"image\": uploaded_filename  # Variable de cellule précédente\n",
        "        }\n",
        "    },\n",
        "    \"3\": {  # VAE Encode (pixels → latent)\n",
        "        \"class_type\": \"VAEEncode\",\n",
        "        \"inputs\": {\n",
        "            \"pixels\": [\"2\", 0],  # Image source\n",
        "            \"vae\": [\"1\", 2]  # VAE du checkpoint\n",
        "        }\n",
        "    },\n",
        "    \"4\": {  # CLIP Text Encode (prompt édition)\n",
        "        \"class_type\": \"CLIPTextEncode\",\n",
        "        \"inputs\": {\n",
        "            \"text\": \"watercolor painting style, artistic, vibrant colors\",\n",
        "            \"clip\": [\"1\", 1]\n",
        "        }\n",
        "    },\n",
        "    \"5\": {  # CLIP Text Encode (prompt négatif)\n",
        "        \"class_type\": \"CLIPTextEncode\",\n",
        "        \"inputs\": {\n",
        "            \"text\": \"blurry, low quality, pixelated\",\n",
        "            \"clip\": [\"1\", 1]\n",
        "        }\n",
        "    },\n",
        "    \"6\": {  # KSampler (édition avec denoise partiel)\n",
        "        \"class_type\": \"KSampler\",\n",
        "        \"inputs\": {\n",
        "            \"model\": [\"1\", 0],\n",
        "            \"positive\": [\"4\", 0],\n",
        "            \"negative\": [\"5\", 0],\n",
        "            \"latent_image\": [\"3\", 0],  # Latent de l'image source\n",
        "            \"seed\": 42,\n",
        "            \"steps\": 25,\n",
        "            \"cfg\": 7.5,\n",
        "            \"sampler_name\": \"euler\",\n",
        "            \"scheduler\": \"normal\",\n",
        "            \"denoise\": 0.5  # Édition modérée (50%)\n",
        "        }\n",
        "    },\n",
        "    \"7\": {  # VAE Decode (latent → pixels)\n",
        "        \"class_type\": \"VAEDecode\",\n",
        "        \"inputs\": {\n",
        "            \"samples\": [\"6\", 0],\n",
        "            \"vae\": [\"1\", 2]\n",
        "        }\n",
        "    },\n",
        "    \"8\": {  # Save Image\n",
        "        \"class_type\": \"SaveImage\",\n",
        "        \"inputs\": {\n",
        "            \"images\": [\"7\", 0],\n",
        "            \"filename_prefix\": \"Qwen_Edit\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Exécution (seulement si image uploadée précédemment)\n",
        "try:\n",
        "    print(\"🎨 Lancement édition image...\")\n",
        "    result = client.execute_workflow(workflow_img2img, verbose=True)\n",
        "    \n",
        "    # Affichage avant/après\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # Image originale\n",
        "    original_img = Image.open(BytesIO(client.session.get(\n",
        "        f\"{API_BASE_URL}/view\",\n",
        "        params={\"filename\": uploaded_filename, \"type\": \"input\"}\n",
        "    ).content))\n",
        "    axes[0].imshow(original_img)\n",
        "    axes[0].set_title(\"Image Originale\", fontsize=12)\n",
        "    axes[0].axis(\"off\")\n",
        "    \n",
        "    # Image éditée\n",
        "    edited_img = Image.open(BytesIO(result[\"images\"][0][\"data\"]))\n",
        "    axes[1].imshow(edited_img)\n",
        "    axes[1].set_title(f\"Image Éditée (denoise=0.5)\", fontsize=12)\n",
        "    axes[1].axis(\"off\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n✅ Édition complète en {result['duration']:.1f}s\")\n",
        "except NameError:\n",
        "    print(\"⚠️ Exécutez d'abord la cellule d'upload d'image\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e1176e6",
      "metadata": {},
      "source": [
        "## 🔬 Expérimentation: Comparaison Denoise\n",
        "\n",
        "### Objectif Pédagogique\n",
        "\n",
        "Comprendre **impact du paramètre `denoise`** sur qualité édition.\n",
        "\n",
        "### Méthodologie\n",
        "\n",
        "1. Workflow identique (même prompt, seed, etc.)\n",
        "2. Variation **uniquement** du paramètre `denoise`\n",
        "3. Comparaison visuelle résultats\n",
        "\n",
        "### Hypothèse\n",
        "\n",
        "**denoise faible** (0.2) → Édition subtile, image proche de l'originale\n",
        "**denoise moyen** (0.5) → Bon compromis édition/préservation\n",
        "**denoise élevé** (0.8) → Édition forte, risque divergence\n",
        "\n",
        "### Configuration Test\n",
        "\n",
        "```python\n",
        "denoise_values = [0.2, 0.5, 0.8]\n",
        "prompt = \"convert to dramatic black and white, high contrast\"\n",
        "seed = 42  # Fixe pour comparabilité\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84553629",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparaison denoise (denoise=0.2, 0.5, 0.8)\n",
        "denoise_values = [0.2, 0.5, 0.8]\n",
        "edit_prompt = \"convert to dramatic black and white, high contrast\"\n",
        "\n",
        "results_denoise = []\n",
        "\n",
        "for denoise_val in denoise_values:\n",
        "    print(f\"\\n🔄 Test denoise={denoise_val}...\")\n",
        "    \n",
        "    # Créer workflow avec denoise spécifique\n",
        "    workflow_denoise_test = {\n",
        "        \"1\": {\n",
        "            \"class_type\": \"CheckpointLoaderSimple\",\n",
        "            \"inputs\": {\"ckpt_name\": \"qwen-image-edit-2509-fp8.safetensors\"}\n",
        "        },\n",
        "        \"2\": {\n",
        "            \"class_type\": \"LoadImage\",\n",
        "            \"inputs\": {\"image\": uploaded_filename}\n",
        "        },\n",
        "        \"3\": {\n",
        "            \"class_type\": \"VAEEncode\",\n",
        "            \"inputs\": {\n",
        "                \"pixels\": [\"2\", 0],\n",
        "                \"vae\": [\"1\", 2]\n",
        "            }\n",
        "        },\n",
        "        \"4\": {\n",
        "            \"class_type\": \"CLIPTextEncode\",\n",
        "            \"inputs\": {\n",
        "                \"text\": edit_prompt,\n",
        "                \"clip\": [\"1\", 1]\n",
        "            }\n",
        "        },\n",
        "        \"5\": {\n",
        "            \"class_type\": \"CLIPTextEncode\",\n",
        "            \"inputs\": {\n",
        "                \"text\": \"blurry, low quality\",\n",
        "                \"clip\": [\"1\", 1]\n",
        "            }\n",
        "        },\n",
        "        \"6\": {\n",
        "            \"class_type\": \"KSampler\",\n",
        "            \"inputs\": {\n",
        "                \"model\": [\"1\", 0],\n",
        "                \"positive\": [\"4\", 0],\n",
        "                \"negative\": [\"5\", 0],\n",
        "                \"latent_image\": [\"3\", 0],\n",
        "                \"seed\": 42,\n",
        "                \"steps\": 25,\n",
        "                \"cfg\": 7.5,\n",
        "                \"sampler_name\": \"euler\",\n",
        "                \"scheduler\": \"normal\",\n",
        "                \"denoise\": denoise_val  # Variable testée\n",
        "            }\n",
        "        },\n",
        "        \"7\": {\n",
        "            \"class_type\": \"VAEDecode\",\n",
        "            \"inputs\": {\n",
        "                \"samples\": [\"6\", 0],\n",
        "                \"vae\": [\"1\", 2]\n",
        "            }\n",
        "        },\n",
        "        \"8\": {\n",
        "            \"class_type\": \"SaveImage\",\n",
        "            \"inputs\": {\n",
        "                \"images\": [\"7\", 0],\n",
        "                \"filename_prefix\": f\"Denoise_{denoise_val}\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        result = client.execute_workflow(workflow_denoise_test, verbose=False)\n",
        "        results_denoise.append({\n",
        "            \"denoise\": denoise_val,\n",
        "            \"result\": result\n",
        "        })\n",
        "        print(f\"✅ Complété en {result['duration']:.1f}s\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur: {e}\")\n",
        "        results_denoise.append({\"denoise\": denoise_val, \"error\": str(e)})\n",
        "\n",
        "# Affichage comparatif\n",
        "if len(results_denoise) == 3 and all('result' in r for r in results_denoise):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    for i, res in enumerate(results_denoise):\n",
        "        img = Image.open(BytesIO(res[\"result\"][\"images\"][0][\"data\"]))\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f\"denoise={res['denoise']}\", fontsize=12)\n",
        "        axes[i].axis(\"off\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n📊 Observations:\")\n",
        "    print(\"- denoise=0.2: Édition subtile, préserve détails originaux\")\n",
        "    print(\"- denoise=0.5: Équilibre édition/préservation\")\n",
        "    print(\"- denoise=0.8: Édition forte, peut diverger de l'original\")\n",
        "else:\n",
        "    print(\"⚠️ Certains tests ont échoué, vérifiez les erreurs ci-dessus\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 🖼️ COMPARAISON AVANT/APRÈS: Side-by-Side\n",
        "# ========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "def compare_before_after(\n",
        "    original_path: str,\n",
        "    edited_path: str,\n",
        "    title_original: str = \"Image Originale\",\n",
        "    title_edited: str = \"Image Éditée\",\n",
        "    show_metrics: bool = True\n",
        ") -> None:\n",
        "    \"\"\"Affiche comparaison side-by-side avec métriques qualité\n",
        "    \n",
        "    Args:\n",
        "        original_path: Chemin image originale\n",
        "        edited_path: Chemin image éditée\n",
        "        title_original: Titre image originale\n",
        "        title_edited: Titre image éditée\n",
        "        show_metrics: Afficher métriques qualité (PSNR, SSIM)\n",
        "    \"\"\"\n",
        "    # Charger images\n",
        "    img_original = Image.open(original_path)\n",
        "    img_edited = Image.open(edited_path)\n",
        "    \n",
        "    # Convertir en numpy arrays\n",
        "    arr_original = np.array(img_original)\n",
        "    arr_edited = np.array(img_edited)\n",
        "    \n",
        "    # Créer figure avec 2 colonnes\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    \n",
        "    # Image originale\n",
        "    axes[0].imshow(arr_original)\n",
        "    axes[0].set_title(f\"{title_original}\\n{img_original.size[0]}x{img_original.size[1]}\", \n",
        "                      fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Image éditée\n",
        "    axes[1].imshow(arr_edited)\n",
        "    axes[1].set_title(f\"{title_edited}\\n{img_edited.size[0]}x{img_edited.size[1]}\", \n",
        "                      fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    # Métriques qualité\n",
        "    if show_metrics and arr_original.shape == arr_edited.shape:\n",
        "        # PSNR (Peak Signal-to-Noise Ratio)\n",
        "        mse = np.mean((arr_original - arr_edited) ** 2)\n",
        "        if mse == 0:\n",
        "            psnr = float('inf')\n",
        "        else:\n",
        "            max_pixel = 255.0\n",
        "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "        \n",
        "        # Différence absolue moyenne\n",
        "        mae = np.mean(np.abs(arr_original - arr_edited))\n",
        "        \n",
        "        # Afficher métriques\n",
        "        metrics_text = f\"📊 Métriques:\\n\"\n",
        "        metrics_text += f\"   PSNR: {psnr:.2f} dB\\n\"\n",
        "        metrics_text += f\"   MAE: {mae:.2f}\\n\"\n",
        "        metrics_text += f\"   Pixels modifiés: {np.sum(arr_original != arr_edited):,}\"\n",
        "        \n",
        "        fig.text(0.5, 0.02, metrics_text, ha='center', fontsize=12, \n",
        "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Interprétation PSNR\n",
        "    if show_metrics and arr_original.shape == arr_edited.shape:\n",
        "        print(\"\\n📈 Interprétation PSNR:\")\n",
        "        if psnr > 40:\n",
        "            print(\"   ✅ Excellente qualité (PSNR > 40 dB) - Changements subtils\")\n",
        "        elif psnr > 30:\n",
        "            print(\"   ✅ Bonne qualité (PSNR 30-40 dB) - Changements visibles mais contrôlés\")\n",
        "        elif psnr > 20:\n",
        "            print(\"   ⚠️  Qualité acceptable (PSNR 20-30 dB) - Changements significatifs\")\n",
        "        else:\n",
        "            print(\"   ⚠️  Qualité faible (PSNR < 20 dB) - Changements majeurs\")\n",
        "\n",
        "def create_difference_map(\n",
        "    original_path: str,\n",
        "    edited_path: str,\n",
        "    amplification: float = 5.0\n",
        ") -> None:\n",
        "    \"\"\"Crée une carte visuelle des différences entre 2 images\n",
        "    \n",
        "    Args:\n",
        "        original_path: Chemin image originale\n",
        "        edited_path: Chemin image éditée\n",
        "        amplification: Facteur amplification différences pour visibilité\n",
        "    \"\"\"\n",
        "    img_original = np.array(Image.open(original_path))\n",
        "    img_edited = np.array(Image.open(edited_path))\n",
        "    \n",
        "    if img_original.shape != img_edited.shape:\n",
        "        print(\"❌ Images de tailles différentes, impossible de comparer\")\n",
        "        return\n",
        "    \n",
        "    # Calculer différence absolue\n",
        "    diff = np.abs(img_original.astype(float) - img_edited.astype(float))\n",
        "    \n",
        "    # Amplifier pour visibilité\n",
        "    diff_amplified = np.clip(diff * amplification, 0, 255).astype(np.uint8)\n",
        "    \n",
        "    # Afficher\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "    \n",
        "    axes[0].imshow(img_original)\n",
        "    axes[0].set_title(\"Originale\", fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    axes[1].imshow(img_edited)\n",
        "    axes[1].set_title(\"Éditée\", fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    axes[2].imshow(diff_amplified)\n",
        "    axes[2].set_title(f\"Carte Différences (×{amplification})\", fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Statistiques différences\n",
        "    print(f\"\\n📊 Statistiques différences:\")\n",
        "    print(f\"   Moyenne: {np.mean(diff):.2f}\")\n",
        "    print(f\"   Max: {np.max(diff):.2f}\")\n",
        "    print(f\"   Pixels modifiés (>5): {np.sum(diff > 5):,} ({100*np.sum(diff > 5)/diff.size:.2f}%)\")\n",
        "\n",
        "# ========================================\n",
        "# EXEMPLE D'UTILISATION\n",
        "# ========================================\n",
        "\n",
        "# Cas d'usage: Comparer original vs édition Qwen\n",
        "# compare_before_after(\n",
        "#     original_path=\"cat_original.png\",\n",
        "#     edited_path=\"cat_with_sunglasses.png\",\n",
        "#     title_original=\"Chat Original\",\n",
        "#     title_edited=\"Chat avec Lunettes (Qwen Edit)\",\n",
        "#     show_metrics=True\n",
        "# )\n",
        "\n",
        "# Cas d'usage: Carte différences pour analyse détaillée\n",
        "# create_difference_map(\n",
        "#     original_path=\"cat_original.png\",\n",
        "#     edited_path=\"cat_with_sunglasses.png\",\n",
        "#     amplification=10.0\n",
        "# )\n",
        "\n",
        "print(\"✅ Fonctions comparaison avant/après définies\")\n",
        "print(\"   - compare_before_after(): Affichage side-by-side + métriques\")\n",
        "print(\"   - create_difference_map(): Carte visuelle différences\")\n",
        "print(\"\\n💡 Décommentez les exemples ci-dessus pour tester avec vos images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6db2fc7",
      "metadata": {},
      "source": [
        "## ⚙️ Bonnes Pratiques ComfyUI\n",
        "\n",
        "### 1. Gestion des Erreurs Courantes\n",
        "\n",
        "#### Timeout (>120s)\n",
        "**Cause**: GPU surchargé, workflow trop complexe\n",
        "**Solution**:\n",
        "```python\n",
        "client.execute_workflow(workflow, max_wait=300)  # Augmenter timeout\n",
        "```\n",
        "\n",
        "#### CUDA Out of Memory\n",
        "**Cause**: Résolution trop élevée (>768x768)\n",
        "**Solution**:\n",
        "- Réduire résolution (512x512 optimal)\n",
        "- Diminuer `batch_size`\n",
        "- Simplifier workflow\n",
        "\n",
        "#### Node Not Found\n",
        "**Cause**: `class_type` invalide ou custom node manquant\n",
        "**Solution**: Vérifier documentation custom nodes Qwen\n",
        "\n",
        "### 2. Optimisation Performance\n",
        "\n",
        "| Paramètre | Impact Performance | Recommandation |\n",
        "|-----------|-------------------|----------------|\n",
        "| **steps** | ↑ steps = ↑ temps | 20-25 optimal |\n",
        "| **resolution** | ↑ résolution = ↑↑ VRAM | 512x512 par défaut |\n",
        "| **denoise** | Minimal | N/A |\n",
        "| **cfg** | Minimal | 7-8 optimal |\n",
        "\n",
        "### 3. Workflow Reproductible\n",
        "\n",
        "**Toujours fixer seed pour debugging**:\n",
        "```python\n",
        "\"seed\": 42  # Même résultat à chaque exécution\n",
        "```\n",
        "\n",
        "**Seed aléatoire pour variété**:\n",
        "```python\n",
        "\"seed\": int(time.time())  # Différent à chaque fois\n",
        "```\n",
        "\n",
        "### 4. Logs et Debugging\n",
        "\n",
        "**Activer verbose**:\n",
        "```python\n",
        "result = client.execute_workflow(workflow, verbose=True)\n",
        "```\n",
        "\n",
        "**Inspecter outputs**:\n",
        "```python\n",
        "print(json.dumps(result[\"outputs\"], indent=2))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7af4b97",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 🎯 EXERCICE PRATIQUE\n",
        "# ========================================\n",
        "# Créez votre propre workflow d'édition d'image\n",
        "\n",
        "# OBJECTIF:\n",
        "# Modifier une image en ajoutant un effet spécifique via Qwen VLM\n",
        "\n",
        "# INSTRUCTIONS:\n",
        "# 1. Choisissez une image de test (ou utilisez celle générée précédemment)\n",
        "# 2. Créez un workflow image-to-image avec un prompt créatif\n",
        "# 3. Testez différentes valeurs de denoise (0.3, 0.5, 0.7)\n",
        "# 4. Comparez visuellement les résultats\n",
        "\n",
        "# TODO: Complétez le workflow ci-dessous\n",
        "workflow_exercice = {\n",
        "    \"5\": {\n",
        "        \"class_type\": \"CheckpointLoaderSimple\",\n",
        "        \"inputs\": {\n",
        "            \"ckpt_name\": \"qwen2vl.safetensors\"  # Modèle Qwen\n",
        "        }\n",
        "    },\n",
        "    \"10\": {\n",
        "        \"class_type\": \"LoadImage\",\n",
        "        \"inputs\": {\n",
        "            # TODO: Ajoutez le chemin de votre image source\n",
        "            \"image\": \"___VOTRE_IMAGE_ICI___\"\n",
        "        }\n",
        "    },\n",
        "    \"6\": {\n",
        "        \"class_type\": \"CLIPTextEncode\",\n",
        "        \"inputs\": {\n",
        "            # TODO: Écrivez un prompt créatif (ex: \"convert to watercolor painting\")\n",
        "            \"text\": \"___VOTRE_PROMPT_ICI___\",\n",
        "            \"clip\": [\"5\", 1]\n",
        "        }\n",
        "    },\n",
        "    \"3\": {\n",
        "        \"class_type\": \"KSampler\",\n",
        "        \"inputs\": {\n",
        "            \"seed\": 42,\n",
        "            # TODO: Testez différentes valeurs de denoise (0.3 à 0.7)\n",
        "            \"denoise\": 0.5,\n",
        "            \"steps\": 20,\n",
        "            \"cfg\": 7.5,\n",
        "            \"sampler_name\": \"euler\",\n",
        "            \"scheduler\": \"normal\",\n",
        "            \"model\": [\"5\", 0],\n",
        "            \"positive\": [\"6\", 0],\n",
        "            \"negative\": [\"7\", 0],\n",
        "            \"latent_image\": [\"13\", 0]\n",
        "        }\n",
        "    },\n",
        "    \"7\": {\n",
        "        \"class_type\": \"CLIPTextEncode\",\n",
        "        \"inputs\": {\n",
        "            \"text\": \"blurry, low quality, distorted\",\n",
        "            \"clip\": [\"5\", 1]\n",
        "        }\n",
        "    },\n",
        "    \"13\": {\n",
        "        \"class_type\": \"VAEEncode\",\n",
        "        \"inputs\": {\n",
        "            \"pixels\": [\"10\", 0],\n",
        "            \"vae\": [\"5\", 2]\n",
        "        }\n",
        "    },\n",
        "    \"8\": {\n",
        "        \"class_type\": \"VAEDecode\",\n",
        "        \"inputs\": {\n",
        "            \"samples\": [\"3\", 0],\n",
        "            \"vae\": [\"5\", 2]\n",
        "        }\n",
        "    },\n",
        "    \"9\": {\n",
        "        \"class_type\": \"SaveImage\",\n",
        "        \"inputs\": {\n",
        "            \"filename_prefix\": \"exercice_qwen\",\n",
        "            \"images\": [\"8\", 0]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# AIDE:\n",
        "# - Pour l'image source: utilisez le nom d'une image uploadée (ex: \"cat_512x512.png\")\n",
        "# - Pour le prompt: soyez créatif! (ex: \"make it look like a Van Gogh painting\")\n",
        "# - Pour denoise: commencez par 0.5, puis testez 0.3 et 0.7\n",
        "\n",
        "# BONUS:\n",
        "# Créez une fonction pour tester plusieurs prompts automatiquement\n",
        "def tester_prompts_exercice(prompts_list, image_source, denoise=0.5):\n",
        "    \"\"\"\n",
        "    Teste plusieurs prompts sur la même image\n",
        "    \n",
        "    Args:\n",
        "        prompts_list: Liste de prompts à tester\n",
        "        image_source: Nom de l'image source\n",
        "        denoise: Valeur denoise (défaut 0.5)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, prompt in enumerate(prompts_list):\n",
        "        print(f\"\\n🎨 Test {i+1}/{len(prompts_list)}: {prompt}\")\n",
        "        \n",
        "        # TODO: Modifiez le workflow pour chaque prompt\n",
        "        workflow_test = workflow_exercice.copy()\n",
        "        # ... (ajoutez votre code ici)\n",
        "        \n",
        "        # Exécution workflow\n",
        "        # result = client.execute_workflow(workflow_test)\n",
        "        # results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# TESTEZ VOTRE CODE:\n",
        "# prompts_test = [\n",
        "#     \"convert to watercolor painting\",\n",
        "#     \"add dramatic sunset lighting\",\n",
        "#     \"make it look like a pencil sketch\"\n",
        "# ]\n",
        "# resultats = tester_prompts_exercice(prompts_test, \"cat_512x512.png\")\n",
        "\n",
        "print(\"\\n✅ Exercice prêt! Complétez les TODO et exécutez la cellule.\")\n",
        "print(\"💡 Conseil: Commencez simple, puis ajoutez de la complexité progressivement.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a694ae8",
      "metadata": {},
      "source": [
        "## 📚 Ressources Complémentaires\n",
        "\n",
        "### Documentation Officielle\n",
        "\n",
        "#### ComfyUI\n",
        "- **GitHub**: [https://github.com/comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)\n",
        "- **Documentation API**: [https://github.com/comfyanonymous/ComfyUI/wiki/API](https://github.com/comfyanonymous/ComfyUI/wiki/API)\n",
        "- **Custom Nodes**: [https://github.com/ltdrdata/ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager)\n",
        "\n",
        "#### Qwen Vision-Language Model\n",
        "- **Paper Officiel**: *Qwen-VL: A Versatile Vision-Language Model*\n",
        "- **Modèle Hugging Face**: [https://huggingface.co/Qwen/Qwen-VL](https://huggingface.co/Qwen/Qwen-VL)\n",
        "- **Documentation Technique**: [https://qwenlm.github.io/](https://qwenlm.github.io/)\n",
        "\n",
        "### Workflows Avancés\n",
        "\n",
        "#### Workflows ComfyUI Communautaires\n",
        "- **ComfyUI Workflows Gallery**: [https://comfyworkflows.com/](https://comfyworkflows.com/)\n",
        "- **CivitAI ComfyUI Section**: [https://civitai.com/tag/comfyui](https://civitai.com/tag/comfyui)\n",
        "\n",
        "#### Custom Nodes Recommandés\n",
        "- **ComfyUI-Impact-Pack**: Outils post-processing avancés\n",
        "- **ComfyUI-AnimateDiff**: Animations et vidéos\n",
        "- **ComfyUI-ControlNet**: Contrôle spatial précis\n",
        "\n",
        "### Tutoriels et Guides\n",
        "\n",
        "#### Débutants\n",
        "1. **ComfyUI Basics** (YouTube): Introduction complète workflows\n",
        "2. **Qwen-VL Quick Start**: Guide rapide édition images\n",
        "3. **JSON Workflows 101**: Comprendre structure workflows\n",
        "\n",
        "#### Intermédiaires\n",
        "1. **Advanced Prompting Techniques**: Optimisation prompts Qwen\n",
        "2. **Workflow Optimization**: Réduire temps génération\n",
        "3. **Multi-Step Workflows**: Chaînage nodes complexes\n",
        "\n",
        "#### Avancés\n",
        "1. **Custom Node Development**: Créer vos propres nodes\n",
        "2. **API Integration**: Intégrer ComfyUI dans applications\n",
        "3. **Batch Processing**: Automatisation workflows\n",
        "\n",
        "### Communauté et Support\n",
        "\n",
        "- **Discord ComfyUI**: [https://discord.gg/comfyui](https://discord.gg/comfyui)\n",
        "- **Reddit r/comfyui**: Forum communautaire\n",
        "- **GitHub Discussions**: Questions techniques\n",
        "\n",
        "### Ressources MyIA.io\n",
        "\n",
        "- **Guide APIs Étudiants**: [`GUIDE-APIS-ETUDIANTS.md`](../../../../docs/suivis/genai-image/GUIDE-APIS-ETUDIANTS.md)\n",
        "- **Workflows Qwen Phase 12C**: [`2025-10-16_12C_architectures-5-workflows-qwen.md`](../../../../docs/genai-suivis/2025-10-16_12C_architectures-5-workflows-qwen.md)\n",
        "- **Notebook Forge SD-XL**: [`01-4-Forge-SD-XL-Turbo.ipynb`](01-4-Forge-SD-XL-Turbo.ipynb) (API REST similaire)\n",
        "\n",
        "---\n",
        "\n",
        "### 🎓 Prochaines Étapes Apprentissage\n",
        "\n",
        "1. **Maîtriser les bases**: Reproduire tous les exemples de ce notebook\n",
        "2. **Expérimenter**: Modifier workflows, tester nouveaux prompts\n",
        "3. **Explorer workflows avancés**: ControlNet, AnimateDiff, Multi-Model\n",
        "4. **Créer projets personnels**: Application web intégrant API Qwen\n",
        "5. **Contribuer communauté**: Partager vos workflows innovants\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Notebook terminé! Bon apprentissage avec Qwen Image Edit! 🚀**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}