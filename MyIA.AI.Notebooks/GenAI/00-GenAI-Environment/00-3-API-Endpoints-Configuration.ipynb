{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "genai": {
      "enabled": true,
      "level": "environment",
      "module": "00-GenAI-Environment",
      "dependencies": [
        "requests",
        "python-dotenv",
        "matplotlib",
        "pandas",
        "time"
      ],
      "estimated_duration_minutes": 20,
      "difficulty": "beginner",
      "learning_outcomes": [
        "Configurer et tester les APIs OpenRouter et OpenAI",
        "Valider les clés API et permissions",
        "Effectuer des benchmarks de performance",
        "Diagnostiquer les problèmes de connectivité",
        "Monitorer l'utilisation des APIs"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# 🔗 Configuration des API Endpoints\n\n**Module :** 00-GenAI-Environment  \n**Niveau :** 🟢 Débutant  \n**Technologies :** OpenRouter, OpenAI API, Monitoring  \n**Durée estimée :** 20 minutes  \n\n## 🎯 Objectifs d'Apprentissage\n\n- [ ] Configurer et tester les connexions API\n- [ ] Valider les clés API et permissions\n- [ ] Effectuer des benchmarks de performance\n- [ ] Diagnostiquer les problèmes courants\n- [ ] Monitorer l'utilisation et les coûts\n- [ ] Optimiser les paramètres de connexion\n\n## 📚 APIs Configurées\n\n### 🚀 OpenRouter API\n- **DALL-E 3** : `openai/dall-e-3`\n- **GPT-5** : `openai/gpt-5` \n- **Qwen-Image-Edit-2509** : `qwen/qwen-image-edit-2509`\n- **FLUX-1** : Modèles de génération avancée\n\n### 🤖 OpenAI Direct API\n- **DALL-E 3** : Accès direct OpenAI\n- **GPT-4 Vision** : Analyse d'images\n- **Whisper** : Transcription audio\n\n## ⚙️ Prérequis\n\n- Fichier `.env` configuré avec les clés API\n- Accès internet stable\n- Python 3.11+ avec packages requis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Paramètres Papermill - JAMAIS modifier ce commentaire\n\n# Configuration des tests\nnotebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\nskip_widgets = False               # True pour mode batch MCP\ndebug_level = \"INFO\"               \n\n# Tests à effectuer\ntest_openrouter = True             # Tester OpenRouter API\ntest_openai_direct = True          # Tester OpenAI API directe\nrun_benchmarks = True              # Benchmarks de performance\ncheck_rate_limits = True           # Vérifier les limites de taux\nmonitor_costs = True               # Monitoring des coûts\n\n# Configuration benchmarks\nbenchmark_iterations = 3           # Nombre de tests par endpoint\nbenchmark_timeout = 30             # Timeout par test (secondes)\nmodel_list_limit = 10              # Limite modèles à tester\n\n# Paramètres monitoring\nexport_results = True              # Exporter résultats JSON\ngenerate_report = True             # Générer rapport HTML\nshow_detailed_logs = False         # Logs détaillés (verbose)",
      "metadata": {
        "tags": ["parameters"]
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Setup environnement et imports\nimport os\nimport sys\nimport json\nimport requests\nimport time\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Optional\nimport logging\nfrom urllib.parse import urljoin\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom dotenv import load_dotenv\n\n# Import helpers GenAI\nGENAI_ROOT = Path.cwd()\nwhile GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n    GENAI_ROOT = GENAI_ROOT.parent\n\nHELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\nif HELPERS_PATH.exists():\n    sys.path.insert(0, str(HELPERS_PATH.parent))\n    try:\n        from helpers.genai_helpers import setup_genai_logging, load_genai_config\n        print(\"✅ Helpers GenAI importés\")\n    except ImportError:\n        print(\"⚠️  Helpers GenAI non disponibles - mode autonome\")\n\n# Configuration logging\nlogging.basicConfig(level=getattr(logging, debug_level))\nlogger = logging.getLogger('api_configuration')\n\n# Chargement des variables d'environnement\nenv_file = GENAI_ROOT / '.env'\nif env_file.exists():\n    load_dotenv(env_file)\n    print(f\"✅ Variables d'environnement chargées depuis {env_file}\")\nelse:\n    print(f\"⚠️  Fichier .env non trouvé: {env_file}\")\n    print(\"💡 Utilisez .env.template comme base\")\n\nprint(f\"\\n🔗 Configuration des API Endpoints\")\nprint(f\"📅 {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"🔧 Mode: {notebook_mode}, Benchmarks: {run_benchmarks}\")\nprint(f\"🏷️  Tests: OpenRouter={test_openrouter}, OpenAI={test_openai_direct}\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Configuration et validation des clés API\nprint(\"\\n🔑 VALIDATION DES CLÉS API\")\nprint(\"=\" * 35)\n\n# Dictionnaire de configuration des APIs\napi_config = {\n    \"openrouter\": {\n        \"name\": \"OpenRouter\",\n        \"base_url\": \"https://openrouter.ai/api/v1\",\n        \"key_env\": \"OPENROUTER_API_KEY\",\n        \"key_value\": None,\n        \"headers\": {},\n        \"endpoints\": {\n            \"models\": \"/models\",\n            \"chat\": \"/chat/completions\",\n            \"images\": \"/images/generations\"\n        },\n        \"test_model\": \"openai/gpt-4o-mini\",\n        \"status\": \"unknown\"\n    },\n    \n    \"openai\": {\n        \"name\": \"OpenAI Direct\",\n        \"base_url\": \"https://api.openai.com/v1\",\n        \"key_env\": \"OPENAI_API_KEY\",\n        \"key_value\": None,\n        \"headers\": {},\n        \"endpoints\": {\n            \"models\": \"/models\",\n            \"chat\": \"/chat/completions\",\n            \"images\": \"/images/generations\"\n        },\n        \"test_model\": \"gpt-4o-mini\",\n        \"status\": \"unknown\"\n    }\n}\n\n# Validation et configuration des clés\nfor api_name, config in api_config.items():\n    print(f\"\\n🔍 {config['name']}\")\n    print(\"-\" * 25)\n    \n    # Récupération de la clé API\n    api_key = os.getenv(config['key_env'])\n    \n    if api_key:\n        config['key_value'] = api_key\n        \n        # Configuration headers\n        if api_name == \"openrouter\":\n            config['headers'] = {\n                \"Authorization\": f\"Bearer {api_key}\",\n                \"HTTP-Referer\": \"https://coursia.myia.io\",\n                \"X-Title\": \"CoursIA GenAI Configuration\",\n                \"Content-Type\": \"application/json\"\n            }\n        else:  # OpenAI\n            config['headers'] = {\n                \"Authorization\": f\"Bearer {api_key}\",\n                \"Content-Type\": \"application/json\"\n            }\n        \n        # Masquage partiel de la clé pour l'affichage\n        masked_key = f\"{api_key[:8]}...{api_key[-4:]}\" if len(api_key) > 12 else \"***\"\n        print(f\"✅ Clé API: {masked_key}\")\n        print(f\"📍 URL base: {config['base_url']}\")\n        print(f\"🎯 Modèle test: {config['test_model']}\")\n        config['status'] = \"configured\"\n    else:\n        print(f\"❌ Variable manquante: {config['key_env']}\")\n        print(f\"💡 Ajoutez {config['key_env']}=your_key_here dans .env\")\n        config['status'] = \"missing\"\n\n# Résumé de la configuration\nprint(f\"\\n📊 RÉSUMÉ CONFIGURATION\")\nprint(\"=\" * 30)\nconfigured_apis = sum(1 for c in api_config.values() if c['status'] == 'configured')\ntotal_apis = len(api_config)\nprint(f\"🎯 APIs configurées: {configured_apis}/{total_apis}\")\n\nif configured_apis == 0:\n    print(\"⚠️  Aucune API configurée - vérifiez votre fichier .env\")\n    print(\"📝 Utilisez .env.template comme référence\")\nelif configured_apis < total_apis:\n    print(\"⚠️  Configuration partielle - certains tests seront ignorés\")\nelse:\n    print(\"✅ Toutes les APIs sont configurées - tests complets possibles\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Tests de connectivité des APIs\nprint(\"\\n🌐 TESTS DE CONNECTIVITÉ\")\nprint(\"=\" * 32)\n\n# Fonction de test de connectivité\ndef test_api_connectivity(api_name: str, config: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Teste la connectivité d'une API.\n    \n    Args:\n        api_name: Nom de l'API\n        config: Configuration de l'API\n        \n    Returns:\n        Dict avec résultats du test\n    \"\"\"\n    result = {\n        \"api\": api_name,\n        \"name\": config['name'],\n        \"success\": False,\n        \"response_time\": None,\n        \"status_code\": None,\n        \"models_count\": 0,\n        \"error\": None,\n        \"timestamp\": datetime.now().isoformat()\n    }\n    \n    if config['status'] != 'configured':\n        result['error'] = \"API non configurée\"\n        return result\n    \n    try:\n        print(f\"\\n🔍 Test {config['name']}...\")\n        \n        # Test endpoint /models\n        start_time = time.time()\n        \n        response = requests.get(\n            urljoin(config['base_url'], config['endpoints']['models']),\n            headers=config['headers'],\n            timeout=benchmark_timeout\n        )\n        \n        end_time = time.time()\n        result['response_time'] = round(end_time - start_time, 3)\n        result['status_code'] = response.status_code\n        \n        if response.status_code == 200:\n            data = response.json()\n            \n            if api_name == \"openrouter\":\n                models = data.get('data', [])\n                result['models_count'] = len(models)\n                \n                # Filtrage des modèles GenAI Images\n                genai_models = [\n                    m for m in models \n                    if any(keyword in m.get('id', '').lower() \n                          for keyword in ['dall-e', 'gpt-5', 'gpt-4', 'flux', 'qwen'])\n                ]\n                \n                print(f\"✅ Connexion réussie - {result['response_time']}s\")\n                print(f\"📊 Modèles totaux: {result['models_count']}\")\n                print(f\"🎨 Modèles GenAI Images: {len(genai_models)}\")\n                \n                # Affichage des modèles GenAI principaux\n                if genai_models:\n                    print(f\"\\n🎯 Modèles prioritaires détectés:\")\n                    for model in genai_models[:5]:\n                        context = model.get('context_length', 'N/A')\n                        pricing = model.get('pricing', {})\n                        prompt_price = pricing.get('prompt', 'N/A')\n                        print(f\"  • {model['id']} - Contexte: {context}, Prix: ${prompt_price}\")\n            \n            else:  # OpenAI\n                models = data.get('data', [])\n                result['models_count'] = len(models)\n                \n                print(f\"✅ Connexion réussie - {result['response_time']}s\")\n                print(f\"📊 Modèles disponibles: {result['models_count']}\")\n                \n                # Affichage des modèles principaux\n                key_models = [m for m in models if m.get('id') in ['gpt-4o', 'gpt-4o-mini', 'dall-e-3']]\n                if key_models:\n                    print(f\"\\n🎯 Modèles clés détectés:\")\n                    for model in key_models:\n                        print(f\"  • {model['id']} - Propriétaire: {model.get('owned_by', 'N/A')}\")\n            \n            result['success'] = True\n            \n        else:\n            error_msg = f\"HTTP {response.status_code}\"\n            try:\n                error_data = response.json()\n                error_msg += f\": {error_data.get('error', {}).get('message', 'Erreur inconnue')}\"\n            except:\n                pass\n            \n            result['error'] = error_msg\n            print(f\"❌ Échec: {error_msg}\")\n            \n    except requests.exceptions.Timeout:\n        result['error'] = f\"Timeout après {benchmark_timeout}s\"\n        print(f\"⏱️  Timeout: {result['error']}\")\n    except requests.exceptions.ConnectionError:\n        result['error'] = \"Erreur de connexion réseau\"\n        print(f\"🌐 Réseau: {result['error']}\")\n    except Exception as e:\n        result['error'] = str(e)[:100]\n        print(f\"❌ Erreur: {result['error']}\")\n    \n    return result\n\n# Exécution des tests de connectivité\nconnectivity_results = []\n\nfor api_name, config in api_config.items():\n    if (api_name == \"openrouter\" and test_openrouter) or \\\n       (api_name == \"openai\" and test_openai_direct):\n        result = test_api_connectivity(api_name, config)\n        connectivity_results.append(result)\n    else:\n        print(f\"\\n⏭️  {config['name']} - Test désactivé\")\n\n# Résumé des tests de connectivité\nprint(f\"\\n📈 RÉSUMÉ CONNECTIVITÉ\")\nprint(\"=\" * 28)\nsuccessful_tests = sum(1 for r in connectivity_results if r['success'])\ntotal_tests = len(connectivity_results)\n\nprint(f\"🎯 Tests réussis: {successful_tests}/{total_tests}\")\n\nif successful_tests > 0:\n    avg_response_time = sum(r['response_time'] for r in connectivity_results if r['success']) / successful_tests\n    total_models = sum(r['models_count'] for r in connectivity_results if r['success'])\n    \n    print(f\"⚡ Temps moyen: {avg_response_time:.3f}s\")\n    print(f\"🔢 Modèles totaux: {total_models}\")\nelse:\n    print(\"❌ Aucune connexion réussie\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Benchmarks de performance des APIs\nif run_benchmarks and connectivity_results:\n    print(\"\\n⚡ BENCHMARKS DE PERFORMANCE\")\n    print(\"=\" * 35)\n    \n    # Fonction de benchmark\n    def benchmark_api_endpoint(api_name: str, config: Dict[str, Any], \n                              endpoint: str, iterations: int = 3) -> Dict[str, Any]:\n        \"\"\"\n        Benchmark d'un endpoint API.\n        \n        Args:\n            api_name: Nom de l'API\n            config: Configuration de l'API\n            endpoint: Endpoint à tester\n            iterations: Nombre d'itérations\n            \n        Returns:\n            Dict avec métriques de performance\n        \"\"\"\n        \n        if config['status'] != 'configured':\n            return {\"error\": \"API non configurée\"}\n        \n        response_times = []\n        errors = 0\n        \n        print(f\"\\n🏃 Benchmark {config['name']} - {endpoint}\")\n        print(f\"🔄 {iterations} itérations...\")\n        \n        for i in range(iterations):\n            try:\n                start_time = time.time()\n                \n                response = requests.get(\n                    urljoin(config['base_url'], config['endpoints'][endpoint]),\n                    headers=config['headers'],\n                    timeout=benchmark_timeout\n                )\n                \n                end_time = time.time()\n                response_time = end_time - start_time\n                \n                if response.status_code == 200:\n                    response_times.append(response_time)\n                    print(f\"  ✅ Itération {i+1}: {response_time:.3f}s\")\n                else:\n                    errors += 1\n                    print(f\"  ❌ Itération {i+1}: HTTP {response.status_code}\")\n                \n                # Délai entre les requêtes pour éviter le rate limiting\n                if i < iterations - 1:\n                    time.sleep(0.5)\n                    \n            except Exception as e:\n                errors += 1\n                print(f\"  ❌ Itération {i+1}: {str(e)[:50]}...\")\n        \n        if response_times:\n            return {\n                \"api\": api_name,\n                \"endpoint\": endpoint,\n                \"success_rate\": len(response_times) / iterations,\n                \"avg_response_time\": sum(response_times) / len(response_times),\n                \"min_response_time\": min(response_times),\n                \"max_response_time\": max(response_times),\n                \"total_requests\": iterations,\n                \"successful_requests\": len(response_times),\n                \"errors\": errors,\n                \"timestamp\": datetime.now().isoformat()\n            }\n        else:\n            return {\n                \"api\": api_name,\n                \"endpoint\": endpoint,\n                \"error\": \"Aucune requête réussie\",\n                \"errors\": errors,\n                \"total_requests\": iterations\n            }\n    \n    # Exécution des benchmarks\n    benchmark_results = []\n    \n    for result in connectivity_results:\n        if result['success']:\n            api_name = result['api']\n            config = api_config[api_name]\n            \n            # Test de l'endpoint /models (le plus stable)\n            bench_result = benchmark_api_endpoint(\n                api_name, config, \"models\", benchmark_iterations\n            )\n            benchmark_results.append(bench_result)\n    \n    # Analyse des résultats de benchmark\n    if benchmark_results:\n        print(f\"\\n📊 ANALYSE DES BENCHMARKS\")\n        print(\"=\" * 30)\n        \n        # Tableau comparatif\n        print(f\"\\n{'API':<15} {'Succès':<8} {'Temps Moy':<12} {'Min':<8} {'Max':<8} {'Erreurs':<8}\")\n        print(\"-\" * 65)\n        \n        for bench in benchmark_results:\n            if 'error' not in bench:\n                api = bench['api'].title()\n                success_pct = f\"{bench['success_rate']*100:.1f}%\"\n                avg_time = f\"{bench['avg_response_time']:.3f}s\"\n                min_time = f\"{bench['min_response_time']:.3f}s\"\n                max_time = f\"{bench['max_response_time']:.3f}s\"\n                errors = str(bench['errors'])\n                \n                print(f\"{api:<15} {success_pct:<8} {avg_time:<12} {min_time:<8} {max_time:<8} {errors:<8}\")\n        \n        # Recommandations de performance\n        print(f\"\\n💡 RECOMMANDATIONS\")\n        print(\"=\" * 22)\n        \n        best_apis = [b for b in benchmark_results if 'error' not in b and b['success_rate'] > 0.8]\n        \n        if best_apis:\n            fastest_api = min(best_apis, key=lambda x: x['avg_response_time'])\n            most_reliable = max(best_apis, key=lambda x: x['success_rate'])\n            \n            print(f\"🚀 API la plus rapide: {fastest_api['api'].title()} ({fastest_api['avg_response_time']:.3f}s)\")\n            print(f\"🛡️  API la plus fiable: {most_reliable['api'].title()} ({most_reliable['success_rate']*100:.1f}% succès)\")\n            \n            # Conseils d'optimisation\n            slow_apis = [b for b in best_apis if b['avg_response_time'] > 2.0]\n            if slow_apis:\n                print(f\"\\n⚠️  APIs lentes détectées:\")\n                for api in slow_apis:\n                    print(f\"   • {api['api'].title()}: {api['avg_response_time']:.3f}s\")\n                print(f\"💡 Considérez un timeout adapté et des retries\")\n        else:\n            print(f\"❌ Aucune API avec performance acceptable\")\n            print(f\"🔧 Vérifiez votre connexion réseau et les clés API\")\nelse:\n    if not run_benchmarks:\n        print(f\"\\n⏭️  Benchmarks désactivés (run_benchmarks = False)\")\n    else:\n        print(f\"\\n⚠️  Aucune connexion réussie pour les benchmarks\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Vérification des limites de taux (Rate limits)\nif check_rate_limits and connectivity_results:\n    print(\"\\n🚦 VÉRIFICATION DES RATE LIMITS\")\n    print(\"=\" * 40)\n    \n    def check_rate_limits_api(api_name: str, config: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Vérifie les limites de taux d'une API.\n        \"\"\"\n        \n        if config['status'] != 'configured':\n            return {\"error\": \"API non configurée\"}\n        \n        rate_info = {\n            \"api\": api_name,\n            \"limits_detected\": False,\n            \"headers_info\": {},\n            \"request_count\": 0,\n            \"rate_limited\": False,\n            \"reset_time\": None\n        }\n        \n        print(f\"\\n🔍 Test rate limits {config['name']}\")\n        \n        try:\n            # Test avec plusieurs requêtes rapprochées\n            for i in range(5):\n                response = requests.get(\n                    urljoin(config['base_url'], config['endpoints']['models']),\n                    headers=config['headers'],\n                    timeout=10\n                )\n                \n                rate_info['request_count'] += 1\n                \n                # Analyse des headers de rate limiting\n                headers_to_check = [\n                    'x-ratelimit-limit',\n                    'x-ratelimit-remaining', \n                    'x-ratelimit-reset',\n                    'retry-after',\n                    'x-ratelimit-limit-requests',\n                    'x-ratelimit-remaining-requests'\n                ]\n                \n                found_headers = {}\n                for header in headers_to_check:\n                    value = response.headers.get(header)\n                    if value:\n                        found_headers[header] = value\n                        rate_info['limits_detected'] = True\n                \n                if found_headers:\n                    rate_info['headers_info'].update(found_headers)\n                \n                if response.status_code == 429:\n                    rate_info['rate_limited'] = True\n                    retry_after = response.headers.get('retry-after')\n                    if retry_after:\n                        rate_info['reset_time'] = int(retry_after)\n                    print(f\"⚠️  Rate limit atteint après {i+1} requêtes\")\n                    break\n                elif response.status_code != 200:\n                    print(f\"❌ Erreur HTTP {response.status_code} à la requête {i+1}\")\n                    break\n                \n                # Petit délai entre requêtes\n                time.sleep(0.1)\n            \n            # Affichage des résultats\n            if rate_info['limits_detected']:\n                print(f\"✅ Headers de rate limiting détectés\")\n                for header, value in rate_info['headers_info'].items():\n                    print(f\"   {header}: {value}\")\n            else:\n                print(f\"ℹ️  Aucun header de rate limiting détecté\")\n            \n            if rate_info['rate_limited']:\n                reset_info = f\" (reset dans {rate_info['reset_time']}s)\" if rate_info['reset_time'] else \"\"\n                print(f\"🚨 Rate limiting actif{reset_info}\")\n            else:\n                print(f\"✅ Aucun rate limiting rencontré\")\n        \n        except Exception as e:\n            rate_info['error'] = str(e)[:100]\n            print(f\"❌ Erreur test rate limits: {rate_info['error']}\")\n        \n        return rate_info\n    \n    # Test des rate limits pour chaque API\n    rate_limit_results = []\n    \n    for result in connectivity_results:\n        if result['success']:\n            api_name = result['api']\n            config = api_config[api_name]\n            \n            rate_result = check_rate_limits_api(api_name, config)\n            rate_limit_results.append(rate_result)\n    \n    # Résumé des rate limits\n    if rate_limit_results:\n        print(f\"\\n📋 RÉSUMÉ RATE LIMITS\")\n        print(\"=\" * 25)\n        \n        for result in rate_limit_results:\n            api = result['api'].title()\n            detected = \"Oui\" if result.get('limits_detected') else \"Non\"\n            limited = \"Oui\" if result.get('rate_limited') else \"Non\"\n            \n            print(f\"🔧 {api}:\")\n            print(f\"   Headers détectés: {detected}\")\n            print(f\"   Rate limiting: {limited}\")\n            \n            if result.get('headers_info'):\n                limit = result['headers_info'].get('x-ratelimit-limit') or result['headers_info'].get('x-ratelimit-limit-requests')\n                remaining = result['headers_info'].get('x-ratelimit-remaining') or result['headers_info'].get('x-ratelimit-remaining-requests')\n                \n                if limit and remaining:\n                    usage = ((int(limit) - int(remaining)) / int(limit)) * 100\n                    print(f\"   Utilisation: {usage:.1f}% ({remaining}/{limit})\")\n        \n        # Conseils pour gérer les rate limits\n        print(f\"\\n💡 BONNES PRATIQUES RATE LIMITING\")\n        print(\"=\" * 38)\n        print(f\"• Implementez des retries avec backoff exponentiel\")\n        print(f\"• Respectez les headers Retry-After\")\n        print(f\"• Utilisez des pools de connexions\")\n        print(f\"• Monitorer l'utilisation en temps réel\")\n        print(f\"• Avoir des fallbacks entre APIs\")\nelse:\n    if not check_rate_limits:\n        print(f\"\\n⏭️  Vérification rate limits désactivée\")\n    else:\n        print(f\"\\n⚠️  Aucune API disponible pour test rate limits\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Monitoring des coûts et utilisation\nif monitor_costs and connectivity_results:\n    print(\"\\n💰 MONITORING DES COÛTS\")\n    print(\"=\" * 30)\n    \n    # Estimation des coûts basée sur les modèles disponibles\n    def estimate_usage_costs() -> Dict[str, Any]:\n        \"\"\"\n        Estime les coûts d'utilisation des APIs.\n        \"\"\"\n        \n        cost_estimates = {\n            \"openrouter\": {\n                \"dall-e-3\": {\n                    \"standard\": {\"1024x1024\": 0.04, \"1792x1024\": 0.08, \"1024x1792\": 0.08},\n                    \"hd\": {\"1024x1024\": 0.08, \"1792x1024\": 0.12, \"1024x1792\": 0.12}\n                },\n                \"gpt-5\": {\n                    \"input_tokens_per_k\": 0.005,  # Estimation\n                    \"output_tokens_per_k\": 0.015\n                },\n                \"gpt-4o\": {\n                    \"input_tokens_per_k\": 0.0025,\n                    \"output_tokens_per_k\": 0.01\n                }\n            },\n            \"openai\": {\n                \"dall-e-3\": {\n                    \"standard\": {\"1024x1024\": 0.04, \"1792x1024\": 0.08, \"1024x1792\": 0.08},\n                    \"hd\": {\"1024x1024\": 0.08, \"1792x1024\": 0.12, \"1024x1792\": 0.12}\n                },\n                \"gpt-4o\": {\n                    \"input_tokens_per_k\": 0.0025,\n                    \"output_tokens_per_k\": 0.01\n                }\n            }\n        }\n        \n        return cost_estimates\n    \n    cost_data = estimate_usage_costs()\n    \n    # Calculs d'exemple pour un usage typique CoursIA\n    print(\"\\n💡 ESTIMATION COÛTS USAGE COURSEIA\")\n    print(\"=\" * 42)\n    \n    typical_usage = {\n        \"images_per_month\": 100,  # Images générées par mois\n        \"chat_sessions_per_month\": 50,  # Sessions d'analyse\n        \"avg_tokens_per_session\": 2000  # Tokens moyens par session\n    }\n    \n    print(f\"📊 Usage typique mensuel estimé:\")\n    print(f\"   🖼️  Images générées: {typical_usage['images_per_month']}\")\n    print(f\"   💬 Sessions d'analyse: {typical_usage['chat_sessions_per_month']}\")\n    print(f\"   🔤 Tokens par session: {typical_usage['avg_tokens_per_session']:,}\")\n    \n    # Calcul des coûts par API\n    monthly_costs = {}\n    \n    for api_name, api_costs in cost_data.items():\n        api_total = 0\n        \n        print(f\"\\n💳 {api_name.title()} - Coûts mensuels estimés:\")\n        \n        # DALL-E 3 costs\n        if 'dall-e-3' in api_costs:\n            dalle_cost = typical_usage['images_per_month'] * api_costs['dall-e-3']['standard']['1024x1024']\n            api_total += dalle_cost\n            print(f\"   🎨 DALL-E 3 (100 images): ${dalle_cost:.2f}\")\n        \n        # GPT costs (estimation basée sur input + output)\n        gpt_models = [k for k in api_costs.keys() if 'gpt' in k]\n        if gpt_models:\n            model = gpt_models[0]  # Premier modèle GPT disponible\n            if 'input_tokens_per_k' in api_costs[model]:\n                total_tokens = typical_usage['chat_sessions_per_month'] * typical_usage['avg_tokens_per_session']\n                # Estimation: 70% input, 30% output\n                input_cost = (total_tokens * 0.7 / 1000) * api_costs[model]['input_tokens_per_k']\n                output_cost = (total_tokens * 0.3 / 1000) * api_costs[model]['output_tokens_per_k']\n                gpt_total = input_cost + output_cost\n                api_total += gpt_total\n                print(f\"   🤖 {model.upper()} ({total_tokens:,} tokens): ${gpt_total:.2f}\")\n        \n        monthly_costs[api_name] = api_total\n        print(f\"   💰 Total {api_name.title()}: ${api_total:.2f}/mois\")\n    \n    # Comparaison et recommandations\n    if monthly_costs:\n        print(f\"\\n🏆 COMPARAISON ET RECOMMANDATIONS\")\n        print(\"=\" * 40)\n        \n        cheapest_api = min(monthly_costs, key=monthly_costs.get)\n        cheapest_cost = monthly_costs[cheapest_api]\n        \n        print(f\"💡 API la plus économique: {cheapest_api.title()} (${cheapest_cost:.2f}/mois)\")\n        \n        cost_diff = max(monthly_costs.values()) - min(monthly_costs.values())\n        if cost_diff > 1:\n            print(f\"💸 Économie potentielle: ${cost_diff:.2f}/mois\")\n        \n        # Tips d'optimisation des coûts\n        print(f\"\\n🎯 CONSEILS D'OPTIMISATION\")\n        print(\"=\" * 30)\n        print(f\"• Utilisez les modèles les plus adaptés au besoin\")\n        print(f\"• Optimisez la longueur des prompts\")\n        print(f\"• Implémentez du caching pour les réponses similaires\")\n        print(f\"• Surveillez l'utilisation avec des alerts\")\n        print(f\"• Testez différentes résolutions d'images selon l'usage\")\nelse:\n    if not monitor_costs:\n        print(f\"\\n⏭️  Monitoring coûts désactivé\")\n    else:\n        print(f\"\\n⚠️  Aucune API disponible pour monitoring coûts\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Guide de diagnostic et troubleshooting\nprint(\"\\n🔧 GUIDE DE DIAGNOSTIC ET TROUBLESHOOTING\")\nprint(\"=\" * 55)\n\n# Fonction de diagnostic automatique\ndef run_diagnostics() -> Dict[str, Any]:\n    \"\"\"\n    Execute un diagnostic complet de la configuration.\n    \"\"\"\n    \n    diagnostics = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"env_file_status\": \"unknown\",\n        \"api_keys_status\": {},\n        \"connectivity_status\": {},\n        \"common_issues\": [],\n        \"recommendations\": []\n    }\n    \n    # Vérification fichier .env\n    env_file = GENAI_ROOT / '.env'\n    if env_file.exists():\n        diagnostics['env_file_status'] = \"found\"\n        try:\n            with open(env_file, 'r') as f:\n                env_content = f.read()\n            if 'OPENROUTER_API_KEY' in env_content or 'OPENAI_API_KEY' in env_content:\n                diagnostics['env_file_status'] = \"configured\"\n        except:\n            diagnostics['env_file_status'] = \"read_error\"\n    else:\n        diagnostics['env_file_status'] = \"missing\"\n    \n    # Status des clés API\n    for api_name, config in api_config.items():\n        key_status = \"missing\"\n        if config.get('key_value'):\n            key_status = \"configured\"\n            # Test basique de format\n            key = config['key_value']\n            if len(key) < 20:\n                key_status = \"too_short\"\n            elif not key.replace('-', '').replace('_', '').isalnum():\n                key_status = \"invalid_format\"\n        \n        diagnostics['api_keys_status'][api_name] = key_status\n    \n    # Status de connectivité\n    for result in connectivity_results:\n        api = result['api']\n        status = \"success\" if result['success'] else \"failed\"\n        error = result.get('error')\n        \n        diagnostics['connectivity_status'][api] = {\n            \"status\": status,\n            \"error\": error,\n            \"response_time\": result.get('response_time')\n        }\n    \n    # Détection des problèmes courants\n    if diagnostics['env_file_status'] == \"missing\":\n        diagnostics['common_issues'].append({\n            \"issue\": \"Fichier .env manquant\",\n            \"solution\": \"Copiez .env.template vers .env et configurez vos clés API\"\n        })\n    \n    missing_keys = [api for api, status in diagnostics['api_keys_status'].items() if status == \"missing\"]\n    if missing_keys:\n        diagnostics['common_issues'].append({\n            \"issue\": f\"Clés API manquantes: {', '.join(missing_keys)}\",\n            \"solution\": \"Ajoutez les clés manquantes dans le fichier .env\"\n        })\n    \n    failed_connections = [api for api, data in diagnostics['connectivity_status'].items() if data['status'] == \"failed\"]\n    if failed_connections:\n        diagnostics['common_issues'].append({\n            \"issue\": f\"Échecs de connexion: {', '.join(failed_connections)}\",\n            \"solution\": \"Vérifiez votre connexion internet et les clés API\"\n        })\n    \n    # Recommandations générales\n    if not diagnostics['common_issues']:\n        diagnostics['recommendations'].append(\"✅ Configuration optimale détectée\")\n    else:\n        diagnostics['recommendations'].append(\"🔧 Corrigez les problèmes identifiés ci-dessus\")\n    \n    if any(data.get('response_time', 0) > 3 for data in diagnostics['connectivity_status'].values()):\n        diagnostics['recommendations'].append(\"⚡ Connexion lente détectée - vérifiez votre réseau\")\n    \n    return diagnostics\n\n# Exécution du diagnostic\ndiag_results = run_diagnostics()\n\nprint(f\"\\n📋 RÉSULTATS DU DIAGNOSTIC\")\nprint(\"=\" * 32)\n\n# Status fichier .env\nenv_status_icons = {\n    \"missing\": \"❌\",\n    \"found\": \"⚠️\",\n    \"configured\": \"✅\",\n    \"read_error\": \"🔧\"\n}\nenv_icon = env_status_icons.get(diag_results['env_file_status'], \"❓\")\nprint(f\"{env_icon} Fichier .env: {diag_results['env_file_status']}\")\n\n# Status clés API\nprint(f\"\\n🔑 Status des clés API:\")\nfor api, status in diag_results['api_keys_status'].items():\n    status_icon = \"✅\" if status == \"configured\" else \"❌\"\n    print(f\"   {status_icon} {api.title()}: {status}\")\n\n# Status connectivité\nif diag_results['connectivity_status']:\n    print(f\"\\n🌐 Status connectivité:\")\n    for api, data in diag_results['connectivity_status'].items():\n        status_icon = \"✅\" if data['status'] == \"success\" else \"❌\"\n        time_info = f\" ({data['response_time']:.3f}s)\" if data.get('response_time') else \"\"\n        print(f\"   {status_icon} {api.title()}: {data['status']}{time_info}\")\n\n# Problèmes détectés\nif diag_results['common_issues']:\n    print(f\"\\n🚨 PROBLÈMES DÉTECTÉS\")\n    print(\"=\" * 25)\n    for i, issue in enumerate(diag_results['common_issues'], 1):\n        print(f\"{i}. ❌ {issue['issue']}\")\n        print(f\"   💡 Solution: {issue['solution']}\")\n\n# Recommandations\nif diag_results['recommendations']:\n    print(f\"\\n💡 RECOMMANDATIONS\")\n    print(\"=\" * 20)\n    for rec in diag_results['recommendations']:\n        print(f\"• {rec}\")\n\nprint(f\"\\n📞 SUPPORT SUPPLÉMENTAIRE\")\nprint(\"=\" * 28)\nprint(f\"• Documentation: docs/genai-troubleshooting-guide.md\")\nprint(f\"• Templates: docs/genai-phase2-templates.md\")\nprint(f\"• Issues GitHub: https://github.com/jsboige/CoursIA/issues\")\nprint(f\"• Logs détaillés: Activez debug_level = 'DEBUG'\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Export des résultats et génération de rapport\nif export_results:\n    print(\"\\n📄 EXPORT DES RÉSULTATS\")\n    print(\"=\" * 30)\n    \n    # Compilation des résultats\n    export_data = {\n        \"configuration_test\": {\n            \"timestamp\": datetime.now().isoformat(),\n            \"notebook_version\": \"1.0.0\",\n            \"test_parameters\": {\n                \"test_openrouter\": test_openrouter,\n                \"test_openai_direct\": test_openai_direct,\n                \"run_benchmarks\": run_benchmarks,\n                \"benchmark_iterations\": benchmark_iterations,\n                \"benchmark_timeout\": benchmark_timeout\n            }\n        },\n        \"api_configuration\": {api: {k: v for k, v in config.items() if k != 'key_value'} for api, config in api_config.items()},\n        \"connectivity_results\": connectivity_results,\n        \"benchmark_results\": benchmark_results if 'benchmark_results' in locals() else [],\n        \"rate_limit_results\": rate_limit_results if 'rate_limit_results' in locals() else [],\n        \"diagnostics\": diag_results,\n        \"summary\": {\n            \"total_apis_tested\": len(connectivity_results),\n            \"successful_connections\": sum(1 for r in connectivity_results if r['success']),\n            \"average_response_time\": sum(r['response_time'] for r in connectivity_results if r['success']) / len([r for r in connectivity_results if r['success']]) if connectivity_results else 0,\n            \"configuration_status\": \"healthy\" if all(r['success'] for r in connectivity_results) else \"issues_detected\"\n        }\n    }\n    \n    # Sauvegarde JSON\n    output_dir = GENAI_ROOT / 'outputs' / 'api_configuration'\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    json_file = output_dir / f\"api_config_test_{timestamp}.json\"\n    \n    with open(json_file, 'w', encoding='utf-8') as f:\n        json.dump(export_data, f, indent=2, ensure_ascii=False)\n    \n    print(f\"💾 Résultats exportés: {json_file}\")\n    print(f\"📊 Données incluses: {len(export_data)} sections\")\n    \n    # Génération rapport HTML simple si demandé\n    if generate_report:\n        html_content = f\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Rapport Configuration APIs CoursIA</title>\n    <meta charset=\"utf-8\">\n    <style>\n        body {{ font-family: Arial, sans-serif; margin: 20px; }}\n        .header {{ background: #f0f8ff; padding: 20px; border-radius: 8px; }}\n        .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}\n        .success {{ color: #28a745; }}\n        .error {{ color: #dc3545; }}\n        .warning {{ color: #ffc107; }}\n        table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}\n        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n        th {{ background-color: #f2f2f2; }}\n    </style>\n</head>\n<body>\n    <div class=\"header\">\n        <h1>🔗 Rapport Configuration APIs CoursIA</h1>\n        <p>Généré le: {datetime.now().strftime('%d/%m/%Y à %H:%M:%S')}</p>\n    </div>\n    \n    <div class=\"section\">\n        <h2>📊 Résumé</h2>\n        <ul>\n            <li>APIs testées: {export_data['summary']['total_apis_tested']}</li>\n            <li>Connexions réussies: {export_data['summary']['successful_connections']}</li>\n            <li>Temps de réponse moyen: {export_data['summary']['average_response_time']:.3f}s</li>\n            <li>Status global: {export_data['summary']['configuration_status']}</li>\n        </ul>\n    </div>\n    \n    <div class=\"section\">\n        <h2>🌐 Connectivité</h2>\n        <table>\n            <tr><th>API</th><th>Status</th><th>Temps (s)</th><th>Modèles</th><th>Erreur</th></tr>\n        \"\"\"\n        \n        for result in connectivity_results:\n            status_class = \"success\" if result['success'] else \"error\"\n            status_text = \"✅ Succès\" if result['success'] else \"❌ Échec\"\n            response_time = f\"{result['response_time']:.3f}\" if result.get('response_time') else \"N/A\"\n            models_count = result.get('models_count', 0)\n            error = result.get('error', 'Aucune') if not result['success'] else 'Aucune'\n            \n            html_content += f\"\"\"\n            <tr>\n                <td>{result['name']}</td>\n                <td class=\"{status_class}\">{status_text}</td>\n                <td>{response_time}</td>\n                <td>{models_count}</td>\n                <td>{error}</td>\n            </tr>\n            \"\"\"\n        \n        html_content += \"\"\"\n        </table>\n    </div>\n    \n    <div class=\"section\">\n        <h2>🔧 Diagnostic</h2>\n        <h3>Problèmes détectés:</h3>\n        <ul>\n        \"\"\"\n        \n        if diag_results['common_issues']:\n            for issue in diag_results['common_issues']:\n                html_content += f\"<li class='error'>❌ {issue['issue']}<br><small>💡 {issue['solution']}</small></li>\"\n        else:\n            html_content += \"<li class='success'>✅ Aucun problème détecté</li>\"\n        \n        html_content += \"\"\"\n        </ul>\n        <h3>Recommandations:</h3>\n        <ul>\n        \"\"\"\n        \n        for rec in diag_results['recommendations']:\n            html_content += f\"<li>{rec}</li>\"\n        \n        html_content += \"\"\"\n        </ul>\n    </div>\n    \n    <div class=\"section\">\n        <h2>📝 Fichiers générés</h2>\n        <ul>\n            <li>Données JSON: api_config_test_{timestamp}.json</li>\n            <li>Rapport HTML: api_config_report_{timestamp}.html</li>\n        </ul>\n    </div>\n</body>\n</html>\n        \"\"\"\n        \n        html_file = output_dir / f\"api_config_report_{timestamp}.html\"\n        with open(html_file, 'w', encoding='utf-8') as f:\n            f.write(html_content)\n        \n        print(f\"📄 Rapport HTML généré: {html_file}\")\n        print(f\"💡 Ouvrez le fichier HTML dans votre navigateur\")\nelse:\n    print(f\"\\n⏭️  Export désactivé (export_results = False)\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 🎯 Résumé et Prochaines Étapes\n\n### ✅ Configuration Validée\n\n- [ ] **APIs configurées** : Clés et endpoints vérifiés\n- [ ] **Connectivité testée** : Tous les endpoints fonctionnels\n- [ ] **Performance mesurée** : Benchmarks et temps de réponse\n- [ ] **Rate limits analysés** : Limites et recommandations\n- [ ] **Coûts estimés** : Projections d'usage mensuel\n- [ ] **Diagnostic complet** : Problèmes détectés et solutions\n\n### 🚀 Utilisation des APIs\n\nMaintenant que vos APIs sont configurées, vous pouvez utiliser :\n\n1. **Notebooks Foundation** :\n   - `01-1-OpenAI-DALL-E-3.ipynb` - Génération d'images\n   - `01-2-GPT-5-Image-Generation.ipynb` - Analyse multimodale\n\n2. **Notebooks Advanced** :\n   - `02-1-Qwen-Image-Edit-2509.ipynb` - Édition d'images\n   - `02-2-FLUX-1-Advanced-Generation.ipynb` - Génération avancée\n\n3. **Notebooks Applications** :\n   - `04-1-Educational-Content-Generation.ipynb` - Contenu pédagogique\n   - `04-2-Creative-Workflows.ipynb` - Workflows créatifs\n\n### 💡 Bonnes Pratiques\n\n**✅ Configuration:**\n- Gardez vos clés API sécurisées dans `.env`\n- Testez régulièrement la connectivité\n- Surveillez vos quotas et coûts\n- Implémentez des retries pour la résilience\n\n**❌ Évitez:**\n- Partager vos clés API dans le code\n- Ignorer les rate limits\n- Utiliser des timeouts trop courts\n- Oublier de monitorer les coûts\n\n### 🔗 Ressources\n\n- **Documentation OpenRouter** : [openrouter.ai/docs](https://openrouter.ai/docs)\n- **API OpenAI** : [platform.openai.com/docs](https://platform.openai.com/docs)\n- **Troubleshooting** : `docs/genai-troubleshooting-guide.md`\n- **Templates** : `docs/genai-phase2-templates.md`\n\n### 📊 Fichiers Générés\n\nCe notebook génère automatiquement :\n- **Rapport JSON** : Données complètes de configuration\n- **Rapport HTML** : Visualisation des résultats\n- **Logs détaillés** : Pour le debugging\n\n---\n\n**🎉 Configuration terminée !** Vos APIs sont prêtes pour l'utilisation dans l'écosystème CoursIA GenAI."
    ],
      "metadata": {}
    }
  ]
}
