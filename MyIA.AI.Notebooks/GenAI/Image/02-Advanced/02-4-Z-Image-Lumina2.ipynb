{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Z-Image (Lumina-2) : Generation Avancee avec ComfyUI\n\n**Module :** 02-Images-Advanced\n**Niveau :** Avance\n**Duree estimee :** 30 minutes\n**Statut :** FONCTIONNEL\n\n## Introduction\n\nZ-Image utilise l'architecture **Lumina-Next-SFT** via le wrapper Diffusers pour la generation d'images haute qualite. Ce notebook utilise le node `LuminaDiffusersNode` qui integre le pipeline HuggingFace Diffusers directement dans ComfyUI.\n\n### Architecture\n\n| Composant | Details |\n|-----------|---------|\n| **Pipeline** | LuminaPipeline (diffusers 0.34+) |\n| **Modele** | Alpha-VLLM/Lumina-Next-SFT-diffusers (~10GB) |\n| **VAE** | SDXL VAE (sdxl_vae.safetensors) |\n| **Resolution** | 1024x1024 (natif) |\n\n### Note sur l'approche GGUF\n\nL'approche GGUF (z_image_turbo + gemma CLIP) a ete abandonnee en raison d'incompatibilites dimensionnelles (2560 vs 2304 entre RecurrentGemma et Gemma-2).\n\n## Prerequis\n- Service `comfyui-qwen` actif\n- Node installe : LuminaDiffusersNode (ComfyUI-Lumina-Next-SFT-DiffusersWrapper)\n- VAE : sdxl_vae.safetensors dans models/vae/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration de l'environnement\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chargement des variables d'environnement\n",
    "load_dotenv(\"../00-GenAI-Environment/.env\")\n",
    "\n",
    "COMFYUI_URL = \"http://localhost:8188\"\n",
    "COMFYUI_TOKEN = os.getenv(\"COMFYUI_AUTH_TOKEN\")\n",
    "\n",
    "if not COMFYUI_TOKEN:\n",
    "    raise ValueError(\"❌ Token COMFYUI_AUTH_TOKEN manquant dans le fichier .env\")\n",
    "\n",
    "HEADERS = {\"Authorization\": f\"Bearer {COMFYUI_TOKEN}\"}\n",
    "\n",
    "print(\"✅ Configuration chargée\")\n",
    "print(f\"Target URL: {COMFYUI_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2. Definition du Workflow Z-Image (Diffusers)\n# Ce workflow utilise LuminaDiffusersNode qui telecharge automatiquement\n# le modele depuis HuggingFace (Alpha-VLLM/Lumina-Next-SFT-diffusers)\n\ndef create_zimage_workflow(prompt: str, \n                           negative_prompt: str = \"blurry, low quality, text, watermark\",\n                           width: int = 1024, \n                           height: int = 1024,\n                           steps: int = 30,\n                           guidance: float = 4.0,\n                           seed: int = None) -> dict:\n    \"\"\"\n    Cree un workflow Z-Image avec le wrapper Diffusers.\n    \n    Args:\n        prompt: Description de l'image a generer\n        negative_prompt: Elements a eviter\n        width/height: Dimensions (1024x1024 recommande)\n        steps: Etapes de diffusion (20-40 optimal)\n        guidance: Guidance scale (3-5 optimal pour Lumina)\n        seed: Graine pour reproductibilite (-1 = aleatoire)\n    \n    Returns:\n        Workflow JSON pret pour ComfyUI\n    \"\"\"\n    if seed is None:\n        seed = -1  # -1 = random dans LuminaDiffusersNode\n    \n    return {\n        # LuminaDiffusersNode - Le coeur de la generation\n        # Note: Le node gere en interne la resolution via le scheduler\n        \"1\": {\n            \"class_type\": \"LuminaDiffusersNode\",\n            \"inputs\": {\n                \"model_path\": \"Alpha-VLLM/Lumina-Next-SFT-diffusers\",\n                \"prompt\": prompt,\n                \"negative_prompt\": negative_prompt,\n                \"num_inference_steps\": steps,\n                \"guidance_scale\": guidance,\n                \"seed\": seed,\n                \"batch_size\": 1,\n                \"scaling_watershed\": 0.3,\n                \"proportional_attn\": True,\n                \"clean_caption\": True,\n                \"max_sequence_length\": 256,\n                \"use_time_shift\": False,\n                \"t_shift\": 4,\n                \"strength\": 1.0\n            }\n        },\n        # VAELoader - SDXL VAE pour le decodage\n        \"2\": {\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\n                \"vae_name\": \"sdxl_vae.safetensors\"\n            }\n        },\n        # VAEDecode - Conversion latent vers image\n        \"3\": {\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\n                \"samples\": [\"1\", 0],\n                \"vae\": [\"2\", 0]\n            }\n        },\n        # SaveImage - Sauvegarde du resultat\n        \"4\": {\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\n                \"filename_prefix\": \"Z-Image-Lumina\",\n                \"images\": [\"3\", 0]\n            }\n        }\n    }\n\n# Workflow de test basique\nz_image_workflow = create_zimage_workflow(\n    prompt=\"A vibrant sunset over a mountain lake, reflection in water, photorealistic\",\n    steps=20,\n    guidance=4.0,\n    seed=42\n)\n\nprint(\"Workflow Z-Image (Diffusers) defini\")\nprint(\"  - Node principal: LuminaDiffusersNode\")\nprint(\"  - Modele: Alpha-VLLM/Lumina-Next-SFT-diffusers\")\nprint(\"  - VAE: SDXL\")\nprint(\"  - Output node: '4'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 3. Fonction de Generation Helper\ndef generate_z_image(prompt, seed=None, steps=20, guidance=4.0, width=1024, height=1024):\n    \"\"\"\n    Genere une image avec Z-Image (Lumina Diffusers).\n    \n    Args:\n        prompt: Description de l'image\n        seed: Graine pour reproductibilite (None/-1 = aleatoire)\n        steps: Nombre d'etapes (20-40 recommande)\n        guidance: Guidance scale (3-5 recommande)\n        width/height: Dimensions\n    \n    Returns:\n        PIL.Image ou None en cas d'erreur\n    \"\"\"\n    if seed is None:\n        seed = -1\n    \n    # Creer le workflow\n    workflow = create_zimage_workflow(\n        prompt=prompt,\n        seed=seed,\n        steps=steps,\n        guidance=guidance,\n        width=width,\n        height=height\n    )\n    \n    payload = {\"prompt\": workflow}\n    \n    print(f\"Envoi de la requete... (Seed: {seed})\")\n    resp = requests.post(f\"{COMFYUI_URL}/prompt\", json=payload, headers=HEADERS)\n    \n    if resp.status_code != 200:\n        print(f\"Erreur API: {resp.text}\")\n        return None\n        \n    prompt_id = resp.json()[\"prompt_id\"]\n    print(f\"Tache ID: {prompt_id}\")\n    print(\"Generation en cours (premiere execution peut prendre plusieurs minutes pour telecharger le modele)...\")\n    \n    # Polling avec timeout etendu (premier run telecharge ~10GB)\n    max_wait = 600  # 10 minutes pour le premier run\n    start_time = time.time()\n    \n    while time.time() - start_time < max_wait:\n        history_resp = requests.get(f\"{COMFYUI_URL}/history/{prompt_id}\", headers=HEADERS)\n        if history_resp.status_code == 200:\n            history_data = history_resp.json()\n            if prompt_id in history_data:\n                prompt_data = history_data[prompt_id]\n                status = prompt_data.get('status', {})\n                \n                if status.get('completed'):\n                    elapsed = time.time() - start_time\n                    print(f\"Generation terminee en {elapsed:.1f}s\")\n                    \n                    # Recuperation image (node 4 = SaveImage)\n                    outputs = prompt_data.get('outputs', {})\n                    if '4' in outputs and 'images' in outputs['4']:\n                        output_data = outputs['4']['images'][0]\n                        filename = output_data['filename']\n                        subfolder = output_data.get('subfolder', '')\n                        img_type = output_data.get('type', 'output')\n                        \n                        img_url = f\"{COMFYUI_URL}/view?filename={filename}&subfolder={subfolder}&type={img_type}\"\n                        img_resp = requests.get(img_url, headers=HEADERS)\n                        \n                        if img_resp.status_code == 200:\n                            return Image.open(BytesIO(img_resp.content))\n                    \n                    print(\"Pas d'image dans les outputs\")\n                    return None\n                    \n                if status.get('status_str') == 'error':\n                    print(f\"Erreur: {status.get('messages', 'Unknown')}\")\n                    return None\n        \n        time.sleep(2)\n    \n    print(f\"Timeout apres {max_wait}s\")\n    return None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4. Test de Generation\n# Note: La premiere execution telecharge le modele (~10GB), prevoir 5-10 minutes\n\nprompt = \"Cinematic photography of a samurai robot in a neon cyberpunk city, raining, reflections, 8k, highly detailed\"\n\nprint(\"Lancement de la generation Z-Image (Lumina Diffusers)...\")\nprint(f\"Prompt: {prompt}\")\nprint(\"\\nNote: Le premier lancement telecharge le modele (~10GB)\")\n\nimage = generate_z_image(prompt, seed=42, steps=20, guidance=4.0)\n\nif image:\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)\n    plt.title(f\"Z-Image: {prompt[:50]}...\", fontsize=10)\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nImage generee: {image.size[0]}x{image.size[1]}\")\nelse:\n    print(\"\\nEchec de la generation. Verifiez:\")\n    print(\"  1. Le service ComfyUI est actif\")\n    print(\"  2. Le node LuminaDiffusersNode est installe\")\n    print(\"  3. Le VAE SDXL est present dans models/vae/\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercices\n\n### Exercice 1: Exploration des parametres\nTestez differentes valeurs de guidance (2.0, 4.0, 6.0) avec le meme prompt et seed pour observer l'impact sur la fidelite au prompt.\n\n### Exercice 2: Comparaison de styles\nGenerez des images avec des styles differents:\n- \"A peaceful mountain lake at sunrise, photorealistic\"\n- \"A peaceful mountain lake at sunrise, watercolor painting style\"\n- \"A peaceful mountain lake at sunrise, anime style\"\n\n### Exercice 3: Resolution\nTestez differentes resolutions (512x512, 768x768, 1024x1024) et observez l'impact sur la qualite et le temps de generation.\n\n## Notes Techniques\n\n### Parametres recommandes\n| Parametre | Plage | Recommande | Impact |\n|-----------|-------|------------|--------|\n| steps | 15-50 | 20-30 | Qualite vs vitesse |\n| guidance | 2-7 | 3-5 | Fidelite au prompt |\n| scaling_watershed | 0.0-1.0 | 0.3 | Reduction saturation |\n\n### Differences avec Qwen\n- **Lumina/Z-Image**: Generation text-to-image de haute qualite, meilleur pour scenes complexes\n- **Qwen**: Specialise dans l'edition d'images existantes, image-to-image\n- Lumina utilise un VAE 4-channel SDXL, Qwen utilise un VAE 16-channel\n\n### Premier lancement\nLe premier lancement telecharge automatiquement le modele (~10GB) depuis HuggingFace.\nCela peut prendre 5-10 minutes selon votre connexion.\n\n### Fix technique (Janvier 2025)\nLe node `LuminaDiffusersNode` a ete mis a jour pour utiliser `LuminaPipeline` au lieu de\n`LuminaText2ImgPipeline` (renomme dans diffusers 0.34+)."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}