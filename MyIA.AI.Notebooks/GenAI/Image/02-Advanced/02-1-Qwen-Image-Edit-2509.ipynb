{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Qwen Image Edit 2509 - Ã‰dition AvancÃ©e d'Images\n",
    "\n",
    "**Module :** 02-Images-Advanced  \n",
    "**Niveau :** IntermÃ©diaire/AvancÃ©  \n",
    "**DurÃ©e estimÃ©e :** 45 minutes  \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ce notebook explore les capacitÃ©s avancÃ©es de **Qwen-Image-Edit 2509** (version Septembre 2025), un modÃ¨le d'Ã©dition d'images de pointe intÃ©grÃ© via ComfyUI. Par rapport au notebook d'introduction (01-5), nous abordons ici :\n",
    "\n",
    "- **Ã‰dition prÃ©cise de texte** dans les images\n",
    "- **Inpainting avancÃ©** avec masques personnalisÃ©s\n",
    "- **Workflows multi-Ã©tapes** pour des transformations complexes\n",
    "- **Batch processing** pour l'efficacitÃ©\n",
    "- **Analyse comparative** des paramÃ¨tres\n",
    "\n",
    "### Architecture Qwen-Image-Edit 2509\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              Qwen-Image-Edit 2509 Pipeline              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Image Input â†’ Qwen2.5-VL Encoder â†’ Diffusion Model    â”‚\n",
    "â”‚       â†“              â†“                    â†“             â”‚\n",
    "â”‚  [Tokenizer]    [16-ch VAE]        [UNet 1024Â²]        â”‚\n",
    "â”‚       â†“              â†“                    â†“             â”‚\n",
    "â”‚   Text Prompt â†’ Cross-Attention â†’ Latent Space         â”‚\n",
    "â”‚                                        â†“                â”‚\n",
    "â”‚                               VAE Decode â†’ Output      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## PrÃ©requis\n",
    "\n",
    "- Module 00-GenAI-Environment complÃ©tÃ©\n",
    "- Service `comfyui-qwen` actif (`docker compose up -d`)\n",
    "- Notebook 01-5-Qwen-Image-Edit terminÃ© (concepts de base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dw2367dvov",
   "source": "**Navigation** : [Index](../../README.md) | [<< Precedent](../01-Foundation/01-5-Qwen-Image-Edit.ipynb) | [Suivant >>](02-2-FLUX-1-Advanced-Generation.ipynb)\n\n---\n\n# Qwen Image Edit 2509 - Ã‰dition AvancÃ©e d'Images\n\n**Module :** 02-Images-Advanced  \n**Niveau :** IntermÃ©diaire/AvancÃ©  \n**DurÃ©e estimÃ©e :** 45 minutes  \n\n## Objectifs d'apprentissage\n\nA la fin de ce notebook, vous saurez :\n1. Maitriser l'architecture Phase 29 de Qwen-Image-Edit 2509\n2. Comprendre l'impact du parametre `denoise` sur l'edition d'images\n3. Implementer l'inpainting avec des masques personnalises\n4. Optimiser le traitement par lots (batch processing)\n5. Analyser l'effet du parametre CFG avec CFGNorm\n\n### Prerequis\n\n- Module 00-GenAI-Environment complÃ©tÃ©\n- Service `comfyui-qwen` actif (`docker compose up -d`)\n- Notebook 01-5-Qwen-Image-Edit terminÃ© (concepts de base)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 1. CONFIGURATION ET IMPORTS\n# =============================================================================\n\nimport os\nimport sys\nimport json\nimport uuid\nimport time\nimport base64\nimport requests\nfrom io import BytesIO\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Dict, List, Tuple, Any\n\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\nimport matplotlib.pyplot as plt\n\n# Chargement variables d'environnement\n# Recherche du .env en remontant l'arborescence (plus robuste que chemins relatifs)\nfrom dotenv import load_dotenv\n\ncurrent_path = Path.cwd()\nfound_env = False\nfor _ in range(5):  # Remonter jusqu'a 5 niveaux\n    env_path = current_path / '.env'\n    if env_path.exists():\n        load_dotenv(env_path)\n        found_env = True\n        break\n    current_path = current_path.parent\n\nif not found_env:\n    # Fallback: essayer des chemins relatifs connus\n    for fallback in [\"../../.env\", \"../.env\", \"../00-GenAI-Environment/.env\"]:\n        if Path(fallback).exists():\n            load_dotenv(fallback)\n            found_env = True\n            break\n\n# Configuration ComfyUI\n# URL par defaut: service myia.io pour etudiants\nCOMFYUI_URL = os.getenv(\"COMFYUI_API_URL\", \"https://qwen-image-edit.myia.io\")\n# Support des deux noms de variable pour le token\nCOMFYUI_TOKEN = os.getenv(\"COMFYUI_AUTH_TOKEN\") or os.getenv(\"COMFYUI_API_TOKEN\")\nCLIENT_ID = str(uuid.uuid4())\n\n# Validation\nif not COMFYUI_TOKEN:\n    raise ValueError(\"COMFYUI_AUTH_TOKEN ou COMFYUI_API_TOKEN manquant dans .env\")\n\nprint(\"Qwen-Image-Edit 2509 - Edition Avancee\")\nprint(f\"\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"API URL: {COMFYUI_URL}\")\nprint(f\"Token: {'Configure' if COMFYUI_TOKEN else 'Manquant'}\")\nif found_env:\n    print(f\".env: charge\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. CLIENT COMFYUI AVANCÃ‰\n",
    "# =============================================================================\n",
    "\n",
    "class QwenImageEditClient:\n",
    "    \"\"\"\n",
    "    Client avancÃ© pour Qwen-Image-Edit via ComfyUI.\n",
    "    Supporte l'authentification, le batch processing et les workflows multi-Ã©tapes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = COMFYUI_URL, auth_token: str = COMFYUI_TOKEN):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.client_id = str(uuid.uuid4())\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        if auth_token:\n",
    "            self.session.headers.update({\"Authorization\": f\"Bearer {auth_token}\"})\n",
    "    \n",
    "    def check_health(self) -> Dict[str, Any]:\n",
    "        \"\"\"VÃ©rifie la santÃ© du service ComfyUI.\"\"\"\n",
    "        try:\n",
    "            resp = self.session.get(f\"{self.base_url}/system_stats\", timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                stats = resp.json()\n",
    "                return {\n",
    "                    \"status\": \"healthy\",\n",
    "                    \"vram_free\": stats.get(\"devices\", [{}])[0].get(\"vram_free\", 0) / 1e9,\n",
    "                    \"vram_total\": stats.get(\"devices\", [{}])[0].get(\"vram_total\", 0) / 1e9\n",
    "                }\n",
    "            return {\"status\": \"error\", \"code\": resp.status_code}\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"unreachable\", \"error\": str(e)}\n",
    "    \n",
    "    def upload_image(self, image: Image.Image, name: str = \"input.png\") -> str:\n",
    "        \"\"\"Upload une image vers ComfyUI.\"\"\"\n",
    "        buffer = BytesIO()\n",
    "        image.save(buffer, format=\"PNG\")\n",
    "        buffer.seek(0)\n",
    "        \n",
    "        files = {\"image\": (name, buffer, \"image/png\")}\n",
    "        data = {\"overwrite\": \"true\"}\n",
    "        \n",
    "        resp = self.session.post(f\"{self.base_url}/upload/image\", files=files, data=data)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.json().get(\"name\", name)\n",
    "        raise Exception(f\"Upload failed: {resp.text}\")\n",
    "    \n",
    "    def upload_mask(self, mask: Image.Image, name: str = \"mask.png\") -> str:\n",
    "        \"\"\"Upload un masque pour l'inpainting.\"\"\"\n",
    "        # Convertir en grayscale si nÃ©cessaire\n",
    "        if mask.mode != 'L':\n",
    "            mask = mask.convert('L')\n",
    "        return self.upload_image(mask, name)\n",
    "    \n",
    "    def queue_workflow(self, workflow: Dict) -> str:\n",
    "        \"\"\"Soumet un workflow et retourne le prompt_id.\"\"\"\n",
    "        payload = {\"prompt\": workflow, \"client_id\": self.client_id}\n",
    "        resp = self.session.post(f\"{self.base_url}/prompt\", json=payload)\n",
    "        \n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Queue failed: {resp.text}\")\n",
    "        return resp.json()[\"prompt_id\"]\n",
    "    \n",
    "    def wait_for_completion(self, prompt_id: str, timeout: int = 120) -> Dict:\n",
    "        \"\"\"Attend la fin d'un workflow avec timeout.\"\"\"\n",
    "        start = time.time()\n",
    "        while time.time() - start < timeout:\n",
    "            resp = self.session.get(f\"{self.base_url}/history/{prompt_id}\")\n",
    "            if resp.status_code == 200:\n",
    "                history = resp.json()\n",
    "                if prompt_id in history:\n",
    "                    return history[prompt_id]\n",
    "            time.sleep(1)\n",
    "        raise TimeoutError(f\"Workflow {prompt_id} timeout after {timeout}s\")\n",
    "    \n",
    "    def get_image(self, filename: str, subfolder: str = \"\", img_type: str = \"output\") -> Image.Image:\n",
    "        \"\"\"RÃ©cupÃ¨re une image gÃ©nÃ©rÃ©e.\"\"\"\n",
    "        params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": img_type}\n",
    "        resp = self.session.get(f\"{self.base_url}/view\", params=params)\n",
    "        if resp.status_code == 200:\n",
    "            return Image.open(BytesIO(resp.content))\n",
    "        raise Exception(f\"Failed to get image: {resp.text}\")\n",
    "    \n",
    "    def execute_and_get_images(self, workflow: Dict, output_node: str = \"9\", \n",
    "                                timeout: int = 120, verbose: bool = True) -> List[Image.Image]:\n",
    "        \"\"\"ExÃ©cute un workflow et retourne les images gÃ©nÃ©rÃ©es.\"\"\"\n",
    "        if verbose:\n",
    "            print(\"ğŸš€ Soumission du workflow...\")\n",
    "        \n",
    "        prompt_id = self.queue_workflow(workflow)\n",
    "        if verbose:\n",
    "            print(f\"ğŸ“‹ ID: {prompt_id}\")\n",
    "            print(\"â³ GÃ©nÃ©ration en cours...\", end=\"\", flush=True)\n",
    "        \n",
    "        result = self.wait_for_completion(prompt_id, timeout)\n",
    "        if verbose:\n",
    "            print(\" âœ…\")\n",
    "        \n",
    "        # Extraire les images\n",
    "        images = []\n",
    "        if \"outputs\" in result and output_node in result[\"outputs\"]:\n",
    "            for img_data in result[\"outputs\"][output_node].get(\"images\", []):\n",
    "                img = self.get_image(\n",
    "                    img_data[\"filename\"],\n",
    "                    img_data.get(\"subfolder\", \"\"),\n",
    "                    img_data.get(\"type\", \"output\")\n",
    "                )\n",
    "                images.append(img)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ğŸ–¼ï¸ {len(images)} image(s) rÃ©cupÃ©rÃ©e(s)\")\n",
    "        return images\n",
    "\n",
    "# Instanciation du client\n",
    "client = QwenImageEditClient()\n",
    "\n",
    "# Test de connexion\n",
    "health = client.check_health()\n",
    "print(f\"\\nğŸ¥ Ã‰tat du service: {health['status']}\")\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"   VRAM: {health['vram_free']:.1f} / {health['vram_total']:.1f} GB libre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 3. WORKFLOWS QWEN-IMAGE-EDIT 2509 - ARCHITECTURE PHASE 29\n# =============================================================================\n# Ces workflows utilisent l'architecture native ComfyUI validee Phase 29\n# avec les modeles FP8 officiels Comfy-Org\n\ndef create_text2img_workflow(prompt: str, \n                              width: int = 1024, height: int = 1024,\n                              steps: int = 20, cfg: float = 1.0,\n                              seed: int = None) -> Dict:\n    \"\"\"\n    Workflow Text-to-Image avec Qwen-Image-Edit 2509.\n    \n    Architecture Phase 29 validee:\n    - VAELoader + CLIPLoader + UNETLoader (modeles separes)\n    - ModelSamplingAuraFlow (shift=3.0) + CFGNorm (strength=1.0)\n    - TextEncodeQwenImageEdit (encodeur natif Qwen)\n    - KSampler avec scheduler=beta, sampler=euler, cfg=1.0\n    \n    Args:\n        prompt: Description de l'image a generer\n        width/height: Dimensions (multiples de 32, defaut 1024)\n        steps: Nombre d'etapes de diffusion (20 recommande)\n        cfg: Guidance scale (1.0 recommande pour Qwen avec CFGNorm)\n        seed: Graine pour reproductibilite\n    \"\"\"\n    if seed is None:\n        seed = np.random.randint(0, 2**32)\n    \n    return {\n        # Chargement des modeles\n        \"1\": {\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"2\": {\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"3\": {\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        # Configuration modele pour Qwen\n        \"4\": {\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}\n        },\n        \"5\": {\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}\n        },\n        # Encodage du prompt avec TextEncodeQwenImageEdit\n        \"6\": {\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"2\", 0],\n                \"prompt\": prompt,\n                \"vae\": [\"1\", 0]\n            }\n        },\n        # Conditioning negatif (vide pour Qwen)\n        \"7\": {\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"6\", 0]}\n        },\n        # Latent vide (16 canaux pour Qwen)\n        \"8\": {\n            \"class_type\": \"EmptySD3LatentImage\",\n            \"inputs\": {\"width\": width, \"height\": height, \"batch_size\": 1}\n        },\n        # Sampling\n        \"9\": {\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"model\": [\"5\", 0],\n                \"positive\": [\"6\", 0],\n                \"negative\": [\"7\", 0],\n                \"latent_image\": [\"8\", 0],\n                \"seed\": seed,\n                \"steps\": steps,\n                \"cfg\": cfg,\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",\n                \"denoise\": 1.0\n            }\n        },\n        # Decodage VAE\n        \"10\": {\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\"samples\": [\"9\", 0], \"vae\": [\"1\", 0]}\n        },\n        # Sauvegarde\n        \"11\": {\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\"filename_prefix\": \"Qwen2509_t2i\", \"images\": [\"10\", 0]}\n        }\n    }\n\n\ndef create_img2img_workflow(image_name: str, prompt: str, \n                            denoise: float = 0.7, steps: int = 20,\n                            cfg: float = 1.0, seed: int = None) -> Dict:\n    \"\"\"\n    Workflow Image-to-Image pour l'edition avec Qwen.\n    \n    Note: Qwen-Image-Edit 2509 est optimise pour l'edition d'images.\n    Le parametre denoise controle la force de l'edition:\n    - 0.3-0.5: Ajustements subtils\n    - 0.5-0.7: Modifications moderees  \n    - 0.7-0.9: Transformations significatives\n    \n    Args:\n        image_name: Nom du fichier image uploade\n        prompt: Instructions d'edition\n        denoise: Force de l'edition (0.0=rien, 1.0=regeneration complete)\n    \"\"\"\n    if seed is None:\n        seed = np.random.randint(0, 2**32)\n    \n    return {\n        # Chargement image source\n        \"1\": {\n            \"class_type\": \"LoadImage\",\n            \"inputs\": {\"image\": image_name}\n        },\n        # Chargement des modeles\n        \"2\": {\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"3\": {\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"4\": {\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        # Configuration modele\n        \"5\": {\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"4\", 0], \"shift\": 3.0}\n        },\n        \"6\": {\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"5\", 0], \"strength\": 1.0}\n        },\n        # Encodage VAE de l'image source\n        \"7\": {\n            \"class_type\": \"VAEEncode\",\n            \"inputs\": {\"pixels\": [\"1\", 0], \"vae\": [\"2\", 0]}\n        },\n        # Encodage du prompt\n        \"8\": {\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"3\", 0],\n                \"prompt\": prompt,\n                \"vae\": [\"2\", 0]\n            }\n        },\n        # Conditioning negatif\n        \"9\": {\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"8\", 0]}\n        },\n        # Sampling\n        \"10\": {\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"model\": [\"6\", 0],\n                \"positive\": [\"8\", 0],\n                \"negative\": [\"9\", 0],\n                \"latent_image\": [\"7\", 0],\n                \"seed\": seed,\n                \"steps\": steps,\n                \"cfg\": cfg,\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",\n                \"denoise\": denoise\n            }\n        },\n        # Decodage\n        \"11\": {\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\"samples\": [\"10\", 0], \"vae\": [\"2\", 0]}\n        },\n        # Sauvegarde\n        \"12\": {\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\"filename_prefix\": \"Qwen2509_i2i\", \"images\": [\"11\", 0]}\n        }\n    }\n\n\ndef create_inpaint_workflow(image_name: str, mask_name: str, prompt: str,\n                            denoise: float = 0.9, steps: int = 25,\n                            cfg: float = 1.0, seed: int = None) -> Dict:\n    \"\"\"\n    Workflow Inpainting pour Qwen-Image-Edit 2509.\n\n    Note: L'inpainting Qwen utilise SetLatentNoiseMask pour appliquer\n    le masque sur le latent encode. Le masque definit les zones a regenerer.\n\n    Args:\n        image_name: Nom du fichier image uploade\n        mask_name: Nom du fichier masque (blanc = zone a modifier)\n        prompt: Description de ce qui doit remplacer la zone masquee\n        denoise: Force de regeneration (0.8-1.0 recommande pour inpaint)\n    \"\"\"\n    if seed is None:\n        seed = np.random.randint(0, 2**32)\n\n    return {\n        # Chargement image et masque\n        \"1\": {\n            \"class_type\": \"LoadImage\",\n            \"inputs\": {\"image\": image_name}\n        },\n        \"2\": {\n            \"class_type\": \"LoadImage\",\n            \"inputs\": {\"image\": mask_name}\n        },\n        # Chargement des modeles\n        \"3\": {\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"4\": {\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"5\": {\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        # Configuration modele\n        \"6\": {\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"5\", 0], \"shift\": 3.0}\n        },\n        \"7\": {\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"6\", 0], \"strength\": 1.0}\n        },\n        # Encodage VAE de l'image source\n        \"8\": {\n            \"class_type\": \"VAEEncode\",\n            \"inputs\": {\"pixels\": [\"1\", 0], \"vae\": [\"3\", 0]}\n        },\n        # Application du masque sur le latent\n        \"9\": {\n            \"class_type\": \"SetLatentNoiseMask\",\n            \"inputs\": {\n                \"samples\": [\"8\", 0],\n                \"mask\": [\"2\", 0]\n            }\n        },\n        # Encodage du prompt\n        \"10\": {\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"4\", 0],\n                \"prompt\": prompt,\n                \"vae\": [\"3\", 0]\n            }\n        },\n        # Conditioning negatif\n        \"11\": {\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"10\", 0]}\n        },\n        # Sampling avec masque\n        \"12\": {\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"model\": [\"7\", 0],\n                \"positive\": [\"10\", 0],\n                \"negative\": [\"11\", 0],\n                \"latent_image\": [\"9\", 0],\n                \"seed\": seed,\n                \"steps\": steps,\n                \"cfg\": cfg,\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",\n                \"denoise\": denoise\n            }\n        },\n        # Decodage\n        \"13\": {\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\"samples\": [\"12\", 0], \"vae\": [\"3\", 0]}\n        },\n        # Sauvegarde\n        \"14\": {\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\"filename_prefix\": \"Qwen2509_inpaint\", \"images\": [\"13\", 0]}\n        }\n    }\n\n\nprint(\"Workflows Qwen-Image-Edit 2509 definis (Architecture Phase 29)\")\nprint(\"   - create_text2img_workflow() -> node output: '11'\")\nprint(\"   - create_img2img_workflow() -> node output: '12'\")\nprint(\"   - create_inpaint_workflow() -> node output: '14'\")\nprint(\"\\nNotes importantes:\")\nprint(\"   - CFG=1.0 recommande (CFGNorm gere l'amplification)\")\nprint(\"   - Scheduler 'beta' optimise pour Qwen\")\nprint(\"   - 20 steps suffisent pour de bons resultats\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 4. GÃ©nÃ©ration Text-to-Image\n",
    "\n",
    "CommenÃ§ons par gÃ©nÃ©rer une image de base que nous Ã©diterons ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 4. TEXT-TO-IMAGE: Creation d'une image de base\n# =============================================================================\n\n# Prompt creatif pour une scene editable\nprompt_base = \"\"\"\nA cozy coffee shop interior, wooden tables, warm lighting,\nlarge window with rain outside, vintage aesthetic,\nempty cup on table, potted plant, high quality photography\n\"\"\".strip()\n\n# Creer le workflow (Architecture Phase 29)\nworkflow_t2i = create_text2img_workflow(\n    prompt=prompt_base,\n    width=1024,\n    height=768,\n    steps=20,      # 20 steps optimaux pour Qwen\n    cfg=1.0,       # CFG=1.0 avec CFGNorm\n    seed=42        # Fixe pour reproductibilite\n)\n\n# Executer (output_node=\"11\" pour le nouveau workflow)\nprint(\"\\nGeneration de l'image de base...\")\nprint(f\"Prompt: {prompt_base[:80]}...\")\n\nimages_t2i = client.execute_and_get_images(workflow_t2i, output_node=\"11\")\n\nif images_t2i:\n    base_image = images_t2i[0]\n    \n    # Affichage\n    plt.figure(figsize=(12, 9))\n    plt.imshow(base_image)\n    plt.title(\"Image de Base - Coffee Shop (Qwen 2509)\", fontsize=14)\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nDimensions: {base_image.size}\")\nelse:\n    print(\"Erreur de generation - verifiez que les modeles Qwen sont presents\")\n    print(\"Modeles requis dans ComfyUI/models/:\")\n    print(\"  - vae/qwen_image_vae.safetensors\")\n    print(\"  - text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors\")\n    print(\"  - diffusion_models/qwen_image_edit_2509_fp8_e4m3fn.safetensors\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 5. Ã‰dition Image-to-Image\n",
    "\n",
    "Explorons diffÃ©rents niveaux de `denoise` pour comprendre son impact sur l'Ã©dition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 5. IMAGE-TO-IMAGE: Analyse comparative du parametre denoise\n# =============================================================================\n\n# Upload de l'image de base\nif 'base_image' in dir():\n    uploaded_name = client.upload_image(base_image, \"base_coffee_shop.png\")\n    print(f\"Image uploadee: {uploaded_name}\")\n    \n    # Test avec differents niveaux de denoise\n    denoise_levels = [0.3, 0.5, 0.7, 0.9]\n    edit_prompt = \"Same scene but with snow falling outside the window, winter atmosphere\"\n    \n    results = []\n    seed_fixed = 12345  # Meme seed pour comparer\n    \n    print(f\"\\nEdition avec prompt: '{edit_prompt}'\")\n    print(\"\\nComparaison des niveaux de denoise:\")\n    \n    for denoise in denoise_levels:\n        print(f\"\\n--- Denoise = {denoise} ---\")\n        \n        workflow = create_img2img_workflow(\n            image_name=uploaded_name,\n            prompt=edit_prompt,\n            denoise=denoise,\n            steps=20,\n            cfg=1.0,\n            seed=seed_fixed\n        )\n        \n        # output_node=\"12\" pour img2img\n        images = client.execute_and_get_images(workflow, output_node=\"12\", verbose=False)\n        if images:\n            results.append((denoise, images[0]))\n            print(f\"   Genere\")\n    \n    # Affichage comparatif\n    if results:\n        fig, axes = plt.subplots(1, len(results) + 1, figsize=(20, 5))\n        \n        # Image originale\n        axes[0].imshow(base_image)\n        axes[0].set_title(\"Original\", fontsize=12)\n        axes[0].axis('off')\n        \n        # Resultats\n        for i, (denoise, img) in enumerate(results):\n            axes[i+1].imshow(img)\n            axes[i+1].set_title(f\"Denoise = {denoise}\", fontsize=12)\n            axes[i+1].axis('off')\n        \n        plt.suptitle(\"Impact du parametre Denoise sur l'edition\", fontsize=14)\n        plt.tight_layout()\n        plt.show()\n        \n        print(\"\\nObservations:\")\n        print(\"   0.3: Changements subtils, structure tres preservee\")\n        print(\"   0.5: Modifications visibles, bonne balance\")\n        print(\"   0.7: Transformations significatives\")\n        print(\"   0.9: Quasi-regeneration, peu de l'original conserve\")\nelse:\n    print(\"Executez d'abord la cellule Text-to-Image\")"
  },
  {
   "cell_type": "markdown",
   "id": "dadagfpmkwo",
   "source": "### Interpretation : Impact du parametre Denoise\n\nLes resultats montrent l'effet du parametre `denoise` sur l'intensite de l'edition :\n\n| Denoise | Observation | Cas d'usage |\n|---------|-------------|-------------|\n| **0.3** | Changements subtils, structure tres preservee | Retouche photo, correction couleur |\n| **0.5** | Modifications visibles, bonne balance | Style transfer modere |\n| **0.7** | Transformations significatives | Edition importante |\n| **0.9** | Quasi-regeneration, peu de l'original conserve | Reconstruction complete |\n\n> **Note technique** : Le parametre `denoise` controle la quantite de bruit ajoutee lors du processus de diffusion. Plus il est eleve, plus le modele s'eloigne de l'image originale pour creer quelque chose de nouveau.\n\n**Choix strategique** : Pour une edition qui preserve la composition tout en changeant l'ambiance, `denoise=0.5` offre le meilleur compromis.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 6. Inpainting AvancÃ© avec Masque PersonnalisÃ©\n",
    "\n",
    "L'inpainting permet de modifier uniquement certaines zones de l'image. Nous allons crÃ©er un masque programmatique pour remplacer un Ã©lÃ©ment spÃ©cifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. INPAINTING: Ã‰dition localisÃ©e avec masque\n",
    "# =============================================================================\n",
    "\n",
    "def create_rectangular_mask(width: int, height: int, \n",
    "                            x1: int, y1: int, x2: int, y2: int,\n",
    "                            feather: int = 10) -> Image.Image:\n",
    "    \"\"\"\n",
    "    CrÃ©e un masque rectangulaire avec bords adoucis.\n",
    "    \n",
    "    Args:\n",
    "        width, height: Dimensions du masque\n",
    "        x1, y1, x2, y2: CoordonnÃ©es du rectangle (zone Ã  modifier)\n",
    "        feather: Adoucissement des bords en pixels\n",
    "    \n",
    "    Returns:\n",
    "        Image grayscale (blanc = zone Ã  modifier)\n",
    "    \"\"\"\n",
    "    mask = Image.new('L', (width, height), 0)  # Noir = prÃ©server\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    draw.rectangle([x1, y1, x2, y2], fill=255)  # Blanc = modifier\n",
    "    \n",
    "    # Adoucissement optionnel (blur simple)\n",
    "    if feather > 0:\n",
    "        from PIL import ImageFilter\n",
    "        mask = mask.filter(ImageFilter.GaussianBlur(radius=feather))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_circular_mask(width: int, height: int,\n",
    "                         cx: int, cy: int, radius: int,\n",
    "                         feather: int = 15) -> Image.Image:\n",
    "    \"\"\"\n",
    "    CrÃ©e un masque circulaire pour l'inpainting.\n",
    "    \"\"\"\n",
    "    mask = Image.new('L', (width, height), 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    draw.ellipse([cx-radius, cy-radius, cx+radius, cy+radius], fill=255)\n",
    "    \n",
    "    if feather > 0:\n",
    "        from PIL import ImageFilter\n",
    "        mask = mask.filter(ImageFilter.GaussianBlur(radius=feather))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "# Exemple: Remplacer la tasse sur la table\n",
    "if 'base_image' in dir():\n",
    "    w, h = base_image.size\n",
    "    \n",
    "    # CrÃ©er un masque pour le centre-bas de l'image (oÃ¹ la table/tasse serait)\n",
    "    mask = create_rectangular_mask(\n",
    "        w, h,\n",
    "        x1=int(w*0.35), y1=int(h*0.55),\n",
    "        x2=int(w*0.65), y2=int(h*0.85),\n",
    "        feather=20\n",
    "    )\n",
    "    \n",
    "    # Upload du masque\n",
    "    mask_name = client.upload_mask(mask, \"edit_mask.png\")\n",
    "    print(f\"âœ… Masque uploadÃ©: {mask_name}\")\n",
    "    \n",
    "    # Visualisation du masque\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(base_image)\n",
    "    axes[0].set_title(\"Image Originale\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(mask, cmap='gray')\n",
    "    axes[1].set_title(\"Masque (blanc = zone Ã  modifier)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = base_image.copy().convert('RGBA')\n",
    "    mask_rgba = Image.new('RGBA', overlay.size, (255, 0, 0, 0))\n",
    "    mask_draw = ImageDraw.Draw(mask_rgba)\n",
    "    # Convertir le masque en overlay rouge semi-transparent\n",
    "    mask_array = np.array(mask)\n",
    "    red_overlay = np.zeros((h, w, 4), dtype=np.uint8)\n",
    "    red_overlay[:,:,0] = 255  # Rouge\n",
    "    red_overlay[:,:,3] = (mask_array * 0.5).astype(np.uint8)  # Alpha\n",
    "    overlay_img = Image.alpha_composite(overlay, Image.fromarray(red_overlay))\n",
    "    \n",
    "    axes[2].imshow(overlay_img)\n",
    "    axes[2].set_title(\"Zone d'Ã©dition (rouge)\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ Image de base non disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 6b. EXECUTION DE L'INPAINTING\n# =============================================================================\n\nif 'uploaded_name' in dir() and 'mask_name' in dir():\n    # Prompt pour la zone masquee\n    inpaint_prompt = \"A beautiful laptop with glowing screen, modern design, on wooden table\"\n    \n    print(f\"\\nInpainting: '{inpaint_prompt}'\")\n    \n    workflow_inpaint = create_inpaint_workflow(\n        image_name=uploaded_name,\n        mask_name=mask_name,\n        prompt=inpaint_prompt,\n        denoise=0.95,  # Haut pour remplacer completement\n        steps=25,\n        cfg=1.0,       # CFG=1.0 avec CFGNorm (Phase 29)\n        seed=99999\n    )\n    \n    # output_node=\"14\" pour inpainting workflow\n    images_inpaint = client.execute_and_get_images(workflow_inpaint, output_node=\"14\")\n    \n    if images_inpaint:\n        inpainted_image = images_inpaint[0]\n        \n        # Comparaison avant/apres\n        fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n        \n        axes[0].imshow(base_image)\n        axes[0].set_title(\"Avant Inpainting\", fontsize=14)\n        axes[0].axis('off')\n        \n        axes[1].imshow(inpainted_image)\n        axes[1].set_title(\"Apres Inpainting\", fontsize=14)\n        axes[1].axis('off')\n        \n        plt.suptitle(f\"Inpainting: '{inpaint_prompt}'\", fontsize=12)\n        plt.tight_layout()\n        plt.show()\nelse:\n    print(\"Prerequis manquants (image ou masque)\")"
  },
  {
   "cell_type": "markdown",
   "id": "gjiyry24wsw",
   "source": "### Interpretation : Resultats de l'Inpainting\n\nL'inpainting a remplace la zone masquee par un nouvel element :\n\n| Aspect | Observation |\n|--------|-------------|\n| **CohÃ©rence spatiale** | Le laptop s'integre naturellement dans la scene |\n| **QualitÃ© des bords** | La transition avec l'arriere-plan est fluide grace au masque adouci |\n| **Consistance lumineuse** | L'eclairage du laptop correspond a l'ambiance du coffee shop |\n\n> **Point cle** : Le succes de l'inpainting depend de la qualite du masque. Un masque avec bords adoucis (feathered) permet une transition plus naturelle entre la zone modifiee et l'originale.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 7. Batch Processing: GÃ©nÃ©ration Multiple\n",
    "\n",
    "Pour l'efficacitÃ©, gÃ©nÃ©rons plusieurs variations en parallÃ¨le avec diffÃ©rents prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 7. BATCH PROCESSING: Variations multiples\n# =============================================================================\n\ndef batch_generate(prompts: List[str], base_seed: int = 1000, **kwargs) -> List[Tuple[str, Image.Image]]:\n    \"\"\"\n    Genere plusieurs images a partir d'une liste de prompts.\n    \n    Args:\n        prompts: Liste de descriptions\n        base_seed: Seed de depart (incremente pour chaque image)\n        **kwargs: Arguments passes a create_text2img_workflow\n    \n    Returns:\n        Liste de tuples (prompt, image)\n    \"\"\"\n    results = []\n    \n    for i, prompt in enumerate(prompts):\n        print(f\"\\n[{i+1}/{len(prompts)}] Generation...\")\n        print(f\"   Prompt: {prompt[:60]}...\")\n        \n        workflow = create_text2img_workflow(\n            prompt=prompt,\n            seed=base_seed + i,\n            **kwargs\n        )\n        \n        # output_node=\"11\" pour t2i\n        images = client.execute_and_get_images(workflow, output_node=\"11\", verbose=False)\n        if images:\n            results.append((prompt, images[0]))\n            print(f\"   Succes\")\n        else:\n            print(f\"   Echec\")\n    \n    return results\n\n\n# Generation de variations thematiques\nvariation_prompts = [\n    \"A futuristic cityscape at sunset, flying cars, neon lights, cyberpunk style\",\n    \"An ancient Japanese temple in autumn, red maple leaves, misty mountains\",\n    \"An underwater coral reef, tropical fish, sunlight rays, crystal clear water\",\n    \"A cozy library interior, tall bookshelves, reading nook, warm lamp light\"\n]\n\nprint(\"\\nBatch Generation - 4 Themes\")\nprint(\"=\" * 40)\n\nbatch_results = batch_generate(\n    variation_prompts,\n    base_seed=2024,\n    width=768,\n    height=768,\n    steps=20,\n    cfg=1.0  # CFG=1.0 avec CFGNorm\n)\n\n# Affichage grille\nif batch_results:\n    n = len(batch_results)\n    cols = 2\n    rows = (n + cols - 1) // cols\n    \n    fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n    axes = axes.flatten()\n    \n    for i, (prompt, img) in enumerate(batch_results):\n        axes[i].imshow(img)\n        # Titre court\n        short_title = prompt.split(',')[0][:40]\n        axes[i].set_title(short_title, fontsize=10)\n        axes[i].axis('off')\n    \n    # Masquer les axes vides\n    for i in range(len(batch_results), len(axes)):\n        axes[i].axis('off')\n    \n    plt.suptitle(\"Batch Generation - Variations Thematiques\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\n{len(batch_results)} images generees avec succes\")"
  },
  {
   "cell_type": "markdown",
   "id": "8txuf4lm17g",
   "source": "### Interpretation : Batch Processing\n\nLa generation en lot a permis de creer 4 variations thematiques distinctes :\n\n| Theme | Prompt cle | Resultat |\n|-------|-----------|----------|\n| **Cyberpunk** | \"futuristic cityscape\", \"neon lights\" | Ambiance urbaine futuriste |\n| **Japonais** | \"ancient temple\", \"autumn\", \"maple leaves\" | Scene traditionnelle paisible |\n| **Sous-marin** | \"coral reef\", \"tropical fish\", \"sunlight\" | Eclairage aquatique realise |\n| **Bibliotheque** | \"library interior\", \"warm lamp light\" | Atmosphere intime et studieuse |\n\n> **Avantage du batch** : Le traitement par lots permet d'explorer plusieurs directions creatives en un seul passage, optimisant ainsi le temps de creation.\n\n**Performance** : Avec 4 images generes en sequence, le temps total reste raisonnable car chaque generation utilise la meme architecture de modeles (pas de rechargement necessaire).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 8. Analyse Comparative: CFG Scale\n",
    "\n",
    "Le paramÃ¨tre `cfg` (Classifier-Free Guidance) contrÃ´le l'adhÃ©rence au prompt. Explorons son impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 8. ANALYSE CFG: Impact du Guidance Scale avec CFGNorm\n# =============================================================================\n# NOTE IMPORTANTE (Phase 29):\n# L'architecture Qwen utilise CFGNorm qui normalise le guidance.\n# Avec CFGNorm, CFG=1.0 est optimal car la normalisation gere l'amplification.\n# Cette cellule explore differentes valeurs CFG a titre educatif pour\n# comprendre le comportement du modele.\n\ntest_prompt = \"A majestic dragon breathing fire, fantasy art, highly detailed scales, dramatic lighting\"\n\n# Valeurs CFG a tester (1.0 est recommande avec CFGNorm)\ncfg_values = [1.0, 2.0, 4.0, 7.0]\nfixed_seed = 7777\n\nprint(f\"\\nAnalyse CFG Scale (avec CFGNorm)\")\nprint(f\"Prompt: '{test_prompt[:50]}...'\")\nprint(f\"Valeurs testees: {cfg_values}\")\nprint(\"\\nNote: CFG=1.0 est recommande avec CFGNorm (architecture Phase 29)\")\n\ncfg_results = []\n\nfor cfg in cfg_values:\n    print(f\"\\n--- CFG = {cfg} ---\")\n    \n    workflow = create_text2img_workflow(\n        prompt=test_prompt,\n        width=768,\n        height=768,\n        steps=20,\n        cfg=cfg,\n        seed=fixed_seed\n    )\n    \n    # output_node=\"11\" pour t2i\n    images = client.execute_and_get_images(workflow, output_node=\"11\", verbose=False)\n    if images:\n        cfg_results.append((cfg, images[0]))\n        print(f\"   Genere\")\n\n# Affichage comparatif\nif cfg_results:\n    fig, axes = plt.subplots(1, len(cfg_results), figsize=(16, 5))\n    \n    for i, (cfg, img) in enumerate(cfg_results):\n        axes[i].imshow(img)\n        title = f\"CFG = {cfg}\"\n        if cfg == 1.0:\n            title += \" (recommande)\"\n        axes[i].set_title(title, fontsize=12)\n        axes[i].axis('off')\n    \n    plt.suptitle(\"Impact du CFG avec CFGNorm (Architecture Phase 29)\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nObservations CFG avec CFGNorm:\")\n    print(\"   1.0: Valeur optimale - CFGNorm gere l'amplification automatiquement\")\n    print(\"   2.0: Leger renforcement du prompt\")\n    print(\"   4.0: Adherence plus forte, details accentues\")\n    print(\"   7.0: Peut introduire des artefacts avec CFGNorm\")\n    print(\"\\nRecommandation: Utiliser CFG=1.0 avec l'architecture Phase 29\")"
  },
  {
   "cell_type": "markdown",
   "id": "mzzz5x02in9",
   "source": "### Interpretation : Impact du CFG avec CFGNorm\n\nL'analyse des differentes valeurs CFG avec l'architecture Phase 29 revele des comportements specifiques :\n\n| CFG | Comportement | Qualite resultats |\n|-----|-------------|-------------------|\n| **1.0** | Valeur optimale avec CFGNorm | Equilibre parfait entre creativite et adherence au prompt |\n| **2.0** | Leger renforcement du prompt | Details un peu plus prononces sans artefacts |\n| **4.0** | Adherence forte au prompt | Risque de surcontrainte, details forces |\n| **7.0** | Trop eleve pour CFGNorm | Artefacts possibles, degradation de la qualite |\n\n> **Comprehension de CFGNorm** : Le node CFGNorm normalise automatiquement le signal de guidance. C'est pourquoi CFG=1.0 est optimal - la normalisation gere deja l'amplification du prompt.\n\n**Recommandation definitive** : Avec l'architecture Phase 29 (Qwen Image Edit 2509), utilisez toujours `cfg=1.0` pour les meilleurs resultats. Des valeurs plus elevees n'apportent pas d'amelioration significative et peuvent reduire la qualite.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 9. Exercices Pratiques\n",
    "\n",
    "### Exercice 1: Ã‰dition de Style\n",
    "Prenez l'image de base du coffee shop et appliquez diffÃ©rents styles artistiques (impressionniste, anime, rÃ©aliste) en utilisant img2img avec un denoise de 0.6.\n",
    "\n",
    "### Exercice 2: Inpainting CrÃ©atif\n",
    "CrÃ©ez un masque circulaire au centre de l'image et remplacez cette zone par un personnage de votre choix.\n",
    "\n",
    "### Exercice 3: Exploration des Schedulers\n",
    "Modifiez le workflow pour tester diffÃ©rents schedulers (`normal`, `karras`, `exponential`) et comparez les rÃ©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9. ESPACE D'EXERCICES\n",
    "# =============================================================================\n",
    "\n",
    "# Exercice 1: Style Transfer\n",
    "# DÃ©commentez et complÃ©tez:\n",
    "\n",
    "# style_prompts = [\n",
    "#     \"Same scene, impressionist painting style, visible brushstrokes\",\n",
    "#     \"Same scene, anime style, vibrant colors, Studio Ghibli\",\n",
    "#     \"Same scene, photorealistic, DSLR quality, 8k resolution\"\n",
    "# ]\n",
    "# \n",
    "# for style in style_prompts:\n",
    "#     workflow = create_img2img_workflow(\n",
    "#         image_name=uploaded_name,\n",
    "#         prompt=style,\n",
    "#         denoise=0.6\n",
    "#     )\n",
    "#     # ... gÃ©nÃ©rer et afficher\n",
    "\n",
    "print(\"ğŸ“ Espace d'exercices - DÃ©commentez le code ci-dessus pour commencer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "## 10. Recapitulatif et Points Cles\n\n### Architecture Phase 29 - Qwen-Image-Edit 2509\n\nCette architecture utilise les composants natifs ComfyUI suivants :\n\n| Composant | Node ComfyUI | Fichier Modele |\n|-----------|--------------|----------------|\n| VAE | VAELoader | qwen_image_vae.safetensors (243MB) |\n| Text Encoder | CLIPLoader (type=sd3) | qwen_2.5_vl_7b_fp8_scaled.safetensors (8.8GB) |\n| Diffusion | UNETLoader (fp8_e4m3fn) | qwen_image_edit_2509_fp8_e4m3fn.safetensors (20GB) |\n\n### Parametres Essentiels (Architecture Phase 29)\n\n| Parametre | Plage | Recommande | Impact |\n|-----------|-------|------------|--------|\n| `steps` | 15-30 | 20 | Qualite vs. Vitesse |\n| `cfg` | 1-4 | **1.0** | CFGNorm gere l'amplification |\n| `denoise` | 0-1 | 0.5-0.7 | Force de l'edition |\n| `shift` | 2-4 | 3.0 | ModelSamplingAuraFlow |\n| `scheduler` | - | **beta** | Optimise pour Qwen |\n| `sampler` | - | **euler** | Stable et rapide |\n\n### Bonnes Pratiques\n\n1. **Toujours utiliser CFG=1.0** avec l'architecture Phase 29 (CFGNorm normalise automatiquement)\n2. **Scheduler 'beta'** est optimise pour les modeles Qwen\n3. **20 steps** suffisent pour de bons resultats (qualite/vitesse optimal)\n4. **Pour l'inpainting, denoise >= 0.8** pour un remplacement complet\n5. **Utiliser seed fixe** pour comparer les parametres et reproduire les resultats\n\n### Nodes Requis\n\n```\nTextEncodeQwenImageEdit   # Encodeur texte natif Qwen\nModelSamplingAuraFlow     # Configuration sampling (shift=3.0)\nCFGNorm                   # Normalisation CFG (strength=1.0)\nEmptySD3LatentImage       # Latent 16 canaux pour Qwen\nConditioningZeroOut       # Conditioning negatif vide\n```\n\n### Ressources\n\n- [Documentation ComfyUI](https://docs.comfy.org/)\n- [Qwen-VL Papers](https://arxiv.org/abs/2308.12966)\n- [Modeles Comfy-Org](https://huggingface.co/Comfy-Org/Qwen-Image-Edit_ComfyUI)\n- Notebook suivant: **02-2-FLUX-1-Advanced-Generation**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIN DU NOTEBOOK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"   âœ… Notebook Qwen-Image-Edit 2509 ComplÃ©tÃ©\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“… TerminÃ©: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nğŸ“š Concepts couverts:\")\n",
    "print(\"   â€¢ Text-to-Image avec Qwen 2509\")\n",
    "print(\"   â€¢ Image-to-Image et analyse du denoise\")\n",
    "print(\"   â€¢ Inpainting avec masques personnalisÃ©s\")\n",
    "print(\"   â€¢ Batch processing\")\n",
    "print(\"   â€¢ Analyse comparative CFG\")\n",
    "print(\"\\nâ¡ï¸  Prochain notebook: 02-2-FLUX-1-Advanced-Generation.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4g9g5qfo8ux",
   "source": "## Conclusion\n\n### Synthese des apprentissages\n\nCe notebook a explore les capacites avancees de Qwen-Image-Edit 2509 avec l'architecture Phase 29 :\n\n| Concept | Retenu |\n|---------|--------|\n| **Architecture Phase 29** | Loaders separes (VAE, CLIP, UNET) + CFGNorm |\n| **Denoise** | Controle l'intensite de l'edition (0.3-0.9) |\n| **Inpainting** | Modification locale avec masques personnalises |\n| **Batch processing** | Generation efficace de multiples variations |\n| **CFG avec CFGNorm** | CFG=1.0 est optimal pour cette architecture |\n\n### Tableau recapitulatif des parametres\n\n| Parametre | Valeur recommandee | Usage |\n|-----------|-------------------|-------|\n| `steps` | 20 | Equilibre qualite/vitesse |\n| `cfg` | 1.0 | Optimal avec CFGNorm |\n| `denoise` (img2img) | 0.5-0.7 | Edition preservee |\n| `denoise` (inpaint) | 0.9+ | Remplacement complet |\n| `scheduler` | beta | Optimise pour Qwen |\n| `sampler` | euler | Stable et rapide |\n\n### Prochaines etapes\n\n1. Experimentez avec differents types de masques (circulaires, irreguliers)\n2. Combinez plusieurs editions en sequence pour des transformations complexes\n3. Explorez le notebook suivant : **02-2-FLUX-1-Advanced-Generation**\n\n---\n\n**Navigation** : [Index](../../README.md) | [<< Precedent](../01-Foundation/01-5-Qwen-Image-Edit.ipynb) | [Suivant >>](02-2-FLUX-1-Advanced-Generation.ipynb)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}