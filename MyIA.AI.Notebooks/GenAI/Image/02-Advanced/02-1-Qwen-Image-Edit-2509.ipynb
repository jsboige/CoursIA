{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Qwen Image Edit 2509 - √âdition Avanc√©e d'Images\n",
    "\n",
    "**Module :** 02-Images-Advanced  \n",
    "**Niveau :** Interm√©diaire/Avanc√©  \n",
    "**Dur√©e estim√©e :** 45 minutes  \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ce notebook explore les capacit√©s avanc√©es de **Qwen-Image-Edit 2509** (version Septembre 2025), un mod√®le d'√©dition d'images de pointe int√©gr√© via ComfyUI. Par rapport au notebook d'introduction (01-5), nous abordons ici :\n",
    "\n",
    "- **√âdition pr√©cise de texte** dans les images\n",
    "- **Inpainting avanc√©** avec masques personnalis√©s\n",
    "- **Workflows multi-√©tapes** pour des transformations complexes\n",
    "- **Batch processing** pour l'efficacit√©\n",
    "- **Analyse comparative** des param√®tres\n",
    "\n",
    "### Architecture Qwen-Image-Edit 2509\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              Qwen-Image-Edit 2509 Pipeline              ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Image Input ‚Üí Qwen2.5-VL Encoder ‚Üí Diffusion Model    ‚îÇ\n",
    "‚îÇ       ‚Üì              ‚Üì                    ‚Üì             ‚îÇ\n",
    "‚îÇ  [Tokenizer]    [16-ch VAE]        [UNet 1024¬≤]        ‚îÇ\n",
    "‚îÇ       ‚Üì              ‚Üì                    ‚Üì             ‚îÇ\n",
    "‚îÇ   Text Prompt ‚Üí Cross-Attention ‚Üí Latent Space         ‚îÇ\n",
    "‚îÇ                                        ‚Üì                ‚îÇ\n",
    "‚îÇ                               VAE Decode ‚Üí Output      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Pr√©requis\n",
    "\n",
    "- Module 00-GenAI-Environment compl√©t√©\n",
    "- Service `comfyui-qwen` actif (`docker compose up -d`)\n",
    "- Notebook 01-5-Qwen-Image-Edit termin√© (concepts de base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 1. CONFIGURATION ET IMPORTS\n# =============================================================================\n\nimport os\nimport sys\nimport json\nimport uuid\nimport time\nimport base64\nimport requests\nfrom io import BytesIO\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Dict, List, Tuple, Any\n\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\nimport matplotlib.pyplot as plt\n\n# Chargement variables d'environnement\n# Recherche du .env en remontant l'arborescence (plus robuste que chemins relatifs)\nfrom dotenv import load_dotenv\n\ncurrent_path = Path.cwd()\nfound_env = False\nfor _ in range(5):  # Remonter jusqu'a 5 niveaux\n    env_path = current_path / '.env'\n    if env_path.exists():\n        load_dotenv(env_path)\n        found_env = True\n        break\n    current_path = current_path.parent\n\nif not found_env:\n    # Fallback: essayer des chemins relatifs connus\n    for fallback in [\"../../.env\", \"../.env\", \"../00-GenAI-Environment/.env\"]:\n        if Path(fallback).exists():\n            load_dotenv(fallback)\n            found_env = True\n            break\n\n# Configuration ComfyUI\n# URL par defaut: service myia.io pour etudiants\nCOMFYUI_URL = os.getenv(\"COMFYUI_API_URL\", \"https://qwen-image-edit.myia.io\")\n# Support des deux noms de variable pour le token\nCOMFYUI_TOKEN = os.getenv(\"COMFYUI_AUTH_TOKEN\") or os.getenv(\"COMFYUI_API_TOKEN\")\nCLIENT_ID = str(uuid.uuid4())\n\n# Validation\nif not COMFYUI_TOKEN:\n    raise ValueError(\"COMFYUI_AUTH_TOKEN ou COMFYUI_API_TOKEN manquant dans .env\")\n\nprint(\"Qwen-Image-Edit 2509 - Edition Avancee\")\nprint(f\"\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"API URL: {COMFYUI_URL}\")\nprint(f\"Token: {'Configure' if COMFYUI_TOKEN else 'Manquant'}\")\nif found_env:\n    print(f\".env: charge\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. CLIENT COMFYUI AVANC√â\n",
    "# =============================================================================\n",
    "\n",
    "class QwenImageEditClient:\n",
    "    \"\"\"\n",
    "    Client avanc√© pour Qwen-Image-Edit via ComfyUI.\n",
    "    Supporte l'authentification, le batch processing et les workflows multi-√©tapes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = COMFYUI_URL, auth_token: str = COMFYUI_TOKEN):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.client_id = str(uuid.uuid4())\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        if auth_token:\n",
    "            self.session.headers.update({\"Authorization\": f\"Bearer {auth_token}\"})\n",
    "    \n",
    "    def check_health(self) -> Dict[str, Any]:\n",
    "        \"\"\"V√©rifie la sant√© du service ComfyUI.\"\"\"\n",
    "        try:\n",
    "            resp = self.session.get(f\"{self.base_url}/system_stats\", timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                stats = resp.json()\n",
    "                return {\n",
    "                    \"status\": \"healthy\",\n",
    "                    \"vram_free\": stats.get(\"devices\", [{}])[0].get(\"vram_free\", 0) / 1e9,\n",
    "                    \"vram_total\": stats.get(\"devices\", [{}])[0].get(\"vram_total\", 0) / 1e9\n",
    "                }\n",
    "            return {\"status\": \"error\", \"code\": resp.status_code}\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"unreachable\", \"error\": str(e)}\n",
    "    \n",
    "    def upload_image(self, image: Image.Image, name: str = \"input.png\") -> str:\n",
    "        \"\"\"Upload une image vers ComfyUI.\"\"\"\n",
    "        buffer = BytesIO()\n",
    "        image.save(buffer, format=\"PNG\")\n",
    "        buffer.seek(0)\n",
    "        \n",
    "        files = {\"image\": (name, buffer, \"image/png\")}\n",
    "        data = {\"overwrite\": \"true\"}\n",
    "        \n",
    "        resp = self.session.post(f\"{self.base_url}/upload/image\", files=files, data=data)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.json().get(\"name\", name)\n",
    "        raise Exception(f\"Upload failed: {resp.text}\")\n",
    "    \n",
    "    def upload_mask(self, mask: Image.Image, name: str = \"mask.png\") -> str:\n",
    "        \"\"\"Upload un masque pour l'inpainting.\"\"\"\n",
    "        # Convertir en grayscale si n√©cessaire\n",
    "        if mask.mode != 'L':\n",
    "            mask = mask.convert('L')\n",
    "        return self.upload_image(mask, name)\n",
    "    \n",
    "    def queue_workflow(self, workflow: Dict) -> str:\n",
    "        \"\"\"Soumet un workflow et retourne le prompt_id.\"\"\"\n",
    "        payload = {\"prompt\": workflow, \"client_id\": self.client_id}\n",
    "        resp = self.session.post(f\"{self.base_url}/prompt\", json=payload)\n",
    "        \n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Queue failed: {resp.text}\")\n",
    "        return resp.json()[\"prompt_id\"]\n",
    "    \n",
    "    def wait_for_completion(self, prompt_id: str, timeout: int = 120) -> Dict:\n",
    "        \"\"\"Attend la fin d'un workflow avec timeout.\"\"\"\n",
    "        start = time.time()\n",
    "        while time.time() - start < timeout:\n",
    "            resp = self.session.get(f\"{self.base_url}/history/{prompt_id}\")\n",
    "            if resp.status_code == 200:\n",
    "                history = resp.json()\n",
    "                if prompt_id in history:\n",
    "                    return history[prompt_id]\n",
    "            time.sleep(1)\n",
    "        raise TimeoutError(f\"Workflow {prompt_id} timeout after {timeout}s\")\n",
    "    \n",
    "    def get_image(self, filename: str, subfolder: str = \"\", img_type: str = \"output\") -> Image.Image:\n",
    "        \"\"\"R√©cup√®re une image g√©n√©r√©e.\"\"\"\n",
    "        params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": img_type}\n",
    "        resp = self.session.get(f\"{self.base_url}/view\", params=params)\n",
    "        if resp.status_code == 200:\n",
    "            return Image.open(BytesIO(resp.content))\n",
    "        raise Exception(f\"Failed to get image: {resp.text}\")\n",
    "    \n",
    "    def execute_and_get_images(self, workflow: Dict, output_node: str = \"9\", \n",
    "                                timeout: int = 120, verbose: bool = True) -> List[Image.Image]:\n",
    "        \"\"\"Ex√©cute un workflow et retourne les images g√©n√©r√©es.\"\"\"\n",
    "        if verbose:\n",
    "            print(\"üöÄ Soumission du workflow...\")\n",
    "        \n",
    "        prompt_id = self.queue_workflow(workflow)\n",
    "        if verbose:\n",
    "            print(f\"üìã ID: {prompt_id}\")\n",
    "            print(\"‚è≥ G√©n√©ration en cours...\", end=\"\", flush=True)\n",
    "        \n",
    "        result = self.wait_for_completion(prompt_id, timeout)\n",
    "        if verbose:\n",
    "            print(\" ‚úÖ\")\n",
    "        \n",
    "        # Extraire les images\n",
    "        images = []\n",
    "        if \"outputs\" in result and output_node in result[\"outputs\"]:\n",
    "            for img_data in result[\"outputs\"][output_node].get(\"images\", []):\n",
    "                img = self.get_image(\n",
    "                    img_data[\"filename\"],\n",
    "                    img_data.get(\"subfolder\", \"\"),\n",
    "                    img_data.get(\"type\", \"output\")\n",
    "                )\n",
    "                images.append(img)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üñºÔ∏è {len(images)} image(s) r√©cup√©r√©e(s)\")\n",
    "        return images\n",
    "\n",
    "# Instanciation du client\n",
    "client = QwenImageEditClient()\n",
    "\n",
    "# Test de connexion\n",
    "health = client.check_health()\n",
    "print(f\"\\nüè• √âtat du service: {health['status']}\")\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"   VRAM: {health['vram_free']:.1f} / {health['vram_total']:.1f} GB libre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 3. WORKFLOWS QWEN-IMAGE-EDIT 2509 - ARCHITECTURE PHASE 29\n# =============================================================================\n# Ces workflows utilisent l'architecture native ComfyUI validee Phase 29\n# avec les modeles FP8 officiels Comfy-Org\n\ndef create_text2img_workflow(prompt: str, \n                              width: int = 1024, height: int = 1024,\n                              steps: int = 20, cfg: float = 1.0,\n                              seed: int = None) -> Dict:\n    \"\"\"\n    Workflow Text-to-Image avec Qwen-Image-Edit 2509.\n    \n    Architecture Phase 29 validee:\n    - VAELoader + CLIPLoader + UNETLoader (modeles separes)\n    - ModelSamplingAuraFlow (shift=3.0) + CFGNorm (strength=1.0)\n    - TextEncodeQwenImageEdit (encodeur natif Qwen)\n    - KSampler avec scheduler=beta, sampler=euler, cfg=1.0\n    \n    Args:\n        prompt: Description de l'image a generer\n        width/height: Dimensions (multiples de 32, defaut 1024)\n        steps: Nombre d'etapes de diffusion (20 recommande)\n        cfg: Guidance scale (1.0 recommande pour Qwen avec CFGNorm)\n        seed: Graine pour reproductibilite\n    \"\"\"\n    if seed is None:\n        seed = np.random.randint(0, 2**32)\n    \n    return {\n        # Chargement des modeles\n        \"1\": {\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"2\": {\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"3\": {\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        # Configuration modele pour Qwen\n        \"4\": {\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}\n        },\n        \"5\": {\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}\n        },\n        # Encodage du prompt avec TextEncodeQwenImageEdit\n        \"6\": {\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"2\", 0],\n                \"prompt\": prompt,\n                \"vae\": [\"1\", 0]\n            }\n        },\n        # Conditioning negatif (vide pour Qwen)\n        \"7\": {\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"6\", 0]}\n        },\n        # Latent vide (16 canaux pour Qwen)\n        \"8\": {\n            \"class_type\": \"EmptySD3LatentImage\",\n            \"inputs\": {\"width\": width, \"height\": height, \"batch_size\": 1}\n        },\n        # Sampling\n        \"9\": {\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"model\": [\"5\", 0],\n                \"positive\": [\"6\", 0],\n                \"negative\": [\"7\", 0],\n                \"latent_image\": [\"8\", 0],\n                \"seed\": seed,\n                \"steps\": steps,\n                \"cfg\": cfg,\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",\n                \"denoise\": 1.0\n            }\n        },\n        # Decodage VAE\n        \"10\": {\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\"samples\": [\"9\", 0], \"vae\": [\"1\", 0]}\n        },\n        # Sauvegarde\n        \"11\": {\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\"filename_prefix\": \"Qwen2509_t2i\", \"images\": [\"10\", 0]}\n        }\n    }\n\n\ndef create_img2img_workflow(image_name: str, prompt: str, \n                            denoise: float = 0.7, steps: int = 20,\n                            cfg: float = 1.0, seed: int = None) -> Dict:\n    \"\"\"\n    Workflow Image-to-Image pour l'edition avec Qwen.\n    \n    Note: Qwen-Image-Edit 2509 est optimise pour l'edition d'images.\n    Le parametre denoise controle la force de l'edition:\n    - 0.3-0.5: Ajustements subtils\n    - 0.5-0.7: Modifications moderees  \n    - 0.7-0.9: Transformations significatives\n    \n    Args:\n        image_name: Nom du fichier image uploade\n        prompt: Instructions d'edition\n        denoise: Force de l'edition (0.0=rien, 1.0=regeneration complete)\n    \"\"\"\n    if seed is None:\n        seed = np.random.randint(0, 2**32)\n    \n    return {\n        # Chargement image source\n        \"1\": {\n            \"class_type\": \"LoadImage\",\n            \"inputs\": {\"image\": image_name}\n        },\n        # Chargement des modeles\n        \"2\": {\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"3\": {\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"4\": {\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        # Configuration modele\n        \"5\": {\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"4\", 0], \"shift\": 3.0}\n        },\n        \"6\": {\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"5\", 0], \"strength\": 1.0}\n        },\n        # Encodage VAE de l'image source\n        \"7\": {\n            \"class_type\": \"VAEEncode\",\n            \"inputs\": {\"pixels\": [\"1\", 0], \"vae\": [\"2\", 0]}\n        },\n        # Encodage du prompt\n        \"8\": {\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"3\", 0],\n                \"prompt\": prompt,\n                \"vae\": [\"2\", 0]\n            }\n        },\n        # Conditioning negatif\n        \"9\": {\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"8\", 0]}\n        },\n        # Sampling\n        \"10\": {\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"model\": [\"6\", 0],\n                \"positive\": [\"8\", 0],\n                \"negative\": [\"9\", 0],\n                \"latent_image\": [\"7\", 0],\n                \"seed\": seed,\n                \"steps\": steps,\n                \"cfg\": cfg,\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",\n                \"denoise\": denoise\n            }\n        },\n        # Decodage\n        \"11\": {\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\"samples\": [\"10\", 0], \"vae\": [\"2\", 0]}\n        },\n        # Sauvegarde\n        \"12\": {\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\"filename_prefix\": \"Qwen2509_i2i\", \"images\": [\"11\", 0]}\n        }\n    }\n\n\ndef create_inpaint_workflow(image_name: str, mask_name: str, prompt: str,\n                            denoise: float = 0.9, steps: int = 25,\n                            cfg: float = 1.0, seed: int = None) -> Dict:\n    \"\"\"\n    Workflow Inpainting pour Qwen-Image-Edit 2509.\n\n    Note: L'inpainting Qwen utilise SetLatentNoiseMask pour appliquer\n    le masque sur le latent encode. Le masque definit les zones a regenerer.\n\n    Args:\n        image_name: Nom du fichier image uploade\n        mask_name: Nom du fichier masque (blanc = zone a modifier)\n        prompt: Description de ce qui doit remplacer la zone masquee\n        denoise: Force de regeneration (0.8-1.0 recommande pour inpaint)\n    \"\"\"\n    if seed is None:\n        seed = np.random.randint(0, 2**32)\n\n    return {\n        # Chargement image et masque\n        \"1\": {\n            \"class_type\": \"LoadImage\",\n            \"inputs\": {\"image\": image_name}\n        },\n        \"2\": {\n            \"class_type\": \"LoadImage\",\n            \"inputs\": {\"image\": mask_name}\n        },\n        # Chargement des modeles\n        \"3\": {\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"4\": {\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"5\": {\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        # Configuration modele\n        \"6\": {\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"5\", 0], \"shift\": 3.0}\n        },\n        \"7\": {\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"6\", 0], \"strength\": 1.0}\n        },\n        # Encodage VAE de l'image source\n        \"8\": {\n            \"class_type\": \"VAEEncode\",\n            \"inputs\": {\"pixels\": [\"1\", 0], \"vae\": [\"3\", 0]}\n        },\n        # Application du masque sur le latent\n        \"9\": {\n            \"class_type\": \"SetLatentNoiseMask\",\n            \"inputs\": {\n                \"samples\": [\"8\", 0],\n                \"mask\": [\"2\", 0]\n            }\n        },\n        # Encodage du prompt\n        \"10\": {\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"4\", 0],\n                \"prompt\": prompt,\n                \"vae\": [\"3\", 0]\n            }\n        },\n        # Conditioning negatif\n        \"11\": {\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"10\", 0]}\n        },\n        # Sampling avec masque\n        \"12\": {\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"model\": [\"7\", 0],\n                \"positive\": [\"10\", 0],\n                \"negative\": [\"11\", 0],\n                \"latent_image\": [\"9\", 0],\n                \"seed\": seed,\n                \"steps\": steps,\n                \"cfg\": cfg,\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",\n                \"denoise\": denoise\n            }\n        },\n        # Decodage\n        \"13\": {\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\"samples\": [\"12\", 0], \"vae\": [\"3\", 0]}\n        },\n        # Sauvegarde\n        \"14\": {\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\"filename_prefix\": \"Qwen2509_inpaint\", \"images\": [\"13\", 0]}\n        }\n    }\n\n\nprint(\"Workflows Qwen-Image-Edit 2509 definis (Architecture Phase 29)\")\nprint(\"   - create_text2img_workflow() -> node output: '11'\")\nprint(\"   - create_img2img_workflow() -> node output: '12'\")\nprint(\"   - create_inpaint_workflow() -> node output: '14'\")\nprint(\"\\nNotes importantes:\")\nprint(\"   - CFG=1.0 recommande (CFGNorm gere l'amplification)\")\nprint(\"   - Scheduler 'beta' optimise pour Qwen\")\nprint(\"   - 20 steps suffisent pour de bons resultats\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 4. G√©n√©ration Text-to-Image\n",
    "\n",
    "Commen√ßons par g√©n√©rer une image de base que nous √©diterons ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 4. TEXT-TO-IMAGE: Creation d'une image de base\n# =============================================================================\n\n# Prompt creatif pour une scene editable\nprompt_base = \"\"\"\nA cozy coffee shop interior, wooden tables, warm lighting,\nlarge window with rain outside, vintage aesthetic,\nempty cup on table, potted plant, high quality photography\n\"\"\".strip()\n\n# Creer le workflow (Architecture Phase 29)\nworkflow_t2i = create_text2img_workflow(\n    prompt=prompt_base,\n    width=1024,\n    height=768,\n    steps=20,      # 20 steps optimaux pour Qwen\n    cfg=1.0,       # CFG=1.0 avec CFGNorm\n    seed=42        # Fixe pour reproductibilite\n)\n\n# Executer (output_node=\"11\" pour le nouveau workflow)\nprint(\"\\nGeneration de l'image de base...\")\nprint(f\"Prompt: {prompt_base[:80]}...\")\n\nimages_t2i = client.execute_and_get_images(workflow_t2i, output_node=\"11\")\n\nif images_t2i:\n    base_image = images_t2i[0]\n    \n    # Affichage\n    plt.figure(figsize=(12, 9))\n    plt.imshow(base_image)\n    plt.title(\"Image de Base - Coffee Shop (Qwen 2509)\", fontsize=14)\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nDimensions: {base_image.size}\")\nelse:\n    print(\"Erreur de generation - verifiez que les modeles Qwen sont presents\")\n    print(\"Modeles requis dans ComfyUI/models/:\")\n    print(\"  - vae/qwen_image_vae.safetensors\")\n    print(\"  - text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors\")\n    print(\"  - diffusion_models/qwen_image_edit_2509_fp8_e4m3fn.safetensors\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 5. √âdition Image-to-Image\n",
    "\n",
    "Explorons diff√©rents niveaux de `denoise` pour comprendre son impact sur l'√©dition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 5. IMAGE-TO-IMAGE: Analyse comparative du parametre denoise\n# =============================================================================\n\n# Upload de l'image de base\nif 'base_image' in dir():\n    uploaded_name = client.upload_image(base_image, \"base_coffee_shop.png\")\n    print(f\"Image uploadee: {uploaded_name}\")\n    \n    # Test avec differents niveaux de denoise\n    denoise_levels = [0.3, 0.5, 0.7, 0.9]\n    edit_prompt = \"Same scene but with snow falling outside the window, winter atmosphere\"\n    \n    results = []\n    seed_fixed = 12345  # Meme seed pour comparer\n    \n    print(f\"\\nEdition avec prompt: '{edit_prompt}'\")\n    print(\"\\nComparaison des niveaux de denoise:\")\n    \n    for denoise in denoise_levels:\n        print(f\"\\n--- Denoise = {denoise} ---\")\n        \n        workflow = create_img2img_workflow(\n            image_name=uploaded_name,\n            prompt=edit_prompt,\n            denoise=denoise,\n            steps=20,\n            cfg=1.0,\n            seed=seed_fixed\n        )\n        \n        # output_node=\"12\" pour img2img\n        images = client.execute_and_get_images(workflow, output_node=\"12\", verbose=False)\n        if images:\n            results.append((denoise, images[0]))\n            print(f\"   Genere\")\n    \n    # Affichage comparatif\n    if results:\n        fig, axes = plt.subplots(1, len(results) + 1, figsize=(20, 5))\n        \n        # Image originale\n        axes[0].imshow(base_image)\n        axes[0].set_title(\"Original\", fontsize=12)\n        axes[0].axis('off')\n        \n        # Resultats\n        for i, (denoise, img) in enumerate(results):\n            axes[i+1].imshow(img)\n            axes[i+1].set_title(f\"Denoise = {denoise}\", fontsize=12)\n            axes[i+1].axis('off')\n        \n        plt.suptitle(\"Impact du parametre Denoise sur l'edition\", fontsize=14)\n        plt.tight_layout()\n        plt.show()\n        \n        print(\"\\nObservations:\")\n        print(\"   0.3: Changements subtils, structure tres preservee\")\n        print(\"   0.5: Modifications visibles, bonne balance\")\n        print(\"   0.7: Transformations significatives\")\n        print(\"   0.9: Quasi-regeneration, peu de l'original conserve\")\nelse:\n    print(\"Executez d'abord la cellule Text-to-Image\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 6. Inpainting Avanc√© avec Masque Personnalis√©\n",
    "\n",
    "L'inpainting permet de modifier uniquement certaines zones de l'image. Nous allons cr√©er un masque programmatique pour remplacer un √©l√©ment sp√©cifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. INPAINTING: √âdition localis√©e avec masque\n",
    "# =============================================================================\n",
    "\n",
    "def create_rectangular_mask(width: int, height: int, \n",
    "                            x1: int, y1: int, x2: int, y2: int,\n",
    "                            feather: int = 10) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Cr√©e un masque rectangulaire avec bords adoucis.\n",
    "    \n",
    "    Args:\n",
    "        width, height: Dimensions du masque\n",
    "        x1, y1, x2, y2: Coordonn√©es du rectangle (zone √† modifier)\n",
    "        feather: Adoucissement des bords en pixels\n",
    "    \n",
    "    Returns:\n",
    "        Image grayscale (blanc = zone √† modifier)\n",
    "    \"\"\"\n",
    "    mask = Image.new('L', (width, height), 0)  # Noir = pr√©server\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    draw.rectangle([x1, y1, x2, y2], fill=255)  # Blanc = modifier\n",
    "    \n",
    "    # Adoucissement optionnel (blur simple)\n",
    "    if feather > 0:\n",
    "        from PIL import ImageFilter\n",
    "        mask = mask.filter(ImageFilter.GaussianBlur(radius=feather))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_circular_mask(width: int, height: int,\n",
    "                         cx: int, cy: int, radius: int,\n",
    "                         feather: int = 15) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Cr√©e un masque circulaire pour l'inpainting.\n",
    "    \"\"\"\n",
    "    mask = Image.new('L', (width, height), 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    draw.ellipse([cx-radius, cy-radius, cx+radius, cy+radius], fill=255)\n",
    "    \n",
    "    if feather > 0:\n",
    "        from PIL import ImageFilter\n",
    "        mask = mask.filter(ImageFilter.GaussianBlur(radius=feather))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "# Exemple: Remplacer la tasse sur la table\n",
    "if 'base_image' in dir():\n",
    "    w, h = base_image.size\n",
    "    \n",
    "    # Cr√©er un masque pour le centre-bas de l'image (o√π la table/tasse serait)\n",
    "    mask = create_rectangular_mask(\n",
    "        w, h,\n",
    "        x1=int(w*0.35), y1=int(h*0.55),\n",
    "        x2=int(w*0.65), y2=int(h*0.85),\n",
    "        feather=20\n",
    "    )\n",
    "    \n",
    "    # Upload du masque\n",
    "    mask_name = client.upload_mask(mask, \"edit_mask.png\")\n",
    "    print(f\"‚úÖ Masque upload√©: {mask_name}\")\n",
    "    \n",
    "    # Visualisation du masque\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(base_image)\n",
    "    axes[0].set_title(\"Image Originale\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(mask, cmap='gray')\n",
    "    axes[1].set_title(\"Masque (blanc = zone √† modifier)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = base_image.copy().convert('RGBA')\n",
    "    mask_rgba = Image.new('RGBA', overlay.size, (255, 0, 0, 0))\n",
    "    mask_draw = ImageDraw.Draw(mask_rgba)\n",
    "    # Convertir le masque en overlay rouge semi-transparent\n",
    "    mask_array = np.array(mask)\n",
    "    red_overlay = np.zeros((h, w, 4), dtype=np.uint8)\n",
    "    red_overlay[:,:,0] = 255  # Rouge\n",
    "    red_overlay[:,:,3] = (mask_array * 0.5).astype(np.uint8)  # Alpha\n",
    "    overlay_img = Image.alpha_composite(overlay, Image.fromarray(red_overlay))\n",
    "    \n",
    "    axes[2].imshow(overlay_img)\n",
    "    axes[2].set_title(\"Zone d'√©dition (rouge)\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Image de base non disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 6b. EXECUTION DE L'INPAINTING\n# =============================================================================\n\nif 'uploaded_name' in dir() and 'mask_name' in dir():\n    # Prompt pour la zone masquee\n    inpaint_prompt = \"A beautiful laptop with glowing screen, modern design, on wooden table\"\n    \n    print(f\"\\nInpainting: '{inpaint_prompt}'\")\n    \n    workflow_inpaint = create_inpaint_workflow(\n        image_name=uploaded_name,\n        mask_name=mask_name,\n        prompt=inpaint_prompt,\n        denoise=0.95,  # Haut pour remplacer completement\n        steps=25,\n        cfg=1.0,       # CFG=1.0 avec CFGNorm (Phase 29)\n        seed=99999\n    )\n    \n    # output_node=\"14\" pour inpainting workflow\n    images_inpaint = client.execute_and_get_images(workflow_inpaint, output_node=\"14\")\n    \n    if images_inpaint:\n        inpainted_image = images_inpaint[0]\n        \n        # Comparaison avant/apres\n        fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n        \n        axes[0].imshow(base_image)\n        axes[0].set_title(\"Avant Inpainting\", fontsize=14)\n        axes[0].axis('off')\n        \n        axes[1].imshow(inpainted_image)\n        axes[1].set_title(\"Apres Inpainting\", fontsize=14)\n        axes[1].axis('off')\n        \n        plt.suptitle(f\"Inpainting: '{inpaint_prompt}'\", fontsize=12)\n        plt.tight_layout()\n        plt.show()\nelse:\n    print(\"Prerequis manquants (image ou masque)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 7. Batch Processing: G√©n√©ration Multiple\n",
    "\n",
    "Pour l'efficacit√©, g√©n√©rons plusieurs variations en parall√®le avec diff√©rents prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 7. BATCH PROCESSING: Variations multiples\n# =============================================================================\n\ndef batch_generate(prompts: List[str], base_seed: int = 1000, **kwargs) -> List[Tuple[str, Image.Image]]:\n    \"\"\"\n    Genere plusieurs images a partir d'une liste de prompts.\n    \n    Args:\n        prompts: Liste de descriptions\n        base_seed: Seed de depart (incremente pour chaque image)\n        **kwargs: Arguments passes a create_text2img_workflow\n    \n    Returns:\n        Liste de tuples (prompt, image)\n    \"\"\"\n    results = []\n    \n    for i, prompt in enumerate(prompts):\n        print(f\"\\n[{i+1}/{len(prompts)}] Generation...\")\n        print(f\"   Prompt: {prompt[:60]}...\")\n        \n        workflow = create_text2img_workflow(\n            prompt=prompt,\n            seed=base_seed + i,\n            **kwargs\n        )\n        \n        # output_node=\"11\" pour t2i\n        images = client.execute_and_get_images(workflow, output_node=\"11\", verbose=False)\n        if images:\n            results.append((prompt, images[0]))\n            print(f\"   Succes\")\n        else:\n            print(f\"   Echec\")\n    \n    return results\n\n\n# Generation de variations thematiques\nvariation_prompts = [\n    \"A futuristic cityscape at sunset, flying cars, neon lights, cyberpunk style\",\n    \"An ancient Japanese temple in autumn, red maple leaves, misty mountains\",\n    \"An underwater coral reef, tropical fish, sunlight rays, crystal clear water\",\n    \"A cozy library interior, tall bookshelves, reading nook, warm lamp light\"\n]\n\nprint(\"\\nBatch Generation - 4 Themes\")\nprint(\"=\" * 40)\n\nbatch_results = batch_generate(\n    variation_prompts,\n    base_seed=2024,\n    width=768,\n    height=768,\n    steps=20,\n    cfg=1.0  # CFG=1.0 avec CFGNorm\n)\n\n# Affichage grille\nif batch_results:\n    n = len(batch_results)\n    cols = 2\n    rows = (n + cols - 1) // cols\n    \n    fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n    axes = axes.flatten()\n    \n    for i, (prompt, img) in enumerate(batch_results):\n        axes[i].imshow(img)\n        # Titre court\n        short_title = prompt.split(',')[0][:40]\n        axes[i].set_title(short_title, fontsize=10)\n        axes[i].axis('off')\n    \n    # Masquer les axes vides\n    for i in range(len(batch_results), len(axes)):\n        axes[i].axis('off')\n    \n    plt.suptitle(\"Batch Generation - Variations Thematiques\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\n{len(batch_results)} images generees avec succes\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 8. Analyse Comparative: CFG Scale\n",
    "\n",
    "Le param√®tre `cfg` (Classifier-Free Guidance) contr√¥le l'adh√©rence au prompt. Explorons son impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 8. ANALYSE CFG: Impact du Guidance Scale avec CFGNorm\n# =============================================================================\n# NOTE IMPORTANTE (Phase 29):\n# L'architecture Qwen utilise CFGNorm qui normalise le guidance.\n# Avec CFGNorm, CFG=1.0 est optimal car la normalisation gere l'amplification.\n# Cette cellule explore differentes valeurs CFG a titre educatif pour\n# comprendre le comportement du modele.\n\ntest_prompt = \"A majestic dragon breathing fire, fantasy art, highly detailed scales, dramatic lighting\"\n\n# Valeurs CFG a tester (1.0 est recommande avec CFGNorm)\ncfg_values = [1.0, 2.0, 4.0, 7.0]\nfixed_seed = 7777\n\nprint(f\"\\nAnalyse CFG Scale (avec CFGNorm)\")\nprint(f\"Prompt: '{test_prompt[:50]}...'\")\nprint(f\"Valeurs testees: {cfg_values}\")\nprint(\"\\nNote: CFG=1.0 est recommande avec CFGNorm (architecture Phase 29)\")\n\ncfg_results = []\n\nfor cfg in cfg_values:\n    print(f\"\\n--- CFG = {cfg} ---\")\n    \n    workflow = create_text2img_workflow(\n        prompt=test_prompt,\n        width=768,\n        height=768,\n        steps=20,\n        cfg=cfg,\n        seed=fixed_seed\n    )\n    \n    # output_node=\"11\" pour t2i\n    images = client.execute_and_get_images(workflow, output_node=\"11\", verbose=False)\n    if images:\n        cfg_results.append((cfg, images[0]))\n        print(f\"   Genere\")\n\n# Affichage comparatif\nif cfg_results:\n    fig, axes = plt.subplots(1, len(cfg_results), figsize=(16, 5))\n    \n    for i, (cfg, img) in enumerate(cfg_results):\n        axes[i].imshow(img)\n        title = f\"CFG = {cfg}\"\n        if cfg == 1.0:\n            title += \" (recommande)\"\n        axes[i].set_title(title, fontsize=12)\n        axes[i].axis('off')\n    \n    plt.suptitle(\"Impact du CFG avec CFGNorm (Architecture Phase 29)\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nObservations CFG avec CFGNorm:\")\n    print(\"   1.0: Valeur optimale - CFGNorm gere l'amplification automatiquement\")\n    print(\"   2.0: Leger renforcement du prompt\")\n    print(\"   4.0: Adherence plus forte, details accentues\")\n    print(\"   7.0: Peut introduire des artefacts avec CFGNorm\")\n    print(\"\\nRecommandation: Utiliser CFG=1.0 avec l'architecture Phase 29\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 9. Exercices Pratiques\n",
    "\n",
    "### Exercice 1: √âdition de Style\n",
    "Prenez l'image de base du coffee shop et appliquez diff√©rents styles artistiques (impressionniste, anime, r√©aliste) en utilisant img2img avec un denoise de 0.6.\n",
    "\n",
    "### Exercice 2: Inpainting Cr√©atif\n",
    "Cr√©ez un masque circulaire au centre de l'image et remplacez cette zone par un personnage de votre choix.\n",
    "\n",
    "### Exercice 3: Exploration des Schedulers\n",
    "Modifiez le workflow pour tester diff√©rents schedulers (`normal`, `karras`, `exponential`) et comparez les r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9. ESPACE D'EXERCICES\n",
    "# =============================================================================\n",
    "\n",
    "# Exercice 1: Style Transfer\n",
    "# D√©commentez et compl√©tez:\n",
    "\n",
    "# style_prompts = [\n",
    "#     \"Same scene, impressionist painting style, visible brushstrokes\",\n",
    "#     \"Same scene, anime style, vibrant colors, Studio Ghibli\",\n",
    "#     \"Same scene, photorealistic, DSLR quality, 8k resolution\"\n",
    "# ]\n",
    "# \n",
    "# for style in style_prompts:\n",
    "#     workflow = create_img2img_workflow(\n",
    "#         image_name=uploaded_name,\n",
    "#         prompt=style,\n",
    "#         denoise=0.6\n",
    "#     )\n",
    "#     # ... g√©n√©rer et afficher\n",
    "\n",
    "print(\"üìù Espace d'exercices - D√©commentez le code ci-dessus pour commencer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "## 10. Recapitulatif et Points Cles\n\n### Architecture Phase 29 - Qwen-Image-Edit 2509\n\nCette architecture utilise les composants natifs ComfyUI suivants :\n\n| Composant | Node ComfyUI | Fichier Modele |\n|-----------|--------------|----------------|\n| VAE | VAELoader | qwen_image_vae.safetensors (243MB) |\n| Text Encoder | CLIPLoader (type=sd3) | qwen_2.5_vl_7b_fp8_scaled.safetensors (8.8GB) |\n| Diffusion | UNETLoader (fp8_e4m3fn) | qwen_image_edit_2509_fp8_e4m3fn.safetensors (20GB) |\n\n### Parametres Essentiels (Architecture Phase 29)\n\n| Parametre | Plage | Recommande | Impact |\n|-----------|-------|------------|--------|\n| `steps` | 15-30 | 20 | Qualite vs. Vitesse |\n| `cfg` | 1-4 | **1.0** | CFGNorm gere l'amplification |\n| `denoise` | 0-1 | 0.5-0.7 | Force de l'edition |\n| `shift` | 2-4 | 3.0 | ModelSamplingAuraFlow |\n| `scheduler` | - | **beta** | Optimise pour Qwen |\n| `sampler` | - | **euler** | Stable et rapide |\n\n### Bonnes Pratiques\n\n1. **Toujours utiliser CFG=1.0** avec l'architecture Phase 29 (CFGNorm normalise automatiquement)\n2. **Scheduler 'beta'** est optimise pour les modeles Qwen\n3. **20 steps** suffisent pour de bons resultats (qualite/vitesse optimal)\n4. **Pour l'inpainting, denoise >= 0.8** pour un remplacement complet\n5. **Utiliser seed fixe** pour comparer les parametres et reproduire les resultats\n\n### Nodes Requis\n\n```\nTextEncodeQwenImageEdit   # Encodeur texte natif Qwen\nModelSamplingAuraFlow     # Configuration sampling (shift=3.0)\nCFGNorm                   # Normalisation CFG (strength=1.0)\nEmptySD3LatentImage       # Latent 16 canaux pour Qwen\nConditioningZeroOut       # Conditioning negatif vide\n```\n\n### Ressources\n\n- [Documentation ComfyUI](https://docs.comfy.org/)\n- [Qwen-VL Papers](https://arxiv.org/abs/2308.12966)\n- [Modeles Comfy-Org](https://huggingface.co/Comfy-Org/Qwen-Image-Edit_ComfyUI)\n- Notebook suivant: **02-2-FLUX-1-Advanced-Generation**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIN DU NOTEBOOK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"   ‚úÖ Notebook Qwen-Image-Edit 2509 Compl√©t√©\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÖ Termin√©: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nüìö Concepts couverts:\")\n",
    "print(\"   ‚Ä¢ Text-to-Image avec Qwen 2509\")\n",
    "print(\"   ‚Ä¢ Image-to-Image et analyse du denoise\")\n",
    "print(\"   ‚Ä¢ Inpainting avec masques personnalis√©s\")\n",
    "print(\"   ‚Ä¢ Batch processing\")\n",
    "print(\"   ‚Ä¢ Analyse comparative CFG\")\n",
    "print(\"\\n‚û°Ô∏è  Prochain notebook: 02-2-FLUX-1-Advanced-Generation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}