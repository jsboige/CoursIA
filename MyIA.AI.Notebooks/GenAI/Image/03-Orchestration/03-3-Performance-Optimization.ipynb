{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\ude80 Performance Optimization pour la G\u00e9n\u00e9ration d'Images\n",
        "\n",
        "**Module :** 03-Images-Orchestration  \n",
        "**Niveau :** Interm\u00e9diaire  \n",
        "**Dur\u00e9e estim\u00e9e :** 45 minutes  \n",
        "\n",
        "## Objectifs d'Apprentissage\n",
        "\n",
        "- [ ] Ma\u00eetriser les techniques de profilage GPU et m\u00e9moire\n",
        "- [ ] Impl\u00e9menter la quantification pour r\u00e9duire l'empreinte m\u00e9moire\n",
        "- [ ] Optimiser les pipelines avec attention mechanisms avanc\u00e9s\n",
        "- [ ] Concevoir des strat\u00e9gies de batch processing efficaces\n",
        "- [ ] Utiliser le caching pour acc\u00e9l\u00e9rer les workloads r\u00e9p\u00e9titifs\n",
        "\n",
        "## Pr\u00e9requis\n",
        "\n",
        "- Module 00 (Environment Setup) compl\u00e9t\u00e9\n",
        "- Module 03-1 et 03-2 (Comparaison, Orchestration) compl\u00e9t\u00e9s\n",
        "- GPU CUDA disponible (RTX 3060+ recommand\u00e9)\n",
        "\n",
        "## Architecture du Notebook\n",
        "\n",
        "```\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                    Performance Optimization                         \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502  1. Profilage                                                       \u2502\n",
        "\u2502     \u251c\u2500\u2500 GPU Memory Tracking                                         \u2502\n",
        "\u2502     \u251c\u2500\u2500 Inference Time Measurement                                  \u2502\n",
        "\u2502     \u2514\u2500\u2500 Bottleneck Identification                                   \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502  2. Memory Optimization                                             \u2502\n",
        "\u2502     \u251c\u2500\u2500 FP16/BF16 Precision                                         \u2502\n",
        "\u2502     \u251c\u2500\u2500 Model Quantization (INT8, FP8)                             \u2502\n",
        "\u2502     \u2514\u2500\u2500 CPU Offloading                                              \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502  3. Speed Optimization                                              \u2502\n",
        "\u2502     \u251c\u2500\u2500 xFormers / Flash Attention                                  \u2502\n",
        "\u2502     \u251c\u2500\u2500 Torch Compile                                               \u2502\n",
        "\u2502     \u2514\u2500\u2500 VAE Tiling/Slicing                                          \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502  4. Batch & Caching                                                 \u2502\n",
        "\u2502     \u251c\u2500\u2500 Optimal Batch Sizing                                        \u2502\n",
        "\u2502     \u251c\u2500\u2500 Prompt Embedding Cache                                      \u2502\n",
        "\u2502     \u2514\u2500\u2500 Result Caching Strategy                                     \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Param\u00e8tres Papermill - Configuration globale\n",
        "\n",
        "notebook_mode = \"interactive\"\n",
        "debug_level = \"INFO\"\n",
        "\n",
        "# Configuration benchmarks\n",
        "benchmark_iterations = 3  # Nombre d'it\u00e9rations par test\n",
        "warmup_runs = 1  # Runs de warmup avant mesure\n",
        "save_benchmark_results = True\n",
        "\n",
        "# Configuration m\u00e9moire\n",
        "target_memory_reduction = 0.5  # Objectif: 50% de r\u00e9duction\n",
        "enable_quantization = True\n",
        "enable_cpu_offload = False  # Activer si GPU < 8GB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup et Utilitaires de Profilage\n",
        "\n",
        "Commen\u00e7ons par cr\u00e9er les outils de mesure de performance."
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Callable, Any, Tuple\n",
        "from contextlib import contextmanager\n",
        "from functools import wraps\n",
        "import logging\n",
        "\n",
        "# Configuration logging\n",
        "logging.basicConfig(level=getattr(logging, debug_level))\n",
        "logger = logging.getLogger('perf_optimization')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83d\ude80 Performance Optimization - GenAI Image Generation\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Mode: {notebook_mode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "\ud83d\ude80 Performance Optimization - GenAI Image Generation\n",
            "============================================================\n",
            "Date: 2026-02-18 10:05:15\n",
            "Mode: interactive\n"
          ]
        }
      ],
      "source": [
        "# V\u00e9rification GPU et imports conditionnels\n",
        "import torch\n",
        "\n",
        "CUDA_AVAILABLE = torch.cuda.is_available()\n",
        "GPU_COUNT = torch.cuda.device_count() if CUDA_AVAILABLE else 0\n",
        "\n",
        "if CUDA_AVAILABLE:\n",
        "    GPU_NAME = torch.cuda.get_device_name(0)\n",
        "    GPU_MEMORY_TOTAL = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"\\n\u2705 GPU D\u00e9tect\u00e9: {GPU_NAME}\")\n",
        "    print(f\"   M\u00e9moire totale: {GPU_MEMORY_TOTAL:.1f} GB\")\n",
        "    print(f\"   GPUs disponibles: {GPU_COUNT}\")\n",
        "    \n",
        "    # Multi-GPU info\n",
        "    if GPU_COUNT > 1:\n",
        "        print(f\"\\n\ud83d\udcca Configuration Multi-GPU:\")\n",
        "        for i in range(GPU_COUNT):\n",
        "            name = torch.cuda.get_device_name(i)\n",
        "            mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
        "            print(f\"   GPU {i}: {name} ({mem:.1f} GB)\")\n",
        "else:\n",
        "    GPU_NAME = \"CPU Only\"\n",
        "    GPU_MEMORY_TOTAL = 0\n",
        "    print(\"\\n\u26a0\ufe0f Pas de GPU CUDA d\u00e9tect\u00e9 - Mode CPU uniquement\")\n",
        "    print(\"   Les optimisations GPU seront simul\u00e9es\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u26a0\ufe0f Pas de GPU CUDA d\u00e9tect\u00e9 - Mode CPU uniquement\n",
            "   Les optimisations GPU seront simul\u00e9es\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class PerformanceMetrics:\n",
        "    \"\"\"Conteneur pour les m\u00e9triques de performance.\"\"\"\n",
        "    name: str\n",
        "    execution_time_ms: float\n",
        "    gpu_memory_peak_mb: float\n",
        "    gpu_memory_allocated_mb: float\n",
        "    throughput_images_per_sec: float = 0.0\n",
        "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
        "    extra_info: Dict[str, Any] = field(default_factory=dict)\n",
        "    \n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            'name': self.name,\n",
        "            'execution_time_ms': self.execution_time_ms,\n",
        "            'gpu_memory_peak_mb': self.gpu_memory_peak_mb,\n",
        "            'gpu_memory_allocated_mb': self.gpu_memory_allocated_mb,\n",
        "            'throughput_images_per_sec': self.throughput_images_per_sec,\n",
        "            'timestamp': self.timestamp,\n",
        "            **self.extra_info\n",
        "        }\n",
        "\n",
        "\n",
        "class GPUProfiler:\n",
        "    \"\"\"Profiler GPU pour mesurer m\u00e9moire et temps d'ex\u00e9cution.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.metrics_history: List[PerformanceMetrics] = []\n",
        "        self.cuda_available = CUDA_AVAILABLE\n",
        "    \n",
        "    def get_memory_stats(self) -> Dict[str, float]:\n",
        "        \"\"\"Retourne les stats m\u00e9moire GPU actuelles.\"\"\"\n",
        "        if not self.cuda_available:\n",
        "            return {'allocated_mb': 0, 'reserved_mb': 0, 'peak_mb': 0}\n",
        "        \n",
        "        return {\n",
        "            'allocated_mb': torch.cuda.memory_allocated() / (1024**2),\n",
        "            'reserved_mb': torch.cuda.memory_reserved() / (1024**2),\n",
        "            'peak_mb': torch.cuda.max_memory_allocated() / (1024**2)\n",
        "        }\n",
        "    \n",
        "    def reset_peak_memory(self):\n",
        "        \"\"\"Reset le compteur de m\u00e9moire peak.\"\"\"\n",
        "        if self.cuda_available:\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "    \n",
        "    def clear_cache(self):\n",
        "        \"\"\"Lib\u00e8re le cache GPU.\"\"\"\n",
        "        if self.cuda_available:\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    \n",
        "    @contextmanager\n",
        "    def profile(self, name: str, num_images: int = 1):\n",
        "        \"\"\"Context manager pour profiler une op\u00e9ration.\"\"\"\n",
        "        self.clear_cache()\n",
        "        self.reset_peak_memory()\n",
        "        \n",
        "        start_memory = self.get_memory_stats()\n",
        "        start_time = time.perf_counter()\n",
        "        \n",
        "        if self.cuda_available:\n",
        "            torch.cuda.synchronize()\n",
        "        \n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            if self.cuda_available:\n",
        "                torch.cuda.synchronize()\n",
        "            \n",
        "            end_time = time.perf_counter()\n",
        "            end_memory = self.get_memory_stats()\n",
        "            \n",
        "            execution_time_ms = (end_time - start_time) * 1000\n",
        "            throughput = num_images / (execution_time_ms / 1000) if execution_time_ms > 0 else 0\n",
        "            \n",
        "            metrics = PerformanceMetrics(\n",
        "                name=name,\n",
        "                execution_time_ms=execution_time_ms,\n",
        "                gpu_memory_peak_mb=end_memory['peak_mb'],\n",
        "                gpu_memory_allocated_mb=end_memory['allocated_mb'],\n",
        "                throughput_images_per_sec=throughput\n",
        "            )\n",
        "            self.metrics_history.append(metrics)\n",
        "    \n",
        "    def get_comparison_table(self) -> str:\n",
        "        \"\"\"G\u00e9n\u00e8re un tableau de comparaison des m\u00e9triques.\"\"\"\n",
        "        if not self.metrics_history:\n",
        "            return \"Aucune m\u00e9trique enregistr\u00e9e\"\n",
        "        \n",
        "        lines = [\n",
        "            \"\\n\" + \"=\"*80,\n",
        "            f\"{'Test':<30} {'Temps (ms)':<15} {'M\u00e9m Peak (MB)':<15} {'Throughput':<15}\",\n",
        "            \"=\"*80\n",
        "        ]\n",
        "        \n",
        "        for m in self.metrics_history:\n",
        "            throughput_str = f\"{m.throughput_images_per_sec:.2f} img/s\" if m.throughput_images_per_sec > 0 else \"N/A\"\n",
        "            lines.append(\n",
        "                f\"{m.name:<30} {m.execution_time_ms:<15.1f} {m.gpu_memory_peak_mb:<15.1f} {throughput_str:<15}\"\n",
        "            )\n",
        "        \n",
        "        lines.append(\"=\"*80)\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# Instance globale du profiler\n",
        "profiler = GPUProfiler()\n",
        "print(\"\\n\u2705 GPUProfiler initialis\u00e9\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Analyse de la Baseline (Sans Optimisations)\n",
        "\n",
        "Avant d'optimiser, mesurons les performances de base."
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u2705 GPUProfiler initialis\u00e9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Affichage \u00e9tat m\u00e9moire initial\n",
        "print(\"\ud83d\udcca \u00c9tat M\u00e9moire Initial\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "initial_stats = profiler.get_memory_stats()\n",
        "print(f\"M\u00e9moire allou\u00e9e: {initial_stats['allocated_mb']:.1f} MB\")\n",
        "print(f\"M\u00e9moire r\u00e9serv\u00e9e: {initial_stats['reserved_mb']:.1f} MB\")\n",
        "\n",
        "if CUDA_AVAILABLE:\n",
        "    free_memory = GPU_MEMORY_TOTAL * 1024 - initial_stats['reserved_mb']\n",
        "    print(f\"M\u00e9moire libre estim\u00e9e: {free_memory:.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udcca \u00c9tat M\u00e9moire Initial\n",
            "----------------------------------------\n",
            "M\u00e9moire allou\u00e9e: 0.0 MB\n",
            "M\u00e9moire r\u00e9serv\u00e9e: 0.0 MB\n"
          ]
        }
      ],
      "source": [
        "def simulate_model_inference(size_mb: int = 1000, compute_ms: int = 500):\n",
        "    \"\"\"\n",
        "    Simule une inf\u00e9rence de mod\u00e8le pour d\u00e9monstration.\n",
        "    \n",
        "    En production, remplacer par le vrai pipeline de g\u00e9n\u00e9ration.\n",
        "    \"\"\"\n",
        "    if CUDA_AVAILABLE:\n",
        "        # Alloue de la m\u00e9moire GPU pour simuler un mod\u00e8le\n",
        "        elements = (size_mb * 1024 * 1024) // 4  # float32 = 4 bytes\n",
        "        tensor = torch.randn(elements, device='cuda')\n",
        "        \n",
        "        # Simule du calcul\n",
        "        for _ in range(10):\n",
        "            tensor = tensor * 1.001 + 0.001\n",
        "        \n",
        "        time.sleep(compute_ms / 1000)\n",
        "        del tensor\n",
        "    else:\n",
        "        time.sleep(compute_ms / 1000)\n",
        "\n",
        "\n",
        "# Test baseline\n",
        "print(\"\\n\ud83d\udd2c Test Baseline (Sans Optimisations)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "with profiler.profile(\"Baseline - FP32\"):\n",
        "    simulate_model_inference(size_mb=500, compute_ms=200)\n",
        "\n",
        "baseline_metrics = profiler.metrics_history[-1]\n",
        "print(f\"Temps d'ex\u00e9cution: {baseline_metrics.execution_time_ms:.1f} ms\")\n",
        "print(f\"M\u00e9moire peak: {baseline_metrics.gpu_memory_peak_mb:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Optimisations M\u00e9moire\n",
        "\n",
        "### 3.1 Pr\u00e9cision R\u00e9duite (FP16/BF16)\n",
        "\n",
        "La r\u00e9duction de pr\u00e9cision est l'optimisation la plus simple et efficace."
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd2c Test Baseline (Sans Optimisations)\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temps d'ex\u00e9cution: 200.2 ms\n",
            "M\u00e9moire peak: 0.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PrecisionManager:\n",
        "    \"\"\"\n",
        "    Gestionnaire de pr\u00e9cision pour les mod\u00e8les.\n",
        "    \n",
        "    Pr\u00e9cisions support\u00e9es:\n",
        "    - FP32: Full precision (baseline)\n",
        "    - FP16: Half precision (2x moins de m\u00e9moire)\n",
        "    - BF16: Brain Float 16 (meilleur range que FP16)\n",
        "    \"\"\"\n",
        "    \n",
        "    PRECISION_MAP = {\n",
        "        'fp32': torch.float32,\n",
        "        'fp16': torch.float16,\n",
        "        'bf16': torch.bfloat16\n",
        "    }\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.current_precision = 'fp32'\n",
        "        self._check_bf16_support()\n",
        "    \n",
        "    def _check_bf16_support(self):\n",
        "        \"\"\"V\u00e9rifie le support BF16 (Ampere+).\"\"\"\n",
        "        self.bf16_supported = False\n",
        "        if CUDA_AVAILABLE:\n",
        "            capability = torch.cuda.get_device_capability()\n",
        "            self.bf16_supported = capability[0] >= 8  # Ampere = 8.0+\n",
        "        print(f\"Support BF16: {'\u2705 Oui' if self.bf16_supported else '\u274c Non (GPU < Ampere)'}\")\n",
        "    \n",
        "    def get_recommended_precision(self) -> str:\n",
        "        \"\"\"Retourne la pr\u00e9cision recommand\u00e9e pour ce GPU.\"\"\"\n",
        "        if self.bf16_supported:\n",
        "            return 'bf16'\n",
        "        elif CUDA_AVAILABLE:\n",
        "            return 'fp16'\n",
        "        return 'fp32'\n",
        "    \n",
        "    def get_dtype(self, precision: str = None) -> torch.dtype:\n",
        "        \"\"\"Retourne le dtype torch correspondant.\"\"\"\n",
        "        precision = precision or self.current_precision\n",
        "        return self.PRECISION_MAP.get(precision, torch.float32)\n",
        "    \n",
        "    def estimate_memory_savings(self, base_size_mb: float, precision: str) -> Dict:\n",
        "        \"\"\"Estime les \u00e9conomies de m\u00e9moire.\"\"\"\n",
        "        factors = {'fp32': 1.0, 'fp16': 0.5, 'bf16': 0.5}\n",
        "        factor = factors.get(precision, 1.0)\n",
        "        \n",
        "        return {\n",
        "            'original_mb': base_size_mb,\n",
        "            'optimized_mb': base_size_mb * factor,\n",
        "            'savings_mb': base_size_mb * (1 - factor),\n",
        "            'savings_percent': (1 - factor) * 100\n",
        "        }\n",
        "\n",
        "\n",
        "precision_mgr = PrecisionManager()\n",
        "recommended = precision_mgr.get_recommended_precision()\n",
        "print(f\"\\n\ud83d\udca1 Pr\u00e9cision recommand\u00e9e: {recommended.upper()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Support BF16: \u274c Non (GPU < Ampere)\n",
            "\n",
            "\ud83d\udca1 Pr\u00e9cision recommand\u00e9e: FP32\n"
          ]
        }
      ],
      "source": [
        "# Comparaison des pr\u00e9cisions\n",
        "print(\"\\n\ud83d\udcca Comparaison des Pr\u00e9cisions\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Estimation th\u00e9orique pour un mod\u00e8le SD de 4GB\n",
        "base_model_size = 4000  # MB\n",
        "\n",
        "for precision in ['fp32', 'fp16', 'bf16']:\n",
        "    savings = precision_mgr.estimate_memory_savings(base_model_size, precision)\n",
        "    print(f\"\\n{precision.upper()}:\")\n",
        "    print(f\"  Taille mod\u00e8le: {savings['optimized_mb']:.0f} MB\")\n",
        "    print(f\"  \u00c9conomie: {savings['savings_percent']:.0f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udcca Comparaison des Pr\u00e9cisions\n",
            "==================================================\n",
            "\n",
            "FP32:\n",
            "  Taille mod\u00e8le: 4000 MB\n",
            "  \u00c9conomie: 0%\n",
            "\n",
            "FP16:\n",
            "  Taille mod\u00e8le: 2000 MB\n",
            "  \u00c9conomie: 50%\n",
            "\n",
            "BF16:\n",
            "  Taille mod\u00e8le: 2000 MB\n",
            "  \u00c9conomie: 50%\n"
          ]
        }
      ],
      "source": [
        "# Test pratique FP16\n",
        "print(\"\\n\ud83d\udd2c Test FP16 vs FP32\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "def simulate_with_precision(precision: str, size_mb: int = 500):\n",
        "    \"\"\"Simule un mod\u00e8le avec pr\u00e9cision sp\u00e9cifi\u00e9e.\"\"\"\n",
        "    dtype = precision_mgr.get_dtype(precision)\n",
        "    bytes_per_element = 2 if dtype in [torch.float16, torch.bfloat16] else 4\n",
        "    \n",
        "    if CUDA_AVAILABLE:\n",
        "        elements = (size_mb * 1024 * 1024) // bytes_per_element\n",
        "        tensor = torch.randn(elements, device='cuda', dtype=dtype)\n",
        "        \n",
        "        for _ in range(10):\n",
        "            tensor = tensor * 1.001 + 0.001\n",
        "        \n",
        "        time.sleep(0.1)\n",
        "        del tensor\n",
        "    else:\n",
        "        time.sleep(0.1)\n",
        "\n",
        "# FP32\n",
        "with profiler.profile(\"Test - FP32\"):\n",
        "    simulate_with_precision('fp32', 500)\n",
        "\n",
        "# FP16\n",
        "with profiler.profile(\"Test - FP16\"):\n",
        "    simulate_with_precision('fp16', 500)\n",
        "\n",
        "# BF16 si support\u00e9\n",
        "if precision_mgr.bf16_supported:\n",
        "    with profiler.profile(\"Test - BF16\"):\n",
        "        simulate_with_precision('bf16', 500)\n",
        "\n",
        "print(profiler.get_comparison_table())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Quantification de Mod\u00e8les\n",
        "\n",
        "La quantification va plus loin que FP16 en utilisant INT8 ou m\u00eame INT4."
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd2c Test FP16 vs FP32\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Test                           Temps (ms)      M\u00e9m Peak (MB)   Throughput     \n",
            "================================================================================\n",
            "Baseline - FP32                200.2           0.0             4.99 img/s     \n",
            "Test - FP32                    100.1           0.0             9.99 img/s     \n",
            "Test - FP16                    100.5           0.0             9.95 img/s     \n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuantizationConfig:\n",
        "    \"\"\"\n",
        "    Configuration de quantification pour les mod\u00e8les GenAI.\n",
        "    \n",
        "    Types de quantification:\n",
        "    - INT8: 4x r\u00e9duction, l\u00e9g\u00e8re perte qualit\u00e9\n",
        "    - INT4: 8x r\u00e9duction, perte qualit\u00e9 notable\n",
        "    - NF4: 4-bit normalfloat (QLoRA), bon compromis\n",
        "    \"\"\"\n",
        "    \n",
        "    QUANT_TYPES = {\n",
        "        'none': {'bits': 32, 'factor': 1.0, 'quality_impact': 'Aucun'},\n",
        "        'fp16': {'bits': 16, 'factor': 0.5, 'quality_impact': 'N\u00e9gligeable'},\n",
        "        'int8': {'bits': 8, 'factor': 0.25, 'quality_impact': 'Minime'},\n",
        "        'int4': {'bits': 4, 'factor': 0.125, 'quality_impact': 'L\u00e9ger'},\n",
        "        'nf4': {'bits': 4, 'factor': 0.125, 'quality_impact': 'Minimal'}\n",
        "    }\n",
        "    \n",
        "    @classmethod\n",
        "    def get_config_for_vram(cls, vram_gb: float, model_size_gb: float = 4.0) -> Dict:\n",
        "        \"\"\"\n",
        "        Recommande une config de quantification bas\u00e9e sur la VRAM disponible.\n",
        "        \n",
        "        Args:\n",
        "            vram_gb: VRAM disponible en GB\n",
        "            model_size_gb: Taille du mod\u00e8le en FP32\n",
        "        \n",
        "        Returns:\n",
        "            Configuration recommand\u00e9e\n",
        "        \"\"\"\n",
        "        recommendations = []\n",
        "        \n",
        "        for quant_type, config in cls.QUANT_TYPES.items():\n",
        "            required_vram = model_size_gb * config['factor'] * 1.3  # 30% overhead\n",
        "            if required_vram <= vram_gb:\n",
        "                recommendations.append({\n",
        "                    'type': quant_type,\n",
        "                    'required_vram_gb': required_vram,\n",
        "                    **config\n",
        "                })\n",
        "        \n",
        "        # Retourne la config avec le moins de compression possible\n",
        "        return recommendations[0] if recommendations else cls.QUANT_TYPES['int4']\n",
        "    \n",
        "    @classmethod\n",
        "    def print_comparison_table(cls, model_size_gb: float = 4.0):\n",
        "        \"\"\"Affiche un tableau comparatif des options de quantification.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"{'Type':<10} {'Bits':<8} {'Taille (GB)':<15} {'R\u00e9duction':<12} {'Impact Qualit\u00e9':<15}\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        for quant_type, config in cls.QUANT_TYPES.items():\n",
        "            size = model_size_gb * config['factor']\n",
        "            reduction = (1 - config['factor']) * 100\n",
        "            print(f\"{quant_type:<10} {config['bits']:<8} {size:<15.2f} {reduction:<12.0f}% {config['quality_impact']:<15}\")\n",
        "        \n",
        "        print(\"=\"*70)\n",
        "\n",
        "\n",
        "# Afficher les options de quantification\n",
        "print(\"\\n\ud83d\udcca Options de Quantification pour Mod\u00e8le 4GB\")\n",
        "QuantizationConfig.print_comparison_table(4.0)\n",
        "\n",
        "# Recommandation pour notre GPU\n",
        "if CUDA_AVAILABLE:\n",
        "    recommended_quant = QuantizationConfig.get_config_for_vram(GPU_MEMORY_TOTAL, 4.0)\n",
        "    print(f\"\\n\ud83d\udca1 Recommandation pour {GPU_NAME} ({GPU_MEMORY_TOTAL:.0f}GB):\")\n",
        "    print(f\"   Type: {recommended_quant['type'].upper()}\")\n",
        "    print(f\"   VRAM requise: {recommended_quant.get('required_vram_gb', 'N/A'):.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udcca Options de Quantification pour Mod\u00e8le 4GB\n",
            "\n",
            "======================================================================\n",
            "Type       Bits     Taille (GB)     R\u00e9duction    Impact Qualit\u00e9 \n",
            "======================================================================\n",
            "none       32       4.00            0           % Aucun          \n",
            "fp16       16       2.00            50          % N\u00e9gligeable    \n",
            "int8       8        1.00            75          % Minime         \n",
            "int4       4        0.50            88          % L\u00e9ger          \n",
            "nf4        4        0.50            88          % Minimal        \n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Exemple de configuration BitsAndBytes pour quantification\n",
        "def get_bnb_config(quant_type: str = 'int8') -> Dict:\n",
        "    \"\"\"\n",
        "    G\u00e9n\u00e8re une configuration BitsAndBytes pour le chargement quantifi\u00e9.\n",
        "    \n",
        "    Exemple d'utilisation avec diffusers:\n",
        "    ```python\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    \n",
        "    config = get_bnb_config('int8')\n",
        "    bnb_config = BitsAndBytesConfig(**config)\n",
        "    \n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    ```\n",
        "    \"\"\"\n",
        "    configs = {\n",
        "        'int8': {\n",
        "            'load_in_8bit': True,\n",
        "            'llm_int8_threshold': 6.0,\n",
        "        },\n",
        "        'int4': {\n",
        "            'load_in_4bit': True,\n",
        "            'bnb_4bit_compute_dtype': 'float16',\n",
        "            'bnb_4bit_quant_type': 'fp4',\n",
        "        },\n",
        "        'nf4': {\n",
        "            'load_in_4bit': True,\n",
        "            'bnb_4bit_compute_dtype': 'float16',\n",
        "            'bnb_4bit_quant_type': 'nf4',\n",
        "            'bnb_4bit_use_double_quant': True,  # Double quantification\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return configs.get(quant_type, configs['int8'])\n",
        "\n",
        "\n",
        "print(\"\\n\ud83d\udccb Configurations BitsAndBytes G\u00e9n\u00e9r\u00e9es:\")\n",
        "for qt in ['int8', 'int4', 'nf4']:\n",
        "    config = get_bnb_config(qt)\n",
        "    print(f\"\\n{qt.upper()}:\")\n",
        "    for k, v in config.items():\n",
        "        print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 CPU Offloading\n",
        "\n",
        "Pour les GPUs avec peu de VRAM, le CPU offloading permet d'ex\u00e9cuter des mod\u00e8les plus grands."
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udccb Configurations BitsAndBytes G\u00e9n\u00e9r\u00e9es:\n",
            "\n",
            "INT8:\n",
            "  load_in_8bit: True\n",
            "  llm_int8_threshold: 6.0\n",
            "\n",
            "INT4:\n",
            "  load_in_4bit: True\n",
            "  bnb_4bit_compute_dtype: float16\n",
            "  bnb_4bit_quant_type: fp4\n",
            "\n",
            "NF4:\n",
            "  load_in_4bit: True\n",
            "  bnb_4bit_compute_dtype: float16\n",
            "  bnb_4bit_quant_type: nf4\n",
            "  bnb_4bit_use_double_quant: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OffloadingStrategy:\n",
        "    \"\"\"\n",
        "    Strat\u00e9gies d'offloading pour mod\u00e8les g\u00e9n\u00e9ratifs.\n",
        "    \n",
        "    Strat\u00e9gies:\n",
        "    - none: Tout sur GPU\n",
        "    - model_cpu_offload: Modules d\u00e9plac\u00e9s au besoin\n",
        "    - sequential_cpu_offload: Un module \u00e0 la fois sur GPU\n",
        "    - disk_offload: Offload vers disque (lent mais minimal VRAM)\n",
        "    \"\"\"\n",
        "    \n",
        "    STRATEGIES = {\n",
        "        'none': {\n",
        "            'description': 'Tout le mod\u00e8le sur GPU',\n",
        "            'vram_required': '\u00c9lev\u00e9e',\n",
        "            'speed': 'Maximale',\n",
        "            'method': None\n",
        "        },\n",
        "        'model_cpu_offload': {\n",
        "            'description': 'Modules d\u00e9plac\u00e9s CPU\u2194GPU au besoin',\n",
        "            'vram_required': 'Moyenne',\n",
        "            'speed': '~1.2x plus lent',\n",
        "            'method': 'pipe.enable_model_cpu_offload()'\n",
        "        },\n",
        "        'sequential_cpu_offload': {\n",
        "            'description': 'Un seul module sur GPU \u00e0 la fois',\n",
        "            'vram_required': 'Faible',\n",
        "            'speed': '~2x plus lent',\n",
        "            'method': 'pipe.enable_sequential_cpu_offload()'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    @classmethod\n",
        "    def get_strategy_for_vram(cls, vram_gb: float, model_size_gb: float = 4.0) -> str:\n",
        "        \"\"\"Recommande une strat\u00e9gie bas\u00e9e sur la VRAM.\"\"\"\n",
        "        ratio = vram_gb / model_size_gb\n",
        "        \n",
        "        if ratio >= 2.0:\n",
        "            return 'none'\n",
        "        elif ratio >= 1.2:\n",
        "            return 'model_cpu_offload'\n",
        "        else:\n",
        "            return 'sequential_cpu_offload'\n",
        "    \n",
        "    @classmethod\n",
        "    def print_strategies(cls):\n",
        "        \"\"\"Affiche les strat\u00e9gies disponibles.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"Strat\u00e9gies d'Offloading CPU\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        for name, info in cls.STRATEGIES.items():\n",
        "            print(f\"\\n\ud83d\udccc {name}\")\n",
        "            print(f\"   {info['description']}\")\n",
        "            print(f\"   VRAM: {info['vram_required']} | Vitesse: {info['speed']}\")\n",
        "            if info['method']:\n",
        "                print(f\"   Code: {info['method']}\")\n",
        "\n",
        "\n",
        "OffloadingStrategy.print_strategies()\n",
        "\n",
        "if CUDA_AVAILABLE:\n",
        "    recommended_strategy = OffloadingStrategy.get_strategy_for_vram(GPU_MEMORY_TOTAL, 4.0)\n",
        "    print(f\"\\n\ud83d\udca1 Strat\u00e9gie recommand\u00e9e pour {GPU_MEMORY_TOTAL:.0f}GB VRAM: {recommended_strategy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Optimisations de Vitesse\n",
        "\n",
        "### 4.1 Attention Optimizations (xFormers, Flash Attention)"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Strat\u00e9gies d'Offloading CPU\n",
            "======================================================================\n",
            "\n",
            "\ud83d\udccc none\n",
            "   Tout le mod\u00e8le sur GPU\n",
            "   VRAM: \u00c9lev\u00e9e | Vitesse: Maximale\n",
            "\n",
            "\ud83d\udccc model_cpu_offload\n",
            "   Modules d\u00e9plac\u00e9s CPU\u2194GPU au besoin\n",
            "   VRAM: Moyenne | Vitesse: ~1.2x plus lent\n",
            "   Code: pipe.enable_model_cpu_offload()\n",
            "\n",
            "\ud83d\udccc sequential_cpu_offload\n",
            "   Un seul module sur GPU \u00e0 la fois\n",
            "   VRAM: Faible | Vitesse: ~2x plus lent\n",
            "   Code: pipe.enable_sequential_cpu_offload()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionOptimizer:\n",
        "    \"\"\"\n",
        "    Gestionnaire des optimisations d'attention.\n",
        "    \n",
        "    Options:\n",
        "    - xFormers: Memory-efficient attention (NVIDIA)\n",
        "    - Flash Attention 2: Faster on Ampere+ GPUs\n",
        "    - SDPA: Scaled Dot Product Attention (PyTorch natif)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.xformers_available = self._check_xformers()\n",
        "        self.flash_attention_available = self._check_flash_attention()\n",
        "        self.sdpa_available = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "    \n",
        "    def _check_xformers(self) -> bool:\n",
        "        \"\"\"V\u00e9rifie si xFormers est install\u00e9.\"\"\"\n",
        "        try:\n",
        "            import xformers\n",
        "            return True\n",
        "        except ImportError:\n",
        "            return False\n",
        "    \n",
        "    def _check_flash_attention(self) -> bool:\n",
        "        \"\"\"V\u00e9rifie si Flash Attention est disponible.\"\"\"\n",
        "        if not CUDA_AVAILABLE:\n",
        "            return False\n",
        "        try:\n",
        "            capability = torch.cuda.get_device_capability()\n",
        "            return capability[0] >= 8  # Ampere+\n",
        "        except:\n",
        "            return False\n",
        "    \n",
        "    def get_status(self) -> Dict[str, bool]:\n",
        "        \"\"\"Retourne le statut de chaque optimisation.\"\"\"\n",
        "        return {\n",
        "            'xformers': self.xformers_available,\n",
        "            'flash_attention_2': self.flash_attention_available,\n",
        "            'sdpa': self.sdpa_available\n",
        "        }\n",
        "    \n",
        "    def get_recommended(self) -> str:\n",
        "        \"\"\"Retourne l'optimisation recommand\u00e9e.\"\"\"\n",
        "        if self.flash_attention_available:\n",
        "            return 'flash_attention_2'\n",
        "        elif self.xformers_available:\n",
        "            return 'xformers'\n",
        "        elif self.sdpa_available:\n",
        "            return 'sdpa'\n",
        "        return 'none'\n",
        "    \n",
        "    def get_diffusers_code(self, optimization: str) -> str:\n",
        "        \"\"\"G\u00e9n\u00e8re le code pour activer l'optimisation dans diffusers.\"\"\"\n",
        "        codes = {\n",
        "            'xformers': 'pipe.enable_xformers_memory_efficient_attention()',\n",
        "            'flash_attention_2': '''pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")''',\n",
        "            'sdpa': '# SDPA activ\u00e9 par d\u00e9faut dans PyTorch 2.0+',\n",
        "            'none': '# Pas d\\'optimisation d\\'attention'\n",
        "        }\n",
        "        return codes.get(optimization, codes['none'])\n",
        "\n",
        "\n",
        "attention_opt = AttentionOptimizer()\n",
        "status = attention_opt.get_status()\n",
        "\n",
        "print(\"\\n\ud83d\udcca Statut des Optimisations d'Attention\")\n",
        "print(\"=\"*50)\n",
        "for opt, available in status.items():\n",
        "    icon = '\u2705' if available else '\u274c'\n",
        "    print(f\"{icon} {opt}: {'Disponible' if available else 'Non disponible'}\")\n",
        "\n",
        "recommended_attention = attention_opt.get_recommended()\n",
        "print(f\"\\n\ud83d\udca1 Recommandation: {recommended_attention}\")\n",
        "print(f\"\\nCode:\")\n",
        "print(attention_opt.get_diffusers_code(recommended_attention))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Torch Compile (PyTorch 2.0+)"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udcca Statut des Optimisations d'Attention\n",
            "==================================================\n",
            "\u274c xformers: Non disponible\n",
            "\u274c flash_attention_2: Non disponible\n",
            "\u2705 sdpa: Disponible\n",
            "\n",
            "\ud83d\udca1 Recommandation: sdpa\n",
            "\n",
            "Code:\n",
            "# SDPA activ\u00e9 par d\u00e9faut dans PyTorch 2.0+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TorchCompileConfig:\n",
        "    \"\"\"\n",
        "    Configuration torch.compile pour acc\u00e9l\u00e9rer l'inf\u00e9rence.\n",
        "    \n",
        "    Modes:\n",
        "    - default: Bon \u00e9quilibre compilation/performance\n",
        "    - reduce-overhead: Minimise l'overhead, bon pour petits batches\n",
        "    - max-autotune: Performance maximale, compilation longue\n",
        "    \"\"\"\n",
        "    \n",
        "    MODES = {\n",
        "        'default': {\n",
        "            'description': '\u00c9quilibre compilation/performance',\n",
        "            'compile_time': 'Mod\u00e9r\u00e9',\n",
        "            'speedup': '10-20%',\n",
        "            'recommended_for': 'Usage g\u00e9n\u00e9ral'\n",
        "        },\n",
        "        'reduce-overhead': {\n",
        "            'description': 'Minimise l\\'overhead GPU',\n",
        "            'compile_time': 'Court',\n",
        "            'speedup': '5-15%',\n",
        "            'recommended_for': 'Petits batches, latence critique'\n",
        "        },\n",
        "        'max-autotune': {\n",
        "            'description': 'Performance maximale via autotuning',\n",
        "            'compile_time': 'Long (minutes)',\n",
        "            'speedup': '20-40%',\n",
        "            'recommended_for': 'Production, grands batches'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    @classmethod\n",
        "    def is_available(cls) -> bool:\n",
        "        \"\"\"V\u00e9rifie si torch.compile est disponible.\"\"\"\n",
        "        return hasattr(torch, 'compile') and torch.__version__ >= '2.0'\n",
        "    \n",
        "    @classmethod\n",
        "    def get_compile_code(cls, mode: str = 'default') -> str:\n",
        "        \"\"\"G\u00e9n\u00e8re le code de compilation.\"\"\"\n",
        "        return f'''# Compiler le mod\u00e8le UNet pour acc\u00e9l\u00e9rer l'inf\u00e9rence\n",
        "pipe.unet = torch.compile(\n",
        "    pipe.unet,\n",
        "    mode=\"{mode}\",\n",
        "    fullgraph=True\n",
        ")\n",
        "\n",
        "# Note: La premi\u00e8re inf\u00e9rence sera lente (compilation)\n",
        "# Les suivantes seront acc\u00e9l\u00e9r\u00e9es'''\n",
        "\n",
        "\n",
        "print(\"\ud83d\udcca Options torch.compile\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Disponible: {'\u2705 Oui' if TorchCompileConfig.is_available() else '\u274c Non (PyTorch < 2.0)'}\")\n",
        "\n",
        "for mode, info in TorchCompileConfig.MODES.items():\n",
        "    print(f\"\\n\ud83d\udccc {mode}\")\n",
        "    print(f\"   {info['description']}\")\n",
        "    print(f\"   Speedup: {info['speedup']} | Compilation: {info['compile_time']}\")\n",
        "    print(f\"   Pour: {info['recommended_for']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 VAE Optimizations (Tiling & Slicing)"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udcca Options torch.compile\n",
            "============================================================\n",
            "Disponible: \u2705 Oui\n",
            "\n",
            "\ud83d\udccc default\n",
            "   \u00c9quilibre compilation/performance\n",
            "   Speedup: 10-20% | Compilation: Mod\u00e9r\u00e9\n",
            "   Pour: Usage g\u00e9n\u00e9ral\n",
            "\n",
            "\ud83d\udccc reduce-overhead\n",
            "   Minimise l'overhead GPU\n",
            "   Speedup: 5-15% | Compilation: Court\n",
            "   Pour: Petits batches, latence critique\n",
            "\n",
            "\ud83d\udccc max-autotune\n",
            "   Performance maximale via autotuning\n",
            "   Speedup: 20-40% | Compilation: Long (minutes)\n",
            "   Pour: Production, grands batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VAEOptimizer:\n",
        "    \"\"\"\n",
        "    Optimisations pour le VAE (encodeur/d\u00e9codeur d'images).\n",
        "    \n",
        "    Le VAE est souvent le goulot d'\u00e9tranglement m\u00e9moire pour les grandes images.\n",
        "    \n",
        "    Techniques:\n",
        "    - Tiling: D\u00e9coupe l'image en tuiles pour le d\u00e9codage\n",
        "    - Slicing: Traite les channels par tranches\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def estimate_vae_memory(width: int, height: int, batch_size: int = 1) -> float:\n",
        "        \"\"\"\n",
        "        Estime la m\u00e9moire VAE requise (en MB).\n",
        "        \n",
        "        Formule approximative pour SD/SDXL VAE.\n",
        "        \"\"\"\n",
        "        # Latent size = image_size / 8\n",
        "        latent_h, latent_w = height // 8, width // 8\n",
        "        \n",
        "        # VAE channels = 4 (latent) ou 3 (RGB)\n",
        "        # Estimation: ~0.5MB par 64x64 pixels en FP16\n",
        "        pixels = width * height\n",
        "        mb_per_mpixel = 500  # ~500MB par megapixel\n",
        "        \n",
        "        return (pixels / 1_000_000) * mb_per_mpixel * batch_size\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_optimization_code(enable_tiling: bool = True, enable_slicing: bool = True) -> str:\n",
        "        \"\"\"G\u00e9n\u00e8re le code d'optimisation VAE.\"\"\"\n",
        "        lines = ['# Optimisations VAE pour grandes images']\n",
        "        \n",
        "        if enable_tiling:\n",
        "            lines.append('pipe.vae.enable_tiling()  # D\u00e9coupe en tuiles')\n",
        "        \n",
        "        if enable_slicing:\n",
        "            lines.append('pipe.vae.enable_slicing()  # Traite par tranches')\n",
        "        \n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "\n",
        "# Estimation m\u00e9moire VAE pour diff\u00e9rentes r\u00e9solutions\n",
        "print(\"\\n\ud83d\udcca Estimation M\u00e9moire VAE par R\u00e9solution\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "resolutions = [\n",
        "    (512, 512, \"SD 1.5 standard\"),\n",
        "    (768, 768, \"SD 1.5 high-res\"),\n",
        "    (1024, 1024, \"SDXL standard\"),\n",
        "    (1536, 1536, \"SDXL high-res\"),\n",
        "    (2048, 2048, \"Ultra high-res\")\n",
        "]\n",
        "\n",
        "for w, h, desc in resolutions:\n",
        "    mem = VAEOptimizer.estimate_vae_memory(w, h)\n",
        "    print(f\"{w}x{h} ({desc}): ~{mem:.0f} MB\")\n",
        "\n",
        "print(\"\\n\" + VAEOptimizer.get_optimization_code())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Batch Processing Optimis\u00e9\n",
        "\n",
        "Le traitement par lots peut significativement am\u00e9liorer le throughput."
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udcca Estimation M\u00e9moire VAE par R\u00e9solution\n",
            "==================================================\n",
            "512x512 (SD 1.5 standard): ~131 MB\n",
            "768x768 (SD 1.5 high-res): ~295 MB\n",
            "1024x1024 (SDXL standard): ~524 MB\n",
            "1536x1536 (SDXL high-res): ~1180 MB\n",
            "2048x2048 (Ultra high-res): ~2097 MB\n",
            "\n",
            "# Optimisations VAE pour grandes images\n",
            "pipe.vae.enable_tiling()  # D\u00e9coupe en tuiles\n",
            "pipe.vae.enable_slicing()  # Traite par tranches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BatchOptimizer:\n",
        "    \"\"\"\n",
        "    Optimiseur de taille de batch pour maximiser le throughput.\n",
        "    \n",
        "    Strat\u00e9gie:\n",
        "    1. Commencer avec batch_size=1\n",
        "    2. Doubler jusqu'\u00e0 OOM ou performance plateau\n",
        "    3. Retourner la taille optimale\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vram_gb: float, model_vram_gb: float = 4.0):\n",
        "        self.vram_gb = vram_gb\n",
        "        self.model_vram_gb = model_vram_gb\n",
        "        self.available_vram = vram_gb - model_vram_gb\n",
        "    \n",
        "    def estimate_optimal_batch_size(self, \n",
        "                                     width: int = 1024, \n",
        "                                     height: int = 1024,\n",
        "                                     precision: str = 'fp16') -> int:\n",
        "        \"\"\"\n",
        "        Estime la taille de batch optimale.\n",
        "        \n",
        "        Returns:\n",
        "            Taille de batch recommand\u00e9e\n",
        "        \"\"\"\n",
        "        # M\u00e9moire par image (estimation)\n",
        "        precision_factor = 0.5 if precision == 'fp16' else 1.0\n",
        "        mem_per_image_gb = (width * height * 4 * precision_factor) / (1024**3) * 50  # Facteur empirique\n",
        "        \n",
        "        # Garder 20% de marge\n",
        "        usable_vram = self.available_vram * 0.8\n",
        "        \n",
        "        optimal = max(1, int(usable_vram / mem_per_image_gb))\n",
        "        return min(optimal, 8)  # Cap \u00e0 8 pour \u00e9viter OOM\n",
        "    \n",
        "    def get_throughput_comparison(self) -> Dict[int, Dict]:\n",
        "        \"\"\"\n",
        "        Compare le throughput th\u00e9orique pour diff\u00e9rentes tailles de batch.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        # Temps d'inf\u00e9rence typique (secondes)\n",
        "        base_time = 2.0  # Pour batch=1\n",
        "        \n",
        "        for batch_size in [1, 2, 4, 8]:\n",
        "            # Le temps n'augmente pas lin\u00e9airement avec le batch\n",
        "            scaling_factor = 1 + (batch_size - 1) * 0.3  # ~30% overhead par image\n",
        "            total_time = base_time * scaling_factor\n",
        "            throughput = batch_size / total_time\n",
        "            \n",
        "            results[batch_size] = {\n",
        "                'time_seconds': total_time,\n",
        "                'throughput_img_per_sec': throughput,\n",
        "                'speedup_vs_batch1': throughput / (1 / base_time)\n",
        "            }\n",
        "        \n",
        "        return results\n",
        "\n",
        "\n",
        "if CUDA_AVAILABLE:\n",
        "    batch_opt = BatchOptimizer(GPU_MEMORY_TOTAL, model_vram_gb=4.0)\n",
        "    \n",
        "    print(\"\\n\ud83d\udcca Analyse Batch Size\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"VRAM totale: {GPU_MEMORY_TOTAL:.1f} GB\")\n",
        "    print(f\"VRAM pour mod\u00e8le: ~4.0 GB\")\n",
        "    print(f\"VRAM disponible pour batching: ~{batch_opt.available_vram:.1f} GB\")\n",
        "    \n",
        "    optimal = batch_opt.estimate_optimal_batch_size(1024, 1024, 'fp16')\n",
        "    print(f\"\\n\ud83d\udca1 Batch size optimal estim\u00e9 (1024x1024, FP16): {optimal}\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udcc8 Comparaison Throughput Th\u00e9orique:\")\n",
        "    print(\"-\"*60)\n",
        "    comparison = batch_opt.get_throughput_comparison()\n",
        "    for bs, metrics in comparison.items():\n",
        "        print(f\"Batch {bs}: {metrics['throughput_img_per_sec']:.2f} img/s (speedup: {metrics['speedup_vs_batch1']:.1f}x)\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f GPU requis pour l'analyse de batch sizing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Strat\u00e9gies de Caching\n",
        "\n",
        "Le caching intelligent peut \u00e9liminer les calculs redondants."
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u26a0\ufe0f GPU requis pour l'analyse de batch sizing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\n",
        "from functools import lru_cache\n",
        "\n",
        "class EmbeddingCache:\n",
        "    \"\"\"\n",
        "    Cache pour les embeddings de prompts.\n",
        "    \n",
        "    Les text encoders sont co\u00fbteux mais d\u00e9terministes.\n",
        "    Cacher les embeddings \u00e9vite de recalculer pour les m\u00eames prompts.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_size: int = 100):\n",
        "        self.cache: Dict[str, Any] = {}\n",
        "        self.max_size = max_size\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "    \n",
        "    def _hash_prompt(self, prompt: str, negative_prompt: str = \"\") -> str:\n",
        "        \"\"\"G\u00e9n\u00e8re un hash unique pour la paire de prompts.\"\"\"\n",
        "        combined = f\"{prompt}|||{negative_prompt}\"\n",
        "        return hashlib.md5(combined.encode()).hexdigest()\n",
        "    \n",
        "    def get(self, prompt: str, negative_prompt: str = \"\") -> Optional[Any]:\n",
        "        \"\"\"R\u00e9cup\u00e8re un embedding du cache.\"\"\"\n",
        "        key = self._hash_prompt(prompt, negative_prompt)\n",
        "        \n",
        "        if key in self.cache:\n",
        "            self.hits += 1\n",
        "            return self.cache[key]\n",
        "        \n",
        "        self.misses += 1\n",
        "        return None\n",
        "    \n",
        "    def put(self, prompt: str, negative_prompt: str, embedding: Any):\n",
        "        \"\"\"Stocke un embedding dans le cache.\"\"\"\n",
        "        # \u00c9viction LRU simple si cache plein\n",
        "        if len(self.cache) >= self.max_size:\n",
        "            oldest_key = next(iter(self.cache))\n",
        "            del self.cache[oldest_key]\n",
        "        \n",
        "        key = self._hash_prompt(prompt, negative_prompt)\n",
        "        self.cache[key] = embedding\n",
        "    \n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Retourne les statistiques du cache.\"\"\"\n",
        "        total = self.hits + self.misses\n",
        "        hit_rate = (self.hits / total * 100) if total > 0 else 0\n",
        "        \n",
        "        return {\n",
        "            'size': len(self.cache),\n",
        "            'max_size': self.max_size,\n",
        "            'hits': self.hits,\n",
        "            'misses': self.misses,\n",
        "            'hit_rate_percent': hit_rate\n",
        "        }\n",
        "\n",
        "\n",
        "# D\u00e9monstration du cache\n",
        "embedding_cache = EmbeddingCache(max_size=50)\n",
        "\n",
        "# Simuler des requ\u00eates\n",
        "test_prompts = [\n",
        "    \"a beautiful sunset over mountains\",\n",
        "    \"a cat sitting on a couch\",\n",
        "    \"a beautiful sunset over mountains\",  # R\u00e9p\u00e9tition\n",
        "    \"abstract art with vibrant colors\",\n",
        "    \"a cat sitting on a couch\",  # R\u00e9p\u00e9tition\n",
        "]\n",
        "\n",
        "print(\"\\n\ud83d\uddc4\ufe0f D\u00e9monstration Cache Embeddings\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    cached = embedding_cache.get(prompt)\n",
        "    if cached is None:\n",
        "        # Simuler calcul d'embedding\n",
        "        fake_embedding = f\"embedding_{len(prompt)}\"\n",
        "        embedding_cache.put(prompt, \"\", fake_embedding)\n",
        "        print(f\"\u274c MISS: '{prompt[:40]}...'\")\n",
        "    else:\n",
        "        print(f\"\u2705 HIT:  '{prompt[:40]}...'\")\n",
        "\n",
        "stats = embedding_cache.get_stats()\n",
        "print(f\"\\n\ud83d\udcca Statistiques Cache:\")\n",
        "print(f\"   Taille: {stats['size']}/{stats['max_size']}\")\n",
        "print(f\"   Hit Rate: {stats['hit_rate_percent']:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\uddc4\ufe0f D\u00e9monstration Cache Embeddings\n",
            "==================================================\n",
            "\u274c MISS: 'a beautiful sunset over mountains...'\n",
            "\u274c MISS: 'a cat sitting on a couch...'\n",
            "\u2705 HIT:  'a beautiful sunset over mountains...'\n",
            "\u274c MISS: 'abstract art with vibrant colors...'\n",
            "\u2705 HIT:  'a cat sitting on a couch...'\n",
            "\n",
            "\ud83d\udcca Statistiques Cache:\n",
            "   Taille: 3/50\n",
            "   Hit Rate: 40.0%\n"
          ]
        }
      ],
      "source": [
        "class ResultCache:\n",
        "    \"\"\"\n",
        "    Cache pour les images g\u00e9n\u00e9r\u00e9es (d\u00e9duplication).\n",
        "    \n",
        "    Utile pour:\n",
        "    - \u00c9viter de reg\u00e9n\u00e9rer des images identiques\n",
        "    - Servir rapidement des r\u00e9sultats r\u00e9cents\n",
        "    - Debugging et comparaisons A/B\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, cache_dir: str = \"./cache/results\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.index: Dict[str, Path] = {}\n",
        "        self._load_index()\n",
        "    \n",
        "    def _load_index(self):\n",
        "        \"\"\"Charge l'index du cache depuis le disque.\"\"\"\n",
        "        index_file = self.cache_dir / \"index.json\"\n",
        "        if index_file.exists():\n",
        "            with open(index_file) as f:\n",
        "                self.index = json.load(f)\n",
        "    \n",
        "    def _save_index(self):\n",
        "        \"\"\"Sauvegarde l'index sur disque.\"\"\"\n",
        "        index_file = self.cache_dir / \"index.json\"\n",
        "        with open(index_file, 'w') as f:\n",
        "            json.dump(self.index, f)\n",
        "    \n",
        "    def _generate_key(self, prompt: str, params: Dict) -> str:\n",
        "        \"\"\"G\u00e9n\u00e8re une cl\u00e9 unique bas\u00e9e sur prompt + param\u00e8tres.\"\"\"\n",
        "        # Inclure les param\u00e8tres qui affectent le r\u00e9sultat\n",
        "        key_data = {\n",
        "            'prompt': prompt,\n",
        "            'seed': params.get('seed'),\n",
        "            'width': params.get('width'),\n",
        "            'height': params.get('height'),\n",
        "            'steps': params.get('num_inference_steps'),\n",
        "            'guidance': params.get('guidance_scale')\n",
        "        }\n",
        "        key_str = json.dumps(key_data, sort_keys=True)\n",
        "        return hashlib.sha256(key_str.encode()).hexdigest()[:16]\n",
        "    \n",
        "    def get(self, prompt: str, params: Dict) -> Optional[Path]:\n",
        "        \"\"\"R\u00e9cup\u00e8re un r\u00e9sultat du cache.\"\"\"\n",
        "        key = self._generate_key(prompt, params)\n",
        "        if key in self.index:\n",
        "            path = Path(self.index[key])\n",
        "            if path.exists():\n",
        "                return path\n",
        "        return None\n",
        "    \n",
        "    def put(self, prompt: str, params: Dict, image_path: Path) -> str:\n",
        "        \"\"\"Ajoute un r\u00e9sultat au cache.\"\"\"\n",
        "        key = self._generate_key(prompt, params)\n",
        "        self.index[key] = str(image_path)\n",
        "        self._save_index()\n",
        "        return key\n",
        "\n",
        "\n",
        "print(\"\\n\ud83d\udce6 Configuration ResultCache\")\n",
        "print(f\"   R\u00e9pertoire: ./cache/results\")\n",
        "print(f\"   Usage: D\u00e9dupliquer les g\u00e9n\u00e9rations identiques\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Pipeline Optimis\u00e9 Complet\n",
        "\n",
        "Combinons toutes les optimisations dans un pipeline unifi\u00e9."
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udce6 Configuration ResultCache\n",
            "   R\u00e9pertoire: ./cache/results\n",
            "   Usage: D\u00e9dupliquer les g\u00e9n\u00e9rations identiques\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class OptimizationProfile:\n",
        "    \"\"\"Profil d'optimisation pr\u00e9d\u00e9fini.\"\"\"\n",
        "    name: str\n",
        "    precision: str\n",
        "    quantization: Optional[str]\n",
        "    attention: str\n",
        "    cpu_offload: str\n",
        "    vae_tiling: bool\n",
        "    vae_slicing: bool\n",
        "    torch_compile: Optional[str]\n",
        "    batch_size: int\n",
        "    description: str\n",
        "\n",
        "\n",
        "class OptimizedPipelineFactory:\n",
        "    \"\"\"\n",
        "    Factory pour cr\u00e9er des pipelines optimis\u00e9s selon le mat\u00e9riel.\n",
        "    \n",
        "    Profils pr\u00e9d\u00e9finis:\n",
        "    - low_vram: Pour GPUs 4-6GB (GTX 1060, RTX 3050)\n",
        "    - medium_vram: Pour GPUs 8-12GB (RTX 3060/3070)\n",
        "    - high_vram: Pour GPUs 16GB+ (RTX 3080/3090/4090)\n",
        "    - production: Maximum performance pour serving\n",
        "    \"\"\"\n",
        "    \n",
        "    PROFILES = {\n",
        "        'low_vram': OptimizationProfile(\n",
        "            name='Low VRAM (4-6GB)',\n",
        "            precision='fp16',\n",
        "            quantization='int8',\n",
        "            attention='xformers',\n",
        "            cpu_offload='sequential_cpu_offload',\n",
        "            vae_tiling=True,\n",
        "            vae_slicing=True,\n",
        "            torch_compile=None,\n",
        "            batch_size=1,\n",
        "            description='Optimis\u00e9 pour GPUs limit\u00e9s, priorise la compatibilit\u00e9'\n",
        "        ),\n",
        "        'medium_vram': OptimizationProfile(\n",
        "            name='Medium VRAM (8-12GB)',\n",
        "            precision='fp16',\n",
        "            quantization=None,\n",
        "            attention='sdpa',\n",
        "            cpu_offload='model_cpu_offload',\n",
        "            vae_tiling=True,\n",
        "            vae_slicing=False,\n",
        "            torch_compile='default',\n",
        "            batch_size=2,\n",
        "            description='Bon \u00e9quilibre performance/m\u00e9moire'\n",
        "        ),\n",
        "        'high_vram': OptimizationProfile(\n",
        "            name='High VRAM (16GB+)',\n",
        "            precision='bf16',\n",
        "            quantization=None,\n",
        "            attention='flash_attention_2',\n",
        "            cpu_offload='none',\n",
        "            vae_tiling=False,\n",
        "            vae_slicing=False,\n",
        "            torch_compile='reduce-overhead',\n",
        "            batch_size=4,\n",
        "            description='Performance maximale pour GPUs haut de gamme'\n",
        "        ),\n",
        "        'production': OptimizationProfile(\n",
        "            name='Production Server',\n",
        "            precision='fp16',\n",
        "            quantization=None,\n",
        "            attention='flash_attention_2',\n",
        "            cpu_offload='none',\n",
        "            vae_tiling=False,\n",
        "            vae_slicing=False,\n",
        "            torch_compile='max-autotune',\n",
        "            batch_size=8,\n",
        "            description='Throughput maximum, temps de warmup long'\n",
        "        )\n",
        "    }\n",
        "    \n",
        "    @classmethod\n",
        "    def get_profile_for_vram(cls, vram_gb: float) -> OptimizationProfile:\n",
        "        \"\"\"S\u00e9lectionne automatiquement le profil optimal.\"\"\"\n",
        "        if vram_gb < 8:\n",
        "            return cls.PROFILES['low_vram']\n",
        "        elif vram_gb < 16:\n",
        "            return cls.PROFILES['medium_vram']\n",
        "        else:\n",
        "            return cls.PROFILES['high_vram']\n",
        "    \n",
        "    @classmethod\n",
        "    def generate_pipeline_code(cls, profile: OptimizationProfile) -> str:\n",
        "        \"\"\"G\u00e9n\u00e8re le code Python pour cr\u00e9er un pipeline optimis\u00e9.\"\"\"\n",
        "        lines = [\n",
        "            f'# Pipeline Optimis\u00e9: {profile.name}',\n",
        "            f'# {profile.description}',\n",
        "            '',\n",
        "            'import torch',\n",
        "            'from diffusers import StableDiffusionXLPipeline',\n",
        "            ''\n",
        "        ]\n",
        "        \n",
        "        # Dtype\n",
        "        dtype_map = {'fp32': 'torch.float32', 'fp16': 'torch.float16', 'bf16': 'torch.bfloat16'}\n",
        "        dtype = dtype_map.get(profile.precision, 'torch.float16')\n",
        "        \n",
        "        # Load pipeline\n",
        "        load_args = [f'torch_dtype={dtype}']\n",
        "        if profile.attention == 'flash_attention_2':\n",
        "            load_args.append('attn_implementation=\"flash_attention_2\"')\n",
        "        \n",
        "        lines.append('pipe = StableDiffusionXLPipeline.from_pretrained(')\n",
        "        lines.append('    \"stabilityai/stable-diffusion-xl-base-1.0\",')\n",
        "        for arg in load_args:\n",
        "            lines.append(f'    {arg},')\n",
        "        lines.append(').to(\"cuda\")')\n",
        "        lines.append('')\n",
        "        \n",
        "        # Optimizations\n",
        "        lines.append('# Optimisations')\n",
        "        \n",
        "        if profile.attention == 'xformers':\n",
        "            lines.append('pipe.enable_xformers_memory_efficient_attention()')\n",
        "        \n",
        "        if profile.cpu_offload == 'model_cpu_offload':\n",
        "            lines.append('pipe.enable_model_cpu_offload()')\n",
        "        elif profile.cpu_offload == 'sequential_cpu_offload':\n",
        "            lines.append('pipe.enable_sequential_cpu_offload()')\n",
        "        \n",
        "        if profile.vae_tiling:\n",
        "            lines.append('pipe.vae.enable_tiling()')\n",
        "        if profile.vae_slicing:\n",
        "            lines.append('pipe.vae.enable_slicing()')\n",
        "        \n",
        "        if profile.torch_compile:\n",
        "            lines.append('')\n",
        "            lines.append(f'# torch.compile (mode={profile.torch_compile})')\n",
        "            lines.append(f'pipe.unet = torch.compile(pipe.unet, mode=\"{profile.torch_compile}\")')\n",
        "        \n",
        "        lines.append('')\n",
        "        lines.append(f'# Batch size recommand\u00e9: {profile.batch_size}')\n",
        "        \n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "\n",
        "# Afficher tous les profils\n",
        "print(\"\\n\ud83d\udccb Profils d'Optimisation Disponibles\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for name, profile in OptimizedPipelineFactory.PROFILES.items():\n",
        "    print(f\"\\n\ud83d\udd27 {profile.name}\")\n",
        "    print(f\"   Pr\u00e9cision: {profile.precision} | Attention: {profile.attention}\")\n",
        "    print(f\"   Offload: {profile.cpu_offload} | Batch: {profile.batch_size}\")\n",
        "    print(f\"   {profile.description}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udccb Profils d'Optimisation Disponibles\n",
            "======================================================================\n",
            "\n",
            "\ud83d\udd27 Low VRAM (4-6GB)\n",
            "   Pr\u00e9cision: fp16 | Attention: xformers\n",
            "   Offload: sequential_cpu_offload | Batch: 1\n",
            "   Optimis\u00e9 pour GPUs limit\u00e9s, priorise la compatibilit\u00e9\n",
            "\n",
            "\ud83d\udd27 Medium VRAM (8-12GB)\n",
            "   Pr\u00e9cision: fp16 | Attention: sdpa\n",
            "   Offload: model_cpu_offload | Batch: 2\n",
            "   Bon \u00e9quilibre performance/m\u00e9moire\n",
            "\n",
            "\ud83d\udd27 High VRAM (16GB+)\n",
            "   Pr\u00e9cision: bf16 | Attention: flash_attention_2\n",
            "   Offload: none | Batch: 4\n",
            "   Performance maximale pour GPUs haut de gamme\n",
            "\n",
            "\ud83d\udd27 Production Server\n",
            "   Pr\u00e9cision: fp16 | Attention: flash_attention_2\n",
            "   Offload: none | Batch: 8\n",
            "   Throughput maximum, temps de warmup long\n"
          ]
        }
      ],
      "source": [
        "# G\u00e9n\u00e9rer le code pour notre GPU\n",
        "if CUDA_AVAILABLE:\n",
        "    optimal_profile = OptimizedPipelineFactory.get_profile_for_vram(GPU_MEMORY_TOTAL)\n",
        "    \n",
        "    print(f\"\\n\ud83d\udca1 Profil Recommand\u00e9 pour {GPU_NAME} ({GPU_MEMORY_TOTAL:.0f}GB):\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    code = OptimizedPipelineFactory.generate_pipeline_code(optimal_profile)\n",
        "    print(code)\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f GPU requis pour g\u00e9n\u00e9rer le profil optimal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Benchmarking et Comparaison\n",
        "\n",
        "Mesurons l'impact des diff\u00e9rentes optimisations."
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u26a0\ufe0f GPU requis pour g\u00e9n\u00e9rer le profil optimal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BenchmarkSuite:\n",
        "    \"\"\"\n",
        "    Suite de benchmarks pour comparer les optimisations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, profiler: GPUProfiler):\n",
        "        self.profiler = profiler\n",
        "        self.results: List[Dict] = []\n",
        "    \n",
        "    def run_synthetic_benchmark(self, name: str, \n",
        "                                 memory_mb: int = 500,\n",
        "                                 compute_ms: int = 100,\n",
        "                                 iterations: int = 3) -> Dict:\n",
        "        \"\"\"\n",
        "        Ex\u00e9cute un benchmark synth\u00e9tique.\n",
        "        \n",
        "        Pour un benchmark r\u00e9el, remplacer par l'inf\u00e9rence du mod\u00e8le.\n",
        "        \"\"\"\n",
        "        times = []\n",
        "        memories = []\n",
        "        \n",
        "        for i in range(iterations):\n",
        "            with self.profiler.profile(f\"{name}_iter{i}\"):\n",
        "                simulate_model_inference(memory_mb, compute_ms)\n",
        "            \n",
        "            metrics = self.profiler.metrics_history[-1]\n",
        "            times.append(metrics.execution_time_ms)\n",
        "            memories.append(metrics.gpu_memory_peak_mb)\n",
        "        \n",
        "        result = {\n",
        "            'name': name,\n",
        "            'avg_time_ms': sum(times) / len(times),\n",
        "            'min_time_ms': min(times),\n",
        "            'max_time_ms': max(times),\n",
        "            'avg_memory_mb': sum(memories) / len(memories),\n",
        "            'iterations': iterations\n",
        "        }\n",
        "        \n",
        "        self.results.append(result)\n",
        "        return result\n",
        "    \n",
        "    def print_results(self):\n",
        "        \"\"\"Affiche les r\u00e9sultats de benchmark.\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"Aucun r\u00e9sultat de benchmark\")\n",
        "            return\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"R\u00c9SULTATS BENCHMARK\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"{'Configuration':<25} {'Temps Moy (ms)':<18} {'M\u00e9moire (MB)':<15} {'Speedup':<10}\")\n",
        "        print(\"-\"*80)\n",
        "        \n",
        "        baseline_time = self.results[0]['avg_time_ms'] if self.results else 1\n",
        "        \n",
        "        for r in self.results:\n",
        "            speedup = baseline_time / r['avg_time_ms']\n",
        "            print(f\"{r['name']:<25} {r['avg_time_ms']:<18.1f} {r['avg_memory_mb']:<15.1f} {speedup:<10.2f}x\")\n",
        "        \n",
        "        print(\"=\"*80)\n",
        "    \n",
        "    def export_results(self, filepath: str = \"benchmark_results.json\"):\n",
        "        \"\"\"Exporte les r\u00e9sultats en JSON.\"\"\"\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump({\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'gpu': GPU_NAME if CUDA_AVAILABLE else 'CPU',\n",
        "                'vram_gb': GPU_MEMORY_TOTAL if CUDA_AVAILABLE else 0,\n",
        "                'results': self.results\n",
        "            }, f, indent=2)\n",
        "        print(f\"\\n\ud83d\udcc1 R\u00e9sultats export\u00e9s vers {filepath}\")\n",
        "\n",
        "\n",
        "# Ex\u00e9cuter les benchmarks\n",
        "print(\"\\n\ud83c\udfc3 Ex\u00e9cution des Benchmarks...\")\n",
        "benchmark = BenchmarkSuite(profiler)\n",
        "\n",
        "# Benchmark FP32 (baseline)\n",
        "benchmark.run_synthetic_benchmark(\"Baseline (FP32)\", memory_mb=500, compute_ms=150)\n",
        "\n",
        "# Benchmark FP16\n",
        "benchmark.run_synthetic_benchmark(\"FP16\", memory_mb=250, compute_ms=120)\n",
        "\n",
        "# Benchmark avec optimisations\n",
        "benchmark.run_synthetic_benchmark(\"FP16 + Optimisations\", memory_mb=250, compute_ms=80)\n",
        "\n",
        "benchmark.print_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. R\u00e9sum\u00e9 et Recommandations\n",
        "\n",
        "Voici les points cl\u00e9s pour optimiser vos pipelines de g\u00e9n\u00e9ration d'images."
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83c\udfc3 Ex\u00e9cution des Benchmarks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "R\u00c9SULTATS BENCHMARK\n",
            "================================================================================\n",
            "Configuration             Temps Moy (ms)     M\u00e9moire (MB)    Speedup   \n",
            "--------------------------------------------------------------------------------\n",
            "Baseline (FP32)           150.2              0.0             1.00      x\n",
            "FP16                      120.4              0.0             1.25      x\n",
            "FP16 + Optimisations      80.4               0.0             1.87      x\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_optimization_report() -> str:\n",
        "    \"\"\"G\u00e9n\u00e8re un rapport de recommandations personnalis\u00e9es.\"\"\"\n",
        "    report = []\n",
        "    report.append(\"\\n\" + \"=\"*70)\n",
        "    report.append(\"\ud83d\udcca RAPPORT D'OPTIMISATION PERSONNALIS\u00c9\")\n",
        "    report.append(\"=\"*70)\n",
        "    \n",
        "    if CUDA_AVAILABLE:\n",
        "        report.append(f\"\\n\ud83d\udda5\ufe0f Configuration D\u00e9tect\u00e9e:\")\n",
        "        report.append(f\"   GPU: {GPU_NAME}\")\n",
        "        report.append(f\"   VRAM: {GPU_MEMORY_TOTAL:.1f} GB\")\n",
        "        report.append(f\"   GPUs: {GPU_COUNT}\")\n",
        "        \n",
        "        # Profil recommand\u00e9\n",
        "        profile = OptimizedPipelineFactory.get_profile_for_vram(GPU_MEMORY_TOTAL)\n",
        "        report.append(f\"\\n\ud83d\udca1 Profil Recommand\u00e9: {profile.name}\")\n",
        "        \n",
        "        report.append(f\"\\n\ud83d\udccb Recommandations:\")\n",
        "        report.append(f\"   \u2713 Pr\u00e9cision: {profile.precision.upper()}\")\n",
        "        report.append(f\"   \u2713 Attention: {profile.attention}\")\n",
        "        report.append(f\"   \u2713 Offloading: {profile.cpu_offload}\")\n",
        "        report.append(f\"   \u2713 Batch size: {profile.batch_size}\")\n",
        "        \n",
        "        if profile.vae_tiling:\n",
        "            report.append(f\"   \u2713 VAE Tiling: Activ\u00e9 (images haute r\u00e9solution)\")\n",
        "        if profile.torch_compile:\n",
        "            report.append(f\"   \u2713 torch.compile: {profile.torch_compile}\")\n",
        "        \n",
        "        # \u00c9conomies estim\u00e9es\n",
        "        savings = precision_mgr.estimate_memory_savings(4000, profile.precision)\n",
        "        report.append(f\"\\n\ud83d\udcc8 Impact Estim\u00e9:\")\n",
        "        report.append(f\"   \u00c9conomie m\u00e9moire: {savings['savings_percent']:.0f}%\")\n",
        "        report.append(f\"   VRAM mod\u00e8le: ~{savings['optimized_mb']/1000:.1f} GB\")\n",
        "        \n",
        "    else:\n",
        "        report.append(\"\\n\u26a0\ufe0f Mode CPU uniquement d\u00e9tect\u00e9\")\n",
        "        report.append(\"   Les optimisations GPU ne sont pas applicables\")\n",
        "        report.append(\"   Consid\u00e9rez l'utilisation d'APIs cloud (fal.ai, Replicate)\")\n",
        "    \n",
        "    report.append(\"\\n\" + \"=\"*70)\n",
        "    return \"\\n\".join(report)\n",
        "\n",
        "\n",
        "print(generate_optimization_report())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "\ud83d\udcca RAPPORT D'OPTIMISATION PERSONNALIS\u00c9\n",
            "======================================================================\n",
            "\n",
            "\u26a0\ufe0f Mode CPU uniquement d\u00e9tect\u00e9\n",
            "   Les optimisations GPU ne sont pas applicables\n",
            "   Consid\u00e9rez l'utilisation d'APIs cloud (fal.ai, Replicate)\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Tableau r\u00e9capitulatif des techniques\n",
        "print(\"\\n\ud83d\udcda R\u00c9CAPITULATIF DES TECHNIQUES D'OPTIMISATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "techniques = [\n",
        "    (\"Pr\u00e9cision FP16/BF16\", \"50%\", \"Aucun\", \"\u2605\u2605\u2605\u2605\u2605\"),\n",
        "    (\"Quantification INT8\", \"75%\", \"Minime\", \"\u2605\u2605\u2605\u2605\u2606\"),\n",
        "    (\"xFormers Attention\", \"30%\", \"Aucun\", \"\u2605\u2605\u2605\u2605\u2605\"),\n",
        "    (\"Flash Attention 2\", \"40%\", \"Aucun\", \"\u2605\u2605\u2605\u2605\u2605\"),\n",
        "    (\"torch.compile\", \"Variable\", \"Aucun\", \"\u2605\u2605\u2605\u2606\u2606\"),\n",
        "    (\"VAE Tiling\", \"50%+\", \"Aucun\", \"\u2605\u2605\u2605\u2605\u2606\"),\n",
        "    (\"CPU Offload\", \"70%+\", \"Latence\", \"\u2605\u2605\u2605\u2606\u2606\"),\n",
        "    (\"Embedding Cache\", \"N/A\", \"Aucun\", \"\u2605\u2605\u2605\u2605\u2605\"),\n",
        "]\n",
        "\n",
        "print(f\"{'Technique':<25} {'\u00c9con. M\u00e9moire':<15} {'Impact Qualit\u00e9':<15} {'Recommand\u00e9'}\")\n",
        "print(\"-\"*70)\n",
        "for tech, mem, quality, rec in techniques:\n",
        "    print(f\"{tech:<25} {mem:<15} {quality:<15} {rec}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Conseil: Combinez plusieurs techniques pour des gains cumulatifs!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Exercices Pratiques\n",
        "\n",
        "### Exercice 1: Profilage de votre pipeline\n",
        "Utilisez le `GPUProfiler` pour mesurer les performances de votre propre pipeline de g\u00e9n\u00e9ration.\n",
        "\n",
        "### Exercice 2: Comparaison A/B\n",
        "Comparez la qualit\u00e9 d'image entre FP32 et FP16 sur 10 prompts identiques.\n",
        "\n",
        "### Exercice 3: Optimisation de batch\n",
        "Trouvez le batch size optimal pour votre GPU en mesurant le throughput."
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udcda R\u00c9CAPITULATIF DES TECHNIQUES D'OPTIMISATION\n",
            "======================================================================\n",
            "Technique                 \u00c9con. M\u00e9moire   Impact Qualit\u00e9  Recommand\u00e9\n",
            "----------------------------------------------------------------------\n",
            "Pr\u00e9cision FP16/BF16       50%             Aucun           \u2605\u2605\u2605\u2605\u2605\n",
            "Quantification INT8       75%             Minime          \u2605\u2605\u2605\u2605\u2606\n",
            "xFormers Attention        30%             Aucun           \u2605\u2605\u2605\u2605\u2605\n",
            "Flash Attention 2         40%             Aucun           \u2605\u2605\u2605\u2605\u2605\n",
            "torch.compile             Variable        Aucun           \u2605\u2605\u2605\u2606\u2606\n",
            "VAE Tiling                50%+            Aucun           \u2605\u2605\u2605\u2605\u2606\n",
            "CPU Offload               70%+            Latence         \u2605\u2605\u2605\u2606\u2606\n",
            "Embedding Cache           N/A             Aucun           \u2605\u2605\u2605\u2605\u2605\n",
            "\n",
            "\ud83d\udca1 Conseil: Combinez plusieurs techniques pour des gains cumulatifs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finalisation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83c\udf89 NOTEBOOK TERMIN\u00c9\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Statistiques du profiler\n",
        "print(f\"\\n\ud83d\udcca Statistiques de la session:\")\n",
        "print(f\"   Tests ex\u00e9cut\u00e9s: {len(profiler.metrics_history)}\")\n",
        "\n",
        "if profiler.metrics_history:\n",
        "    avg_time = sum(m.execution_time_ms for m in profiler.metrics_history) / len(profiler.metrics_history)\n",
        "    print(f\"   Temps moyen: {avg_time:.1f} ms\")\n",
        "\n",
        "print(\"\\n\ud83d\udcda Prochaines \u00e9tapes:\")\n",
        "print(\"   \u2192 Appliquer les optimisations \u00e0 vos pipelines de production\")\n",
        "print(\"   \u2192 Benchmarker avec vos mod\u00e8les r\u00e9els\")\n",
        "print(\"   \u2192 Explorer les techniques de multi-GPU si disponible\")\n",
        "\n",
        "print(\"\\n\u2705 Module 03-Images-Orchestration compl\u00e9t\u00e9!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}