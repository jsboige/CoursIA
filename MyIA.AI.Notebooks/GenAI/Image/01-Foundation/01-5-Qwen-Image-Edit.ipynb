{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79be9a9e",
   "metadata": {},
   "source": [
    "# Notebook: Qwen Image-Edit 2.5 - API ComfyUI\n",
    "\n",
    "**Objectif**: Ma√Ætriser g√©n√©ration et √©dition d'images via API Qwen-Image-Edit (backend ComfyUI)\n",
    "\n",
    "## üéØ Ce que vous allez apprendre\n",
    "\n",
    "1. Diff√©rence API **Forge** (simple) vs **ComfyUI** (workflows JSON)\n",
    "2. Pattern **\"queue and poll\"** pour g√©n√©ration asynchrone\n",
    "3. Cr√©ation workflows **Text-to-Image** et **Image-to-Image**\n",
    "4. Optimisation param√®tres (**steps**, **cfg**, **denoise**)\n",
    "5. Troubleshooting erreurs courantes (**timeout**, **CUDA OOM**)\n",
    "\n",
    "## üöÄ API Qwen-Image-Edit\n",
    "\n",
    "| Caract√©ristique | Valeur |\n",
    "|----------------|--------|\n",
    "| **URL Production** | `https://qwen-image-edit.myia.io` |\n",
    "| **Mod√®le** | Qwen-Image-Edit-2509-FP8 (54GB) |\n",
    "| **GPU** | RTX 3090 (24GB VRAM) |\n",
    "| **Latence Typique** | 5-10 secondes |\n",
    "| **R√©solution Optimale** | 512x512 pixels |\n",
    "\n",
    "## üîç ComfyUI vs Forge\n",
    "\n",
    "**Forge (SD XL Turbo)**:\n",
    "- ‚úÖ API simple (1 requ√™te POST)\n",
    "- ‚úÖ Ultra-rapide (1-3s)\n",
    "- ‚ùå Pas d'√©dition images\n",
    "- ‚ùå Moins flexible\n",
    "\n",
    "**ComfyUI (Qwen)**:\n",
    "- ‚úÖ Workflows JSON complexes\n",
    "- ‚úÖ √âdition images avanc√©e\n",
    "- ‚úÖ Contr√¥le fin (28 custom nodes)\n",
    "- ‚ùå API plus complexe (queue + poll)\n",
    "\n",
    "**Recommandation**: Commencer avec Forge pour prototypes, affiner avec Qwen pour production.\n",
    "\n",
    "## üìö Pr√©requis\n",
    "\n",
    "```bash\n",
    "pip install requests pillow matplotlib\n",
    "```\n",
    "\n",
    "**Temps estim√©**: 90-120 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ikycba75qmf",
   "source": "## ‚öôÔ∏è V√©rification de l'Environnement\n\n**Avant d'ex√©cuter ce notebook**, assurez-vous que tous les packages Python requis sont install√©s :\n\n```bash\npip install pillow requests matplotlib python-dotenv\n```\n\n### Packages Requis\n\n| Package | Utilisation | Installation |\n|---------|-------------|--------------|\n| **Pillow** | Manipulation d'images (API PIL) | `pip install pillow` |\n| **requests** | Appels API ComfyUI | `pip install requests` |\n| **matplotlib** | Visualisation r√©sultats | `pip install matplotlib` |\n| **python-dotenv** | Chargement variables d'environnement | `pip install python-dotenv` |\n\n**‚ö†Ô∏è Note importante** : Le package `Pillow` fournit l'API `PIL`. Si vous obtenez `ModuleNotFoundError: No module named 'PIL'`, installez Pillow avec :\n\n```bash\npip install --upgrade pillow\n```\n\nPour v√©rifier l'installation, ex√©cutez dans un terminal Python :\n\n```python\nimport PIL\nfrom PIL import Image\nprint(f\"‚úÖ PIL {PIL.__version__} install√©\")\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244bd967",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Visualisation\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpatches\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "# Imports standard\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import time\n",
    "import uuid\n",
    "import os\n",
    "from typing import Dict, Optional, List\n",
    "from io import BytesIO\n",
    "\n",
    "# Visualisation\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Configuration environnement\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration API\n",
    "API_BASE_URL = \"https://qwen-image-edit.myia.io\"\n",
    "CLIENT_ID = str(uuid.uuid4())  # ID unique pour tracking\n",
    "\n",
    "# Authentification - Token ComfyUI standardis√©\n",
    "COMFYUI_API_TOKEN = os.getenv(\"COMFYUI_API_TOKEN\")\n",
    "\n",
    "if not COMFYUI_API_TOKEN:\n",
    "    print(\"‚ö†Ô∏è  COMFYUI_API_TOKEN non trouv√© - connexion sans authentification\")\n",
    "    print(\"\\nüìã Pour activer l'authentification :\")\n",
    "    print(\"   1. Cr√©ez un fichier .env dans MyIA.AI.Notebooks/GenAI/\")\n",
    "    print(\"   2. Ajoutez : COMFYUI_API_TOKEN=votre_token_ici\")\n",
    "    print(\"   3. Obtenez le token via scripts/genai-auth/extract-bearer-tokens.ps1\")\n",
    "    print(\"\\n‚úì Le notebook fonctionnera en mode d√©grad√© (si serveur non s√©curis√©)\\n\")\n",
    "    COMFYUI_API_TOKEN = None\n",
    "else:\n",
    "    print(\"‚úÖ Configuration charg√©e\")\n",
    "    print(f\"üì° API: {API_BASE_URL}\")\n",
    "    print(f\"üÜî Client ID: {CLIENT_ID}\")\n",
    "    print(f\"üîê Token: {COMFYUI_API_TOKEN[:15]}...{COMFYUI_API_TOKEN[-5:]}\" if len(COMFYUI_API_TOKEN) > 20 else f\"üîê Token: (court)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5743a6a4",
   "metadata": {},
   "source": "## üèóÔ∏è Architecture ComfyUI: Workflows JSON\n\n### Diff√©rence fondamentale avec Forge\n\n**API Forge (POST direct)**:\n```python\nresponse = requests.post(url, json={\"prompt\": \"astronaut\"})\nimage_base64 = response.json()[\"image\"]\n```\n\n**API ComfyUI (Queue + Poll)**:\n```python\n# 1. Soumettre workflow JSON\nresponse = requests.post(f\"{url}/prompt\", json={\n    \"prompt\": workflow_json,\n    \"client_id\": client_id\n})\nprompt_id = response.json()[\"prompt_id\"]\n\n# 2. Attendre compl√©tion (polling)\nwhile True:\n    history = requests.get(f\"{url}/history/{prompt_id}\")\n    if history.json().get(prompt_id, {}).get(\"status\", {}).get(\"completed\"):\n        break\n    time.sleep(1)\n\n# 3. R√©cup√©rer images\nimages = history.json()[prompt_id][\"outputs\"]\n```\n\n### Structure Workflow ComfyUI\n\nUn **workflow** est un **graph JSON** de **nodes connect√©s**:\n\n```json\n{\n  \"1\": {  // Node VAE Loader\n    \"class_type\": \"VAELoader\",\n    \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n  },\n  \"2\": {  // Node KSampler\n    \"class_type\": \"KSampler\",\n    \"inputs\": {\n      \"model\": [\"5\", 0],  // Connexion: output du node 5 (CFGNorm)\n      \"steps\": 20,\n      \"cfg\": 1.0,\n      \"seed\": 42\n    }\n  },\n  \"3\": {  // Node Save Image\n    \"class_type\": \"SaveImage\",\n    \"inputs\": {\"images\": [\"2\", 0]}\n  }\n}\n```\n\n**Workflow = Pipeline modulaire** ou chaque node effectue une operation (charger modele, sampler, encoder texte, etc.).\n\n### Anatomie d'un Node\n\n| Propri√©t√© | Description | Exemple |\n|-----------|-------------|----------|\n| **`class_type`** | Type de node ComfyUI | `\"TextEncodeQwenImageEdit\"` |\n| **`inputs`** | Param√®tres du node | `{\"prompt\": \"astronaut\"}` |\n| **Connexions** | `[node_id, output_slot]` | `[\"5\", 0]` |\n\n### Architecture Qwen: Loaders Separes (Phase 29)\n\nQwen Image Edit n'utilise **pas** `CheckpointLoaderSimple` car les poids du modele sont stockes dans des fichiers separes :\n\n| Loader | Fichier | Role |\n|--------|---------|------|\n| **VAELoader** | `qwen_image_vae.safetensors` | VAE 16 canaux (pas SDXL standard) |\n| **CLIPLoader** | `qwen_2.5_vl_7b_fp8_scaled.safetensors` (type `sd3`) | Encodeur vision-language |\n| **UNETLoader** | `qwen_image_edit_2509_fp8_e4m3fn.safetensors` | Modele de diffusion |\n\nApres chargement, le UNET passe par **ModelSamplingAuraFlow** (shift=3.0) puis **CFGNorm** (strength=1.0) avant d'arriver au KSampler.\n\nLe prompt est encode avec **TextEncodeQwenImageEdit** (encodeur natif Qwen, pas CLIPTextEncode standard), et le conditioning negatif utilise **ConditioningZeroOut** (pas un second encodeur texte).\n\nL'espace latent utilise **EmptySD3LatentImage** (16 canaux) au lieu de EmptyLatentImage (4 canaux SDXL)."
  },
  {
   "cell_type": "markdown",
   "id": "851cb759",
   "metadata": {},
   "source": "### üîß Visualisation Architecture Workflow ComfyUI (Phase 29 Qwen)\n\n**Diagramme ASCII du workflow Qwen avec loaders separes**:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                  WORKFLOW QWEN PHASE 29                        ‚îÇ\n‚îÇ                                                                 ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n‚îÇ  ‚îÇ VAELoader    ‚îÇ  ‚îÇ CLIPLoader   ‚îÇ  ‚îÇ UNETLoader   ‚îÇ        ‚îÇ\n‚îÇ  ‚îÇ (16ch VAE)   ‚îÇ  ‚îÇ (type: sd3)  ‚îÇ  ‚îÇ (fp8_e4m3fn) ‚îÇ        ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ                 ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ                  ‚ñº                 ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ          ‚îÇ ModelSampling‚îÇ         ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ          ‚îÇ AuraFlow     ‚îÇ         ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ          ‚îÇ (shift=3.0)  ‚îÇ         ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ                  ‚ñº                 ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ          ‚îÇ CFGNorm      ‚îÇ         ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ          ‚îÇ (strength=1) ‚îÇ         ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ                 ‚îÇ\n‚îÇ         ‚îÇ                  ‚ñº                  ‚îÇ                 ‚îÇ\n‚îÇ         ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ                ‚îÇ\n‚îÇ         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ TextEncodeQwen   ‚îÇ       ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ          ‚îÇ ImageEdit        ‚îÇ       ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ                   ‚îÇ                  ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ                   ‚ñº                  ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ          ‚îÇ ConditioningZero ‚îÇ        ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ          ‚îÇ Out (negatif)    ‚îÇ        ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ                   ‚îÇ                  ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                  ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ    ‚îÇ              ‚îÇ                  ‚îÇ                ‚îÇ\n‚îÇ         ‚îÇ    ‚ñº              ‚ñº                  ‚ñº                ‚îÇ\n‚îÇ         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ\n‚îÇ         ‚îÇ  ‚îÇ          KSampler                     ‚îÇ           ‚îÇ\n‚îÇ         ‚îÇ  ‚îÇ  (cfg=1.0, scheduler=beta, euler)    ‚îÇ           ‚îÇ\n‚îÇ         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ\n‚îÇ         ‚îÇ                 ‚îÇ                                    ‚îÇ\n‚îÇ         ‚îÇ                 ‚ñº                                    ‚îÇ\n‚îÇ         ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ\n‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ VAE Decode   ‚îÇ                            ‚îÇ\n‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ\n‚îÇ                          ‚îÇ                                     ‚îÇ\n‚îÇ                          ‚ñº                                     ‚îÇ\n‚îÇ                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ\n‚îÇ                   ‚îÇ Save Image   ‚îÇ                            ‚îÇ\n‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Flux de donn√©es (Phase 29)**:\n1. **VAELoader** -> Charge le VAE 16 canaux Qwen + fournit VAE au TextEncode et au VAEDecode\n2. **CLIPLoader** -> Fournit le CLIP au TextEncodeQwenImageEdit\n3. **UNETLoader** -> Charge le modele, passe par ModelSamplingAuraFlow puis CFGNorm\n4. **TextEncodeQwenImageEdit** -> Encode le prompt avec CLIP + VAE (encodeur natif Qwen)\n5. **ConditioningZeroOut** -> Cree un conditioning negatif vide\n6. **EmptySD3LatentImage** -> Canvas latent 16 canaux (non montre pour simplifier)\n7. **KSampler** -> Genere l'image latente (cfg=1.0, scheduler=beta)\n8. **VAEDecode** -> Convertit latent en image RGB\n9. **SaveImage** -> Sauvegarde l'image finale\n\n**Correspondance JSON**:\n```json\n{\n  \"1\": {\"class_type\": \"VAELoader\", \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}},\n  \"2\": {\"class_type\": \"CLIPLoader\", \"inputs\": {\"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\", \"type\": \"sd3\"}},\n  \"3\": {\"class_type\": \"UNETLoader\", \"inputs\": {\"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\", \"weight_dtype\": \"fp8_e4m3fn\"}},\n  \"4\": {\"class_type\": \"ModelSamplingAuraFlow\", \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}},\n  \"5\": {\"class_type\": \"CFGNorm\", \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}},\n  \"6\": {\"class_type\": \"TextEncodeQwenImageEdit\", \"inputs\": {\"clip\": [\"2\", 0], \"prompt\": \"...\", \"vae\": [\"1\", 0]}},\n  \"7\": {\"class_type\": \"ConditioningZeroOut\", \"inputs\": {\"conditioning\": [\"6\", 0]}},\n  \"8\": {\"class_type\": \"EmptySD3LatentImage\", \"inputs\": {\"width\": 512, \"height\": 512, \"batch_size\": 1}},\n  \"9\": {\"class_type\": \"KSampler\", \"inputs\": {\"model\": [\"5\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"8\", 0], \"cfg\": 1.0, \"scheduler\": \"beta\"}},\n  \"10\": {\"class_type\": \"VAEDecode\", \"inputs\": {\"samples\": [\"9\", 0], \"vae\": [\"1\", 0]}},\n  \"11\": {\"class_type\": \"SaveImage\", \"inputs\": {\"images\": [\"10\", 0]}}\n}\n```\n\n**Notation `[\"ID_NODE\", INDEX_OUTPUT]`**:\n- `[\"1\", 0]` = Output 0 (VAE) du node 1 (VAELoader)\n- `[\"2\", 0]` = Output 0 (CLIP) du node 2 (CLIPLoader)\n- `[\"5\", 0]` = Output 0 (model) du node 5 (CFGNorm, apres ModelSamplingAuraFlow)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComfyUIClient:\n",
    "    \"\"\"Client p√©dagogique API ComfyUI pour Qwen avec authentification Bearer\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=API_BASE_URL, client_id=CLIENT_ID, auth_token=None):\n",
    "        self.base_url = base_url\n",
    "        self.client_id = client_id\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Configuration authentification\n",
    "        if auth_token:\n",
    "            self.session.headers.update({\n",
    "                \"Authorization\": f\"Bearer {auth_token}\"\n",
    "            })\n",
    "        elif COMFYUI_API_TOKEN:  # Utilise variable globale si disponible\n",
    "            self.session.headers.update({\n",
    "                \"Authorization\": f\"Bearer {COMFYUI_API_TOKEN}\"\n",
    "            })\n",
    "        # Pas d'erreur si pas de token - graceful degradation\n",
    "    \n",
    "    def execute_workflow(\n",
    "        self, \n",
    "        workflow_json: Dict, \n",
    "        wait_for_completion: bool = True,\n",
    "        max_wait: int = 120,\n",
    "        verbose: bool = True\n",
    "    ) -> Dict:\n",
    "        \"\"\"Ex√©cute workflow ComfyUI et r√©cup√®re r√©sultats\n",
    "        \n",
    "        Args:\n",
    "            workflow_json: Workflow ComfyUI (dict)\n",
    "            wait_for_completion: Attendre fin g√©n√©ration\n",
    "            max_wait: Timeout en secondes\n",
    "            verbose: Afficher logs progression\n",
    "        \n",
    "        Returns:\n",
    "            dict: {\"prompt_id\", \"outputs\", \"status\"}\n",
    "        \"\"\"\n",
    "        # 1. Soumettre workflow\n",
    "        if verbose:\n",
    "            print(\"üì§ Soumission workflow...\")\n",
    "        \n",
    "        response = self.session.post(\n",
    "            f\"{self.base_url}/prompt\",\n",
    "            json={\"prompt\": workflow_json, \"client_id\": self.client_id}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        prompt_id = response.json()[\"prompt_id\"]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Workflow queued: {prompt_id}\")\n",
    "        \n",
    "        if not wait_for_completion:\n",
    "            return {\"prompt_id\": prompt_id, \"status\": \"queued\"}\n",
    "        \n",
    "        # 2. Polling compl√©tion\n",
    "        start_time = time.time()\n",
    "        while True:\n",
    "            elapsed = time.time() - start_time\n",
    "            if elapsed > max_wait:\n",
    "                raise TimeoutError(f\"Timeout apr√®s {max_wait}s\")\n",
    "            \n",
    "            history_response = self.session.get(\n",
    "                f\"{self.base_url}/history/{prompt_id}\"\n",
    "            )\n",
    "            history_response.raise_for_status()\n",
    "            history = history_response.json()\n",
    "            \n",
    "            if prompt_id not in history:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            \n",
    "            prompt_data = history[prompt_id]\n",
    "            status = prompt_data.get(\"status\", {})\n",
    "            \n",
    "            if status.get(\"completed\"):\n",
    "                if verbose:\n",
    "                    print(f\"‚úÖ Complet en {elapsed:.1f}s\")\n",
    "                \n",
    "                # 3. Extraire outputs\n",
    "                outputs = prompt_data.get(\"outputs\", {})\n",
    "                images = []\n",
    "                \n",
    "                for node_id, node_output in outputs.items():\n",
    "                    if \"images\" in node_output:\n",
    "                        for img_info in node_output[\"images\"]:\n",
    "                            # R√©cup√©rer image\n",
    "                            img_response = self.session.get(\n",
    "                                f\"{self.base_url}/view\",\n",
    "                                params={\n",
    "                                    \"filename\": img_info[\"filename\"],\n",
    "                                    \"subfolder\": img_info.get(\"subfolder\", \"\"),\n",
    "                                    \"type\": img_info.get(\"type\", \"output\")\n",
    "                                }\n",
    "                            )\n",
    "                            img_response.raise_for_status()\n",
    "                            images.append({\n",
    "                                \"data\": img_response.content,\n",
    "                                \"filename\": img_info[\"filename\"]\n",
    "                            })\n",
    "                \n",
    "                return {\n",
    "                    \"prompt_id\": prompt_id,\n",
    "                    \"status\": \"completed\",\n",
    "                    \"outputs\": outputs,\n",
    "                    \"images\": images,\n",
    "                    \"duration\": elapsed\n",
    "                }\n",
    "            \n",
    "            if status.get(\"status_str\") == \"error\":\n",
    "                error_msg = status.get(\"messages\", [\"Unknown error\"])\n",
    "                raise RuntimeError(f\"ComfyUI error: {error_msg}\")\n",
    "            \n",
    "            if verbose and int(elapsed) % 5 == 0:\n",
    "                print(f\"‚è≥ En cours... ({elapsed:.0f}s)\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "    \n",
    "    def display_images(self, result: Dict, figsize=(12, 4)):\n",
    "        \"\"\"Affiche images r√©sultats\"\"\"\n",
    "        images = result.get(\"images\", [])\n",
    "        if not images:\n",
    "            print(\"‚ö†Ô∏è Aucune image g√©n√©r√©e\")\n",
    "            return\n",
    "        \n",
    "        n = len(images)\n",
    "        fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "        if n == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, img_data in zip(axes, images):\n",
    "            img = Image.open(BytesIO(img_data[\"data\"]))\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(img_data[\"filename\"], fontsize=10)\n",
    "            ax.axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Instancier client avec authentification\n",
    "client = ComfyUIClient(auth_token=COMFYUI_API_TOKEN)\n",
    "if COMFYUI_API_TOKEN:\n",
    "    print(\"‚úÖ ComfyUIClient pr√™t avec authentification\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  ComfyUIClient pr√™t en mode sans authentification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de517444",
   "metadata": {},
   "source": "## üöÄ Workflow Minimal: \"Hello World\"\n\n### Objectif\n\nCr√©er le workflow le plus simple possible pour **valider l'API** et comprendre la structure JSON.\n\n### Workflow Text-to-Image Basique (Architecture Phase 29)\n\nQwen Image Edit utilise des **loaders s√©par√©s** (VAE, CLIP, UNET) au lieu d'un checkpoint unique. Le pipeline complet est :\n\n1. **VAELoader** ‚Üí Charger le VAE 16 canaux (`qwen_image_vae.safetensors`)\n2. **CLIPLoader** ‚Üí Charger le CLIP vision-language (`qwen_2.5_vl_7b_fp8_scaled.safetensors`, type `sd3`)\n3. **UNETLoader** ‚Üí Charger le mod√®le UNET (`qwen_image_edit_2509_fp8_e4m3fn.safetensors`)\n4. **ModelSamplingAuraFlow** ‚Üí Configurer l'√©chantillonnage (shift=3.0)\n5. **CFGNorm** ‚Üí Normalisation CFG (strength=1.0)\n6. **TextEncodeQwenImageEdit** ‚Üí Encoder le prompt (encodeur natif Qwen)\n7. **ConditioningZeroOut** ‚Üí Conditioning n√©gatif (vide)\n8. **EmptySD3LatentImage** ‚Üí Cr√©er canvas latent 16 canaux\n9. **KSampler** ‚Üí G√©n√©rer l'image\n10. **VAEDecode** ‚Üí Convertir latent vers pixels\n11. **SaveImage** ‚Üí Sauvegarder r√©sultat\n\n### Param√®tres Critiques\n\n| Param√®tre | Valeur | Impact |\n|-----------|--------|--------|\n| **steps** | 20 | Qualit√© (plus de steps = meilleure qualit√©, plus de temps) |\n| **cfg** | 1.0 | Doit rester a 1.0 car CFGNorm gere l'amplification |\n| **sampler** | euler | Algorithme de generation |\n| **scheduler** | beta | Obligatoire pour Qwen (pas \"normal\") |\n| **denoise** | 1.0 | Force generation (1.0 = 100%) |\n| **seed** | 42 | Reproductibilite |\n| **shift** | 3.0 | Parametre ModelSamplingAuraFlow |\n\n**Temps attendu**: 5-10 secondes (512x512)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b3bf0",
   "metadata": {},
   "outputs": [],
   "source": "# Workflow Text-to-Image minimal (Architecture Phase 29)\nworkflow_hello = {\n    \"1\": {  # VAE Loader (16 canaux Qwen)\n        \"class_type\": \"VAELoader\",\n        \"inputs\": {\n            \"vae_name\": \"qwen_image_vae.safetensors\"\n        }\n    },\n    \"2\": {  # CLIP Loader (vision-language Qwen)\n        \"class_type\": \"CLIPLoader\",\n        \"inputs\": {\n            \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n            \"type\": \"sd3\"\n        }\n    },\n    \"3\": {  # UNET Loader (modele diffusion Qwen)\n        \"class_type\": \"UNETLoader\",\n        \"inputs\": {\n            \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n            \"weight_dtype\": \"fp8_e4m3fn\"\n        }\n    },\n    \"4\": {  # ModelSamplingAuraFlow (configuration echantillonnage)\n        \"class_type\": \"ModelSamplingAuraFlow\",\n        \"inputs\": {\n            \"model\": [\"3\", 0],  # UNET du loader\n            \"shift\": 3.0\n        }\n    },\n    \"5\": {  # CFGNorm (normalisation CFG)\n        \"class_type\": \"CFGNorm\",\n        \"inputs\": {\n            \"model\": [\"4\", 0],  # Modele apres AuraFlow\n            \"strength\": 1.0\n        }\n    },\n    \"6\": {  # TextEncodeQwenImageEdit (encodeur natif Qwen)\n        \"class_type\": \"TextEncodeQwenImageEdit\",\n        \"inputs\": {\n            \"clip\": [\"2\", 0],   # CLIP du loader\n            \"prompt\": \"a majestic astronaut floating in space, photorealistic, 8k, detailed\",\n            \"vae\": [\"1\", 0]     # VAE requis par l'encodeur Qwen\n        }\n    },\n    \"7\": {  # ConditioningZeroOut (conditioning negatif vide)\n        \"class_type\": \"ConditioningZeroOut\",\n        \"inputs\": {\n            \"conditioning\": [\"6\", 0]  # Basee sur le conditioning positif\n        }\n    },\n    \"8\": {  # EmptySD3LatentImage (canvas latent 16 canaux)\n        \"class_type\": \"EmptySD3LatentImage\",\n        \"inputs\": {\n            \"width\": 512,\n            \"height\": 512,\n            \"batch_size\": 1\n        }\n    },\n    \"9\": {  # KSampler (generation)\n        \"class_type\": \"KSampler\",\n        \"inputs\": {\n            \"model\": [\"5\", 0],       # Modele apres CFGNorm\n            \"positive\": [\"6\", 0],    # Prompt positif (TextEncodeQwen)\n            \"negative\": [\"7\", 0],    # Conditioning negatif (ZeroOut)\n            \"latent_image\": [\"8\", 0],  # Canvas latent SD3\n            \"seed\": 42,\n            \"steps\": 20,\n            \"cfg\": 1.0,              # cfg=1.0 car CFGNorm gere l'amplification\n            \"sampler_name\": \"euler\",\n            \"scheduler\": \"beta\",     # scheduler beta obligatoire pour Qwen\n            \"denoise\": 1.0\n        }\n    },\n    \"10\": {  # VAE Decode (latent -> pixels)\n        \"class_type\": \"VAEDecode\",\n        \"inputs\": {\n            \"samples\": [\"9\", 0],  # Latent du sampler\n            \"vae\": [\"1\", 0]       # VAE du loader\n        }\n    },\n    \"11\": {  # Save Image\n        \"class_type\": \"SaveImage\",\n        \"inputs\": {\n            \"images\": [\"10\", 0],  # Pixels du VAE Decode\n            \"filename_prefix\": \"ComfyUI\"\n        }\n    }\n}\n\n# Execution\nprint(\"Lancement generation...\")\nresult = client.execute_workflow(workflow_hello, verbose=True)\n\n# Affichage\nclient.display_images(result)\nprint(f\"\\nImage generee en {result['duration']:.1f}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6a7ad",
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# WORKFLOW REEL 1: Edition Simple Image (Phase 29)\n# ========================================\n\ndef create_simple_edit_workflow(image_name: str, edit_prompt: str, denoise: float = 0.5) -> dict:\n    \"\"\"Workflow edition simple d'une image existante (Architecture Phase 29)\n    \n    Args:\n        image_name: Nom du fichier image uploade sur ComfyUI\n        edit_prompt: Description de l'edition souhaitee\n        denoise: Force de l'edition (0.0 = aucune, 1.0 = complete)\n    \n    Returns:\n        Workflow JSON pret a executer\n    \"\"\"\n    workflow = {\n        \"1\": {  # VAE Loader (16 canaux Qwen)\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"2\": {  # CLIP Loader\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"3\": {  # UNET Loader\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        \"4\": {  # ModelSamplingAuraFlow\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}\n        },\n        \"5\": {  # CFGNorm\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}\n        },\n        \"6\": {  # Load Image (image source)\n            \"class_type\": \"LoadImage\",\n            \"inputs\": {\"image\": image_name}\n        },\n        \"7\": {  # VAE Encode (pixels -> latent de l'image source)\n            \"class_type\": \"VAEEncode\",\n            \"inputs\": {\n                \"pixels\": [\"6\", 0],  # Image source\n                \"vae\": [\"1\", 0]      # VAE Qwen 16 canaux\n            }\n        },\n        \"8\": {  # TextEncodeQwenImageEdit (encodeur natif Qwen)\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"2\", 0],\n                \"prompt\": edit_prompt,\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"9\": {  # ConditioningZeroOut (negatif)\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"8\", 0]}\n        },\n        \"10\": {  # KSampler (edition avec denoise partiel)\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"seed\": 42,\n                \"steps\": 20,\n                \"cfg\": 1.0,              # cfg=1.0 car CFGNorm gere l'amplification\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",     # scheduler beta obligatoire\n                \"denoise\": denoise,\n                \"model\": [\"5\", 0],       # Modele apres CFGNorm\n                \"positive\": [\"8\", 0],    # Prompt positif\n                \"negative\": [\"9\", 0],    # Conditioning negatif ZeroOut\n                \"latent_image\": [\"7\", 0]  # Latent de l'image source\n            }\n        },\n        \"11\": {  # VAE Decode\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\n                \"samples\": [\"10\", 0],\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"12\": {  # Save Image\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\n                \"images\": [\"11\", 0],\n                \"filename_prefix\": \"qwen_edit_simple\"\n            }\n        }\n    }\n    return workflow\n\n# ========================================\n# WORKFLOW REEL 2: Chainage Nodes Avance (Phase 29)\n# ========================================\n\ndef create_chained_workflow(base_prompt: str, refine_prompt: str) -> dict:\n    \"\"\"Workflow avec chainage: generation base + raffinement (Phase 29)\n    \n    Architecture:\n        1. Generation image base (text-to-image) avec loaders separes\n        2. Raffinement avec nouveau prompt (image-to-image)\n    \n    Args:\n        base_prompt: Prompt initial generation\n        refine_prompt: Prompt raffinement/amelioration\n    \n    Returns:\n        Workflow JSON avec 2 etapes KSampler\n    \"\"\"\n    workflow = {\n        # Loaders (partages entre les 2 etapes)\n        \"1\": {  # VAE Loader\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"2\": {  # CLIP Loader\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"3\": {  # UNET Loader\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        \"4\": {  # ModelSamplingAuraFlow\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}\n        },\n        \"5\": {  # CFGNorm\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}\n        },\n        # Etape 1: Generation base (text-to-image)\n        \"6\": {  # TextEncodeQwenImageEdit (prompt base)\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"2\", 0],\n                \"prompt\": base_prompt,\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"7\": {  # ConditioningZeroOut (negatif etape 1)\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"6\", 0]}\n        },\n        \"8\": {  # EmptySD3LatentImage (canvas 16 canaux)\n            \"class_type\": \"EmptySD3LatentImage\",\n            \"inputs\": {\"width\": 512, \"height\": 512, \"batch_size\": 1}\n        },\n        \"9\": {  # KSampler etape 1 (generation complete)\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"seed\": 42,\n                \"steps\": 10,\n                \"cfg\": 1.0,\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",\n                \"denoise\": 1.0,  # Generation complete\n                \"model\": [\"5\", 0],\n                \"positive\": [\"6\", 0],\n                \"negative\": [\"7\", 0],\n                \"latent_image\": [\"8\", 0]\n            }\n        },\n        # Etape 2: Raffinement (image-to-image sur le latent de l'etape 1)\n        \"10\": {  # TextEncodeQwenImageEdit (prompt raffinement)\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"2\", 0],\n                \"prompt\": refine_prompt,\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"11\": {  # ConditioningZeroOut (negatif etape 2)\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"10\", 0]}\n        },\n        \"12\": {  # KSampler etape 2 (raffinement leger)\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"seed\": 43,\n                \"steps\": 10,\n                \"cfg\": 1.0,\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",\n                \"denoise\": 0.3,  # Raffinement leger\n                \"model\": [\"5\", 0],\n                \"positive\": [\"10\", 0],\n                \"negative\": [\"11\", 0],\n                \"latent_image\": [\"9\", 0]  # Sortie de l'etape 1\n            }\n        },\n        \"13\": {  # VAE Decode\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\"samples\": [\"12\", 0], \"vae\": [\"1\", 0]}\n        },\n        \"14\": {  # Save Image\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\n                \"images\": [\"13\", 0],\n                \"filename_prefix\": \"qwen_chained\"\n            }\n        }\n    }\n    return workflow\n\n# ========================================\n# EXEMPLE D'UTILISATION\n# ========================================\n\n# Workflow 1: Edition simple\nworkflow_simple = create_simple_edit_workflow(\n    image_name=\"cat.png\",\n    edit_prompt=\"Add sunglasses to the cat\",\n    denoise=0.5\n)\nprint(\"Workflow edition simple cree (Phase 29)\")\nprint(f\"   Nodes: {len(workflow_simple)}\")\n\n# Workflow 2: Chainage\nworkflow_chained = create_chained_workflow(\n    base_prompt=\"A cat sitting on a chair\",\n    refine_prompt=\"High quality, professional photography, detailed fur\"\n)\nprint(\"Workflow chaine cree (Phase 29)\")\nprint(f\"   Nodes: {len(workflow_chained)}\")\nprint(f\"   KSamplers: 2 (base + raffinement)\")\n\n# Pour executer ces workflows:\n# client = ComfyUIClient(\"https://qwen-image-edit.myia.io\")\n# result = client.execute_workflow(workflow_simple)\n# client.display_images(result)"
  },
  {
   "cell_type": "markdown",
   "id": "e75258e1",
   "metadata": {},
   "source": [
    "## üñºÔ∏è √âdition Images avec Qwen VLM\n",
    "\n",
    "### Capacit√©s Qwen Vision-Language Model\n",
    "\n",
    "**Qwen-Image-Edit** combine:\n",
    "- üß† **Vision Encoder** (CLIP-like) pour comprendre images\n",
    "- ‚úçÔ∏è **Language Model** pour interpr√©ter instructions texte\n",
    "- üé® **Diffusion Model** pour √©diter images\n",
    "\n",
    "### Cas d'Usage Typiques\n",
    "\n",
    "| T√¢che | Exemple Prompt |\n",
    "|-------|----------------|\n",
    "| **Style Transfer** | `\"Convert to watercolor painting\"` |\n",
    "| **Object Addition** | `\"Add a red balloon in the sky\"` |\n",
    "| **Color Grading** | `\"Make the image warmer, golden hour lighting\"` |\n",
    "| **Background Change** | `\"Replace background with snowy mountains\"` |\n",
    "| **Detail Enhancement** | `\"Enhance facial details, 8k quality\"` |\n",
    "\n",
    "### Pattern Image-to-Image\n",
    "\n",
    "**Diff√©rence cl√© avec Text-to-Image**:\n",
    "- **Nouveau node**: `LoadImage` pour charger image source\n",
    "- **Param√®tre critique**: `denoise` (0.0-1.0)\n",
    "  - `denoise=0.1`: √âdition subtile (retouche l√©g√®re)\n",
    "  - `denoise=0.5`: √âdition mod√©r√©e (style transfer)\n",
    "  - `denoise=0.9`: √âdition forte (reconstruction quasi-totale)\n",
    "\n",
    "**Workflow**:\n",
    "1. **Load Image** ‚Üí Charger image source\n",
    "2. **CLIP Vision Encode** ‚Üí Encoder image\n",
    "3. **CLIP Text Encode** ‚Üí Encoder prompt √©dition\n",
    "4. **VAE Encode** ‚Üí Convertir pixels ‚Üí latent\n",
    "5. **KSampler** (denoise < 1.0) ‚Üí √âditer latent\n",
    "6. **VAE Decode** ‚Üí Convertir latent ‚Üí pixels\n",
    "7. **Save Image** ‚Üí Sauvegarder r√©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba896395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_image_to_comfyui(image_path: str) -> str:\n",
    "    \"\"\"Upload image vers ComfyUI pour √©dition\n",
    "    \n",
    "    Args:\n",
    "        image_path: Chemin image locale ou URL\n",
    "    \n",
    "    Returns:\n",
    "        str: Nom fichier upload√© dans ComfyUI\n",
    "    \"\"\"\n",
    "    # Charger image\n",
    "    if image_path.startswith('http'):\n",
    "        response = requests.get(image_path)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        img = Image.open(image_path)\n",
    "    \n",
    "    # Convertir en bytes\n",
    "    img_bytes = BytesIO()\n",
    "    img.save(img_bytes, format='PNG')\n",
    "    img_bytes.seek(0)\n",
    "    \n",
    "    # Upload vers ComfyUI\n",
    "    files = {'image': ('input.png', img_bytes, 'image/png')}\n",
    "    response = client.session.post(\n",
    "        f\"{API_BASE_URL}/upload/image\",\n",
    "        files=files\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    filename = response.json()['name']\n",
    "    print(f\"‚úÖ Image upload√©e: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Test upload avec image exemple\n",
    "# Note: Remplacer par votre propre image\n",
    "test_image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/481px-Cat03.jpg\"\n",
    "\n",
    "try:\n",
    "    uploaded_filename = upload_image_to_comfyui(test_image_url)\n",
    "    print(f\"üìÅ Fichier: {uploaded_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur upload: {e}\")\n",
    "    print(\"üí° Conseil: Utiliser une image locale ou v√©rifier URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6a3da",
   "metadata": {},
   "source": "## üé® Workflow Image-to-Image Complet\n\n### Architecture (Phase 29)\n\n**Pipeline edition avec loaders separes**:\n1. **VAELoader / CLIPLoader / UNETLoader** ‚Üí Charger les 3 composants du modele\n2. **ModelSamplingAuraFlow + CFGNorm** ‚Üí Configurer le modele\n3. **LoadImage** ‚Üí Charger image source uploadee\n4. **VAEEncode** ‚Üí Convertir pixels ‚Üí latent (16 canaux)\n5. **TextEncodeQwenImageEdit** ‚Üí Encoder instructions edition (encodeur natif Qwen)\n6. **ConditioningZeroOut** ‚Üí Conditioning negatif vide\n7. **KSampler** (denoise partiel, cfg=1.0, scheduler=beta) ‚Üí Editer latent\n8. **VAEDecode** ‚Üí Reconvertir latent ‚Üí pixels\n9. **SaveImage** ‚Üí Sauvegarder resultat\n\n### Parametre Critique: denoise\n\n**Impact sur edition**:\n\n| denoise | Type Edition | Exemple |\n|---------|-------------|----------|\n| **0.1-0.3** | Retouche subtile | Correction couleurs, amelioration details |\n| **0.4-0.6** | Edition moderee | Style transfer, ajout elements mineurs |\n| **0.7-0.9** | Edition forte | Changement scene, reconstruction majeure |\n| **1.0** | Generation totale | Ignore quasi totalement image source |\n\n**Recommandation**: Commencer avec `denoise=0.5` et ajuster selon resultat.\n\n**Rappel Phase 29**: `cfg=1.0` (CFGNorm gere l'amplification), `scheduler=\"beta\"` (obligatoire)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747201c",
   "metadata": {},
   "outputs": [],
   "source": "# Workflow Image-to-Image (edition) - Architecture Phase 29\nworkflow_img2img = {\n    \"1\": {  # VAE Loader (16 canaux Qwen)\n        \"class_type\": \"VAELoader\",\n        \"inputs\": {\n            \"vae_name\": \"qwen_image_vae.safetensors\"\n        }\n    },\n    \"2\": {  # CLIP Loader\n        \"class_type\": \"CLIPLoader\",\n        \"inputs\": {\n            \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n            \"type\": \"sd3\"\n        }\n    },\n    \"3\": {  # UNET Loader\n        \"class_type\": \"UNETLoader\",\n        \"inputs\": {\n            \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n            \"weight_dtype\": \"fp8_e4m3fn\"\n        }\n    },\n    \"4\": {  # ModelSamplingAuraFlow\n        \"class_type\": \"ModelSamplingAuraFlow\",\n        \"inputs\": {\n            \"model\": [\"3\", 0],\n            \"shift\": 3.0\n        }\n    },\n    \"5\": {  # CFGNorm\n        \"class_type\": \"CFGNorm\",\n        \"inputs\": {\n            \"model\": [\"4\", 0],\n            \"strength\": 1.0\n        }\n    },\n    \"6\": {  # Load Image (image source uploadee)\n        \"class_type\": \"LoadImage\",\n        \"inputs\": {\n            \"image\": uploaded_filename  # Variable de cellule precedente\n        }\n    },\n    \"7\": {  # VAE Encode (pixels -> latent)\n        \"class_type\": \"VAEEncode\",\n        \"inputs\": {\n            \"pixels\": [\"6\", 0],  # Image source\n            \"vae\": [\"1\", 0]      # VAE Qwen 16 canaux\n        }\n    },\n    \"8\": {  # TextEncodeQwenImageEdit (prompt edition)\n        \"class_type\": \"TextEncodeQwenImageEdit\",\n        \"inputs\": {\n            \"clip\": [\"2\", 0],\n            \"prompt\": \"watercolor painting style, artistic, vibrant colors\",\n            \"vae\": [\"1\", 0]\n        }\n    },\n    \"9\": {  # ConditioningZeroOut (conditioning negatif)\n        \"class_type\": \"ConditioningZeroOut\",\n        \"inputs\": {\n            \"conditioning\": [\"8\", 0]\n        }\n    },\n    \"10\": {  # KSampler (edition avec denoise partiel)\n        \"class_type\": \"KSampler\",\n        \"inputs\": {\n            \"model\": [\"5\", 0],       # Modele apres CFGNorm\n            \"positive\": [\"8\", 0],    # Prompt positif\n            \"negative\": [\"9\", 0],    # Conditioning negatif ZeroOut\n            \"latent_image\": [\"7\", 0],  # Latent de l'image source\n            \"seed\": 42,\n            \"steps\": 25,\n            \"cfg\": 1.0,              # cfg=1.0 car CFGNorm gere l'amplification\n            \"sampler_name\": \"euler\",\n            \"scheduler\": \"beta\",     # scheduler beta obligatoire pour Qwen\n            \"denoise\": 0.5           # Edition moderee (50%)\n        }\n    },\n    \"11\": {  # VAE Decode (latent -> pixels)\n        \"class_type\": \"VAEDecode\",\n        \"inputs\": {\n            \"samples\": [\"10\", 0],\n            \"vae\": [\"1\", 0]\n        }\n    },\n    \"12\": {  # Save Image\n        \"class_type\": \"SaveImage\",\n        \"inputs\": {\n            \"images\": [\"11\", 0],\n            \"filename_prefix\": \"Qwen_Edit\"\n        }\n    }\n}\n\n# Execution (seulement si image uploadee precedemment)\ntry:\n    print(\"Lancement edition image...\")\n    result = client.execute_workflow(workflow_img2img, verbose=True)\n    \n    # Affichage avant/apres\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Image originale\n    original_img = Image.open(BytesIO(client.session.get(\n        f\"{API_BASE_URL}/view\",\n        params={\"filename\": uploaded_filename, \"type\": \"input\"}\n    ).content))\n    axes[0].imshow(original_img)\n    axes[0].set_title(\"Image Originale\", fontsize=12)\n    axes[0].axis(\"off\")\n    \n    # Image editee\n    edited_img = Image.open(BytesIO(result[\"images\"][0][\"data\"]))\n    axes[1].imshow(edited_img)\n    axes[1].set_title(f\"Image Editee (denoise=0.5)\", fontsize=12)\n    axes[1].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nEdition complete en {result['duration']:.1f}s\")\nexcept NameError:\n    print(\"Executez d'abord la cellule d'upload d'image\")\nexcept Exception as e:\n    print(f\"Erreur: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "7e1176e6",
   "metadata": {},
   "source": [
    "## üî¨ Exp√©rimentation: Comparaison Denoise\n",
    "\n",
    "### Objectif P√©dagogique\n",
    "\n",
    "Comprendre **impact du param√®tre `denoise`** sur qualit√© √©dition.\n",
    "\n",
    "### M√©thodologie\n",
    "\n",
    "1. Workflow identique (m√™me prompt, seed, etc.)\n",
    "2. Variation **uniquement** du param√®tre `denoise`\n",
    "3. Comparaison visuelle r√©sultats\n",
    "\n",
    "### Hypoth√®se\n",
    "\n",
    "**denoise faible** (0.2) ‚Üí √âdition subtile, image proche de l'originale\n",
    "**denoise moyen** (0.5) ‚Üí Bon compromis √©dition/pr√©servation\n",
    "**denoise √©lev√©** (0.8) ‚Üí √âdition forte, risque divergence\n",
    "\n",
    "### Configuration Test\n",
    "\n",
    "```python\n",
    "denoise_values = [0.2, 0.5, 0.8]\n",
    "prompt = \"convert to dramatic black and white, high contrast\"\n",
    "seed = 42  # Fixe pour comparabilit√©\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84553629",
   "metadata": {},
   "outputs": [],
   "source": "# Comparaison denoise (denoise=0.2, 0.5, 0.8) - Architecture Phase 29\ndenoise_values = [0.2, 0.5, 0.8]\nedit_prompt = \"convert to dramatic black and white, high contrast\"\n\nresults_denoise = []\n\nfor denoise_val in denoise_values:\n    print(f\"\\nTest denoise={denoise_val}...\")\n    \n    # Creer workflow Phase 29 avec denoise specifique\n    workflow_denoise_test = {\n        \"1\": {  # VAE Loader\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"2\": {  # CLIP Loader\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"3\": {  # UNET Loader\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        \"4\": {  # ModelSamplingAuraFlow\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}\n        },\n        \"5\": {  # CFGNorm\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}\n        },\n        \"6\": {  # Load Image\n            \"class_type\": \"LoadImage\",\n            \"inputs\": {\"image\": uploaded_filename}\n        },\n        \"7\": {  # VAE Encode\n            \"class_type\": \"VAEEncode\",\n            \"inputs\": {\n                \"pixels\": [\"6\", 0],\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"8\": {  # TextEncodeQwenImageEdit (prompt edition)\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"2\", 0],\n                \"prompt\": edit_prompt,\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"9\": {  # ConditioningZeroOut (negatif)\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"8\", 0]}\n        },\n        \"10\": {  # KSampler\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"model\": [\"5\", 0],\n                \"positive\": [\"8\", 0],\n                \"negative\": [\"9\", 0],\n                \"latent_image\": [\"7\", 0],\n                \"seed\": 42,\n                \"steps\": 25,\n                \"cfg\": 1.0,              # cfg=1.0 car CFGNorm gere l'amplification\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",     # scheduler beta obligatoire\n                \"denoise\": denoise_val   # Variable testee\n            }\n        },\n        \"11\": {  # VAE Decode\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\n                \"samples\": [\"10\", 0],\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"12\": {  # Save Image\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\n                \"images\": [\"11\", 0],\n                \"filename_prefix\": f\"Denoise_{denoise_val}\"\n            }\n        }\n    }\n    \n    try:\n        result = client.execute_workflow(workflow_denoise_test, verbose=False)\n        results_denoise.append({\n            \"denoise\": denoise_val,\n            \"result\": result\n        })\n        print(f\"Complete en {result['duration']:.1f}s\")\n    except Exception as e:\n        print(f\"Erreur: {e}\")\n        results_denoise.append({\"denoise\": denoise_val, \"error\": str(e)})\n\n# Affichage comparatif\nif len(results_denoise) == 3 and all('result' in r for r in results_denoise):\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    for i, res in enumerate(results_denoise):\n        img = Image.open(BytesIO(res[\"result\"][\"images\"][0][\"data\"]))\n        axes[i].imshow(img)\n        axes[i].set_title(f\"denoise={res['denoise']}\", fontsize=12)\n        axes[i].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nObservations:\")\n    print(\"- denoise=0.2: Edition subtile, preserve details originaux\")\n    print(\"- denoise=0.5: Equilibre edition/preservation\")\n    print(\"- denoise=0.8: Edition forte, peut diverger de l'original\")\nelse:\n    print(\"Certains tests ont echoue, verifiez les erreurs ci-dessus\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacdbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üñºÔ∏è COMPARAISON AVANT/APR√àS: Side-by-Side\n",
    "# ========================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "def compare_before_after(\n",
    "    original_path: str,\n",
    "    edited_path: str,\n",
    "    title_original: str = \"Image Originale\",\n",
    "    title_edited: str = \"Image √âdit√©e\",\n",
    "    show_metrics: bool = True\n",
    ") -> None:\n",
    "    \"\"\"Affiche comparaison side-by-side avec m√©triques qualit√©\n",
    "    \n",
    "    Args:\n",
    "        original_path: Chemin image originale\n",
    "        edited_path: Chemin image √©dit√©e\n",
    "        title_original: Titre image originale\n",
    "        title_edited: Titre image √©dit√©e\n",
    "        show_metrics: Afficher m√©triques qualit√© (PSNR, SSIM)\n",
    "    \"\"\"\n",
    "    # Charger images\n",
    "    img_original = Image.open(original_path)\n",
    "    img_edited = Image.open(edited_path)\n",
    "    \n",
    "    # Convertir en numpy arrays\n",
    "    arr_original = np.array(img_original)\n",
    "    arr_edited = np.array(img_edited)\n",
    "    \n",
    "    # Cr√©er figure avec 2 colonnes\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Image originale\n",
    "    axes[0].imshow(arr_original)\n",
    "    axes[0].set_title(f\"{title_original}\\n{img_original.size[0]}x{img_original.size[1]}\", \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Image √©dit√©e\n",
    "    axes[1].imshow(arr_edited)\n",
    "    axes[1].set_title(f\"{title_edited}\\n{img_edited.size[0]}x{img_edited.size[1]}\", \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # M√©triques qualit√©\n",
    "    if show_metrics and arr_original.shape == arr_edited.shape:\n",
    "        # PSNR (Peak Signal-to-Noise Ratio)\n",
    "        mse = np.mean((arr_original - arr_edited) ** 2)\n",
    "        if mse == 0:\n",
    "            psnr = float('inf')\n",
    "        else:\n",
    "            max_pixel = 255.0\n",
    "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "        \n",
    "        # Diff√©rence absolue moyenne\n",
    "        mae = np.mean(np.abs(arr_original - arr_edited))\n",
    "        \n",
    "        # Afficher m√©triques\n",
    "        metrics_text = f\"üìä M√©triques:\\n\"\n",
    "        metrics_text += f\"   PSNR: {psnr:.2f} dB\\n\"\n",
    "        metrics_text += f\"   MAE: {mae:.2f}\\n\"\n",
    "        metrics_text += f\"   Pixels modifi√©s: {np.sum(arr_original != arr_edited):,}\"\n",
    "        \n",
    "        fig.text(0.5, 0.02, metrics_text, ha='center', fontsize=12, \n",
    "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interpr√©tation PSNR\n",
    "    if show_metrics and arr_original.shape == arr_edited.shape:\n",
    "        print(\"\\nüìà Interpr√©tation PSNR:\")\n",
    "        if psnr > 40:\n",
    "            print(\"   ‚úÖ Excellente qualit√© (PSNR > 40 dB) - Changements subtils\")\n",
    "        elif psnr > 30:\n",
    "            print(\"   ‚úÖ Bonne qualit√© (PSNR 30-40 dB) - Changements visibles mais contr√¥l√©s\")\n",
    "        elif psnr > 20:\n",
    "            print(\"   ‚ö†Ô∏è  Qualit√© acceptable (PSNR 20-30 dB) - Changements significatifs\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Qualit√© faible (PSNR < 20 dB) - Changements majeurs\")\n",
    "\n",
    "def create_difference_map(\n",
    "    original_path: str,\n",
    "    edited_path: str,\n",
    "    amplification: float = 5.0\n",
    ") -> None:\n",
    "    \"\"\"Cr√©e une carte visuelle des diff√©rences entre 2 images\n",
    "    \n",
    "    Args:\n",
    "        original_path: Chemin image originale\n",
    "        edited_path: Chemin image √©dit√©e\n",
    "        amplification: Facteur amplification diff√©rences pour visibilit√©\n",
    "    \"\"\"\n",
    "    img_original = np.array(Image.open(original_path))\n",
    "    img_edited = np.array(Image.open(edited_path))\n",
    "    \n",
    "    if img_original.shape != img_edited.shape:\n",
    "        print(\"‚ùå Images de tailles diff√©rentes, impossible de comparer\")\n",
    "        return\n",
    "    \n",
    "    # Calculer diff√©rence absolue\n",
    "    diff = np.abs(img_original.astype(float) - img_edited.astype(float))\n",
    "    \n",
    "    # Amplifier pour visibilit√©\n",
    "    diff_amplified = np.clip(diff * amplification, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Afficher\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    axes[0].imshow(img_original)\n",
    "    axes[0].set_title(\"Originale\", fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(img_edited)\n",
    "    axes[1].set_title(\"√âdit√©e\", fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(diff_amplified)\n",
    "    axes[2].set_title(f\"Carte Diff√©rences (√ó{amplification})\", fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques diff√©rences\n",
    "    print(f\"\\nüìä Statistiques diff√©rences:\")\n",
    "    print(f\"   Moyenne: {np.mean(diff):.2f}\")\n",
    "    print(f\"   Max: {np.max(diff):.2f}\")\n",
    "    print(f\"   Pixels modifi√©s (>5): {np.sum(diff > 5):,} ({100*np.sum(diff > 5)/diff.size:.2f}%)\")\n",
    "\n",
    "# ========================================\n",
    "# EXEMPLE D'UTILISATION\n",
    "# ========================================\n",
    "\n",
    "# Cas d'usage: Comparer original vs √©dition Qwen\n",
    "# compare_before_after(\n",
    "#     original_path=\"cat_original.png\",\n",
    "#     edited_path=\"cat_with_sunglasses.png\",\n",
    "#     title_original=\"Chat Original\",\n",
    "#     title_edited=\"Chat avec Lunettes (Qwen Edit)\",\n",
    "#     show_metrics=True\n",
    "# )\n",
    "\n",
    "# Cas d'usage: Carte diff√©rences pour analyse d√©taill√©e\n",
    "# create_difference_map(\n",
    "#     original_path=\"cat_original.png\",\n",
    "#     edited_path=\"cat_with_sunglasses.png\",\n",
    "#     amplification=10.0\n",
    "# )\n",
    "\n",
    "print(\"‚úÖ Fonctions comparaison avant/apr√®s d√©finies\")\n",
    "print(\"   - compare_before_after(): Affichage side-by-side + m√©triques\")\n",
    "print(\"   - create_difference_map(): Carte visuelle diff√©rences\")\n",
    "print(\"\\nüí° D√©commentez les exemples ci-dessus pour tester avec vos images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db2fc7",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Bonnes Pratiques ComfyUI\n",
    "\n",
    "### 1. Gestion des Erreurs Courantes\n",
    "\n",
    "#### Timeout (>120s)\n",
    "**Cause**: GPU surcharg√©, workflow trop complexe\n",
    "**Solution**:\n",
    "```python\n",
    "client.execute_workflow(workflow, max_wait=300)  # Augmenter timeout\n",
    "```\n",
    "\n",
    "#### CUDA Out of Memory\n",
    "**Cause**: R√©solution trop √©lev√©e (>768x768)\n",
    "**Solution**:\n",
    "- R√©duire r√©solution (512x512 optimal)\n",
    "- Diminuer `batch_size`\n",
    "- Simplifier workflow\n",
    "\n",
    "#### Node Not Found\n",
    "**Cause**: `class_type` invalide ou custom node manquant\n",
    "**Solution**: V√©rifier documentation custom nodes Qwen\n",
    "\n",
    "### 2. Optimisation Performance\n",
    "\n",
    "| Param√®tre | Impact Performance | Recommandation |\n",
    "|-----------|-------------------|----------------|\n",
    "| **steps** | ‚Üë steps = ‚Üë temps | 20-25 optimal |\n",
    "| **resolution** | ‚Üë r√©solution = ‚Üë‚Üë VRAM | 512x512 par d√©faut |\n",
    "| **denoise** | Minimal | N/A |\n",
    "| **cfg** | Minimal | 7-8 optimal |\n",
    "\n",
    "### 3. Workflow Reproductible\n",
    "\n",
    "**Toujours fixer seed pour debugging**:\n",
    "```python\n",
    "\"seed\": 42  # M√™me r√©sultat √† chaque ex√©cution\n",
    "```\n",
    "\n",
    "**Seed al√©atoire pour vari√©t√©**:\n",
    "```python\n",
    "\"seed\": int(time.time())  # Diff√©rent √† chaque fois\n",
    "```\n",
    "\n",
    "### 4. Logs et Debugging\n",
    "\n",
    "**Activer verbose**:\n",
    "```python\n",
    "result = client.execute_workflow(workflow, verbose=True)\n",
    "```\n",
    "\n",
    "**Inspecter outputs**:\n",
    "```python\n",
    "print(json.dumps(result[\"outputs\"], indent=2))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af4b97",
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# EXERCICE PRATIQUE (Architecture Phase 29)\n# ========================================\n# Creez votre propre workflow d'edition d'image\n\n# OBJECTIF:\n# Modifier une image en ajoutant un effet specifique via Qwen VLM\n\n# INSTRUCTIONS:\n# 1. Choisissez une image de test (ou utilisez celle generee precedemment)\n# 2. Creez un workflow image-to-image avec un prompt creatif\n# 3. Testez differentes valeurs de denoise (0.3, 0.5, 0.7)\n# 4. Comparez visuellement les resultats\n\n# TODO: Completez le workflow ci-dessous\nworkflow_exercice = {\n    # Loaders separes (Phase 29 - ne pas modifier)\n    \"1\": {  # VAE Loader\n        \"class_type\": \"VAELoader\",\n        \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n    },\n    \"2\": {  # CLIP Loader\n        \"class_type\": \"CLIPLoader\",\n        \"inputs\": {\n            \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n            \"type\": \"sd3\"\n        }\n    },\n    \"3\": {  # UNET Loader\n        \"class_type\": \"UNETLoader\",\n        \"inputs\": {\n            \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n            \"weight_dtype\": \"fp8_e4m3fn\"\n        }\n    },\n    \"4\": {  # ModelSamplingAuraFlow\n        \"class_type\": \"ModelSamplingAuraFlow\",\n        \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}\n    },\n    \"5\": {  # CFGNorm\n        \"class_type\": \"CFGNorm\",\n        \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}\n    },\n    # Image source\n    \"6\": {  # Load Image\n        \"class_type\": \"LoadImage\",\n        \"inputs\": {\n            # TODO: Ajoutez le nom de votre image source\n            \"image\": \"___VOTRE_IMAGE_ICI___\"\n        }\n    },\n    \"7\": {  # VAE Encode (pixels -> latent)\n        \"class_type\": \"VAEEncode\",\n        \"inputs\": {\n            \"pixels\": [\"6\", 0],\n            \"vae\": [\"1\", 0]\n        }\n    },\n    # Encodage texte natif Qwen\n    \"8\": {  # TextEncodeQwenImageEdit\n        \"class_type\": \"TextEncodeQwenImageEdit\",\n        \"inputs\": {\n            \"clip\": [\"2\", 0],\n            # TODO: Ecrivez un prompt creatif (ex: \"convert to watercolor painting\")\n            \"prompt\": \"___VOTRE_PROMPT_ICI___\",\n            \"vae\": [\"1\", 0]\n        }\n    },\n    \"9\": {  # ConditioningZeroOut (conditioning negatif)\n        \"class_type\": \"ConditioningZeroOut\",\n        \"inputs\": {\"conditioning\": [\"8\", 0]}\n    },\n    # Sampler\n    \"10\": {  # KSampler\n        \"class_type\": \"KSampler\",\n        \"inputs\": {\n            \"seed\": 42,\n            # TODO: Testez differentes valeurs de denoise (0.3 a 0.7)\n            \"denoise\": 0.5,\n            \"steps\": 20,\n            \"cfg\": 1.0,              # cfg=1.0 obligatoire (CFGNorm gere l'amplification)\n            \"sampler_name\": \"euler\",\n            \"scheduler\": \"beta\",     # scheduler beta obligatoire pour Qwen\n            \"model\": [\"5\", 0],       # Modele apres CFGNorm\n            \"positive\": [\"8\", 0],    # Prompt positif\n            \"negative\": [\"9\", 0],    # Conditioning negatif ZeroOut\n            \"latent_image\": [\"7\", 0]  # Latent de l'image source\n        }\n    },\n    # Decodage et sauvegarde\n    \"11\": {  # VAE Decode\n        \"class_type\": \"VAEDecode\",\n        \"inputs\": {\n            \"samples\": [\"10\", 0],\n            \"vae\": [\"1\", 0]\n        }\n    },\n    \"12\": {  # Save Image\n        \"class_type\": \"SaveImage\",\n        \"inputs\": {\n            \"filename_prefix\": \"exercice_qwen\",\n            \"images\": [\"11\", 0]\n        }\n    }\n}\n\n# AIDE:\n# - Pour l'image source: utilisez le nom d'une image uploadee (ex: \"cat_512x512.png\")\n# - Pour le prompt: soyez creatif! (ex: \"make it look like a Van Gogh painting\")\n# - Pour denoise: commencez par 0.5, puis testez 0.3 et 0.7\n# - cfg doit rester a 1.0 (CFGNorm gere l'amplification)\n# - scheduler doit rester \"beta\" (obligatoire pour Qwen)\n\n# BONUS:\n# Creez une fonction pour tester plusieurs prompts automatiquement\ndef tester_prompts_exercice(prompts_list, image_source, denoise=0.5):\n    \"\"\"\n    Teste plusieurs prompts sur la meme image (Phase 29)\n    \n    Args:\n        prompts_list: Liste de prompts a tester\n        image_source: Nom de l'image source\n        denoise: Valeur denoise (defaut 0.5)\n    \"\"\"\n    results = []\n    \n    for i, prompt in enumerate(prompts_list):\n        print(f\"\\nTest {i+1}/{len(prompts_list)}: {prompt}\")\n        \n        # Utilise create_simple_edit_workflow defini precedemment (Phase 29)\n        workflow_test = create_simple_edit_workflow(\n            image_name=image_source,\n            edit_prompt=prompt,\n            denoise=denoise\n        )\n        \n        # Execution workflow\n        # result = client.execute_workflow(workflow_test)\n        # results.append(result)\n    \n    return results\n\n# TESTEZ VOTRE CODE:\n# prompts_test = [\n#     \"convert to watercolor painting\",\n#     \"add dramatic sunset lighting\",\n#     \"make it look like a pencil sketch\"\n# ]\n# resultats = tester_prompts_exercice(prompts_test, \"cat_512x512.png\")\n\nprint(\"\\nExercice pret! Completez les TODO et executez la cellule.\")\nprint(\"Conseil: Commencez simple, puis ajoutez de la complexite progressivement.\")\nprint(\"\\nRappel Phase 29: cfg=1.0, scheduler='beta', TextEncodeQwenImageEdit, ConditioningZeroOut\")"
  },
  {
   "cell_type": "markdown",
   "id": "0a694ae8",
   "metadata": {},
   "source": [
    "## üìö Ressources Compl√©mentaires\n",
    "\n",
    "### Documentation Officielle\n",
    "\n",
    "#### ComfyUI\n",
    "- **GitHub**: [https://github.com/comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)\n",
    "- **Documentation API**: [https://github.com/comfyanonymous/ComfyUI/wiki/API](https://github.com/comfyanonymous/ComfyUI/wiki/API)\n",
    "- **Custom Nodes**: [https://github.com/ltdrdata/ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager)\n",
    "\n",
    "#### Qwen Vision-Language Model\n",
    "- **Paper Officiel**: *Qwen-VL: A Versatile Vision-Language Model*\n",
    "- **Mod√®le Hugging Face**: [https://huggingface.co/Qwen/Qwen-VL](https://huggingface.co/Qwen/Qwen-VL)\n",
    "- **Documentation Technique**: [https://qwenlm.github.io/](https://qwenlm.github.io/)\n",
    "\n",
    "### Workflows Avanc√©s\n",
    "\n",
    "#### Workflows ComfyUI Communautaires\n",
    "- **ComfyUI Workflows Gallery**: [https://comfyworkflows.com/](https://comfyworkflows.com/)\n",
    "- **CivitAI ComfyUI Section**: [https://civitai.com/tag/comfyui](https://civitai.com/tag/comfyui)\n",
    "\n",
    "#### Custom Nodes Recommand√©s\n",
    "- **ComfyUI-Impact-Pack**: Outils post-processing avanc√©s\n",
    "- **ComfyUI-AnimateDiff**: Animations et vid√©os\n",
    "- **ComfyUI-ControlNet**: Contr√¥le spatial pr√©cis\n",
    "\n",
    "### Tutoriels et Guides\n",
    "\n",
    "#### D√©butants\n",
    "1. **ComfyUI Basics** (YouTube): Introduction compl√®te workflows\n",
    "2. **Qwen-VL Quick Start**: Guide rapide √©dition images\n",
    "3. **JSON Workflows 101**: Comprendre structure workflows\n",
    "\n",
    "#### Interm√©diaires\n",
    "1. **Advanced Prompting Techniques**: Optimisation prompts Qwen\n",
    "2. **Workflow Optimization**: R√©duire temps g√©n√©ration\n",
    "3. **Multi-Step Workflows**: Cha√Ænage nodes complexes\n",
    "\n",
    "#### Avanc√©s\n",
    "1. **Custom Node Development**: Cr√©er vos propres nodes\n",
    "2. **API Integration**: Int√©grer ComfyUI dans applications\n",
    "3. **Batch Processing**: Automatisation workflows\n",
    "\n",
    "### Communaut√© et Support\n",
    "\n",
    "- **Discord ComfyUI**: [https://discord.gg/comfyui](https://discord.gg/comfyui)\n",
    "- **Reddit r/comfyui**: Forum communautaire\n",
    "- **GitHub Discussions**: Questions techniques\n",
    "\n",
    "### Ressources MyIA.io\n",
    "\n",
    "- **Guide APIs √âtudiants**: [`GUIDE-APIS-ETUDIANTS.md`](../../../../docs/suivis/genai-image/GUIDE-APIS-ETUDIANTS.md)\n",
    "- **Workflows Qwen Phase 12C**: [`2025-10-16_12C_architectures-5-workflows-qwen.md`](../../../../docs/genai-suivis/2025-10-16_12C_architectures-5-workflows-qwen.md)\n",
    "- **Notebook Forge SD-XL**: [`01-4-Forge-SD-XL-Turbo.ipynb`](01-4-Forge-SD-XL-Turbo.ipynb) (API REST similaire)\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Prochaines √âtapes Apprentissage\n",
    "\n",
    "1. **Ma√Ætriser les bases**: Reproduire tous les exemples de ce notebook\n",
    "2. **Exp√©rimenter**: Modifier workflows, tester nouveaux prompts\n",
    "3. **Explorer workflows avanc√©s**: ControlNet, AnimateDiff, Multi-Model\n",
    "4. **Cr√©er projets personnels**: Application web int√©grant API Qwen\n",
    "5. **Contribuer communaut√©**: Partager vos workflows innovants\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ Notebook termin√©! Bon apprentissage avec Qwen Image Edit! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp-jupyter-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}