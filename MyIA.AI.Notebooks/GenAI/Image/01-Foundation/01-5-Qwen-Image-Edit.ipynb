{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "79be9a9e",
      "metadata": {},
      "source": [
        "# Notebook: Qwen Image-Edit 2.5 - API ComfyUI\n",
        "\n",
        "**Objectif**: Ma\u00eetriser g\u00e9n\u00e9ration et \u00e9dition d'images via API Qwen-Image-Edit (backend ComfyUI)\n",
        "\n",
        "## \ud83c\udfaf Ce que vous allez apprendre\n",
        "\n",
        "1. Diff\u00e9rence API **Forge** (simple) vs **ComfyUI** (workflows JSON)\n",
        "2. Pattern **\"queue and poll\"** pour g\u00e9n\u00e9ration asynchrone\n",
        "3. Cr\u00e9ation workflows **Text-to-Image** et **Image-to-Image**\n",
        "4. Optimisation param\u00e8tres (**steps**, **cfg**, **denoise**)\n",
        "5. Troubleshooting erreurs courantes (**timeout**, **CUDA OOM**)\n",
        "\n",
        "## \ud83d\ude80 API Qwen-Image-Edit\n",
        "\n",
        "| Caract\u00e9ristique | Valeur |\n",
        "|----------------|--------|\n",
        "| **URL Production** | `https://qwen-image-edit.myia.io` |\n",
        "| **Mod\u00e8le** | Qwen-Image-Edit-2509-FP8 (54GB) |\n",
        "| **GPU** | RTX 3090 (24GB VRAM) |\n",
        "| **Latence Typique** | 5-10 secondes |\n",
        "| **R\u00e9solution Optimale** | 512x512 pixels |\n",
        "\n",
        "## \ud83d\udd0d ComfyUI vs Forge\n",
        "\n",
        "**Forge (SD XL Turbo)**:\n",
        "- \u2705 API simple (1 requ\u00eate POST)\n",
        "- \u2705 Ultra-rapide (1-3s)\n",
        "- \u274c Pas d'\u00e9dition images\n",
        "- \u274c Moins flexible\n",
        "\n",
        "**ComfyUI (Qwen)**:\n",
        "- \u2705 Workflows JSON complexes\n",
        "- \u2705 \u00c9dition images avanc\u00e9e\n",
        "- \u2705 Contr\u00f4le fin (28 custom nodes)\n",
        "- \u274c API plus complexe (queue + poll)\n",
        "\n",
        "**Recommandation**: Commencer avec Forge pour prototypes, affiner avec Qwen pour production.\n",
        "\n",
        "## \ud83d\udcda Pr\u00e9requis\n",
        "\n",
        "```bash\n",
        "pip install requests pillow matplotlib\n",
        "```\n",
        "\n",
        "**Temps estim\u00e9**: 90-120 minutes"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ikycba75qmf",
      "source": "## \u2699\ufe0f V\u00e9rification de l'Environnement\n\n**Avant d'ex\u00e9cuter ce notebook**, assurez-vous que tous les packages Python requis sont install\u00e9s :\n\n```bash\npip install pillow requests matplotlib python-dotenv\n```\n\n### Packages Requis\n\n| Package | Utilisation | Installation |\n|---------|-------------|--------------|\n| **Pillow** | Manipulation d'images (API PIL) | `pip install pillow` |\n| **requests** | Appels API ComfyUI | `pip install requests` |\n| **matplotlib** | Visualisation r\u00e9sultats | `pip install matplotlib` |\n| **python-dotenv** | Chargement variables d'environnement | `pip install python-dotenv` |\n\n**\u26a0\ufe0f Note importante** : Le package `Pillow` fournit l'API `PIL`. Si vous obtenez `ModuleNotFoundError: No module named 'PIL'`, installez Pillow avec :\n\n```bash\npip install --upgrade pillow\n```\n\nPour v\u00e9rifier l'installation, ex\u00e9cutez dans un terminal Python :\n\n```python\nimport PIL\nfrom PIL import Image\nprint(f\"\u2705 PIL {PIL.__version__} install\u00e9\")\n```",
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "244bd967",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports standard\n",
        "import requests\n",
        "import json\n",
        "import base64\n",
        "import time\n",
        "import uuid\n",
        "import os\n",
        "from typing import Dict, Optional, List\n",
        "from io import BytesIO\n",
        "\n",
        "# Visualisation\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Configuration environnement\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration API\n",
        "API_BASE_URL = \"https://qwen-image-edit.myia.io\"\n",
        "CLIENT_ID = str(uuid.uuid4())  # ID unique pour tracking\n",
        "\n",
        "# Authentification - Token ComfyUI standardis\u00e9\n",
        "COMFYUI_API_TOKEN = os.getenv(\"COMFYUI_API_TOKEN\")\n",
        "\n",
        "if not COMFYUI_API_TOKEN:\n",
        "    print(\"\u26a0\ufe0f  COMFYUI_API_TOKEN non trouv\u00e9 - connexion sans authentification\")\n",
        "    print(\"\\n\ud83d\udccb Pour activer l'authentification :\")\n",
        "    print(\"   1. Cr\u00e9ez un fichier .env dans MyIA.AI.Notebooks/GenAI/\")\n",
        "    print(\"   2. Ajoutez : COMFYUI_API_TOKEN=votre_token_ici\")\n",
        "    print(\"   3. Obtenez le token via scripts/genai-auth/extract-bearer-tokens.ps1\")\n",
        "    print(\"\\n\u2713 Le notebook fonctionnera en mode d\u00e9grad\u00e9 (si serveur non s\u00e9curis\u00e9)\\n\")\n",
        "    COMFYUI_API_TOKEN = None\n",
        "else:\n",
        "    print(\"\u2705 Configuration charg\u00e9e\")\n",
        "    print(f\"\ud83d\udce1 API: {API_BASE_URL}\")\n",
        "    print(f\"\ud83c\udd94 Client ID: {CLIENT_ID}\")\n",
        "    print(f\"\ud83d\udd10 Token: {COMFYUI_API_TOKEN[:15]}...{COMFYUI_API_TOKEN[-5:]}\" if len(COMFYUI_API_TOKEN) > 20 else f\"\ud83d\udd10 Token: (court)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5743a6a4",
      "metadata": {},
      "source": "## \ud83c\udfd7\ufe0f Architecture ComfyUI: Workflows JSON\n\n### Diff\u00e9rence fondamentale avec Forge\n\n**API Forge (POST direct)**:\n```python\nresponse = requests.post(url, json={\"prompt\": \"astronaut\"})\nimage_base64 = response.json()[\"image\"]\n```\n\n**API ComfyUI (Queue + Poll)**:\n```python\n# 1. Soumettre workflow JSON\nresponse = requests.post(f\"{url}/prompt\", json={\n    \"prompt\": workflow_json,\n    \"client_id\": client_id\n})\nprompt_id = response.json()[\"prompt_id\"]\n\n# 2. Attendre compl\u00e9tion (polling)\nwhile True:\n    history = requests.get(f\"{url}/history/{prompt_id}\")\n    if history.json().get(prompt_id, {}).get(\"status\", {}).get(\"completed\"):\n        break\n    time.sleep(1)\n\n# 3. R\u00e9cup\u00e9rer images\nimages = history.json()[prompt_id][\"outputs\"]\n```\n\n### Structure Workflow ComfyUI\n\nUn **workflow** est un **graph JSON** de **nodes connect\u00e9s**:\n\n```json\n{\n  \"1\": {  // Node VAE Loader\n    \"class_type\": \"VAELoader\",\n    \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n  },\n  \"2\": {  // Node KSampler\n    \"class_type\": \"KSampler\",\n    \"inputs\": {\n      \"model\": [\"5\", 0],  // Connexion: output du node 5 (CFGNorm)\n      \"steps\": 20,\n      \"cfg\": 1.0,\n      \"seed\": 42\n    }\n  },\n  \"3\": {  // Node Save Image\n    \"class_type\": \"SaveImage\",\n    \"inputs\": {\"images\": [\"2\", 0]}\n  }\n}\n```\n\n**Workflow = Pipeline modulaire** ou chaque node effectue une operation (charger modele, sampler, encoder texte, etc.).\n\n### Anatomie d'un Node\n\n| Propri\u00e9t\u00e9 | Description | Exemple |\n|-----------|-------------|----------|\n| **`class_type`** | Type de node ComfyUI | `\"TextEncodeQwenImageEdit\"` |\n| **`inputs`** | Param\u00e8tres du node | `{\"prompt\": \"astronaut\"}` |\n| **Connexions** | `[node_id, output_slot]` | `[\"5\", 0]` |\n\n### Architecture Qwen: Loaders Separes (Phase 29)\n\nQwen Image Edit n'utilise **pas** `CheckpointLoaderSimple` car les poids du modele sont stockes dans des fichiers separes :\n\n| Loader | Fichier | Role |\n|--------|---------|------|\n| **VAELoader** | `qwen_image_vae.safetensors` | VAE 16 canaux (pas SDXL standard) |\n| **CLIPLoader** | `qwen_2.5_vl_7b_fp8_scaled.safetensors` (type `sd3`) | Encodeur vision-language |\n| **UNETLoader** | `qwen_image_edit_2509_fp8_e4m3fn.safetensors` | Modele de diffusion |\n\nApres chargement, le UNET passe par **ModelSamplingAuraFlow** (shift=3.0) puis **CFGNorm** (strength=1.0) avant d'arriver au KSampler.\n\nLe prompt est encode avec **TextEncodeQwenImageEdit** (encodeur natif Qwen, pas CLIPTextEncode standard), et le conditioning negatif utilise **ConditioningZeroOut** (pas un second encodeur texte).\n\nL'espace latent utilise **EmptySD3LatentImage** (16 canaux) au lieu de EmptyLatentImage (4 canaux SDXL).",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "851cb759",
      "metadata": {},
      "source": "### \ud83d\udd27 Visualisation Architecture Workflow ComfyUI (Phase 29 Qwen)\n\n**Diagramme ASCII du workflow Qwen avec loaders separes**:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  WORKFLOW QWEN PHASE 29                        \u2502\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502 VAELoader    \u2502  \u2502 CLIPLoader   \u2502  \u2502 UNETLoader   \u2502        \u2502\n\u2502  \u2502 (16ch VAE)   \u2502  \u2502 (type: sd3)  \u2502  \u2502 (fp8_e4m3fn) \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502         \u2502                  \u2502                  \u2502                 \u2502\n\u2502         \u2502                  \u2502                  \u25bc                 \u2502\n\u2502         \u2502                  \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502         \u2502                  \u2502          \u2502 ModelSampling\u2502         \u2502\n\u2502         \u2502                  \u2502          \u2502 AuraFlow     \u2502         \u2502\n\u2502         \u2502                  \u2502          \u2502 (shift=3.0)  \u2502         \u2502\n\u2502         \u2502                  \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502         \u2502                  \u2502                  \u25bc                 \u2502\n\u2502         \u2502                  \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502         \u2502                  \u2502          \u2502 CFGNorm      \u2502         \u2502\n\u2502         \u2502                  \u2502          \u2502 (strength=1) \u2502         \u2502\n\u2502         \u2502                  \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502         \u2502                  \u2502                  \u2502                 \u2502\n\u2502         \u2502                  \u25bc                  \u2502                 \u2502\n\u2502         \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502                \u2502\n\u2502         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 TextEncodeQwen   \u2502       \u2502                \u2502\n\u2502         \u2502          \u2502 ImageEdit        \u2502       \u2502                \u2502\n\u2502         \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502                \u2502\n\u2502         \u2502                   \u2502                  \u2502                \u2502\n\u2502         \u2502                   \u25bc                  \u2502                \u2502\n\u2502         \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502                \u2502\n\u2502         \u2502          \u2502 ConditioningZero \u2502        \u2502                \u2502\n\u2502         \u2502          \u2502 Out (negatif)    \u2502        \u2502                \u2502\n\u2502         \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502                \u2502\n\u2502         \u2502                   \u2502                  \u2502                \u2502\n\u2502         \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                  \u2502                \u2502\n\u2502         \u2502    \u2502              \u2502                  \u2502                \u2502\n\u2502         \u2502    \u25bc              \u25bc                  \u25bc                \u2502\n\u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n\u2502         \u2502  \u2502          KSampler                     \u2502           \u2502\n\u2502         \u2502  \u2502  (cfg=1.0, scheduler=beta, euler)    \u2502           \u2502\n\u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2502         \u2502                 \u2502                                    \u2502\n\u2502         \u2502                 \u25bc                                    \u2502\n\u2502         \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                            \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 VAE Decode   \u2502                            \u2502\n\u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                            \u2502\n\u2502                          \u2502                                     \u2502\n\u2502                          \u25bc                                     \u2502\n\u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                            \u2502\n\u2502                   \u2502 Save Image   \u2502                            \u2502\n\u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n**Flux de donn\u00e9es (Phase 29)**:\n1. **VAELoader** -> Charge le VAE 16 canaux Qwen + fournit VAE au TextEncode et au VAEDecode\n2. **CLIPLoader** -> Fournit le CLIP au TextEncodeQwenImageEdit\n3. **UNETLoader** -> Charge le modele, passe par ModelSamplingAuraFlow puis CFGNorm\n4. **TextEncodeQwenImageEdit** -> Encode le prompt avec CLIP + VAE (encodeur natif Qwen)\n5. **ConditioningZeroOut** -> Cree un conditioning negatif vide\n6. **EmptySD3LatentImage** -> Canvas latent 16 canaux (non montre pour simplifier)\n7. **KSampler** -> Genere l'image latente (cfg=1.0, scheduler=beta)\n8. **VAEDecode** -> Convertit latent en image RGB\n9. **SaveImage** -> Sauvegarde l'image finale\n\n**Correspondance JSON**:\n```json\n{\n  \"1\": {\"class_type\": \"VAELoader\", \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}},\n  \"2\": {\"class_type\": \"CLIPLoader\", \"inputs\": {\"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\", \"type\": \"sd3\"}},\n  \"3\": {\"class_type\": \"UNETLoader\", \"inputs\": {\"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\", \"weight_dtype\": \"fp8_e4m3fn\"}},\n  \"4\": {\"class_type\": \"ModelSamplingAuraFlow\", \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}},\n  \"5\": {\"class_type\": \"CFGNorm\", \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}},\n  \"6\": {\"class_type\": \"TextEncodeQwenImageEdit\", \"inputs\": {\"clip\": [\"2\", 0], \"prompt\": \"...\", \"vae\": [\"1\", 0]}},\n  \"7\": {\"class_type\": \"ConditioningZeroOut\", \"inputs\": {\"conditioning\": [\"6\", 0]}},\n  \"8\": {\"class_type\": \"EmptySD3LatentImage\", \"inputs\": {\"width\": 512, \"height\": 512, \"batch_size\": 1}},\n  \"9\": {\"class_type\": \"KSampler\", \"inputs\": {\"model\": [\"5\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"8\", 0], \"cfg\": 1.0, \"scheduler\": \"beta\"}},\n  \"10\": {\"class_type\": \"VAEDecode\", \"inputs\": {\"samples\": [\"9\", 0], \"vae\": [\"1\", 0]}},\n  \"11\": {\"class_type\": \"SaveImage\", \"inputs\": {\"images\": [\"10\", 0]}}\n}\n```\n\n**Notation `[\"ID_NODE\", INDEX_OUTPUT]`**:\n- `[\"1\", 0]` = Output 0 (VAE) du node 1 (VAELoader)\n- `[\"2\", 0]` = Output 0 (CLIP) du node 2 (CLIPLoader)\n- `[\"5\", 0]` = Output 0 (model) du node 5 (CFGNorm, apres ModelSamplingAuraFlow)",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Configuration charg\u00e9e\n",
            "\ud83d\udce1 API: https://qwen-image-edit.myia.io\n",
            "\ud83c\udd94 Client ID: aa2da349-2b17-47ce-b5de-ed59c48873a0\n",
            "\ud83d\udd10 Token: $2b$12$I7V9gQud...O7ekC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a8ef79",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ComfyUIClient:\n",
        "    \"\"\"Client p\u00e9dagogique API ComfyUI pour Qwen avec authentification Bearer\"\"\"\n",
        "    \n",
        "    def __init__(self, base_url=API_BASE_URL, client_id=CLIENT_ID, auth_token=None):\n",
        "        self.base_url = base_url\n",
        "        self.client_id = client_id\n",
        "        self.session = requests.Session()\n",
        "        \n",
        "        # Configuration authentification\n",
        "        if auth_token:\n",
        "            self.session.headers.update({\n",
        "                \"Authorization\": f\"Bearer {auth_token}\"\n",
        "            })\n",
        "        elif COMFYUI_API_TOKEN:  # Utilise variable globale si disponible\n",
        "            self.session.headers.update({\n",
        "                \"Authorization\": f\"Bearer {COMFYUI_API_TOKEN}\"\n",
        "            })\n",
        "        # Pas d'erreur si pas de token - graceful degradation\n",
        "    \n",
        "    def execute_workflow(\n",
        "        self, \n",
        "        workflow_json: Dict, \n",
        "        wait_for_completion: bool = True,\n",
        "        max_wait: int = 120,\n",
        "        verbose: bool = True\n",
        "    ) -> Dict:\n",
        "        \"\"\"Ex\u00e9cute workflow ComfyUI et r\u00e9cup\u00e8re r\u00e9sultats\n",
        "        \n",
        "        Args:\n",
        "            workflow_json: Workflow ComfyUI (dict)\n",
        "            wait_for_completion: Attendre fin g\u00e9n\u00e9ration\n",
        "            max_wait: Timeout en secondes\n",
        "            verbose: Afficher logs progression\n",
        "        \n",
        "        Returns:\n",
        "            dict: {\"prompt_id\", \"outputs\", \"status\"}\n",
        "        \"\"\"\n",
        "        # 1. Soumettre workflow\n",
        "        if verbose:\n",
        "            print(\"\ud83d\udce4 Soumission workflow...\")\n",
        "        \n",
        "        response = self.session.post(\n",
        "            f\"{self.base_url}/prompt\",\n",
        "            json={\"prompt\": workflow_json, \"client_id\": self.client_id}\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        prompt_id = response.json()[\"prompt_id\"]\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\u2705 Workflow queued: {prompt_id}\")\n",
        "        \n",
        "        if not wait_for_completion:\n",
        "            return {\"prompt_id\": prompt_id, \"status\": \"queued\"}\n",
        "        \n",
        "        # 2. Polling compl\u00e9tion\n",
        "        start_time = time.time()\n",
        "        while True:\n",
        "            elapsed = time.time() - start_time\n",
        "            if elapsed > max_wait:\n",
        "                raise TimeoutError(f\"Timeout apr\u00e8s {max_wait}s\")\n",
        "            \n",
        "            history_response = self.session.get(\n",
        "                f\"{self.base_url}/history/{prompt_id}\"\n",
        "            )\n",
        "            history_response.raise_for_status()\n",
        "            history = history_response.json()\n",
        "            \n",
        "            if prompt_id not in history:\n",
        "                time.sleep(1)\n",
        "                continue\n",
        "            \n",
        "            prompt_data = history[prompt_id]\n",
        "            status = prompt_data.get(\"status\", {})\n",
        "            \n",
        "            if status.get(\"completed\"):\n",
        "                if verbose:\n",
        "                    print(f\"\u2705 Complet en {elapsed:.1f}s\")\n",
        "                \n",
        "                # 3. Extraire outputs\n",
        "                outputs = prompt_data.get(\"outputs\", {})\n",
        "                images = []\n",
        "                \n",
        "                for node_id, node_output in outputs.items():\n",
        "                    if \"images\" in node_output:\n",
        "                        for img_info in node_output[\"images\"]:\n",
        "                            # R\u00e9cup\u00e9rer image\n",
        "                            img_response = self.session.get(\n",
        "                                f\"{self.base_url}/view\",\n",
        "                                params={\n",
        "                                    \"filename\": img_info[\"filename\"],\n",
        "                                    \"subfolder\": img_info.get(\"subfolder\", \"\"),\n",
        "                                    \"type\": img_info.get(\"type\", \"output\")\n",
        "                                }\n",
        "                            )\n",
        "                            img_response.raise_for_status()\n",
        "                            images.append({\n",
        "                                \"data\": img_response.content,\n",
        "                                \"filename\": img_info[\"filename\"]\n",
        "                            })\n",
        "                \n",
        "                return {\n",
        "                    \"prompt_id\": prompt_id,\n",
        "                    \"status\": \"completed\",\n",
        "                    \"outputs\": outputs,\n",
        "                    \"images\": images,\n",
        "                    \"duration\": elapsed\n",
        "                }\n",
        "            \n",
        "            if status.get(\"status_str\") == \"error\":\n",
        "                error_msg = status.get(\"messages\", [\"Unknown error\"])\n",
        "                raise RuntimeError(f\"ComfyUI error: {error_msg}\")\n",
        "            \n",
        "            if verbose and int(elapsed) % 5 == 0:\n",
        "                print(f\"\u23f3 En cours... ({elapsed:.0f}s)\")\n",
        "            \n",
        "            time.sleep(1)\n",
        "    \n",
        "    def display_images(self, result: Dict, figsize=(12, 4)):\n",
        "        \"\"\"Affiche images r\u00e9sultats\"\"\"\n",
        "        images = result.get(\"images\", [])\n",
        "        if not images:\n",
        "            print(\"\u26a0\ufe0f Aucune image g\u00e9n\u00e9r\u00e9e\")\n",
        "            return\n",
        "        \n",
        "        n = len(images)\n",
        "        fig, axes = plt.subplots(1, n, figsize=figsize)\n",
        "        if n == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for ax, img_data in zip(axes, images):\n",
        "            img = Image.open(BytesIO(img_data[\"data\"]))\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(img_data[\"filename\"], fontsize=10)\n",
        "            ax.axis(\"off\")\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Instancier client avec authentification\n",
        "client = ComfyUIClient(auth_token=COMFYUI_API_TOKEN)\n",
        "if COMFYUI_API_TOKEN:\n",
        "    print(\"\u2705 ComfyUIClient pr\u00eat avec authentification\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  ComfyUIClient pr\u00eat en mode sans authentification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de517444",
      "metadata": {},
      "source": "## \ud83d\ude80 Workflow Minimal: \"Hello World\"\n\n### Objectif\n\nCr\u00e9er le workflow le plus simple possible pour **valider l'API** et comprendre la structure JSON.\n\n### Workflow Text-to-Image Basique (Architecture Phase 29)\n\nQwen Image Edit utilise des **loaders s\u00e9par\u00e9s** (VAE, CLIP, UNET) au lieu d'un checkpoint unique. Le pipeline complet est :\n\n1. **VAELoader** \u2192 Charger le VAE 16 canaux (`qwen_image_vae.safetensors`)\n2. **CLIPLoader** \u2192 Charger le CLIP vision-language (`qwen_2.5_vl_7b_fp8_scaled.safetensors`, type `sd3`)\n3. **UNETLoader** \u2192 Charger le mod\u00e8le UNET (`qwen_image_edit_2509_fp8_e4m3fn.safetensors`)\n4. **ModelSamplingAuraFlow** \u2192 Configurer l'\u00e9chantillonnage (shift=3.0)\n5. **CFGNorm** \u2192 Normalisation CFG (strength=1.0)\n6. **TextEncodeQwenImageEdit** \u2192 Encoder le prompt (encodeur natif Qwen)\n7. **ConditioningZeroOut** \u2192 Conditioning n\u00e9gatif (vide)\n8. **EmptySD3LatentImage** \u2192 Cr\u00e9er canvas latent 16 canaux\n9. **KSampler** \u2192 G\u00e9n\u00e9rer l'image\n10. **VAEDecode** \u2192 Convertir latent vers pixels\n11. **SaveImage** \u2192 Sauvegarder r\u00e9sultat\n\n### Param\u00e8tres Critiques\n\n| Param\u00e8tre | Valeur | Impact |\n|-----------|--------|--------|\n| **steps** | 20 | Qualit\u00e9 (plus de steps = meilleure qualit\u00e9, plus de temps) |\n| **cfg** | 1.0 | Doit rester a 1.0 car CFGNorm gere l'amplification |\n| **sampler** | euler | Algorithme de generation |\n| **scheduler** | beta | Obligatoire pour Qwen (pas \"normal\") |\n| **denoise** | 1.0 | Force generation (1.0 = 100%) |\n| **seed** | 42 | Reproductibilite |\n| **shift** | 3.0 | Parametre ModelSamplingAuraFlow |\n\n**Temps attendu**: 5-10 secondes (512x512)",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20b3bf0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 ComfyUIClient pr\u00eat avec authentification\n"
          ]
        }
      ],
      "source": "# Workflow Text-to-Image minimal (Architecture Phase 29)\nworkflow_hello = {\n    \"1\": {  # VAE Loader (16 canaux Qwen)\n        \"class_type\": \"VAELoader\",\n        \"inputs\": {\n            \"vae_name\": \"qwen_image_vae.safetensors\"\n        }\n    },\n    \"2\": {  # CLIP Loader (vision-language Qwen)\n        \"class_type\": \"CLIPLoader\",\n        \"inputs\": {\n            \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n            \"type\": \"sd3\"\n        }\n    },\n    \"3\": {  # UNET Loader (modele diffusion Qwen)\n        \"class_type\": \"UNETLoader\",\n        \"inputs\": {\n            \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n            \"weight_dtype\": \"fp8_e4m3fn\"\n        }\n    },\n    \"4\": {  # ModelSamplingAuraFlow (configuration echantillonnage)\n        \"class_type\": \"ModelSamplingAuraFlow\",\n        \"inputs\": {\n            \"model\": [\"3\", 0],  # UNET du loader\n            \"shift\": 3.0\n        }\n    },\n    \"5\": {  # CFGNorm (normalisation CFG)\n        \"class_type\": \"CFGNorm\",\n        \"inputs\": {\n            \"model\": [\"4\", 0],  # Modele apres AuraFlow\n            \"strength\": 1.0\n        }\n    },\n    \"6\": {  # TextEncodeQwenImageEdit (encodeur natif Qwen)\n        \"class_type\": \"TextEncodeQwenImageEdit\",\n        \"inputs\": {\n            \"clip\": [\"2\", 0],   # CLIP du loader\n            \"prompt\": \"a majestic astronaut floating in space, photorealistic, 8k, detailed\",\n            \"vae\": [\"1\", 0]     # VAE requis par l'encodeur Qwen\n        }\n    },\n    \"7\": {  # ConditioningZeroOut (conditioning negatif vide)\n        \"class_type\": \"ConditioningZeroOut\",\n        \"inputs\": {\n            \"conditioning\": [\"6\", 0]  # Basee sur le conditioning positif\n        }\n    },\n    \"8\": {  # EmptySD3LatentImage (canvas latent 16 canaux)\n        \"class_type\": \"EmptySD3LatentImage\",\n        \"inputs\": {\n            \"width\": 512,\n            \"height\": 512,\n            \"batch_size\": 1\n        }\n    },\n    \"9\": {  # KSampler (generation)\n        \"class_type\": \"KSampler\",\n        \"inputs\": {\n            \"model\": [\"5\", 0],       # Modele apres CFGNorm\n            \"positive\": [\"6\", 0],    # Prompt positif (TextEncodeQwen)\n            \"negative\": [\"7\", 0],    # Conditioning negatif (ZeroOut)\n            \"latent_image\": [\"8\", 0],  # Canvas latent SD3\n            \"seed\": 42,\n            \"steps\": 20,\n            \"cfg\": 1.0,              # cfg=1.0 car CFGNorm gere l'amplification\n            \"sampler_name\": \"euler\",\n            \"scheduler\": \"beta\",     # scheduler beta obligatoire pour Qwen\n            \"denoise\": 1.0\n        }\n    },\n    \"10\": {  # VAE Decode (latent -> pixels)\n        \"class_type\": \"VAEDecode\",\n        \"inputs\": {\n            \"samples\": [\"9\", 0],  # Latent du sampler\n            \"vae\": [\"1\", 0]       # VAE du loader\n        }\n    },\n    \"11\": {  # Save Image\n        \"class_type\": \"SaveImage\",\n        \"inputs\": {\n            \"images\": [\"10\", 0],  # Pixels du VAE Decode\n            \"filename_prefix\": \"ComfyUI\"\n        }\n    }\n}\n\n# Execution\nprint(\"Lancement generation...\")\nresult = client.execute_workflow(workflow_hello, verbose=True)\n\n# Affichage\nclient.display_images(result)\nprint(f\"\\nImage generee en {result['duration']:.1f}s\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d6a7ad",
      "metadata": {},
      "outputs": [],
      "source": "# ========================================\n# WORKFLOW REEL 1: Edition Simple Image (Phase 29)\n# ========================================\n\ndef create_simple_edit_workflow(image_name: str, edit_prompt: str, denoise: float = 0.5) -> dict:\n    \"\"\"Workflow edition simple d'une image existante (Architecture Phase 29)\n    \n    Args:\n        image_name: Nom du fichier image uploade sur ComfyUI\n        edit_prompt: Description de l'edition souhaitee\n        denoise: Force de l'edition (0.0 = aucune, 1.0 = complete)\n    \n    Returns:\n        Workflow JSON pret a executer\n    \"\"\"\n    workflow = {\n        \"1\": {  # VAE Loader (16 canaux Qwen)\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"2\": {  # CLIP Loader\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"3\": {  # UNET Loader\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        \"4\": {  # ModelSamplingAuraFlow\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}\n        },\n        \"5\": {  # CFGNorm\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}\n        },\n        \"6\": {  # Load Image (image source)\n            \"class_type\": \"LoadImage\",\n            \"inputs\": {\"image\": image_name}\n        },\n        \"7\": {  # VAE Encode (pixels -> latent de l'image source)\n            \"class_type\": \"VAEEncode\",\n            \"inputs\": {\n                \"pixels\": [\"6\", 0],  # Image source\n                \"vae\": [\"1\", 0]      # VAE Qwen 16 canaux\n            }\n        },\n        \"8\": {  # TextEncodeQwenImageEdit (encodeur natif Qwen)\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"2\", 0],\n                \"prompt\": edit_prompt,\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"9\": {  # ConditioningZeroOut (negatif)\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"8\", 0]}\n        },\n        \"10\": {  # KSampler (edition avec denoise partiel)\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"seed\": 42,\n                \"steps\": 20,\n                \"cfg\": 1.0,              # cfg=1.0 car CFGNorm gere l'amplification\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",     # scheduler beta obligatoire\n                \"denoise\": denoise,\n                \"model\": [\"5\", 0],       # Modele apres CFGNorm\n                \"positive\": [\"8\", 0],    # Prompt positif\n                \"negative\": [\"9\", 0],    # Conditioning negatif ZeroOut\n                \"latent_image\": [\"7\", 0]  # Latent de l'image source\n            }\n        },\n        \"11\": {  # VAE Decode\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\n                \"samples\": [\"10\", 0],\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"12\": {  # Save Image\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\n                \"images\": [\"11\", 0],\n                \"filename_prefix\": \"qwen_edit_simple\"\n            }\n        }\n    }\n    return workflow\n\n# ========================================\n# WORKFLOW REEL 2: Chainage Nodes Avance (Phase 29)\n# ========================================\n\ndef create_chained_workflow(base_prompt: str, refine_prompt: str) -> dict:\n    \"\"\"Workflow avec chainage: generation base + raffinement (Phase 29)\n    \n    Architecture:\n        1. Generation image base (text-to-image) avec loaders separes\n        2. Raffinement avec nouveau prompt (image-to-image)\n    \n    Args:\n        base_prompt: Prompt initial generation\n        refine_prompt: Prompt raffinement/amelioration\n    \n    Returns:\n        Workflow JSON avec 2 etapes KSampler\n    \"\"\"\n    workflow = {\n        # Loaders (partages entre les 2 etapes)\n        \"1\": {  # VAE Loader\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"2\": {  # CLIP Loader\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"3\": {  # UNET Loader\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        \"4\": {  # ModelSamplingAuraFlow\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}\n        },\n        \"5\": {  # CFGNorm\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}\n        },\n        # Etape 1: Generation base (text-to-image)\n        \"6\": {  # TextEncodeQwenImageEdit (prompt base)\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"2\", 0],\n                \"prompt\": base_prompt,\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"7\": {  # ConditioningZeroOut (negatif etape 1)\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"6\", 0]}\n        },\n        \"8\": {  # EmptySD3LatentImage (canvas 16 canaux)\n            \"class_type\": \"EmptySD3LatentImage\",\n            \"inputs\": {\"width\": 512, \"height\": 512, \"batch_size\": 1}\n        },\n        \"9\": {  # KSampler etape 1 (generation complete)\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"seed\": 42,\n                \"steps\": 10,\n                \"cfg\": 1.0,\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",\n                \"denoise\": 1.0,  # Generation complete\n                \"model\": [\"5\", 0],\n                \"positive\": [\"6\", 0],\n                \"negative\": [\"7\", 0],\n                \"latent_image\": [\"8\", 0]\n            }\n        },\n        # Etape 2: Raffinement (image-to-image sur le latent de l'etape 1)\n        \"10\": {  # TextEncodeQwenImageEdit (prompt raffinement)\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"2\", 0],\n                \"prompt\": refine_prompt,\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"11\": {  # ConditioningZeroOut (negatif etape 2)\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"10\", 0]}\n        },\n        \"12\": {  # KSampler etape 2 (raffinement leger)\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"seed\": 43,\n                \"steps\": 10,\n                \"cfg\": 1.0,\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",\n                \"denoise\": 0.3,  # Raffinement leger\n                \"model\": [\"5\", 0],\n                \"positive\": [\"10\", 0],\n                \"negative\": [\"11\", 0],\n                \"latent_image\": [\"9\", 0]  # Sortie de l'etape 1\n            }\n        },\n        \"13\": {  # VAE Decode\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\"samples\": [\"12\", 0], \"vae\": [\"1\", 0]}\n        },\n        \"14\": {  # Save Image\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\n                \"images\": [\"13\", 0],\n                \"filename_prefix\": \"qwen_chained\"\n            }\n        }\n    }\n    return workflow\n\n# ========================================\n# EXEMPLE D'UTILISATION\n# ========================================\n\n# Workflow 1: Edition simple\nworkflow_simple = create_simple_edit_workflow(\n    image_name=\"cat.png\",\n    edit_prompt=\"Add sunglasses to the cat\",\n    denoise=0.5\n)\nprint(\"Workflow edition simple cree (Phase 29)\")\nprint(f\"   Nodes: {len(workflow_simple)}\")\n\n# Workflow 2: Chainage\nworkflow_chained = create_chained_workflow(\n    base_prompt=\"A cat sitting on a chair\",\n    refine_prompt=\"High quality, professional photography, detailed fur\"\n)\nprint(\"Workflow chaine cree (Phase 29)\")\nprint(f\"   Nodes: {len(workflow_chained)}\")\nprint(f\"   KSamplers: 2 (base + raffinement)\")\n\n# Pour executer ces workflows:\n# client = ComfyUIClient(\"https://qwen-image-edit.myia.io\")\n# result = client.execute_workflow(workflow_simple)\n# client.display_images(result)"
    },
    {
      "cell_type": "markdown",
      "id": "e75258e1",
      "metadata": {},
      "source": [
        "## \ud83d\uddbc\ufe0f \u00c9dition Images avec Qwen VLM\n",
        "\n",
        "### Capacit\u00e9s Qwen Vision-Language Model\n",
        "\n",
        "**Qwen-Image-Edit** combine:\n",
        "- \ud83e\udde0 **Vision Encoder** (CLIP-like) pour comprendre images\n",
        "- \u270d\ufe0f **Language Model** pour interpr\u00e9ter instructions texte\n",
        "- \ud83c\udfa8 **Diffusion Model** pour \u00e9diter images\n",
        "\n",
        "### Cas d'Usage Typiques\n",
        "\n",
        "| T\u00e2che | Exemple Prompt |\n",
        "|-------|----------------|\n",
        "| **Style Transfer** | `\"Convert to watercolor painting\"` |\n",
        "| **Object Addition** | `\"Add a red balloon in the sky\"` |\n",
        "| **Color Grading** | `\"Make the image warmer, golden hour lighting\"` |\n",
        "| **Background Change** | `\"Replace background with snowy mountains\"` |\n",
        "| **Detail Enhancement** | `\"Enhance facial details, 8k quality\"` |\n",
        "\n",
        "### Pattern Image-to-Image\n",
        "\n",
        "**Diff\u00e9rence cl\u00e9 avec Text-to-Image**:\n",
        "- **Nouveau node**: `LoadImage` pour charger image source\n",
        "- **Param\u00e8tre critique**: `denoise` (0.0-1.0)\n",
        "  - `denoise=0.1`: \u00c9dition subtile (retouche l\u00e9g\u00e8re)\n",
        "  - `denoise=0.5`: \u00c9dition mod\u00e9r\u00e9e (style transfer)\n",
        "  - `denoise=0.9`: \u00c9dition forte (reconstruction quasi-totale)\n",
        "\n",
        "**Workflow**:\n",
        "1. **Load Image** \u2192 Charger image source\n",
        "2. **CLIP Vision Encode** \u2192 Encoder image\n",
        "3. **CLIP Text Encode** \u2192 Encoder prompt \u00e9dition\n",
        "4. **VAE Encode** \u2192 Convertir pixels \u2192 latent\n",
        "5. **KSampler** (denoise < 1.0) \u2192 \u00c9diter latent\n",
        "6. **VAE Decode** \u2192 Convertir latent \u2192 pixels\n",
        "7. **Save Image** \u2192 Sauvegarder r\u00e9sultat"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba896395",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lancement generation...\n",
            "\ud83d\udce4 Soumission workflow...\n"
          ]
        },
        {
          "ename": "HTTPError",
          "evalue": "502 Server Error: Bad Gateway for url: https://qwen-image-edit.myia.io/prompt",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Execution\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLancement generation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m result = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkflow_hello\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Affichage\u001b[39;00m\n\u001b[32m     95\u001b[39m client.display_images(result)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mComfyUIClient.execute_workflow\u001b[39m\u001b[34m(self, workflow_json, wait_for_completion, max_wait, verbose)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\ud83d\udce4 Soumission workflow...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m response = \u001b[38;5;28mself\u001b[39m.session.post(\n\u001b[32m     43\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.base_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m     json={\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: workflow_json, \u001b[33m\"\u001b[39m\u001b[33mclient_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.client_id}\n\u001b[32m     45\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m prompt_id = response.json()[\u001b[33m\"\u001b[39m\u001b[33mprompt_id\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1019\u001b[39m     http_error_msg = (\n\u001b[32m   1020\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1021\u001b[39m     )\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 502 Server Error: Bad Gateway for url: https://qwen-image-edit.myia.io/prompt"
          ]
        }
      ],
      "source": [
        "def upload_image_to_comfyui(image_path: str) -> str:\n",
        "    \"\"\"Upload image vers ComfyUI pour \u00e9dition\n",
        "    \n",
        "    Args:\n",
        "        image_path: Chemin image locale ou URL\n",
        "    \n",
        "    Returns:\n",
        "        str: Nom fichier upload\u00e9 dans ComfyUI\n",
        "    \"\"\"\n",
        "    # Charger image\n",
        "    if image_path.startswith('http'):\n",
        "        response = requests.get(image_path)\n",
        "        response.raise_for_status()\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "    else:\n",
        "        img = Image.open(image_path)\n",
        "    \n",
        "    # Convertir en bytes\n",
        "    img_bytes = BytesIO()\n",
        "    img.save(img_bytes, format='PNG')\n",
        "    img_bytes.seek(0)\n",
        "    \n",
        "    # Upload vers ComfyUI\n",
        "    files = {'image': ('input.png', img_bytes, 'image/png')}\n",
        "    response = client.session.post(\n",
        "        f\"{API_BASE_URL}/upload/image\",\n",
        "        files=files\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    filename = response.json()['name']\n",
        "    print(f\"\u2705 Image upload\u00e9e: {filename}\")\n",
        "    return filename\n",
        "\n",
        "# Test upload avec image exemple\n",
        "# Note: Remplacer par votre propre image\n",
        "test_image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/481px-Cat03.jpg\"\n",
        "\n",
        "try:\n",
        "    uploaded_filename = upload_image_to_comfyui(test_image_url)\n",
        "    print(f\"\ud83d\udcc1 Fichier: {uploaded_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f Erreur upload: {e}\")\n",
        "    print(\"\ud83d\udca1 Conseil: Utiliser une image locale ou v\u00e9rifier URL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a6a3da",
      "metadata": {},
      "source": "## \ud83c\udfa8 Workflow Image-to-Image Complet\n\n### Architecture (Phase 29)\n\n**Pipeline edition avec loaders separes**:\n1. **VAELoader / CLIPLoader / UNETLoader** \u2192 Charger les 3 composants du modele\n2. **ModelSamplingAuraFlow + CFGNorm** \u2192 Configurer le modele\n3. **LoadImage** \u2192 Charger image source uploadee\n4. **VAEEncode** \u2192 Convertir pixels \u2192 latent (16 canaux)\n5. **TextEncodeQwenImageEdit** \u2192 Encoder instructions edition (encodeur natif Qwen)\n6. **ConditioningZeroOut** \u2192 Conditioning negatif vide\n7. **KSampler** (denoise partiel, cfg=1.0, scheduler=beta) \u2192 Editer latent\n8. **VAEDecode** \u2192 Reconvertir latent \u2192 pixels\n9. **SaveImage** \u2192 Sauvegarder resultat\n\n### Parametre Critique: denoise\n\n**Impact sur edition**:\n\n| denoise | Type Edition | Exemple |\n|---------|-------------|----------|\n| **0.1-0.3** | Retouche subtile | Correction couleurs, amelioration details |\n| **0.4-0.6** | Edition moderee | Style transfer, ajout elements mineurs |\n| **0.7-0.9** | Edition forte | Changement scene, reconstruction majeure |\n| **1.0** | Generation totale | Ignore quasi totalement image source |\n\n**Recommandation**: Commencer avec `denoise=0.5` et ajuster selon resultat.\n\n**Rappel Phase 29**: `cfg=1.0` (CFGNorm gere l'amplification), `scheduler=\"beta\"` (obligatoire).",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a747201c",
      "metadata": {},
      "outputs": [],
      "source": "# Workflow Image-to-Image (edition) - Architecture Phase 29\nworkflow_img2img = {\n    \"1\": {  # VAE Loader (16 canaux Qwen)\n        \"class_type\": \"VAELoader\",\n        \"inputs\": {\n            \"vae_name\": \"qwen_image_vae.safetensors\"\n        }\n    },\n    \"2\": {  # CLIP Loader\n        \"class_type\": \"CLIPLoader\",\n        \"inputs\": {\n            \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n            \"type\": \"sd3\"\n        }\n    },\n    \"3\": {  # UNET Loader\n        \"class_type\": \"UNETLoader\",\n        \"inputs\": {\n            \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n            \"weight_dtype\": \"fp8_e4m3fn\"\n        }\n    },\n    \"4\": {  # ModelSamplingAuraFlow\n        \"class_type\": \"ModelSamplingAuraFlow\",\n        \"inputs\": {\n            \"model\": [\"3\", 0],\n            \"shift\": 3.0\n        }\n    },\n    \"5\": {  # CFGNorm\n        \"class_type\": \"CFGNorm\",\n        \"inputs\": {\n            \"model\": [\"4\", 0],\n            \"strength\": 1.0\n        }\n    },\n    \"6\": {  # Load Image (image source uploadee)\n        \"class_type\": \"LoadImage\",\n        \"inputs\": {\n            \"image\": uploaded_filename  # Variable de cellule precedente\n        }\n    },\n    \"7\": {  # VAE Encode (pixels -> latent)\n        \"class_type\": \"VAEEncode\",\n        \"inputs\": {\n            \"pixels\": [\"6\", 0],  # Image source\n            \"vae\": [\"1\", 0]      # VAE Qwen 16 canaux\n        }\n    },\n    \"8\": {  # TextEncodeQwenImageEdit (prompt edition)\n        \"class_type\": \"TextEncodeQwenImageEdit\",\n        \"inputs\": {\n            \"clip\": [\"2\", 0],\n            \"prompt\": \"watercolor painting style, artistic, vibrant colors\",\n            \"vae\": [\"1\", 0]\n        }\n    },\n    \"9\": {  # ConditioningZeroOut (conditioning negatif)\n        \"class_type\": \"ConditioningZeroOut\",\n        \"inputs\": {\n            \"conditioning\": [\"8\", 0]\n        }\n    },\n    \"10\": {  # KSampler (edition avec denoise partiel)\n        \"class_type\": \"KSampler\",\n        \"inputs\": {\n            \"model\": [\"5\", 0],       # Modele apres CFGNorm\n            \"positive\": [\"8\", 0],    # Prompt positif\n            \"negative\": [\"9\", 0],    # Conditioning negatif ZeroOut\n            \"latent_image\": [\"7\", 0],  # Latent de l'image source\n            \"seed\": 42,\n            \"steps\": 25,\n            \"cfg\": 1.0,              # cfg=1.0 car CFGNorm gere l'amplification\n            \"sampler_name\": \"euler\",\n            \"scheduler\": \"beta\",     # scheduler beta obligatoire pour Qwen\n            \"denoise\": 0.5           # Edition moderee (50%)\n        }\n    },\n    \"11\": {  # VAE Decode (latent -> pixels)\n        \"class_type\": \"VAEDecode\",\n        \"inputs\": {\n            \"samples\": [\"10\", 0],\n            \"vae\": [\"1\", 0]\n        }\n    },\n    \"12\": {  # Save Image\n        \"class_type\": \"SaveImage\",\n        \"inputs\": {\n            \"images\": [\"11\", 0],\n            \"filename_prefix\": \"Qwen_Edit\"\n        }\n    }\n}\n\n# Execution (seulement si image uploadee precedemment)\ntry:\n    print(\"Lancement edition image...\")\n    result = client.execute_workflow(workflow_img2img, verbose=True)\n    \n    # Affichage avant/apres\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Image originale\n    original_img = Image.open(BytesIO(client.session.get(\n        f\"{API_BASE_URL}/view\",\n        params={\"filename\": uploaded_filename, \"type\": \"input\"}\n    ).content))\n    axes[0].imshow(original_img)\n    axes[0].set_title(\"Image Originale\", fontsize=12)\n    axes[0].axis(\"off\")\n    \n    # Image editee\n    edited_img = Image.open(BytesIO(result[\"images\"][0][\"data\"]))\n    axes[1].imshow(edited_img)\n    axes[1].set_title(f\"Image Editee (denoise=0.5)\", fontsize=12)\n    axes[1].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nEdition complete en {result['duration']:.1f}s\")\nexcept NameError:\n    print(\"Executez d'abord la cellule d'upload d'image\")\nexcept Exception as e:\n    print(f\"Erreur: {e}\")"
    },
    {
      "cell_type": "markdown",
      "id": "7e1176e6",
      "metadata": {},
      "source": [
        "## \ud83d\udd2c Exp\u00e9rimentation: Comparaison Denoise\n",
        "\n",
        "### Objectif P\u00e9dagogique\n",
        "\n",
        "Comprendre **impact du param\u00e8tre `denoise`** sur qualit\u00e9 \u00e9dition.\n",
        "\n",
        "### M\u00e9thodologie\n",
        "\n",
        "1. Workflow identique (m\u00eame prompt, seed, etc.)\n",
        "2. Variation **uniquement** du param\u00e8tre `denoise`\n",
        "3. Comparaison visuelle r\u00e9sultats\n",
        "\n",
        "### Hypoth\u00e8se\n",
        "\n",
        "**denoise faible** (0.2) \u2192 \u00c9dition subtile, image proche de l'originale\n",
        "**denoise moyen** (0.5) \u2192 Bon compromis \u00e9dition/pr\u00e9servation\n",
        "**denoise \u00e9lev\u00e9** (0.8) \u2192 \u00c9dition forte, risque divergence\n",
        "\n",
        "### Configuration Test\n",
        "\n",
        "```python\n",
        "denoise_values = [0.2, 0.5, 0.8]\n",
        "prompt = \"convert to dramatic black and white, high contrast\"\n",
        "seed = 42  # Fixe pour comparabilit\u00e9\n",
        "```"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84553629",
      "metadata": {},
      "outputs": [],
      "source": "# Comparaison denoise (denoise=0.2, 0.5, 0.8) - Architecture Phase 29\ndenoise_values = [0.2, 0.5, 0.8]\nedit_prompt = \"convert to dramatic black and white, high contrast\"\n\nresults_denoise = []\n\nfor denoise_val in denoise_values:\n    print(f\"\\nTest denoise={denoise_val}...\")\n    \n    # Creer workflow Phase 29 avec denoise specifique\n    workflow_denoise_test = {\n        \"1\": {  # VAE Loader\n            \"class_type\": \"VAELoader\",\n            \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n        },\n        \"2\": {  # CLIP Loader\n            \"class_type\": \"CLIPLoader\",\n            \"inputs\": {\n                \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n                \"type\": \"sd3\"\n            }\n        },\n        \"3\": {  # UNET Loader\n            \"class_type\": \"UNETLoader\",\n            \"inputs\": {\n                \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn\"\n            }\n        },\n        \"4\": {  # ModelSamplingAuraFlow\n            \"class_type\": \"ModelSamplingAuraFlow\",\n            \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}\n        },\n        \"5\": {  # CFGNorm\n            \"class_type\": \"CFGNorm\",\n            \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}\n        },\n        \"6\": {  # Load Image\n            \"class_type\": \"LoadImage\",\n            \"inputs\": {\"image\": uploaded_filename}\n        },\n        \"7\": {  # VAE Encode\n            \"class_type\": \"VAEEncode\",\n            \"inputs\": {\n                \"pixels\": [\"6\", 0],\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"8\": {  # TextEncodeQwenImageEdit (prompt edition)\n            \"class_type\": \"TextEncodeQwenImageEdit\",\n            \"inputs\": {\n                \"clip\": [\"2\", 0],\n                \"prompt\": edit_prompt,\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"9\": {  # ConditioningZeroOut (negatif)\n            \"class_type\": \"ConditioningZeroOut\",\n            \"inputs\": {\"conditioning\": [\"8\", 0]}\n        },\n        \"10\": {  # KSampler\n            \"class_type\": \"KSampler\",\n            \"inputs\": {\n                \"model\": [\"5\", 0],\n                \"positive\": [\"8\", 0],\n                \"negative\": [\"9\", 0],\n                \"latent_image\": [\"7\", 0],\n                \"seed\": 42,\n                \"steps\": 25,\n                \"cfg\": 1.0,              # cfg=1.0 car CFGNorm gere l'amplification\n                \"sampler_name\": \"euler\",\n                \"scheduler\": \"beta\",     # scheduler beta obligatoire\n                \"denoise\": denoise_val   # Variable testee\n            }\n        },\n        \"11\": {  # VAE Decode\n            \"class_type\": \"VAEDecode\",\n            \"inputs\": {\n                \"samples\": [\"10\", 0],\n                \"vae\": [\"1\", 0]\n            }\n        },\n        \"12\": {  # Save Image\n            \"class_type\": \"SaveImage\",\n            \"inputs\": {\n                \"images\": [\"11\", 0],\n                \"filename_prefix\": f\"Denoise_{denoise_val}\"\n            }\n        }\n    }\n    \n    try:\n        result = client.execute_workflow(workflow_denoise_test, verbose=False)\n        results_denoise.append({\n            \"denoise\": denoise_val,\n            \"result\": result\n        })\n        print(f\"Complete en {result['duration']:.1f}s\")\n    except Exception as e:\n        print(f\"Erreur: {e}\")\n        results_denoise.append({\"denoise\": denoise_val, \"error\": str(e)})\n\n# Affichage comparatif\nif len(results_denoise) == 3 and all('result' in r for r in results_denoise):\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    for i, res in enumerate(results_denoise):\n        img = Image.open(BytesIO(res[\"result\"][\"images\"][0][\"data\"]))\n        axes[i].imshow(img)\n        axes[i].set_title(f\"denoise={res['denoise']}\", fontsize=12)\n        axes[i].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nObservations:\")\n    print(\"- denoise=0.2: Edition subtile, preserve details originaux\")\n    print(\"- denoise=0.5: Equilibre edition/preservation\")\n    print(\"- denoise=0.8: Edition forte, peut diverger de l'original\")\nelse:\n    print(\"Certains tests ont echoue, verifiez les erreurs ci-dessus\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dacdbf51",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# \ud83d\uddbc\ufe0f COMPARAISON AVANT/APR\u00c8S: Side-by-Side\n",
        "# ========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "def compare_before_after(\n",
        "    original_path: str,\n",
        "    edited_path: str,\n",
        "    title_original: str = \"Image Originale\",\n",
        "    title_edited: str = \"Image \u00c9dit\u00e9e\",\n",
        "    show_metrics: bool = True\n",
        ") -> None:\n",
        "    \"\"\"Affiche comparaison side-by-side avec m\u00e9triques qualit\u00e9\n",
        "    \n",
        "    Args:\n",
        "        original_path: Chemin image originale\n",
        "        edited_path: Chemin image \u00e9dit\u00e9e\n",
        "        title_original: Titre image originale\n",
        "        title_edited: Titre image \u00e9dit\u00e9e\n",
        "        show_metrics: Afficher m\u00e9triques qualit\u00e9 (PSNR, SSIM)\n",
        "    \"\"\"\n",
        "    # Charger images\n",
        "    img_original = Image.open(original_path)\n",
        "    img_edited = Image.open(edited_path)\n",
        "    \n",
        "    # Convertir en numpy arrays\n",
        "    arr_original = np.array(img_original)\n",
        "    arr_edited = np.array(img_edited)\n",
        "    \n",
        "    # Cr\u00e9er figure avec 2 colonnes\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    \n",
        "    # Image originale\n",
        "    axes[0].imshow(arr_original)\n",
        "    axes[0].set_title(f\"{title_original}\\n{img_original.size[0]}x{img_original.size[1]}\", \n",
        "                      fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Image \u00e9dit\u00e9e\n",
        "    axes[1].imshow(arr_edited)\n",
        "    axes[1].set_title(f\"{title_edited}\\n{img_edited.size[0]}x{img_edited.size[1]}\", \n",
        "                      fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    # M\u00e9triques qualit\u00e9\n",
        "    if show_metrics and arr_original.shape == arr_edited.shape:\n",
        "        # PSNR (Peak Signal-to-Noise Ratio)\n",
        "        mse = np.mean((arr_original - arr_edited) ** 2)\n",
        "        if mse == 0:\n",
        "            psnr = float('inf')\n",
        "        else:\n",
        "            max_pixel = 255.0\n",
        "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "        \n",
        "        # Diff\u00e9rence absolue moyenne\n",
        "        mae = np.mean(np.abs(arr_original - arr_edited))\n",
        "        \n",
        "        # Afficher m\u00e9triques\n",
        "        metrics_text = f\"\ud83d\udcca M\u00e9triques:\\n\"\n",
        "        metrics_text += f\"   PSNR: {psnr:.2f} dB\\n\"\n",
        "        metrics_text += f\"   MAE: {mae:.2f}\\n\"\n",
        "        metrics_text += f\"   Pixels modifi\u00e9s: {np.sum(arr_original != arr_edited):,}\"\n",
        "        \n",
        "        fig.text(0.5, 0.02, metrics_text, ha='center', fontsize=12, \n",
        "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Interpr\u00e9tation PSNR\n",
        "    if show_metrics and arr_original.shape == arr_edited.shape:\n",
        "        print(\"\\n\ud83d\udcc8 Interpr\u00e9tation PSNR:\")\n",
        "        if psnr > 40:\n",
        "            print(\"   \u2705 Excellente qualit\u00e9 (PSNR > 40 dB) - Changements subtils\")\n",
        "        elif psnr > 30:\n",
        "            print(\"   \u2705 Bonne qualit\u00e9 (PSNR 30-40 dB) - Changements visibles mais contr\u00f4l\u00e9s\")\n",
        "        elif psnr > 20:\n",
        "            print(\"   \u26a0\ufe0f  Qualit\u00e9 acceptable (PSNR 20-30 dB) - Changements significatifs\")\n",
        "        else:\n",
        "            print(\"   \u26a0\ufe0f  Qualit\u00e9 faible (PSNR < 20 dB) - Changements majeurs\")\n",
        "\n",
        "def create_difference_map(\n",
        "    original_path: str,\n",
        "    edited_path: str,\n",
        "    amplification: float = 5.0\n",
        ") -> None:\n",
        "    \"\"\"Cr\u00e9e une carte visuelle des diff\u00e9rences entre 2 images\n",
        "    \n",
        "    Args:\n",
        "        original_path: Chemin image originale\n",
        "        edited_path: Chemin image \u00e9dit\u00e9e\n",
        "        amplification: Facteur amplification diff\u00e9rences pour visibilit\u00e9\n",
        "    \"\"\"\n",
        "    img_original = np.array(Image.open(original_path))\n",
        "    img_edited = np.array(Image.open(edited_path))\n",
        "    \n",
        "    if img_original.shape != img_edited.shape:\n",
        "        print(\"\u274c Images de tailles diff\u00e9rentes, impossible de comparer\")\n",
        "        return\n",
        "    \n",
        "    # Calculer diff\u00e9rence absolue\n",
        "    diff = np.abs(img_original.astype(float) - img_edited.astype(float))\n",
        "    \n",
        "    # Amplifier pour visibilit\u00e9\n",
        "    diff_amplified = np.clip(diff * amplification, 0, 255).astype(np.uint8)\n",
        "    \n",
        "    # Afficher\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "    \n",
        "    axes[0].imshow(img_original)\n",
        "    axes[0].set_title(\"Originale\", fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    axes[1].imshow(img_edited)\n",
        "    axes[1].set_title(\"\u00c9dit\u00e9e\", fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    axes[2].imshow(diff_amplified)\n",
        "    axes[2].set_title(f\"Carte Diff\u00e9rences (\u00d7{amplification})\", fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Statistiques diff\u00e9rences\n",
        "    print(f\"\\n\ud83d\udcca Statistiques diff\u00e9rences:\")\n",
        "    print(f\"   Moyenne: {np.mean(diff):.2f}\")\n",
        "    print(f\"   Max: {np.max(diff):.2f}\")\n",
        "    print(f\"   Pixels modifi\u00e9s (>5): {np.sum(diff > 5):,} ({100*np.sum(diff > 5)/diff.size:.2f}%)\")\n",
        "\n",
        "# ========================================\n",
        "# EXEMPLE D'UTILISATION\n",
        "# ========================================\n",
        "\n",
        "# Cas d'usage: Comparer original vs \u00e9dition Qwen\n",
        "# compare_before_after(\n",
        "#     original_path=\"cat_original.png\",\n",
        "#     edited_path=\"cat_with_sunglasses.png\",\n",
        "#     title_original=\"Chat Original\",\n",
        "#     title_edited=\"Chat avec Lunettes (Qwen Edit)\",\n",
        "#     show_metrics=True\n",
        "# )\n",
        "\n",
        "# Cas d'usage: Carte diff\u00e9rences pour analyse d\u00e9taill\u00e9e\n",
        "# create_difference_map(\n",
        "#     original_path=\"cat_original.png\",\n",
        "#     edited_path=\"cat_with_sunglasses.png\",\n",
        "#     amplification=10.0\n",
        "# )\n",
        "\n",
        "print(\"\u2705 Fonctions comparaison avant/apr\u00e8s d\u00e9finies\")\n",
        "print(\"   - compare_before_after(): Affichage side-by-side + m\u00e9triques\")\n",
        "print(\"   - create_difference_map(): Carte visuelle diff\u00e9rences\")\n",
        "print(\"\\n\ud83d\udca1 D\u00e9commentez les exemples ci-dessus pour tester avec vos images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6db2fc7",
      "metadata": {},
      "source": [
        "## \u2699\ufe0f Bonnes Pratiques ComfyUI\n",
        "\n",
        "### 1. Gestion des Erreurs Courantes\n",
        "\n",
        "#### Timeout (>120s)\n",
        "**Cause**: GPU surcharg\u00e9, workflow trop complexe\n",
        "**Solution**:\n",
        "```python\n",
        "client.execute_workflow(workflow, max_wait=300)  # Augmenter timeout\n",
        "```\n",
        "\n",
        "#### CUDA Out of Memory\n",
        "**Cause**: R\u00e9solution trop \u00e9lev\u00e9e (>768x768)\n",
        "**Solution**:\n",
        "- R\u00e9duire r\u00e9solution (512x512 optimal)\n",
        "- Diminuer `batch_size`\n",
        "- Simplifier workflow\n",
        "\n",
        "#### Node Not Found\n",
        "**Cause**: `class_type` invalide ou custom node manquant\n",
        "**Solution**: V\u00e9rifier documentation custom nodes Qwen\n",
        "\n",
        "### 2. Optimisation Performance\n",
        "\n",
        "| Param\u00e8tre | Impact Performance | Recommandation |\n",
        "|-----------|-------------------|----------------|\n",
        "| **steps** | \u2191 steps = \u2191 temps | 20-25 optimal |\n",
        "| **resolution** | \u2191 r\u00e9solution = \u2191\u2191 VRAM | 512x512 par d\u00e9faut |\n",
        "| **denoise** | Minimal | N/A |\n",
        "| **cfg** | Minimal | 7-8 optimal |\n",
        "\n",
        "### 3. Workflow Reproductible\n",
        "\n",
        "**Toujours fixer seed pour debugging**:\n",
        "```python\n",
        "\"seed\": 42  # M\u00eame r\u00e9sultat \u00e0 chaque ex\u00e9cution\n",
        "```\n",
        "\n",
        "**Seed al\u00e9atoire pour vari\u00e9t\u00e9**:\n",
        "```python\n",
        "\"seed\": int(time.time())  # Diff\u00e9rent \u00e0 chaque fois\n",
        "```\n",
        "\n",
        "### 4. Logs et Debugging\n",
        "\n",
        "**Activer verbose**:\n",
        "```python\n",
        "result = client.execute_workflow(workflow, verbose=True)\n",
        "```\n",
        "\n",
        "**Inspecter outputs**:\n",
        "```python\n",
        "print(json.dumps(result[\"outputs\"], indent=2))\n",
        "```"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7af4b97",
      "metadata": {},
      "outputs": [],
      "source": "# ========================================\n# EXERCICE PRATIQUE (Architecture Phase 29)\n# ========================================\n# Creez votre propre workflow d'edition d'image\n\n# OBJECTIF:\n# Modifier une image en ajoutant un effet specifique via Qwen VLM\n\n# INSTRUCTIONS:\n# 1. Choisissez une image de test (ou utilisez celle generee precedemment)\n# 2. Creez un workflow image-to-image avec un prompt creatif\n# 3. Testez differentes valeurs de denoise (0.3, 0.5, 0.7)\n# 4. Comparez visuellement les resultats\n\n# TODO: Completez le workflow ci-dessous\nworkflow_exercice = {\n    # Loaders separes (Phase 29 - ne pas modifier)\n    \"1\": {  # VAE Loader\n        \"class_type\": \"VAELoader\",\n        \"inputs\": {\"vae_name\": \"qwen_image_vae.safetensors\"}\n    },\n    \"2\": {  # CLIP Loader\n        \"class_type\": \"CLIPLoader\",\n        \"inputs\": {\n            \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n            \"type\": \"sd3\"\n        }\n    },\n    \"3\": {  # UNET Loader\n        \"class_type\": \"UNETLoader\",\n        \"inputs\": {\n            \"unet_name\": \"qwen_image_edit_2509_fp8_e4m3fn.safetensors\",\n            \"weight_dtype\": \"fp8_e4m3fn\"\n        }\n    },\n    \"4\": {  # ModelSamplingAuraFlow\n        \"class_type\": \"ModelSamplingAuraFlow\",\n        \"inputs\": {\"model\": [\"3\", 0], \"shift\": 3.0}\n    },\n    \"5\": {  # CFGNorm\n        \"class_type\": \"CFGNorm\",\n        \"inputs\": {\"model\": [\"4\", 0], \"strength\": 1.0}\n    },\n    # Image source\n    \"6\": {  # Load Image\n        \"class_type\": \"LoadImage\",\n        \"inputs\": {\n            # TODO: Ajoutez le nom de votre image source\n            \"image\": \"___VOTRE_IMAGE_ICI___\"\n        }\n    },\n    \"7\": {  # VAE Encode (pixels -> latent)\n        \"class_type\": \"VAEEncode\",\n        \"inputs\": {\n            \"pixels\": [\"6\", 0],\n            \"vae\": [\"1\", 0]\n        }\n    },\n    # Encodage texte natif Qwen\n    \"8\": {  # TextEncodeQwenImageEdit\n        \"class_type\": \"TextEncodeQwenImageEdit\",\n        \"inputs\": {\n            \"clip\": [\"2\", 0],\n            # TODO: Ecrivez un prompt creatif (ex: \"convert to watercolor painting\")\n            \"prompt\": \"___VOTRE_PROMPT_ICI___\",\n            \"vae\": [\"1\", 0]\n        }\n    },\n    \"9\": {  # ConditioningZeroOut (conditioning negatif)\n        \"class_type\": \"ConditioningZeroOut\",\n        \"inputs\": {\"conditioning\": [\"8\", 0]}\n    },\n    # Sampler\n    \"10\": {  # KSampler\n        \"class_type\": \"KSampler\",\n        \"inputs\": {\n            \"seed\": 42,\n            # TODO: Testez differentes valeurs de denoise (0.3 a 0.7)\n            \"denoise\": 0.5,\n            \"steps\": 20,\n            \"cfg\": 1.0,              # cfg=1.0 obligatoire (CFGNorm gere l'amplification)\n            \"sampler_name\": \"euler\",\n            \"scheduler\": \"beta\",     # scheduler beta obligatoire pour Qwen\n            \"model\": [\"5\", 0],       # Modele apres CFGNorm\n            \"positive\": [\"8\", 0],    # Prompt positif\n            \"negative\": [\"9\", 0],    # Conditioning negatif ZeroOut\n            \"latent_image\": [\"7\", 0]  # Latent de l'image source\n        }\n    },\n    # Decodage et sauvegarde\n    \"11\": {  # VAE Decode\n        \"class_type\": \"VAEDecode\",\n        \"inputs\": {\n            \"samples\": [\"10\", 0],\n            \"vae\": [\"1\", 0]\n        }\n    },\n    \"12\": {  # Save Image\n        \"class_type\": \"SaveImage\",\n        \"inputs\": {\n            \"filename_prefix\": \"exercice_qwen\",\n            \"images\": [\"11\", 0]\n        }\n    }\n}\n\n# AIDE:\n# - Pour l'image source: utilisez le nom d'une image uploadee (ex: \"cat_512x512.png\")\n# - Pour le prompt: soyez creatif! (ex: \"make it look like a Van Gogh painting\")\n# - Pour denoise: commencez par 0.5, puis testez 0.3 et 0.7\n# - cfg doit rester a 1.0 (CFGNorm gere l'amplification)\n# - scheduler doit rester \"beta\" (obligatoire pour Qwen)\n\n# BONUS:\n# Creez une fonction pour tester plusieurs prompts automatiquement\ndef tester_prompts_exercice(prompts_list, image_source, denoise=0.5):\n    \"\"\"\n    Teste plusieurs prompts sur la meme image (Phase 29)\n    \n    Args:\n        prompts_list: Liste de prompts a tester\n        image_source: Nom de l'image source\n        denoise: Valeur denoise (defaut 0.5)\n    \"\"\"\n    results = []\n    \n    for i, prompt in enumerate(prompts_list):\n        print(f\"\\nTest {i+1}/{len(prompts_list)}: {prompt}\")\n        \n        # Utilise create_simple_edit_workflow defini precedemment (Phase 29)\n        workflow_test = create_simple_edit_workflow(\n            image_name=image_source,\n            edit_prompt=prompt,\n            denoise=denoise\n        )\n        \n        # Execution workflow\n        # result = client.execute_workflow(workflow_test)\n        # results.append(result)\n    \n    return results\n\n# TESTEZ VOTRE CODE:\n# prompts_test = [\n#     \"convert to watercolor painting\",\n#     \"add dramatic sunset lighting\",\n#     \"make it look like a pencil sketch\"\n# ]\n# resultats = tester_prompts_exercice(prompts_test, \"cat_512x512.png\")\n\nprint(\"\\nExercice pret! Completez les TODO et executez la cellule.\")\nprint(\"Conseil: Commencez simple, puis ajoutez de la complexite progressivement.\")\nprint(\"\\nRappel Phase 29: cfg=1.0, scheduler='beta', TextEncodeQwenImageEdit, ConditioningZeroOut\")"
    },
    {
      "cell_type": "markdown",
      "id": "0a694ae8",
      "metadata": {},
      "source": [
        "## \ud83d\udcda Ressources Compl\u00e9mentaires\n",
        "\n",
        "### Documentation Officielle\n",
        "\n",
        "#### ComfyUI\n",
        "- **GitHub**: [https://github.com/comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)\n",
        "- **Documentation API**: [https://github.com/comfyanonymous/ComfyUI/wiki/API](https://github.com/comfyanonymous/ComfyUI/wiki/API)\n",
        "- **Custom Nodes**: [https://github.com/ltdrdata/ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager)\n",
        "\n",
        "#### Qwen Vision-Language Model\n",
        "- **Paper Officiel**: *Qwen-VL: A Versatile Vision-Language Model*\n",
        "- **Mod\u00e8le Hugging Face**: [https://huggingface.co/Qwen/Qwen-VL](https://huggingface.co/Qwen/Qwen-VL)\n",
        "- **Documentation Technique**: [https://qwenlm.github.io/](https://qwenlm.github.io/)\n",
        "\n",
        "### Workflows Avanc\u00e9s\n",
        "\n",
        "#### Workflows ComfyUI Communautaires\n",
        "- **ComfyUI Workflows Gallery**: [https://comfyworkflows.com/](https://comfyworkflows.com/)\n",
        "- **CivitAI ComfyUI Section**: [https://civitai.com/tag/comfyui](https://civitai.com/tag/comfyui)\n",
        "\n",
        "#### Custom Nodes Recommand\u00e9s\n",
        "- **ComfyUI-Impact-Pack**: Outils post-processing avanc\u00e9s\n",
        "- **ComfyUI-AnimateDiff**: Animations et vid\u00e9os\n",
        "- **ComfyUI-ControlNet**: Contr\u00f4le spatial pr\u00e9cis\n",
        "\n",
        "### Tutoriels et Guides\n",
        "\n",
        "#### D\u00e9butants\n",
        "1. **ComfyUI Basics** (YouTube): Introduction compl\u00e8te workflows\n",
        "2. **Qwen-VL Quick Start**: Guide rapide \u00e9dition images\n",
        "3. **JSON Workflows 101**: Comprendre structure workflows\n",
        "\n",
        "#### Interm\u00e9diaires\n",
        "1. **Advanced Prompting Techniques**: Optimisation prompts Qwen\n",
        "2. **Workflow Optimization**: R\u00e9duire temps g\u00e9n\u00e9ration\n",
        "3. **Multi-Step Workflows**: Cha\u00eenage nodes complexes\n",
        "\n",
        "#### Avanc\u00e9s\n",
        "1. **Custom Node Development**: Cr\u00e9er vos propres nodes\n",
        "2. **API Integration**: Int\u00e9grer ComfyUI dans applications\n",
        "3. **Batch Processing**: Automatisation workflows\n",
        "\n",
        "### Communaut\u00e9 et Support\n",
        "\n",
        "- **Discord ComfyUI**: [https://discord.gg/comfyui](https://discord.gg/comfyui)\n",
        "- **Reddit r/comfyui**: Forum communautaire\n",
        "- **GitHub Discussions**: Questions techniques\n",
        "\n",
        "### Ressources MyIA.io\n",
        "\n",
        "- **Guide APIs \u00c9tudiants**: [`GUIDE-APIS-ETUDIANTS.md`](../../../../docs/suivis/genai-image/GUIDE-APIS-ETUDIANTS.md)\n",
        "- **Workflows Qwen Phase 12C**: [`2025-10-16_12C_architectures-5-workflows-qwen.md`](../../../../docs/genai-suivis/2025-10-16_12C_architectures-5-workflows-qwen.md)\n",
        "- **Notebook Forge SD-XL**: [`01-4-Forge-SD-XL-Turbo.ipynb`](01-4-Forge-SD-XL-Turbo.ipynb) (API REST similaire)\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udf93 Prochaines \u00c9tapes Apprentissage\n",
        "\n",
        "1. **Ma\u00eetriser les bases**: Reproduire tous les exemples de ce notebook\n",
        "2. **Exp\u00e9rimenter**: Modifier workflows, tester nouveaux prompts\n",
        "3. **Explorer workflows avanc\u00e9s**: ControlNet, AnimateDiff, Multi-Model\n",
        "4. **Cr\u00e9er projets personnels**: Application web int\u00e9grant API Qwen\n",
        "5. **Contribuer communaut\u00e9**: Partager vos workflows innovants\n",
        "\n",
        "---\n",
        "\n",
        "**\u2705 Notebook termin\u00e9! Bon apprentissage avec Qwen Image Edit! \ud83d\ude80**"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mcp-jupyter-py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}