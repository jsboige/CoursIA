# Tutorial Complet : GPT-5 Multimodal - Analyse d'Images pour CoursIA

## Table des Mati√®res
1. [Introduction](#introduction)
2. [Configuration OpenRouter](#configuration-openrouter)
3. [Analyse d'Images et Description](#analyse-images)
4. [Conversations Multimodales Avanc√©es](#conversations-multimodales)
5. [Templates Cas P√©dagogiques](#templates-pedagogiques)
6. [Alt Text et Accessibilit√©](#alt-text-accessibilite)
7. [Int√©gration avec DALL-E](#integration-dalle)
8. [Performance et Cost Management](#performance-cost)

---

## Introduction

GPT-5 repr√©sente une avanc√©e majeure dans l'analyse multimodale, combinant compr√©hension textuelle et visuelle. Ce guide explore son utilisation via OpenRouter pour cr√©er des exp√©riences p√©dagogiques enrichies dans CoursIA.

### Capacit√©s Cl√©s GPT-5
- **Vision avanc√©e** : Analyse d√©taill√©e d'images complexes
- **Contexte √©tendu** : 200K+ tokens pour conversations longues
- **Raisonnement spatial** : Compr√©hension g√©om√©trique et positionnelle
- **Extraction structur√©e** : Donn√©es depuis graphiques, diagrammes
- **Multi-langue** : Support fran√ßais natif
- **Pr√©cision p√©dagogique** : Descriptions adapt√©es au niveau scolaire

### Cas d'Usage √âducatifs
- Analyse documents historiques
- Extraction donn√©es graphiques scientifiques
- Description illustrations pour accessibilit√©
- √âvaluation travaux visuels √©tudiants
- G√©n√©ration questions depuis images
- Correction exercices visuels

---

## Configuration OpenRouter

### Setup Initial

```python
import os
import base64
from openai import OpenAI
from PIL import Image
import requests
from io import BytesIO

# Configuration OpenRouter pour GPT-5
client = OpenAI(
    api_key=os.getenv("OPENROUTER_API_KEY"),
    base_url="https://openrouter.ai/api/v1"
)

# Mod√®le GPT-5
GPT5_MODEL = "openai/gpt-5-preview"

def verify_openrouter_config():
    """V√©rifie configuration OpenRouter"""
    try:
        response = client.chat.completions.create(
            model=GPT5_MODEL,
            messages=[{"role": "user", "content": "Test connection"}],
            max_tokens=10
        )
        print("‚úÖ OpenRouter configur√© correctement")
        return True
    except Exception as e:
        print(f"‚ùå Erreur configuration: {e}")
        return False
```

### Pr√©paration Images

```python
def encode_image_base64(image_path):
    """
    Encode image en base64 pour API
    
    Args:
        image_path: Chemin vers l'image
    
    Returns:
        String base64 de l'image
    """
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def encode_image_url(image_url):
    """
    T√©l√©charge et encode image depuis URL
    
    Args:
        image_url: URL de l'image
    
    Returns:
        String base64 de l'image
    """
    response = requests.get(image_url)
    return base64.b64encode(response.content).decode('utf-8')

def resize_image_for_api(image_path, max_size=(2048, 2048)):
    """
    Redimensionne image pour optimiser co√ªts API
    
    Args:
        image_path: Chemin image
        max_size: Dimensions maximales
    """
    img = Image.open(image_path)
    img.thumbnail(max_size, Image.Resampling.LANCZOS)
    
    # Sauvegarde temporaire
    temp_path = f"temp_resized_{os.path.basename(image_path)}"
    img.save(temp_path, optimize=True)
    
    return temp_path
```

### Format Messages Multimodaux

```python
def create_image_message(image_path, prompt, detail="auto"):
    """
    Cr√©e message avec image pour GPT-5
    
    Args:
        image_path: Chemin image locale
        prompt: Question/instruction textuelle
        detail: 'low' (rapide/√©conomique), 'high' (d√©taill√©), 'auto'
    
    Returns:
        Format message OpenAI
    """
    base64_image = encode_image_base64(image_path)
    
    return {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": prompt
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}",
                    "detail": detail
                }
            }
        ]
    }
```

---

## Analyse d'Images et Description

### Analyse de Base

```python
def analyze_image(image_path, prompt="D√©cris cette image en d√©tail"):
    """
    Analyse basique d'une image avec GPT-5
    
    Args:
        image_path: Chemin image
        prompt: Instructions d'analyse
    
    Returns:
        Description textuelle
    """
    try:
        message = create_image_message(image_path, prompt)
        
        response = client.chat.completions.create(
            model=GPT5_MODEL,
            messages=[message],
            max_tokens=500,
            temperature=0.7
        )
        
        description = response.choices[0].message.content
        print(f"‚úÖ Analyse compl√©t√©e ({response.usage.total_tokens} tokens)")
        
        return {
            'description': description,
            'tokens_used': response.usage.total_tokens,
            'model': GPT5_MODEL
        }
        
    except Exception as e:
        print(f"‚ùå Erreur analyse: {e}")
        return None
```

### Analyse Structur√©e

```python
def analyze_image_structured(image_path, analysis_type="educational"):
    """
    Analyse structur√©e selon type p√©dagogique
    
    Args:
        image_path: Chemin image
        analysis_type: Type d'analyse (educational, scientific, historical)
    
    Returns:
        Analyse structur√©e par sections
    """
    prompts = {
        "educational": """Analyse cette image de mani√®re p√©dagogique:

1. DESCRIPTION G√âN√âRALE (2-3 phrases)
2. √âL√âMENTS CL√âS (liste bullet points)
3. CONCEPTS P√âDAGOGIQUES (notions enseignables)
4. NIVEAU SCOLAIRE RECOMMAND√â
5. QUESTIONS P√âDAGOGIQUES (3-5 questions)

Format: Markdown structur√©, fran√ßais clair.""",

        "scientific": """Analyse scientifique de cette image:

1. TYPE DE DOCUMENT (diagramme, graphique, photo, etc.)
2. DONN√âES VISUELLES (mesures, valeurs, √©chelles)
3. CONCEPTS SCIENTIFIQUES (th√©ories illustr√©es)
4. INTERPR√âTATION (ce que montre l'image)
5. APPLICATIONS P√âDAGOGIQUES

Format: Pr√©cis et technique.""",

        "historical": """Analyse historique de cette image:

1. P√âRIODE HISTORIQUE (estimation)
2. CONTEXTE (√©v√©nement, lieu, personnages)
3. √âL√âMENTS HISTORIQUES (v√™tements, architecture, objets)
4. SIGNIFICATION HISTORIQUE
5. LIENS PROGRAMME SCOLAIRE

Format: Contextualis√© et p√©dagogique.""",

        "artistic": """Analyse artistique de cette image:

1. STYLE ARTISTIQUE (mouvement, technique)
2. COMPOSITION (√©l√©ments visuels, couleurs)
3. TECHNIQUE (m√©dium, m√©thode)
4. INTERPR√âTATION ARTISTIQUE
5. EXPLOITATION P√âDAGOGIQUE

Format: Descriptif et inspirant."""
    }
    
    prompt = prompts.get(analysis_type, prompts["educational"])
    return analyze_image(image_path, prompt)
```

### Extraction d'Informations Sp√©cifiques

```python
def extract_text_from_image(image_path):
    """Extrait texte visible dans image (OCR multimodal)"""
    prompt = """Identifie et transcris TOUT le texte visible dans cette image.

Format:
- Liste chaque √©l√©ment textuel
- Pr√©serve formatage si pertinent
- Indique position approximative
- Note langues diff√©rentes"""
    
    return analyze_image(image_path, prompt)

def extract_data_from_chart(image_path):
    """Extrait donn√©es depuis graphique ou tableau"""
    prompt = """Extrait les donn√©es de ce graphique/tableau:

1. TYPE DE GRAPHIQUE/TABLEAU
2. AXES/COLONNES (labels et unit√©s)
3. DONN√âES (valeurs num√©riques)
4. L√âGENDE (si pr√©sente)
5. TITRE/CAPTION

Format: Structure JSON ou tableau Markdown."""
    
    return analyze_image(image_path, prompt)

def identify_objects_and_concepts(image_path, subject_area="general"):
    """Identifie objets et concepts pertinents par domaine"""
    prompts = {
        "sciences": "Identifie tous les √©l√©ments scientifiques (organismes, appareils, ph√©nom√®nes) avec nomenclature pr√©cise.",
        "math√©matiques": "Identifie figures g√©om√©triques, symboles math√©matiques, formules, mesures visibles.",
        "histoire": "Identifie √©l√©ments historiques (personnages, lieux, objets d'√©poque, symboles).",
        "g√©ographie": "Identifie √©l√©ments g√©ographiques (relief, cours d'eau, v√©g√©tation, am√©nagements).",
        "g√©n√©ral": "Identifie et cat√©gorise tous les objets, personnes, lieux, concepts visibles."
    }
    
    prompt = prompts.get(subject_area, prompts["g√©n√©ral"])
    return analyze_image(image_path, prompt)
```

---

## Conversations Multimodales Avanc√©es

### Dialogue Contextuel

```python
class MultimodalConversation:
    """Gestion conversations multimodales avec contexte"""
    
    def __init__(self, subject="g√©n√©ral"):
        self.subject = subject
        self.conversation_history = []
        self.images_analyzed = []
    
    def add_image(self, image_path, initial_prompt):
        """Ajoute image √† la conversation"""
        message = create_image_message(image_path, initial_prompt)
        self.conversation_history.append(message)
        self.images_analyzed.append(image_path)
        
        # R√©ponse initiale
        response = self._get_response()
        return response
    
    def ask_followup(self, question):
        """Pose question de suivi sur images pr√©c√©dentes"""
        self.conversation_history.append({
            "role": "user",
            "content": question
        })
        
        response = self._get_response()
        return response
    
    def compare_images(self, image_path_1, image_path_2, comparison_prompt):
        """Compare deux images dans le contexte"""
        # Image 1
        msg1 = create_image_message(
            image_path_1, 
            "Image 1 pour comparaison:"
        )
        
        # Image 2
        msg2 = create_image_message(
            image_path_2,
            "Image 2 pour comparaison:"
        )
        
        # Prompt de comparaison
        comparison_msg = {
            "role": "user",
            "content": f"Maintenant compare ces deux images: {comparison_prompt}"
        }
        
        self.conversation_history.extend([msg1, msg2, comparison_msg])
        response = self._get_response()
        
        return response
    
    def _get_response(self):
        """Obtient r√©ponse GPT-5 avec historique"""
        try:
            response = client.chat.completions.create(
                model=GPT5_MODEL,
                messages=self.conversation_history,
                max_tokens=1000,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            
            # Ajoute √† l'historique
            self.conversation_history.append({
                "role": "assistant",
                "content": assistant_message
            })
            
            return {
                'response': assistant_message,
                'tokens': response.usage.total_tokens
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def get_summary(self):
        """R√©sum√© de la conversation"""
        summary_prompt = f"""R√©sume notre conversation sur {len(self.images_analyzed)} image(s):

1. Images analys√©es
2. Th√®mes principaux
3. Conclusions cl√©s
4. Points p√©dagogiques importants

Format: Markdown structur√©."""
        
        return self.ask_followup(summary_prompt)
```

**Exemple d'utilisation** :
```python
# Conversation p√©dagogique
conv = MultimodalConversation(subject="biologie")

# Analyse initiale
result1 = conv.add_image(
    "cellule_vegetale.png",
    "D√©cris cette cellule v√©g√©tale en d√©tail pour des √©l√®ves de coll√®ge"
)
print(result1['response'])

# Questions de suivi
result2 = conv.ask_followup(
    "Quelles diff√©rences principales avec une cellule animale?"
)

result3 = conv.ask_followup(
    "G√©n√®re 3 questions d'√©valuation sur cette image"
)

# R√©sum√©
summary = conv.get_summary()
```

### Analyse Comparative Multi-Images

```python
def compare_multiple_images(image_paths, comparison_criteria):
    """
    Compare plusieurs images selon crit√®res sp√©cifiques
    
    Args:
        image_paths: Liste de chemins d'images
        comparison_criteria: Crit√®res de comparaison
    
    Returns:
        Analyse comparative structur√©e
    """
    messages = []
    
    # Ajout de chaque image
    for i, path in enumerate(image_paths):
        messages.append(
            create_image_message(
                path,
                f"Image {i+1}/{len(image_paths)} pour analyse comparative:"
            )
        )
    
    # Prompt de comparaison
    comparison_prompt = f"""Compare ces {len(image_paths)} images selon:

{comparison_criteria}

Structure ta r√©ponse:
1. TABLEAU COMPARATIF (markdown)
2. SIMILITUDES
3. DIFF√âRENCES MAJEURES
4. ANALYSE P√âDAGOGIQUE
5. APPLICATIONS EN CLASSE

Format: Clair et structur√©."""
    
    messages.append({
        "role": "user",
        "content": comparison_prompt
    })
    
    # G√©n√©ration r√©ponse
    response = client.chat.completions.create(
        model=GPT5_MODEL,
        messages=messages,
        max_tokens=1500,
        temperature=0.7
    )
    
    return response.choices[0].message.content
```

---

## Templates Cas P√©dagogiques

### Sciences Naturelles

```python
SCIENCE_TEMPLATES = {
    "anatomie": {
        "prompt": """Analyse anatomique p√©dagogique:

1. Identification structure/organe
2. Composants principaux (avec annotations)
3. Fonction biologique
4. Relations avec autres syst√®mes
5. Niveau de complexit√© (primaire/coll√®ge/lyc√©e)
6. Exercices sugg√©r√©s (3 questions progressives)""",
        
        "followup_questions": [
            "Quelles exp√©riences pratiques pour illustrer cette structure?",
            "Quelles erreurs communes des √©l√®ves sur ce sujet?",
            "Comment simplifier pour √©l√®ves en difficult√©?"
        ]
    },
    
    "exp√©rience": {
        "prompt": """Analyse d'exp√©rience scientifique:

1. Type d'exp√©rience
2. Mat√©riel visible
3. Protocole probable
4. Ph√©nom√®ne observ√©
5. Concepts scientifiques illustr√©s
6. Pr√©cautions s√©curit√©
7. Adaptations p√©dagogiques par niveau""",
        
        "safety_check": "Identifie tous risques potentiels pour classe"
    }
}

def analyze_science_image(image_path, template_type="anatomie"):
    """Analyse image scientifique avec template p√©dagogique"""
    template = SCIENCE_TEMPLATES.get(template_type)
    
    result = analyze_image(image_path, template['prompt'])
    
    # Questions compl√©mentaires optionnelles
    if 'followup_questions' in template:
        conv = MultimodalConversation(subject="sciences")
        conv.add_image(image_path, template['prompt'])
        
        followups = []
        for q in template['followup_questions']:
            followups.append(conv.ask_followup(q))
        
        result['followup_analysis'] = followups
    
    return result
```

### Histoire et Documents

```python
HISTORY_TEMPLATES = {
    "document_historique": {
        "prompt": """Analyse historique p√©dagogique:

1. Type de document (photo, tableau, affiche, etc.)
2. Datation estim√©e (indices visuels)
3. Contexte historique (√©v√©nement, p√©riode)
4. Personnages/lieux identifiables
5. Signification historique
6. Biais ou propagande potentiels
7. Exploitation p√©dagogique:
   - Niveau scolaire
   - Th√®mes programme
   - Questions analyse documentaire (3-5)
   - Activit√©s sugg√©r√©es""",
        
        "critical_thinking": "Analyse critique: Que r√©v√®le/cache ce document? Quel point de vue repr√©sente-t-il?"
    },
    
    "chronologie": {
        "prompt": """Analyse chronologique:

1. √âl√©ments datables (v√™tements, technologie, architecture)
2. Estimation p√©riode historique
3. √âvolution visible (avant/apr√®s si comparaison)
4. Marqueurs temporels cl√©s
5. Int√©gration frise chronologique
6. Exercices rep√©rage temporel"""
    }
}

def analyze_historical_document(image_path, template_type="document_historique"):
    """Analyse document historique avec approche critique"""
    template = HISTORY_TEMPLATES.get(template_type)
    
    # Analyse principale
    result = analyze_image(image_path, template['prompt'])
    
    # Analyse critique optionnelle
    if 'critical_thinking' in template:
        conv = MultimodalConversation(subject="histoire")
        conv.add_image(image_path, template['prompt'])
        critical = conv.ask_followup(template['critical_thinking'])
        result['critical_analysis'] = critical
    
    return result
```

### Math√©matiques et G√©om√©trie

```python
MATH_TEMPLATES = {
    "g√©om√©trie": {
        "prompt": """Analyse g√©om√©trique:

1. Figures identifi√©es (noms pr√©cis)
2. Propri√©t√©s g√©om√©triques visibles
3. Mesures/angles not√©s
4. Relations spatiales (parall√®les, perpendiculaires, etc.)
5. Th√©or√®mes applicables
6. Niveau difficult√© (√©cole/coll√®ge/lyc√©e)
7. Exercices g√©n√©rables (3 progressifs)""",
        
        "problem_generation": "Cr√©e 3 probl√®mes de difficult√© croissante bas√©s sur cette figure"
    },
    
    "graphique": {
        "prompt": """Analyse graphique math√©matique:

1. Type de graphique (fonction, statistique, etc.)
2. Donn√©es/valeurs extractibles
3. Domaine et image (si fonction)
4. Propri√©t√©s math√©matiques (croissance, extrema, etc.)
5. Interpr√©tation math√©matique
6. Questions exploitation (5 niveaux progressifs)"""
    }
}

def analyze_math_diagram(image_path, template_type="g√©om√©trie"):
    """Analyse diagramme math√©matique avec g√©n√©ration d'exercices"""
    template = MATH_TEMPLATES.get(template_type)
    
    result = analyze_image(image_path, template['prompt'])
    
    # G√©n√©ration probl√®mes optionnelle
    if 'problem_generation' in template:
        conv = MultimodalConversation(subject="math√©matiques")
        conv.add_image(image_path, template['prompt'])
        problems = conv.ask_followup(template['problem_generation'])
        result['generated_problems'] = problems
    
    return result
```

---

## Alt Text et Accessibilit√©

### G√©n√©ration Alt Text

```python
def generate_alt_text(image_path, context="educational", max_length=125):
    """
    G√©n√®re alt text accessible selon WCAG 2.1
    
    Args:
        image_path: Chemin image
        context: Contexte d'utilisation
        max_length: Longueur maximale (WCAG recommande <125 char)
    
    Returns:
        Alt text optimis√©
    """
    prompts = {
        "educational": f"""G√©n√®re alt text pour cette image p√©dagogique (max {max_length} caract√®res):

R√®gles WCAG 2.1:
- D√©cris contenu essentiel
- Contexte p√©dagogique clair
- Pas "image de..." (implicite)
- Concis mais informatif
- Fran√ßais correct

Format: Texte brut, une phrase.""",

        "decorative": "Cette image est-elle purement d√©corative? Si oui, r√©ponds 'DECORATIVE'. Sinon, g√©n√®re alt text court.",
        
        "complex": f"""Alt text pour image complexe (diagramme/graphique):

Court alt text ({max_length} char max): [description succincte]
Description longue (s√©par√©e): [description d√©taill√©e pour longdesc]"""
    }
    
    result = analyze_image(image_path, prompts.get(context, prompts["educational"]))
    
    # Validation longueur
    alt_text = result['description'].strip()
    if len(alt_text) > max_length:
        # Raccourcir si n√©cessaire
        truncate_prompt = f"Raccourcis ce alt text √† maximum {max_length} caract√®res en gardant l'essentiel: {alt_text}"
        alt_text = analyze_image(image_path, truncate_prompt)['description'].strip()
    
    return alt_text

def generate_long_description(image_path, alt_text):
    """
    G√©n√®re description longue pour images complexes (longdesc)
    
    Args:
        image_path: Chemin image
        alt_text: Alt text court d√©j√† g√©n√©r√©
    """
    prompt = f"""Description longue d√©taill√©e pour accessibilit√©.

Alt text court existant: "{alt_text}"

G√©n√®re description longue HTML (<longdesc>) incluant:
1. Description compl√®te et structur√©e
2. Toutes donn√©es/informations visuelles
3. Relations spatiales importantes
4. Interpr√©tation si pertinent

Format: HTML avec structure s√©mantique (headings, lists)."""
    
    return analyze_image(image_path, prompt)
```

### Audit Accessibilit√© Batch

```python
def audit_images_accessibility(image_directory):
    """
    Audite accessibilit√© de plusieurs images
    
    Args:
        image_directory: R√©pertoire contenant images
    
    Returns:
        Rapport accessibilit√© avec recommandations
    """
    import glob
    
    images = glob.glob(f"{image_directory}/*.png") + glob.glob(f"{image_directory}/*.jpg")
    
    audit_report = []
    
    for img_path in images:
        print(f"üîç Audit: {os.path.basename(img_path)}")
        
        # Alt text
        alt = generate_alt_text(img_path)
        
        # Classification
        classification_prompt = """Classifie cette image selon WCAG:

1. DECORATIVE (purement esth√©tique)
2. INFORMATIVE (contenu essentiel)
3. FUNCTIONAL (bouton, lien, contr√¥le)
4. COMPLEX (diagramme, graphique n√©cessitant description longue)

R√©ponds juste la cat√©gorie."""
        
        classification = analyze_image(img_path, classification_prompt)['description'].strip()
        
        # Recommandations
        needs_longdesc = classification == "COMPLEX"
        
        audit_report.append({
            'filename': os.path.basename(img_path),
            'alt_text': alt,
            'classification': classification,
            'needs_longdesc': needs_longdesc,
            'wcag_compliant': len(alt) <= 125 if classification != "DECORATIVE" else True
        })
    
    return audit_report
```

---

## Int√©gration avec DALL-E

### Pipeline G√©n√©ration ‚Üí Analyse

```python
class ImageCreationAnalysisPipeline:
    """Pipeline int√©gr√© DALL-E g√©n√©ration + GPT-5 analyse"""
    
    def __init__(self):
        self.dalle_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.gpt5_client = OpenAI(
            api_key=os.getenv("OPENROUTER_API_KEY"),
            base_url="https://openrouter.ai/api/v1"
        )
    
    def generate_and_validate(self, prompt, validation_criteria):
        """
        G√©n√®re image et valide qualit√© automatiquement
        
        Args:
            prompt: Prompt DALL-E
            validation_criteria: Crit√®res de validation
        
        Returns:
            Image + rapport validation
        """
        # G√©n√©ration DALL-E
        print("üé® G√©n√©ration image...")
        dalle_response = self.dalle_client.images.generate(
            model="dall-e-3",
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1
        )
        
        image_url = dalle_response.data[0].url
        revised_prompt = dalle_response.data[0].revised_prompt
        
        # T√©l√©chargement
        temp_path = "temp_validation.png"
        response = requests.get(image_url)
        with open(temp_path, 'wb') as f:
            f.write(response.content)
        
        # Validation GPT-5
        print("üîç Validation qualit√©...")
        validation_prompt = f"""Valide cette image g√©n√©r√©e selon crit√®res:

PROMPT ORIGINAL: {prompt}
PROMPT R√âVIS√â: {revised_prompt}

CRIT√àRES DE VALIDATION:
{validation_criteria}

√âvalue:
1. Respect du prompt (0-10)
2. Qualit√© p√©dagogique (0-10)
3. Clart√© visuelle (0-10)
4. Probl√®mes d√©tect√©s
5. Suggestions am√©lioration

Format: JSON structur√©."""
        
        validation = analyze_image(temp_path, validation_prompt)
        
        return {
            'image_url': image_url,
            'image_path': temp_path,
            'original_prompt': prompt,
            'revised_prompt': revised_prompt,
            'validation': validation['description']
        }
    
    def iterate_until_valid(self, base_prompt, validation_criteria, max_iterations=3):
        """
        It√®re g√©n√©ration jusqu'√† validation r√©ussie
        
        Args:
            base_prompt: Prompt de base
            validation_criteria: Crit√®res validation
            max_iterations: Nombre max d'it√©rations
        """
        iterations = []
        current_prompt = base_prompt
        
        for i in range(max_iterations):
            print(f"\n{'='*60}")
            print(f"It√©ration {i+1}/{max_iterations}")
            
            result = self.generate_and_validate(current_prompt, validation_criteria)
            iterations.append(result)
            
            # V√©rification validation
            if "10/10" in result['validation'] or "excellent" in result['validation'].lower():
                print("‚úÖ Image valid√©e!")
                break
            
            # Prompt am√©lioration
            if i < max_iterations - 1:
                improvement_prompt = f"""Am√©liore ce prompt DALL-E bas√© sur validation:

PROMPT ACTUEL: {current_prompt}
VALIDATION: {result['validation']}

G√©n√®re prompt am√©lior√© (garde style p√©dagogique)."""
                
                # Utilisation GPT-5 pour am√©lioration
                improvement = self.gpt5_client.chat.completions.create(
                    model=GPT5_MODEL,
                    messages=[{"role": "user", "content": improvement_prompt}],
                    max_tokens=300
                )
                
                current_prompt = improvement.choices[0].message.content.strip()
                print(f"üîÑ Prompt am√©lior√©: {current_prompt}")
        
        return iterations
    
    def generate_with_accessibility(self, prompt, subject_area):
        """G√©n√®re image avec alt text automatique"""
        # G√©n√©ration
        result = self.generate_and_validate(
            prompt,
            "Image claire, p√©dagogique, accessible"
        )
        
        # Alt text
        alt_text = generate_alt_text(result['image_path'], context="educational")
        
        # Description longue si complexe
        complexity_check = analyze_image(
            result['image_path'],
            "Cette image est-elle complexe (diagramme, graphique)? R√©ponds OUI ou NON."
        )
        
        if "OUI" in complexity_check['description']:
            long_desc = generate_long_description(result['image_path'], alt_text)
            result['long_description'] = long_desc['description']
        
        result['alt_text'] = alt_text
        return result
```

**Exemple d'utilisation** :
```python
pipeline = ImageCreationAnalysisPipeline()

# G√©n√©ration avec validation
result = pipeline.iterate_until_valid(
    base_prompt="Diagramme cellule v√©g√©tale avec labels fran√ßais",
    validation_criteria="""
    - Tous organites visibles et label√©s
    - Labels en fran√ßais correct
    - Style p√©dagogique clair
    - Adapt√© niveau coll√®ge
    """,
    max_iterations=3
)

# G√©n√©ration avec accessibilit√©
accessible_result = pipeline.generate_with_accessibility(
    prompt="Diagramme photosynth√®se √©tapes principales",
    subject_area="sciences"
)

print(f"Alt text: {accessible_result['alt_text']}")
```

---

## Performance et Cost Management

### Optimisation Co√ªts

```python
class GPT5CostOptimizer:
    """Optimise co√ªts utilisation GPT-5 multimodal"""
    
    # Tarifs approximatifs (v√©rifier OpenRouter)
    COST_PER_1K_INPUT_TOKENS = 0.01
    COST_PER_1K_OUTPUT_TOKENS = 0.03
    IMAGE_COST_LOW_DETAIL = 0.002  # ~85 tokens
    IMAGE_COST_HIGH_DETAIL = 0.006  # ~255 tokens
    
    def __init__(self):
        self.total_cost = 0
        self.usage_log = []
    
    def estimate_cost(self, prompt_length, max_tokens, num_images=0, detail="auto"):
        """Estime co√ªt avant appel API"""
        # Tokens prompt
        input_tokens = prompt_length / 4  # Approximation
        
        # Tokens images
        image_tokens = 0
        if detail == "low":
            image_tokens = num_images * 85
        elif detail == "high":
            image_tokens = num_images * 255
        else:  # auto
            image_tokens = num_images * 170  # Moyenne
        
        # Co√ªt total
        input_cost = (input_tokens + image_tokens) / 1000 * self.COST_PER_1K_INPUT_TOKENS
        output_cost = max_tokens / 1000 * self.COST_PER_1K_OUTPUT_TOKENS
        
        total = input_cost + output_cost
        
        return {
            'estimated_cost': total,
            'input_tokens': input_tokens + image_tokens,
            'output_tokens': max_tokens
        }
    
    def analyze_cost_effective(self, image_path, prompt, detail="auto"):
        """Analyse avec tracking co√ªt"""
        estimate = self.estimate_cost(len(prompt), 500, num_images=1, detail=detail)
        
        print(f"üí∞ Co√ªt estim√©: ${estimate['estimated_cost']:.4f}")
        
        result = analyze_image(image_path, prompt)
        
        # Co√ªt r√©el
        actual_cost = (result['tokens_used'] / 1000) * (self.COST_PER_1K_INPUT_TOKENS + self.COST_PER_1K_OUTPUT_TOKENS)
        
        self.total_cost += actual_cost
        self.usage_log.append({
            'timestamp': datetime.now().isoformat(),
            'prompt': prompt[:50],
            'tokens': result['tokens_used'],
            'cost': actual_cost
        })
        
        result['cost'] = actual_cost
        return result
    
    def batch_optimize(self, images_and_prompts):
        """Optimise batch d'analyses"""
        # Trie par priorit√©
        # Utilise cache si images similaires
        # Regroupe prompts similaires
        
        results = []
        for img, prompt in images_and_prompts:
            # Utilise detail="low" par d√©faut pour √©conomie
            result = self.analyze_cost_effective(img, prompt, detail="low")
            results.append(result)
        
        print(f"\nüìä Total batch: ${self.total_cost:.4f}")
        return results
    
    def get_usage_report(self):
        """Rapport utilisation et co√ªts"""
        return {
            'total_cost': self.total_cost,
            'total_requests': len(self.usage_log),
            'average_cost_per_request': self.total_cost / len(self.usage_log) if self.usage_log else 0,
            'detailed_log': self.usage_log
        }
```

### Cache et R√©utilisation

```python
import json
import hashlib
from datetime import datetime, timedelta

class AnalysisCache:
    """Cache analyses pour √©viter duplications"""
    
    def __init__(self, cache_dir="cache/gpt5_analyses"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_index = self._load_index()
    
    def _load_index(self):
        """Charge index cache"""
        index_file = self.cache_dir / "index.json"
        if index_file.exists():
            with open(index_file, 'r') as f:
                return json.load(f)
        return {}
    
    def _save_index(self):
        """Sauvegarde index"""
        with open(self.cache_dir / "index.json", 'w') as f:
            json.dump(self.cache_index, f, indent=2)
    
    def _get_cache_key(self, image_path, prompt):
        """G√©n√®re cl√© cache unique"""
        # Hash image
        with open(image_path, 'rb') as f:
            image_hash = hashlib.md5(f.read()).hexdigest()
        
        # Hash prompt
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        
        return f"{image_hash}_{prompt_hash}"
    
    def get(self, image_path, prompt, max_age_hours=24):
        """R√©cup√®re depuis cache si existe et r√©cent"""
        cache_key = self._get_cache_key(image_path, prompt)
        
        if cache_key in self.cache_index:
            entry = self.cache_index[cache_key]
            cached_time = datetime.fromisoformat(entry['timestamp'])
            
            # V√©rification √¢ge
            if datetime.now() - cached_time < timedelta(hours=max_age_hours):
                cache_file = self.cache_dir / f"{cache_key}.json"
                with open(cache_file, 'r') as f:
                    print(f"‚úÖ Cache hit: {cache_key[:16]}...")
                    return json.load(f)
        
        return None
    
    def set(self, image_path, prompt, result):
        """Sauvegarde dans cache"""
        cache_key = self._get_cache_key(image_path, prompt)
        
        # Sauvegarde r√©sultat
        cache_file = self.cache_dir / f"{cache_key}.json"
        with open(cache_file, 'w') as f:
            json.dump(result, f, indent=2)
        
        # Mise √† jour index
        self.cache_index[cache_key] = {
            'image': os.path.basename(image_path),
            'prompt_preview': prompt[:100],
            'timestamp': datetime.now().isoformat()
        }
        self._save_index()
        
        print(f"üíæ Cached: {cache_key[:16]}...")
    
    def analyze_with_cache(self, image_path, prompt):
        """Analyse avec cache automatique"""
        # V√©rification cache
        cached = self.get(image_path, prompt)
        if cached:
            return cached
        
        # Analyse si pas en cache
        result = analyze_image(image_path, prompt)
        self.set(image_path, prompt, result)
        
        return result
```

**Exemple d'utilisation optimis√©e** :
```python
# Avec optimisation co√ªts
optimizer = GPT5CostOptimizer()
cache = AnalysisCache()

# Analyse avec cache
result = cache.analyze_with_cache(
    "diagram.png",
    "Analyse ce diagramme p√©dagogiquement"
)

# Tracking co√ªt
tracked_result = optimizer.analyze_cost_effective(
    "diagram.png",
    "Analyse d√©taill√©e",
    detail="low"  # √âconomique
)

# Rapport final
report = optimizer.get_usage_report()
print(f"Co√ªt total session: ${report['total_cost']:.4f}")
```

---

## Ressources Compl√©mentaires

### Documentation
- [OpenRouter API Docs](https://openrouter.ai/docs)
- [OpenAI Vision Guide](https://platform.openai.com/docs/guides/vision)
- [WCAG 2.1 Guidelines](https://www.w3.org/WAI/WCAG21/quickref/)

### Notebooks CoursIA
- `01-2-GPT-5-Image-Generation.ipynb` : Introduction GPT-5
- `04-1-Educational-Content-Generation.ipynb` : Applications p√©dagogiques
- `04-2-Creative-Workflows.ipynb` : Workflows avanc√©s

### Templates R√©utilisables
- `examples/science-diagrams.ipynb` : Analyses scientifiques
- `examples/history-geography.ipynb` : Documents historiques
- `examples/literature-visual.ipynb` : Illustrations litt√©raires

---

**Version**: 1.0.0  
**Derni√®re mise √† jour**: 2025-10-08  
**Auteur**: √âquipe CoursIA  
**Licence**: Projet √âducatif CoursIA