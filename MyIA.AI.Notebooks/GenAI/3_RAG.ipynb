{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook : Génération d'Images, Low-Code AI, Function Calling, RAG\n",
    "\n",
    "Dans ce notebook, nous allons tour à tour découvrir :\n",
    "\n",
    "1. Comment générer des **images** à partir de prompts en texte (ex: DALL-E, Midjourney).\n",
    "2. Comment créer des **applications low-code** enrichies par l'IA, grâce à **Power Platform** (Copilot, AI Builder).\n",
    "3. **Function Calling** côté OpenAI : structurer les réponses d’un LLM pour déclencher des actions.\n",
    "4. **RAG** (Retrieval Augmented Generation) et **bases vectorielles** (indexation, recherche sémantique, chunking, etc.).\n",
    "\n",
    "\n",
    "\n",
    "## Prérequis & Installation\n",
    "\n",
    "- **Python 3.9+** (ou version ultérieure).\n",
    "- Un compte [OpenAI](https://platform.openai.com/) et une clé d’API valide.\n",
    "- Le fichier `.env` contenant votre clé d’API :\n",
    "OPENAI_API_KEY=sk-...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi la génération d'images via IA ?\n",
    "\n",
    "Les modèles de génération d'images, tels que **DALL-E** (OpenAI) ou **Midjourney**, ont la capacité de créer des visuels originaux à partir de simples descriptions textuelles (prompts).  \n",
    "- **Applications concrètes** :  \n",
    "  - Design rapide de prototypes (marketing, publicité)  \n",
    "  - Création artistique (concept art, storyboards)  \n",
    "  - Illustrations pédagogiques ou infographiques  \n",
    "- **Limitations** :  \n",
    "  - Les images peuvent contenir des incohérences (proportions bizarres, doigts supplémentaires, etc.)  \n",
    "  - Certaines requêtes contraires aux politiques d’utilisation peuvent être bloquées  \n",
    "\n",
    "Dans la suite, nous allons voir comment **OpenAI** gère la génération d’images via l’API `images.generate()`, et comment intégrer ces visuels dans un flux d'automatisation (Low-Code) ou dans des applications web.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "# from PIL import Image\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger le fichier .env pour la clé OPENAI_API_KEY\n",
    "load_dotenv()\n",
    "\n",
    "# Config globale - initialisation du client OpenAI moderne\n",
    "client = OpenAI()\n",
    "\n",
    "try:\n",
    "    response = client.images.generate(\n",
    "        prompt=\"Lapin sur un cheval tenant une sucette, dans un champ brumeux\"\n",
    "    )\n",
    "    # response est un ImagesResponse\n",
    "    image_url = response.data[0].url\n",
    "    print(\"Image URL:\", image_url)\n",
    "\n",
    "    # Téléchargement de l'image\n",
    "    # img_data = requests.get(image_url).content\n",
    "    # with open(\"my_image.png\",\"wb\") as f:\n",
    "    #     f.write(img_data)\n",
    "\n",
    "    # # Ouverture\n",
    "    # img = Image.open(\"my_image.png\")\n",
    "    # img.show()\n",
    "    \n",
    "    \n",
    "    image = Image(url= image_url)\n",
    "    display(image)\n",
    "\n",
    "except openai.APIConnectionError as e:\n",
    "    print(\"Erreur de connexion réseau:\", e)\n",
    "except openai.RateLimitError as e:\n",
    "    print(\"Limite atteinte ou quota dépassé:\", e)\n",
    "except openai.APIStatusError as e:\n",
    "    print(\"Erreur HTTP renvoyée par l'API (4xx, 5xx, etc.):\", e)\n",
    "except openai.APIError as e:\n",
    "    print(\"Autre erreur OpenAI:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "#  Comparaison de prompts d'images\n",
    "# ============================\n",
    "\n",
    "image_prompts = [\n",
    "    \"A small kitten wearing a hat, cartoon style\",\n",
    "    \"A realistic portrait of a small kitten wearing a cowboy hat in the desert\",\n",
    "    \"A small kitten wearing a futuristic helmet in cyberpunk style, neon colors\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(image_prompts):\n",
    "    try:\n",
    "        print(f\"--- Prompt #{i+1}: {prompt} ---\")\n",
    "        response_img = openai.images.generate(prompt=prompt)\n",
    "        img_url = response_img.data[0].url\n",
    "        print(\"Image URL:\", img_url)\n",
    "        # Optionnel : display() si tu es dans un environnement Jupyter\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Erreur lors de la génération d'image:\", e)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Méta-prompts et usage responsable\n",
    "\n",
    "Pour gérer un usage plus responsable et filtrer des images non souhaitées, on peut ajouter un \n",
    "**meta-prompt** en amont, décrivant les restrictions (ex: Safe for Work, No adult content, etc.).\n",
    "\n",
    "Ex:\n",
    "You are an assistant that only generates children-friendly images. [... consignes ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Low-Code AI Apps (Power Platform)\n",
    "\n",
    "## 2.1 Introduction\n",
    "Power Platform inclut :\n",
    "- Power Apps (construction rapide d'apps)\n",
    "- Power Automate (workflows et automatisations)\n",
    "- Dataverse (stockage de données)\n",
    "- AI Builder (modèles IA pré-construits)\n",
    "- Copilot (assistant pour générer tables, flux, e-mails)\n",
    "\n",
    "Avantages : construction **no-code / low-code** pour mettre en place des solutions rapidement, \n",
    "y compris connectées à des services IA.\n",
    "\n",
    "\n",
    "## 2.2 Copilot dans Power Apps : Student Assignment Tracker\n",
    "\n",
    "Exemple : On veut un **Student Assignment Tracker**.\n",
    "\n",
    "1. Sur la home de [Power Apps](https://make.powerapps.com), on saisit dans la zone Copilot : \n",
    "   \"I want an app to track and manage student assignments.\"\n",
    "2. Copilot propose une table Dataverse (champs Title, DateDue, StudentName, etc.)\n",
    "3. Personnaliser la table (ajouter `StudentEmail`, etc.)\n",
    "4. Cliquer \"Create app\" => Copilot génère une **Canvas App** auto.\n",
    "5. Ajouter une page (screen) pour \"Envoyer un email\" (Prompt : \"I want a screen to send an email to the student\").\n",
    "\n",
    "On obtient en quelques clics un début d'application.\n",
    "\n",
    "\n",
    "## 2.3 Copilot dans Power Automate : Invoice Processing\n",
    "\n",
    "Même concept : Dans [Power Automate](https://make.powerautomate.com),\n",
    "on demande \"Process an invoice when it arrives in my mailbox\", \n",
    "Copilot propose un flux (trigger: new mail arrives + extractions + email)...\n",
    "\n",
    "On peut ensuite y intégrer **AI Builder** : \n",
    "- ex: le prébuilt model \"Invoice Processing\" pour extraire `supplier`, `amount`, etc.\n",
    "- stocker dans Dataverse, \n",
    "- email de confirmation.\n",
    "\n",
    "C’est un gros gain de temps pour la finance ou la logistique !\n",
    "\n",
    "\n",
    "# 3. Function Calling (OpenAI)\n",
    "\n",
    "\n",
    "## 3.1 Pourquoi ?\n",
    "\n",
    "Sans function calling, le LLM renvoie du texte non structuré. \n",
    "Difficile d’automatiser (ex: parse JSON, exécuter une fonction tierce).\n",
    "Avec function calling, on déclare un `schema` JSON, \n",
    "le LLM répond par un `function_call`: \n",
    "- Nom de la fonction \n",
    "- Arguments structurés\n",
    "\n",
    "Ensuite on exécute la fonction en Python (ou autre).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Find me a good course for a beginner developer to learn Roo-code. Join a message to your function calls\"}\n",
    "]\n",
    "\n",
    "functions = [\n",
    "  {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"description\": \"Retrieves relevant courses based on role, product & level\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"role\":   {\"type\":\"string\",\"description\":\"the role of the user\"},\n",
    "        \"product\":{\"type\":\"string\",\"description\":\"the product/tech\"},\n",
    "        \"level\": {\"type\":\"string\",\"description\":\"the user skill level\"}\n",
    "      },\n",
    "      \"required\": [\"role\",\"product\",\"level\"]\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  \n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\"  # laisse le LLM décider s’il appelle la fonction\n",
    "    )\n",
    "\n",
    "    print(\"Réponse brute:\\n\", response.choices[0].message)\n",
    "\n",
    "except openai.RateLimitError as e:\n",
    "    print(\"Limite atteinte:\", e)\n",
    "except openai.APIError as e:\n",
    "    print(\"Autre erreur:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def search_courses(role,product,level):\n",
    "    # Ton code Python => Appel API Microsoft Learn\n",
    "    # On renvoie un JSON/string\n",
    "    return \"Liste de cours: Roo-code For EPF, etc.\"\n",
    "\n",
    "resp_msg = response.choices[0].message\n",
    "if resp_msg.function_call:\n",
    "    fn_name = resp_msg.function_call.name\n",
    "    fn_args = json.loads(resp_msg.function_call.arguments)\n",
    "    \n",
    "    # Exécuter la fonction Python correspondante\n",
    "    result = search_courses(**fn_args)\n",
    "\n",
    "    # On crée deux messages :\n",
    "    # 1) le function_call\n",
    "    # 2) le role=\"function\" + content du résultat\n",
    "    second_messages = [\n",
    "      {\"role\":\"assistant\",\"function_call\": {\"name\":fn_name,\"arguments\":resp_msg.function_call.arguments}},\n",
    "      {\"role\":\"function\",\"name\":fn_name,\"content\": result}\n",
    "    ]\n",
    "\n",
    "    # On relance le chat\n",
    "    final_resp = client.chat.completions.create(\n",
    "       model=\"gpt-4o-mini\",\n",
    "       messages=messages + second_messages\n",
    "    )\n",
    "    print(\"Réponse finale:\\n\", final_resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling Avancé avec OpenAI : Chaînage de Fonctions  \n",
    "\n",
    "Ce bloc de code illustre un exemple avancé de **Function Calling** avec OpenAI, permettant à l'IA d'orchestrer plusieurs appels de fonctions de manière autonome.  \n",
    "\n",
    "### 🛠 Fonctionnalités mises en œuvre :  \n",
    "1. **Premier appel à l'API OpenAI** :  \n",
    "   - L'utilisateur demande de planifier une réunion.  \n",
    "   - L'IA détecte qu'il faut appeler `create_meeting_event` et génère les arguments nécessaires (`topic`, `date`, `participants`).  \n",
    "   - Le modèle ne renvoie pas de texte brut, mais un `function_call` contenant les paramètres de la réunion.  \n",
    "\n",
    "2. **Exécution locale de `create_meeting_event` en Python** :  \n",
    "   - La fonction génère un objet événement fictif avec un `event_id`.  \n",
    "   - Ce résultat est transmis à l'IA en tant que réponse fonctionnelle (`role=\"function\"`).  \n",
    "\n",
    "3. **Deuxième appel à l'API OpenAI** :  \n",
    "   - Sur la base de l'événement créé, l'IA décide de générer un email de confirmation en appelant `send_email`.  \n",
    "   - L'IA fournit les arguments (`subject`, `body`, `recipients`).  \n",
    "\n",
    "4. **Exécution locale de `send_email` en Python** :  \n",
    "   - Simulation de l'envoi d'email avec un affichage console.  \n",
    "   - Message de confirmation `\"Email sent successfully!\"`.  \n",
    "\n",
    "### 📌 Preuve du bon fonctionnement :  \n",
    "✅ **Chaînage réussi** : OpenAI a déclenché **deux appels de fonction distincts**, prouvant la capacité du modèle à raisonner sur plusieurs étapes.  \n",
    "✅ **Exécution hybride** : L'IA décide des actions à effectuer, mais l'exécution est déléguée au code Python.  \n",
    "✅ **Application possible** : Ce workflow peut être adapté pour intégrer des bases de données, envoyer de vrais emails ou automatiser des tâches complexes.  \n",
    "\n",
    "🔹 **Exemple d'affichage console :**  \n",
    "```\n",
    "Raw response: ChatCompletionMessage(..., function_call=FunctionCall(...))\n",
    "=== Simulated Email ===\n",
    "Subject: Confirmation de la réunion sur l'état du projet\n",
    "To: ['Alice', 'Bob']\n",
    "Body: Bonjour Alice et Bob,\n",
    "Je vous confirme que la réunion sur l'état du projet est planifiée pour mardi prochain, le 31 octobre 2023, à 10h.\n",
    "À bientôt,\n",
    "L'équipe de gestion de projet\n",
    "Email function result: Email sent successfully!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Function Calling (avancé) avec 2 fonctions\n",
    "# ============================\n",
    "\n",
    "import json\n",
    "import openai\n",
    "\n",
    "def create_meeting_event(topic, date, participants):\n",
    "    # Exemple simulé : création d'un objet event\n",
    "    return {\n",
    "        \"event_id\": \"evt-001\",\n",
    "        \"topic\": topic,\n",
    "        \"date\": date,\n",
    "        \"participants\": participants\n",
    "    }\n",
    "\n",
    "def send_email(subject, body, recipients):\n",
    "    # Exemple simulé : envoi d'un email\n",
    "    print(\"=== Simulated Email ===\")\n",
    "    print(\"Subject:\", subject)\n",
    "    print(\"To:\", recipients)\n",
    "    print(\"Body:\", body)\n",
    "    return \"Email sent successfully!\"\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"create_meeting_event\",\n",
    "        \"description\": \"Créer un événement de réunion\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"topic\": {\"type\": \"string\"},\n",
    "                \"date\": {\"type\": \"string\"},\n",
    "                \"participants\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"topic\", \"date\", \"participants\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"send_email\",\n",
    "        \"description\": \"Envoyer un email\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"subject\": {\"type\": \"string\"},\n",
    "                \"body\": {\"type\": \"string\"},\n",
    "                \"recipients\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"subject\", \"body\", \"recipients\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "messages_fc = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Planifie une réunion sur l'état du projet pour mardi prochain à 10h \"\n",
    "            \"avec Alice et Bob, puis envoie un email de confirmation.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# L'appel initial : le modèle peut décider d'appeler\n",
    "# create_meeting_event, send_email, ou rien (function_call=\"auto\").\n",
    "response_fc = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages_fc,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\"\n",
    ")\n",
    "\n",
    "assistant_msg = response_fc.choices[0].message\n",
    "print(\"Raw response:\", assistant_msg)\n",
    "\n",
    "# Vérifier si le modèle appelle une fonction\n",
    "if assistant_msg.function_call:\n",
    "    fn_name = assistant_msg.function_call.name\n",
    "    fn_args_str = assistant_msg.function_call.arguments  # chaîne JSON\n",
    "    fn_args = json.loads(fn_args_str)\n",
    "\n",
    "    if fn_name == \"create_meeting_event\":\n",
    "        # 1) Exécuter la fonction create_meeting_event côté Python\n",
    "        result_event = create_meeting_event(**fn_args)\n",
    "\n",
    "        # 2) Créer de nouveaux messages (assistant + function)\n",
    "        second_messages = messages_fc + [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"function_call\": {\n",
    "                    \"name\": fn_name,\n",
    "                    \"arguments\": json.dumps(fn_args)\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": fn_name,\n",
    "                \"content\": json.dumps(result_event)\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # 3) Relancer le chat pour voir si le modèle appelle la 2e fonction\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=second_messages,\n",
    "            functions=functions,\n",
    "            function_call=\"auto\"\n",
    "        )\n",
    "        second_msg = second_response.choices[0].message\n",
    "\n",
    "        if second_msg.function_call:\n",
    "            fn2_name = second_msg.function_call.name\n",
    "            fn2_args_str = second_msg.function_call.arguments\n",
    "            fn2_args = json.loads(fn2_args_str)\n",
    "\n",
    "            if fn2_name == \"send_email\":\n",
    "                # Exécution de la seconde fonction\n",
    "                result_email = send_email(**fn2_args)\n",
    "                print(\"Email function result:\", result_email)\n",
    "            else:\n",
    "                print(f\"The model called a different function: {fn2_name}\")\n",
    "        else:\n",
    "            print(\"No second function call was triggered.\")\n",
    "    else:\n",
    "        print(f\"The model called a different function: {fn_name}\")\n",
    "else:\n",
    "    print(\"No function call triggered by the assistant.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Retrieval Augmented Generation & Vector Databases\n",
    "\n",
    "## 4.1 Principe\n",
    "Un LLM (ex: GPT) a une limite : il ne connaît pas forcément nos documents internes. \n",
    "RAG => on stocke nos docs dans une base vectorielle (embeddings), \n",
    "puis à chaque question, on envoie au LLM les passages pertinents (retrieval + augmentation).\n",
    "\n",
    "## 4.2 Création d’une base vectorielle\n",
    "\n",
    "- On découpe (chunk) nos documents en petits segments (ex: 400 tokens).\n",
    "- On calcule embeddings (ex: text-embedding-ada-002).\n",
    "- On stocke : ex. Cosmos DB, Pinecone, ChromaDB, Elasticsearch, Qdrant, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn numpy pandas requests beautifulsoup4 lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL du débat\n",
    "url = \"https://home.nps.gov/liho/learn/historyculture/debate1.htm\"\n",
    "\n",
    "# Requête HTTP\n",
    "response = requests.get(url)\n",
    "html = response.text  # contenu HTML sous forme de string\n",
    "\n",
    "# print(html)\n",
    "\n",
    "# On parse avec BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Sélection du noeud principal.\n",
    "# Selon ton info: div.ColumnMain:nth-child(2)\n",
    "# (Le \"nth-child(2)\" est parfois incertain, on peut tenter un select plus large.)\n",
    "main_div = soup.select_one(\"div.ColumnMain\")\n",
    "\n",
    "if not main_div:\n",
    "    raise ValueError(\"Impossible de trouver le div.ColumnMain dans la page !\")\n",
    "\n",
    "# Extraction du texte brut (on sépare par \" \" pour éviter les collisions)\n",
    "debate_text = main_div.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "#Enregistrement du texte dans un fichier\n",
    "with open(\"debate.txt\", \"w\") as f:\n",
    "    f.write(debate_text)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement depuis le fichier\n",
    "with open(\"debate.txt\", \"r\") as f:\n",
    "    debate_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour débogage:\n",
    "print(\"=== Longueur du texte récupéré:\", len(debate_text))\n",
    "print(debate_text[:10000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"title\":\"First Debate: Ottawa, Illinois (NPS)\",\n",
    "            \"text\": debate_text\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    splitted = split_text_into_chunks(row[\"text\"], chunk_size=400, overlap=50)\n",
    "    for chunk in splitted:\n",
    "        rows.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"chunk\": chunk\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(rows)\n",
    "print(\"Nombre de chunks =\", len(df_chunks))\n",
    "df_chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialiser le client OpenAI (indispensable avec la nouvelle API)\n",
    "client = OpenAI()\n",
    "\n",
    "def create_embedding(text: str):\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            input=[text]  # ⚠️ Doit être une **liste**\n",
    "        )\n",
    "        return response.data[0].embedding  # Extraction correcte\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur lors de la génération d'embedding: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Générer les embeddings pour les chunks du DataFrame\n",
    "df_chunks[\"embedding\"] = df_chunks[\"chunk\"].apply(create_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nettoyage des embeddings\n",
    "all_vectors = np.array([emb for emb in df_chunks[\"embedding\"] if emb is not None])  # Exclure None\n",
    "nn = NearestNeighbors(n_neighbors=3, metric=\"euclidean\")\n",
    "nn.fit(all_vectors)\n",
    "\n",
    "def retrieve(user_query: str) -> str:\n",
    "    try:\n",
    "        # Générer l'embedding de la requête\n",
    "        q_emb = create_embedding(user_query)\n",
    "        if q_emb is None:\n",
    "            return \"⚠️ Impossible de générer un embedding pour la requête.\"\n",
    "\n",
    "        dist, idx = nn.kneighbors([q_emb])\n",
    "\n",
    "        # Récupérer les meilleurs chunks\n",
    "        best_chunks = df_chunks.iloc[idx[0]][\"chunk\"].tolist()\n",
    "        prompt = user_query + \"\\n\\n\" + \"\\n\".join(best_chunks)\n",
    "        \n",
    "        print(\"🔍 Prompt augmenté générée :\\n\", prompt)\n",
    "\n",
    "        # ✅ Appel OpenAI corrigé\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content if response.choices else \"⚠️ Aucune réponse générée.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Erreur lors de la récupération : {str(e)}\"\n",
    "\n",
    "\n",
    "# 🔥 Test avec la question sur Lincoln\n",
    "question = \"What did Lincoln argue about slavery in that first debate?\"\n",
    "answer = retrieve(question)\n",
    "print(\"🔍 Réponse générée :\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonnes pratiques pour la RAG (Retrieval Augmented Generation)\n",
    "\n",
    "1. **Chunking**  \n",
    "   - Découper vos documents en segments de taille raisonnable (ex. 300-500 tokens), afin de mieux cibler les passages pertinents.\n",
    "2. **Indexation des embeddings**  \n",
    "   - Stocker les vecteurs dans une base adaptée (ex. Pinecone, Chroma, Elasticsearch vectoriel, Qdrant, ou Cosmos DB vector).\n",
    "3. **Filtrage et post-traitement**  \n",
    "   - Après avoir récupéré les chunks les plus proches sémantiquement, il peut être utile de vérifier l’exactitude ou la cohérence des informations extraites.\n",
    "4. **Ré-intégration**  \n",
    "   - Insérer les passages sélectionnés dans le prompt (par ex. “Voici un extrait : ...\\n\\n Maintenant, réponds à la question...”).  \n",
    "   - Ou bien utiliser un outil style [LangChain](https://github.com/hwchase17/langchain) qui facilite ce pipeline.\n",
    "5. **Éviter les hallucinations**  \n",
    "   - Demander explicitement au modèle de s’en tenir aux informations fournies dans les chunks.  \n",
    "   - En cas d’insuffisance de données, demander au modèle de répondre “Je ne sais pas” plutôt que d’inventer.\n",
    "\n",
    "L’objectif est de combiner la **puissance du LLM** et l’**exactitude** de données externes (base documentaire, articles, PDF, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion & Pistes\n",
    "\n",
    "Nous avons exploré :\n",
    "- la génération d’images (DALL-E, prompts, meta-prompts),\n",
    "- la création d’apps low-code Power Apps / Automate,\n",
    "- l’usage de Copilot & AI Builder pour des scénarios métiers (tracking, invoice),\n",
    "- la structuration des réponses via Function Calling,\n",
    "- RAG : indexer nos docs dans une base vectorielle et enrichir un LLM.\n",
    "\n",
    "Pistes d’exercices :\n",
    "- Améliorer les prompts d’images (température, variations, mask, etc.)\n",
    "- Créer un flux complet dans Power Automate avec AI Builder\n",
    "- Mettre en place Function Calling plus complexe (multi-fonctions, error-handling)\n",
    "- Stocker un doc plus large (ex: 10 pages PDF) en chunks + RAG\n",
    "\n",
    "Fin de la synthèse ! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
