{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b2f02a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:13:32.617086Z",
     "iopub.status.busy": "2026-02-18T08:13:32.615132Z",
     "iopub.status.idle": "2026-02-18T08:13:32.627848Z",
     "shell.execute_reply": "2026-02-18T08:13:32.627234Z"
    },
    "papermill": {
     "duration": 0.094624,
     "end_time": "2026-02-18T08:13:32.629312",
     "exception": false,
     "start_time": "2026-02-18T08:13:32.534688",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_MODE = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb39ecbc",
   "metadata": {
    "papermill": {
     "duration": 0.052786,
     "end_time": "2026-02-18T08:13:32.765474",
     "exception": false,
     "start_time": "2026-02-18T08:13:32.712688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10. Hébergement Local de Modèles Génératifs\n",
    "\n",
    "**Durée estimée** : 60 minutes\n",
    "\n",
    "**Prérequis** : Notebook 1 (OpenAI Intro), Docker, GPU (recommandé)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "Ce notebook explore l'hébergement **local** de LLMs via des serveurs compatibles OpenAI API :\n",
    "\n",
    "1. **Configuration multi-endpoints** : Gérer plusieurs modèles/serveurs\n",
    "2. **vLLM et Ollama** : Serveurs d'inférence populaires\n",
    "3. **DeepSeek R1** : Modèle raisonnant local (alternative à o1)\n",
    "4. **Qwen 2.5** : Tool calling et multimodal local\n",
    "5. **Benchmarking** : Comparaison performances et coûts\n",
    "\n",
    "---\n",
    "\n",
    "## Pourquoi héberger localement ?\n",
    "\n",
    "| Aspect | Cloud (OpenAI) | Local (vLLM/Ollama) |\n",
    "|--------|----------------|---------------------|\n",
    "| **Coût** | Par token ($) | Fixe (matériel + électricité) |\n",
    "| **Latence** | Réseau + queue | Direct GPU |\n",
    "| **Confidentialité** | Données envoyées | Données locales |\n",
    "| **Disponibilité** | Dépend du service | 100% contrôle |\n",
    "| **Modèles** | Limité au catalogue | Open-source illimité |\n",
    "\n",
    "---\n",
    "\n",
    "## Modèles locaux recommandés (2025-2026)\n",
    "\n",
    "| Modèle | Taille | VRAM | Capacités |\n",
    "|--------|--------|------|-----------|\n",
    "| **DeepSeek R1** (distill) | 8B-70B | 8-48GB | Raisonnement, code |\n",
    "| **Qwen 2.5** | 7B-72B | 8-48GB | Tool calling, multimodal |\n",
    "| **Llama 3.1** | 8B-70B | 8-48GB | Généraliste |\n",
    "| **Mistral/Mixtral** | 7B-8x7B | 8-48GB | Code, MoE |\n",
    "\n",
    "---\n",
    "\n",
    "## Installation & Import\n",
    "\n",
    "On installe/importe ce qui est nécessaire :\n",
    "- `requests` pour les appels HTTP bruts,\n",
    "- `openai` version 1.0.0+,\n",
    "- `semantic-kernel` si on veut tester SK,\n",
    "- d'autres libs selon besoin (json, time, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b0cdc",
   "metadata": {
    "papermill": {
     "duration": 0.058576,
     "end_time": "2026-02-18T08:13:32.887977",
     "exception": false,
     "start_time": "2026-02-18T08:13:32.829401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Concepts Clés\n",
    "\n",
    "### vLLM (Very Large Language Models)\n",
    "\n",
    "**vLLM** est un serveur d'inférence haute performance pour les LLMs :\n",
    "\n",
    "- **PagedAttention** : Gestion optimisée de la mémoire GPU (KV cache)\n",
    "- **Batching continu** : Traite plusieurs requêtes simultanément\n",
    "- **Compatible OpenAI API** : Drop-in replacement pour les applications existantes\n",
    "- **Support multi-GPU** : Tensor parallelism et pipeline parallelism\n",
    "\n",
    "**Cas d'usage idéaux** :\n",
    "- Production avec fort trafic (>100 req/min)\n",
    "- Applications nécessitant faible latence\n",
    "- Déploiement multi-GPU pour grands modèles (>70B paramètres)\n",
    "\n",
    "### Ollama\n",
    "\n",
    "**Ollama** est une plateforme de déploiement simplifiée pour LLMs locaux :\n",
    "\n",
    "- **Installation ultra-simple** : Une commande pour démarrer\n",
    "- **Gestion de modèles** : Téléchargement et versioning automatiques\n",
    "- **Quantization** : Support Q4, Q5, Q8 pour réduire l'empreinte mémoire\n",
    "- **API REST** : Compatible OpenAI + API native Ollama\n",
    "\n",
    "**Cas d'usage idéaux** :\n",
    "- Prototypage rapide et développement local\n",
    "- Machines avec GPU limité (8-16GB VRAM)\n",
    "- Applications mono-utilisateur ou faible trafic\n",
    "\n",
    "### Comparaison vLLM vs Ollama\n",
    "\n",
    "| Aspect | vLLM | Ollama |\n",
    "|--------|------|--------|\n",
    "| **Performance** | Excellent (batching optimisé) | Bon (single request) |\n",
    "| **Setup** | Complexe (Docker + config) | Simple (1 commande) |\n",
    "| **VRAM requis** | 16-24GB+ | 8GB+ (avec quantization) |\n",
    "| **Multi-GPU** | Support natif | Limité |\n",
    "| **Gestion modèles** | Manuel (HuggingFace) | Automatique (registry) |\n",
    "\n",
    "### Endpoints OpenAI-compatibles\n",
    "\n",
    "Les deux serveurs exposent les mêmes endpoints que l'API OpenAI :\n",
    "\n",
    "-  : Conversations chat\n",
    "-  : Complétion de texte brut\n",
    "-  : Liste des modèles disponibles\n",
    "-  : Génération d'embeddings (vLLM uniquement)\n",
    "\n",
    "**Avantage clé** : Code portable entre cloud (OpenAI) et local (vLLM/Ollama)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8a7aab",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:13:32.945912Z",
     "iopub.status.busy": "2026-02-18T08:13:32.944366Z",
     "iopub.status.idle": "2026-02-18T08:13:42.597950Z",
     "shell.execute_reply": "2026-02-18T08:13:42.597013Z"
    },
    "papermill": {
     "duration": 9.686762,
     "end_time": "2026-02-18T08:13:42.599415",
     "exception": false,
     "start_time": "2026-02-18T08:13:32.912653",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (2.32.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (3.11.18)\n",
      "Requirement already satisfied: openai in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.109.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (0.12.0)\n",
      "Requirement already satisfied: semantic-kernel in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.39.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.1.0)\n",
      "Requirement already satisfied: anyio in c:\\python313\\lib\\site-packages (4.9.0)\n",
      "Requirement already satisfied: httpx in c:\\python313\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: httpcore in c:\\python313\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python313\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python313\\lib\\site-packages (from aiohttp) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp) (1.20.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in c:\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\python313\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: azure-ai-projects~=1.0.0b12 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.0.0)\n",
      "Requirement already satisfied: azure-ai-agents>=1.2.0b3 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.2.0b6)\n",
      "Requirement already satisfied: cloudevents~=1.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.11.0)\n",
      "Requirement already satisfied: pydantic-settings~=2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (2.9.1)\n",
      "Requirement already satisfied: defusedxml~=0.7 in c:\\python313\\lib\\site-packages (from semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: azure-identity>=1.13 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.21.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (2.2.5)\n",
      "Requirement already satisfied: openapi_core<0.20,>=0.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (0.19.5)\n",
      "Requirement already satisfied: websockets<16,>=13 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (15.0.1)\n",
      "Requirement already satisfied: aiortc>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.11.0)\n",
      "Requirement already satisfied: opentelemetry-api~=1.24 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.24 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.33.1)\n",
      "Requirement already satisfied: prance<25.4.9,>=23.6.21 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (25.4.8.0)\n",
      "Requirement already satisfied: pybars4~=0.9 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (0.9.13)\n",
      "Requirement already satisfied: jinja2~=3.1 in c:\\python313\\lib\\site-packages (from semantic-kernel) (3.1.6)\n",
      "Requirement already satisfied: nest-asyncio~=1.6 in c:\\python313\\lib\\site-packages (from semantic-kernel) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.15.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (1.15.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from semantic-kernel) (5.29.4)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\python313\\lib\\site-packages (from httpcore) (0.16.0)\n",
      "Requirement already satisfied: aioice<1.0.0,>=0.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (0.10.1)\n",
      "Requirement already satisfied: av<15.0.0,>=14.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (14.3.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\python313\\lib\\site-packages (from aiortc>=1.9.0->semantic-kernel) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=44.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (44.0.3)\n",
      "Requirement already satisfied: google-crc32c>=1.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (1.7.1)\n",
      "Requirement already satisfied: pyee>=13.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (13.0.0)\n",
      "Requirement already satisfied: pylibsrtp>=0.10.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (0.12.0)\n",
      "Requirement already satisfied: pyopenssl>=25.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aiortc>=1.9.0->semantic-kernel) (25.0.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-ai-agents>=1.2.0b3->semantic-kernel) (0.7.2)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-ai-agents>=1.2.0b3->semantic-kernel) (1.34.0)\n",
      "Requirement already satisfied: azure-storage-blob>=12.15.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-ai-projects~=1.0.0b12->semantic-kernel) (12.25.1)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-identity>=1.13->semantic-kernel) (1.32.3)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from azure-identity>=1.13->semantic-kernel) (1.3.1)\n",
      "Requirement already satisfied: deprecation<3.0,>=2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from cloudevents~=1.0->semantic-kernel) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python313\\lib\\site-packages (from jinja2~=3.1->semantic-kernel) (3.0.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in c:\\python313\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (4.23.0)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.3.4)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (10.7.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.6.3)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: parse in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (1.20.2)\n",
      "Requirement already satisfied: werkzeug<3.1.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (3.1.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-api~=1.24->semantic-kernel) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-api~=1.24->semantic-kernel) (8.6.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-sdk~=1.24->semantic-kernel) (0.54b1)\n",
      "Requirement already satisfied: chardet>=5.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.18.10 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (0.18.10)\n",
      "Requirement already satisfied: packaging>=24.2 in c:\\python313\\lib\\site-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (25.0)\n",
      "Requirement already satisfied: PyMeta3>=0.5.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pybars4~=0.9->semantic-kernel) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aioice<1.0.0,>=0.9.0->aiortc>=1.9.0->semantic-kernel) (2.7.0)\n",
      "Requirement already satisfied: ifaddr>=0.2.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from aioice<1.0.0,>=0.9.0->aiortc>=1.9.0->semantic-kernel) (0.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\python313\\lib\\site-packages (from azure-core>=1.30.0->azure-ai-agents>=1.2.0b3->semantic-kernel) (1.17.0)\n",
      "Requirement already satisfied: pycparser in c:\\python313\\lib\\site-packages (from cffi>=1.0.0->aiortc>=1.9.0->semantic-kernel) (2.22)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from deprecated>=1.2.6->opentelemetry-api~=1.24->semantic-kernel) (1.17.2)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api~=1.24->semantic-kernel) (3.21.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\python313\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\python313\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\python313\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.24.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\python313\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (6.0.2)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (0.4.4)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity>=1.13->semantic-kernel) (2.10.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\python313\\lib\\site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.1.4)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel) (1.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importations OK.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests aiohttp openai tiktoken semantic-kernel python-dotenv anyio httpx httpcore\n",
    "\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import getpass\n",
    "import openai\n",
    "\n",
    "\n",
    "print(\"Importations OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a5637",
   "metadata": {
    "papermill": {
     "duration": 0.020049,
     "end_time": "2026-02-18T08:13:42.633076",
     "exception": false,
     "start_time": "2026-02-18T08:13:42.613027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation de l'installation et des imports\n",
    "\n",
    "Cette cellule prépare **l'environnement Python** nécessaire pour le notebook :\n",
    "\n",
    "**Bibliothèques installées** :\n",
    "\n",
    "| Package | Rôle | Version minimale |\n",
    "|---------|------|------------------|\n",
    "| `requests` | Requêtes HTTP brutes | 2.28+ |\n",
    "| `aiohttp` | Requêtes asynchrones (batching) | 3.8+ |\n",
    "| `openai` | Client officiel OpenAI | 1.0+ |\n",
    "| `tiktoken` | Tokenization (comptage tokens) | 0.5+ |\n",
    "| `semantic-kernel` | Framework orchestration LLM | 0.5+ |\n",
    "| `python-dotenv` | Chargement variables `.env` | 1.0+ |\n",
    "\n",
    "**Imports validés** :\n",
    "\n",
    "- `requests`, `json`, `time` : Tests HTTP basiques\n",
    "- `openai` : Tests avec bibliothèque officielle\n",
    "- `getpass` : Saisie sécurisée d'API keys (si `.env` absent)\n",
    "\n",
    "**Vérification** :\n",
    "\n",
    "Le message \"Importations OK.\" confirme que toutes les bibliothèques sont disponibles. Si une erreur survient, vérifier :\n",
    "\n",
    "1. Environnement virtuel activé\n",
    "2. Pip à jour : `pip install --upgrade pip`\n",
    "3. Connexion internet (pour téléchargement des packages)\n",
    "\n",
    "> **Note** : Les versions spécifiées garantissent la compatibilité avec les endpoints OpenAI-compatibles (vLLM 0.6+, Ollama 0.3+).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e54515",
   "metadata": {
    "papermill": {
     "duration": 0.019452,
     "end_time": "2026-02-18T08:13:42.674592",
     "exception": false,
     "start_time": "2026-02-18T08:13:42.655140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Journalisation colorée\n",
    "\n",
    "Nous allons utiliser un logger (via le module `logging`) configuré avec un\n",
    "**ColorFormatter** pour afficher les messages en couleur dans la console ou la\n",
    "sortie de Jupyter :\n",
    "\n",
    "- Les **informations** et étapes réussies apparaîtront en vert (niveau `INFO`).\n",
    "- Les **erreurs** seront en rouge (niveau `ERROR`).\n",
    "- Les avertissements (`WARNING`) ou messages de debug (`DEBUG`) auront également leurs couleurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff40b5c8",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:13:42.719959Z",
     "iopub.status.busy": "2026-02-18T08:13:42.718975Z",
     "iopub.status.idle": "2026-02-18T08:13:42.736289Z",
     "shell.execute_reply": "2026-02-18T08:13:42.731536Z"
    },
    "papermill": {
     "duration": 0.046369,
     "end_time": "2026-02-18T08:13:42.738459",
     "exception": false,
     "start_time": "2026-02-18T08:13:42.692090",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:42 [INFO] Local Llama - Configuration initiale terminée.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "class ColorFormatter(logging.Formatter):\n",
    "    \"\"\"\n",
    "    Un formatter coloré pour rendre les logs plus lisibles.\n",
    "    \"\"\"\n",
    "    colors = {\n",
    "        'DEBUG': '\\033[94m',\n",
    "        'INFO': '\\033[92m',\n",
    "        'WARNING': '\\033[93m',\n",
    "        'ERROR': '\\033[91m',\n",
    "        'CRITICAL': '\\033[91m\\033[1m'\n",
    "    }\n",
    "    reset = '\\033[0m'\n",
    "\n",
    "    def format(self, record: logging.LogRecord) -> str:\n",
    "        msg = super().format(record)\n",
    "        return f\"{self.colors.get(record.levelname, '')}{msg}{self.reset}\"\n",
    "\n",
    "logger = logging.getLogger(\"Local Llama\")\n",
    "logger.setLevel(logging.DEBUG)  # Peut être paramétré via .env ou variable\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setLevel(logging.DEBUG)\n",
    "    formatter = ColorFormatter(\n",
    "        fmt=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "logger.info(\"Configuration initiale terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49da614",
   "metadata": {
    "papermill": {
     "duration": 0.025215,
     "end_time": "2026-02-18T08:13:42.798226",
     "exception": false,
     "start_time": "2026-02-18T08:13:42.773011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation de la configuration du logger\n",
    "\n",
    "Ce code configure un **logger coloré** pour faciliter le suivi des tests :\n",
    "\n",
    "**Niveaux de log configurés** :\n",
    "\n",
    "| Niveau | Couleur | Usage |\n",
    "|--------|---------|-------|\n",
    "| **DEBUG** | Bleu | Détails techniques (JSON brut, tokens, etc.) |\n",
    "| **INFO** | Vert | Informations normales (succès, résultats) |\n",
    "| **WARNING** | Jaune | Avertissements (réponse inattendue, latence élevée) |\n",
    "| **ERROR** | Rouge | Erreurs (timeout, 401, 500) |\n",
    "| **CRITICAL** | Rouge gras | Erreurs bloquantes |\n",
    "\n",
    "**Avantages** :\n",
    "\n",
    "- **Lisibilité** : Les couleurs permettent de repérer rapidement les erreurs\n",
    "- **Filtrage** : Niveau `DEBUG` pour verbose, `INFO` pour production\n",
    "- **Timestamp** : Format `%H:%M:%S` pour suivre la chronologie\n",
    "\n",
    "**Utilisation** :\n",
    "\n",
    "```python\n",
    "logger.info(\"Test réussi\")       # Vert\n",
    "logger.error(\"Timeout\")          # Rouge\n",
    "logger.debug(\"JSON: {...}\")      # Bleu (détails)\n",
    "```\n",
    "\n",
    "**Configuration** :\n",
    "\n",
    "Le niveau est fixé à `DEBUG` par défaut (tout est affiché). En production, on peut le passer à `INFO` pour réduire le bruit.\n",
    "\n",
    "> **Note** : Le formatter utilise des codes ANSI (`\\033[XXm`), supportés par la plupart des terminaux modernes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c442252",
   "metadata": {
    "papermill": {
     "duration": 0.018796,
     "end_time": "2026-02-18T08:13:42.834425",
     "exception": false,
     "start_time": "2026-02-18T08:13:42.815629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration et définition dynamique des endpoints\n",
    "\n",
    "Pour simplifier la configuration de nos endpoints (URL d’API, clés d’API, modèles, etc.), nous allons externaliser ces informations dans un fichier `.env` placé à la racine de notre projet ou dans un dossier sécurisé. Copiez le fichier .env.example et renommez le fichier résultant en .env avant de le personnaliser.\n",
    "\n",
    "### Déclaration dans `.env`\n",
    "\n",
    "On déclare, par exemple, un premier endpoint dans des variables d’environnement :\n",
    "\n",
    "```\n",
    "OPENAI_ENDPOINT_NAME=OpenAI\n",
    "OPENAI_BASE_URL=https://api.openai.com/v1\n",
    "OPENAI_API_KEY=sk-abcd1234ABCD1423\n",
    "OPENAI_CHAT_MODEL_ID=gpt-5-mini\n",
    "```\n",
    "\n",
    "Et si l’on souhaite tester plusieurs endpoints (ex. mini, medium, large), on ajoute un suffixe `_2`, `_3`... :\n",
    "\n",
    "```\n",
    "OPENAI_ENDPOINT_NAME_2=local-mini\n",
    "OPENAI_BASE_URL_2=https://api.mini.yourdomain.com/v1\n",
    "OPENAI_API_KEY_2=sk-MINI-SECRET-KEY\n",
    "OPENAI_CHAT_MODEL_ID_2=unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit\n",
    "\n",
    "OPENAI_ENDPOINT_NAME_3=medium\n",
    "OPENAI_BASE_URL_3=https://api.medium.text-generation-webui.myia.io/v1\n",
    "OPENAI_API_KEY_3=sk-MEDIUM-SECRET-KEY\n",
    "OPENAI_CHAT_MODEL_ID_3=unsloth/DeepSeek-R1-Distill-Qwen-14B-bnb-4bit\n",
    "```\n",
    "\n",
    "Ainsi, chacun de ces blocs définit un **endpoint** : un service local ou distant OpenAI-compatible (ex. Oobabooga ou vLLM).  \n",
    "\n",
    "### Lecture et création automatique dans le notebook\n",
    "\n",
    "Dans le notebook, nous allons **lire** ces variables pour construire la liste `endpoints`. Chacun contient :\n",
    "\n",
    "- `name` : un label descriptif (ex. `micro`, `mini`, etc.),\n",
    "- `api_base` : l’URL de base de l’API (ex. `https://api.micro.text-generation-webui.myia.io/v1`),\n",
    "- `api_key` : la clé API (fournie par votre conteneur ou config),\n",
    "- `model` (optionnel) : si le modèle n’est pas fourni, nous pourrons interroger `/models` pour récupérer le nom du (ou des) modèle(s) disponibles.\n",
    "\n",
    "Grâce à cette configuration dynamique, on peut aisément **alterner** entre différents backends (p. ex. Oobabooga ou vLLM) ou **interroger plusieurs endpoints** pour **comparer leurs performances**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e653d1b0",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:13:42.877666Z",
     "iopub.status.busy": "2026-02-18T08:13:42.877000Z",
     "iopub.status.idle": "2026-02-18T08:13:42.980651Z",
     "shell.execute_reply": "2026-02-18T08:13:42.973663Z"
    },
    "papermill": {
     "duration": 0.128382,
     "end_time": "2026-02-18T08:13:42.982247",
     "exception": false,
     "start_time": "2026-02-18T08:13:42.853865",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:42 [INFO] Local Llama - === Endpoints chargés ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:42 [INFO] Local Llama - - name=OpenAI, base=https://api.openai.com/v1, key=(len=164), model=gpt-4o-mini\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')  # Charge les variables du fichier .env (dans le répertoire parent GenAI/)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def get_optional_env(var_name, default_value=None):\n",
    "    \"\"\"\n",
    "    Récupère la valeur de la variable d'env `var_name`, \n",
    "    ou la valeur par défaut `default_value` si non définie.\n",
    "    \"\"\"\n",
    "    val = os.getenv(var_name)\n",
    "    if val is None or val.strip() == \"\":\n",
    "        return default_value\n",
    "    return val.strip()\n",
    "\n",
    "def load_endpoint(index=1):\n",
    "    \"\"\"\n",
    "    Lit un ensemble de variables d'environnement.\n",
    "    - index=1 => variables : OPENAI_API_KEY, OPENAI_BASE_URL, etc.\n",
    "    - index>1 => on suffixe : OPENAI_API_KEY_{index}, etc.\n",
    "    Retourne un dict {name, api_base, api_key, model} ou None si 'api_key' manquant.\n",
    "    \"\"\"\n",
    "    suffix = \"\" if index == 1 else f\"_{index}\"\n",
    "\n",
    "    # Lecture des variables\n",
    "    api_key = os.getenv(f\"OPENAI_API_KEY{suffix}\")\n",
    "    if not api_key:\n",
    "        return None  # pas de clé => on arrête\n",
    "\n",
    "    name = get_optional_env(f\"OPENAI_ENDPOINT_NAME{suffix}\", default_value=f\"openai{suffix}\")\n",
    "    base_url = get_optional_env(f\"OPENAI_BASE_URL{suffix}\", default_value=\"https://api.openai.com/v1\")\n",
    "    model_id = get_optional_env(f\"OPENAI_CHAT_MODEL_ID{suffix}\", default_value=None)\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"api_base\": base_url,\n",
    "        \"api_key\": api_key,\n",
    "        \"model\": model_id  # On pourra le compléter si None\n",
    "    }\n",
    "\n",
    "\n",
    "endpoints = []\n",
    "\n",
    "# On tente successivement index=1,2,3... jusqu'à ce qu'on ne trouve plus OPENAI_API_KEY_{i}\n",
    "for i in range(1, 10):  # max 9 endpoints, ajustez si besoin\n",
    "    ep = load_endpoint(i)\n",
    "    if ep is None:\n",
    "        break\n",
    "    endpoints.append(ep)\n",
    "\n",
    "# Vérification (simple)\n",
    "logger.info(\"=== Endpoints chargés ===\")\n",
    "for e in endpoints:\n",
    "    logger.info(f\"- name={e['name']}, base={e['api_base']}, key=(len={len(e['api_key'])}), model={e['model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a62bd",
   "metadata": {
    "papermill": {
     "duration": 0.031878,
     "end_time": "2026-02-18T08:13:43.047232",
     "exception": false,
     "start_time": "2026-02-18T08:13:43.015354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation de la configuration dynamique\n",
    "\n",
    "Le code charge les **endpoints depuis le fichier `.env`** :\n",
    "\n",
    "**Variables détectées** :\n",
    "\n",
    "Pour chaque suffixe (`_1`, `_2`, `_3`...) :\n",
    "\n",
    "- `OPENAI_ENDPOINT_NAME_X` : Nom descriptif (ex: \"mini\", \"medium\", \"large\")\n",
    "- `OPENAI_BASE_URL_X` : URL de l'API (ex: `https://api.mini.myia.io/v1`)\n",
    "- `OPENAI_API_KEY_X` : Bearer token d'authentification\n",
    "- `OPENAI_CHAT_MODEL_ID_X` : Identifiant du modèle (ex: `DeepSeek-R1-Distill-Llama-8B`)\n",
    "\n",
    "**Structure de `endpoints`** :\n",
    "\n",
    "```python\n",
    "endpoints = [\n",
    "    {\n",
    "        \"name\": \"mini\",\n",
    "        \"api_base\": \"https://api.mini.myia.io/v1\",\n",
    "        \"api_key\": \"sk-MINI-SECRET\",\n",
    "        \"model\": \"unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit\"\n",
    "    },\n",
    "    # ...\n",
    "]\n",
    "```\n",
    "\n",
    "**Avantages de cette approche** :\n",
    "\n",
    "- **Centralisation** : Un seul fichier `.env` pour toute la configuration\n",
    "- **Sécurité** : API keys ne sont jamais commitées (`.env` dans `.gitignore`)\n",
    "- **Flexibilité** : Ajouter/supprimer des endpoints sans modifier le code\n",
    "- **Multi-environnement** : Dev/staging/prod avec des `.env` différents\n",
    "\n",
    "**Cas d'usage** :\n",
    "\n",
    "- Tester plusieurs serveurs vLLM simultanément\n",
    "- Comparer performances cloud (OpenAI) vs local\n",
    "- Load balancing manuel entre endpoints\n",
    "\n",
    "> **Important** : Toujours copier `.env.example` vers `.env` et ne jamais committer `.env`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae719d",
   "metadata": {
    "papermill": {
     "duration": 0.0204,
     "end_time": "2026-02-18T08:13:43.086798",
     "exception": false,
     "start_time": "2026-02-18T08:13:43.066398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inspection des modèles disponibles\n",
    "\n",
    "Nous allons appeler l'endpoint `/models` de chaque service \n",
    "pour récupérer la liste des modèles chargés côté serveur.\n",
    "\n",
    "Si vous avez mis `model=None` dans la config, vous pourrez automatiquement \n",
    "récupérer le `model` à partir des données renvoyées. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55fd052b",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:13:43.120574Z",
     "iopub.status.busy": "2026-02-18T08:13:43.120058Z",
     "iopub.status.idle": "2026-02-18T08:13:44.050558Z",
     "shell.execute_reply": "2026-02-18T08:13:44.049546Z"
    },
    "papermill": {
     "duration": 0.95192,
     "end_time": "2026-02-18T08:13:44.051949",
     "exception": false,
     "start_time": "2026-02-18T08:13:43.100029",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:43 [INFO] Local Llama - === OpenAI : /models ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:44 [DEBUG] Local Llama - Réponse brute (tronquée): {\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"gpt-4-0613\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1686588896,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1687882411,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1677610602,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.2-codex\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1766164985,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-tts-2025-12-15\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765610837,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime-mini-2025-12-15\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765612007,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio-mini-2025-12-15\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765760008,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"chatgpt-image-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765925279,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"davinci-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692634301,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"babbage-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692634615,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-instruct\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692901427,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-instruct-0914\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1694122472,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"dall-e-3\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698785189,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"dall-e-2\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698798177,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-1106-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698957206,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698959748,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-hd\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699046015,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699053241,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-hd-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699053533,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-3-small\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1705948997,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-3-large\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1705953180,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-0125-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706037612,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706037777,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-0125\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706048358,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1712361441,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo-2024-04-09\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1712601677,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1715367049,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-05-13\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1715368132,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-2024-07-18\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1721172717,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1721172741,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-08-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1722814719,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727460443,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727659998,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"omni-moderation-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1731689265,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"omni-moderation-2024-09-26\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1732734466,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1733945430,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734034239,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-realtime-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734112601,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-audio-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734115920,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734326976,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-realtime-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734387380,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-audio-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734387424,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"computer-use-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734655677,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1737146383,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-mini-2025-01-31\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1738010200,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-11-20\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1739331543,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"computer-use-preview-2025-03-11\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741377021,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-search-preview-2025-03-11\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741388170,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-search-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741388720,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-search-preview-2025-03-11\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741390858,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-search-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741391161,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-transcribe\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742068463,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-transcribe\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742068596,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-pro-2025-03-19\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742251504,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-pro\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742251791,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-tts\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742403959,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-2025-04-16\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744133301,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini-2025-04-16\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744133506,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744225308,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744225351,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-2025-04-14\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744315746,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744316542,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-mini-2025-04-14\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744317547,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744318173,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-nano-2025-04-14\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744321025,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-nano\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744321707,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-image-1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1745517030,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-pro\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1748475349,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview-2025-06-03\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1748907838,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview-2025-06-03\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1748908498,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-pro-2025-06-10\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1749166761,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini-deep-research\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1749685485,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-deep-research\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1749840121,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-transcribe-diarize\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1750798887,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-deep-research-2025-06-26\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1750865219,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini-deep-research-2025-06-26\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1750866121,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-chat-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754073306,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-2025-08-07\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754075360,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754425777,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-mini-2025-08-07\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754425867,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754425928,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-nano-2025-08-07\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754426303,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-nano\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754426384,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio-2025-08-28\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756256146,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756271701,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime-2025-08-28\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756271773,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756339249,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-codex\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1757527818,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-image-1-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1758845821,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-pro-2025-10-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759469707,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-pro\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759469822,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759512027,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio-mini-2025-10-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759512137,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-search-api\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759514629,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759517133,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime-mini-2025-10-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759517175,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"sora-2\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759708615,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"sora-2-pro\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759708663,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-search-api-2025-10-14\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1760043960,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.1-chat-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1762547951,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.1-2025-11-13\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1762800353,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1762800673,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.1-codex\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1762988221,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.1-codex-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1763007109,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.1-codex-max\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1763671532,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-image-1.5\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1764030620,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.2-2025-12-11\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765313028,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.2\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765313051,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.2-pro-2025-12-11\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765343959,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.2-pro\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765343983,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5.2-chat-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765344352,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-transcribe-2025-12-15\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765610407,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-transcribe-2025-03-20\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765610545,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-tts-2025-03-20\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765610731,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-16k\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1683758102,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1681940951,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"whisper-1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1677532384,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-ada-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1671217299,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    }\n",
      "  ]\n",
      "}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:44 [INFO] Local Llama - Réussite: 118 modèle(s) listé(s) (endpoint=OpenAI)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:44 [INFO] Local Llama -   -> Temps de réponse: 0.91 secondes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def shrink_json(obj, skip_keys=None, max_str=500, _level=0, max_level=4):\n",
    "    if skip_keys is None:\n",
    "        skip_keys = set()\n",
    "\n",
    "    if _level >= max_level:\n",
    "        return \"... (nested)\"\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        new_dict = {}\n",
    "        for k, v in obj.items():\n",
    "            if k in skip_keys:\n",
    "                new_dict[k] = f\"(skipped large data for key: {k})\"\n",
    "            else:\n",
    "                new_dict[k] = shrink_json(v, skip_keys, max_str, _level+1, max_level)\n",
    "        return new_dict\n",
    "\n",
    "    elif isinstance(obj, list):\n",
    "        return [\n",
    "            shrink_json(x, skip_keys, max_str, _level+1, max_level)\n",
    "            for x in obj\n",
    "        ]\n",
    "\n",
    "    elif isinstance(obj, str):\n",
    "        if len(obj) > max_str:\n",
    "            return obj[:max_str] + \"... (truncated)\"\n",
    "        else:\n",
    "            return obj\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def list_models(api_base, api_key):\n",
    "    url = f\"{api_base}/models\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=20)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.json()  # dict\n",
    "        else:\n",
    "            return {\"error\": f\"status={resp.status_code}\", \"text\": resp.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def update_endpoints_with_model():\n",
    "    for ep in endpoints:\n",
    "        logger.info(f\"=== {ep['name']} : /models ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        info = list_models(ep[\"api_base\"], ep[\"api_key\"])\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # On \"rétrécit\" le JSON pour éviter d'afficher les champs volumineux\n",
    "        truncated_info = shrink_json(\n",
    "            info,\n",
    "            skip_keys={\"profile_image_url\", \"raw_modelfile_content\"},\n",
    "            max_str=1000  # Tronque les chaînes > 1000 chars\n",
    "        )\n",
    "\n",
    "        # On journalise la version tronquée ou partiellement ignorée en DEBUG\n",
    "        logger.debug(\"Réponse brute (tronquée): %s\",\n",
    "                     json.dumps(truncated_info, indent=2, ensure_ascii=False))\n",
    "\n",
    "        if \"error\" in info:\n",
    "            # En cas d'erreur, on logue au niveau ERROR\n",
    "            logger.error(\n",
    "                f\"Échec de récupération /models (endpoint={ep['name']}): \"\n",
    "                f\"{info['error']}, texte={info.get('text', '')}\"\n",
    "            )\n",
    "        else:\n",
    "            # Succès : on peut afficher le nombre de modèles\n",
    "            data_models = info.get(\"data\", [])\n",
    "            logger.info(\n",
    "                f\"Réussite: {len(data_models)} modèle(s) listé(s) \"\n",
    "                f\"(endpoint={ep['name']})\"\n",
    "            )\n",
    "\n",
    "        logger.info(f\"  -> Temps de réponse: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "        # On met à jour ep[\"model\"] si besoin\n",
    "        if \"error\" not in info and (\"model\" not in ep or not ep[\"model\"]):\n",
    "            data_list = info.get(\"data\", [])\n",
    "            if data_list:\n",
    "                first_model_id = data_list[0].get(\"id\")\n",
    "                ep[\"model\"] = first_model_id\n",
    "                logger.info(f\"  -> ep['model'] défini à: {first_model_id}\")\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    \"  -> Aucune entrée 'data' dans la réponse pour définir ep['model']\"\n",
    "                )\n",
    "\n",
    "# Test\n",
    "update_endpoints_with_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e2b31",
   "metadata": {
    "papermill": {
     "duration": 0.008581,
     "end_time": "2026-02-18T08:13:44.069247",
     "exception": false,
     "start_time": "2026-02-18T08:13:44.060666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse de la réponse /models\n",
    "\n",
    "L'appel à l'endpoint `/models` permet d'**inspecter les modèles disponibles** sur chaque serveur :\n",
    "\n",
    "**Informations récupérées** :\n",
    "1. **Liste des modèles** : Identifiants (ex: `unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit`)\n",
    "2. **Métadonnées** : Type de modèle, capacités, versions\n",
    "3. **Statut** : Modèle chargé et prêt à recevoir des requêtes\n",
    "\n",
    "**Observations clés** :\n",
    "- **Temps de réponse** : Indicateur de la latence réseau et de la charge du serveur\n",
    "- **Nombre de modèles** : Certains serveurs hébergent plusieurs modèles simultanément\n",
    "- **Erreurs éventuelles** : Timeout, authentification échouée, serveur indisponible\n",
    "\n",
    "**Utilité pratique** :\n",
    "- **Auto-configuration** : Si `model=None` dans le config, on peut récupérer automatiquement le premier modèle disponible\n",
    "- **Monitoring** : Vérifier que le serveur répond correctement\n",
    "- **Découverte** : Lister les modèles accessibles sans connaître leurs noms à l'avance\n",
    "\n",
    "**Champs potentiellement volumineux ignorés** :\n",
    "- `profile_image_url` (images base64)\n",
    "- `raw_modelfile_content` (configurations très longues)\n",
    "\n",
    "Cela évite de saturer les logs tout en conservant les informations essentielles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b5dcb6",
   "metadata": {
    "papermill": {
     "duration": 0.011233,
     "end_time": "2026-02-18T08:13:44.088979",
     "exception": false,
     "start_time": "2026-02-18T08:13:44.077746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test brut via `requests.post`\n",
    "\n",
    "Ce test vérifie le bon fonctionnement de l'endpoint OpenAI-compatible \n",
    "sans passer par la librairie `openai`. \n",
    "\n",
    "On envoie une requête minimaliste en JSON, \n",
    "puis on affiche la réponse brute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff7f215d",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:13:44.116259Z",
     "iopub.status.busy": "2026-02-18T08:13:44.115285Z",
     "iopub.status.idle": "2026-02-18T08:13:45.207008Z",
     "shell.execute_reply": "2026-02-18T08:13:45.205821Z"
    },
    "papermill": {
     "duration": 1.109829,
     "end_time": "2026-02-18T08:13:45.210562",
     "exception": false,
     "start_time": "2026-02-18T08:13:44.100733",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:44 [INFO] Local Llama - \n",
      "=== Test HTTP brut pour OpenAI ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:44 [DEBUG] Local Llama -   -> Envoi de la requête POST...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:45 [DEBUG] Local Llama -   -> Statut HTTP: 200 (durée=1.07s)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:45 [DEBUG] Local Llama - Réponse (début): {\n",
      "  \"id\": \"chatcmpl-DAWynttJjTCx4s4nt5wMTdjKV1KWC\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1771402425,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Bonjour ! Je suis un assistant virtuel, ici pour r\\u00e9pondre \\u00e0 vos questions et vous aider avec divers sujets. Comment puis-je vous aider aujourd'hui ?\",\n",
      "        \"refusal\": null,\n",
      "        \"annotations\": []\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 13,\n",
      "    \"completion_tokens\": 30,\n",
      "    \"total_tokens\": 43,\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"cached_tokens\": 0,\n",
      "      \"audio_tokens\": 0\n",
      "    },\n",
      "    \"completion_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"service_tier\": \"default\",\n",
      "  \"system_fingerprint\": \"fp_373a14eb6f\"\n",
      "}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:45 [INFO] Local Llama - [OK] Réponse HTTP 200 en 1.07s, Tokens utilisés=43\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def test_brut_endpoints():\n",
    "    \"\"\"Test brut via requests.post() sur tous les endpoints.\"\"\"\n",
    "    for ep in endpoints:\n",
    "        logger.info(f\"\\n=== Test HTTP brut pour {ep['name']} ===\")\n",
    "        \n",
    "        url = f\"{ep['api_base']}/chat/completions\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {ep['api_key']}\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": ep[\"model\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"Bonjour, qui es-tu ?\"}\n",
    "            ],\n",
    "            \"max_completion_tokens\": 200\n",
    "        }\n",
    "        \n",
    "        logger.debug(\"  -> Envoi de la requête POST...\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            resp = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            logger.debug(f\"  -> Statut HTTP: {resp.status_code} (durée={elapsed_time:.2f}s)\")\n",
    "            \n",
    "            # On essaie d'obtenir le JSON de la réponse\n",
    "            try:\n",
    "                resp_json = resp.json()\n",
    "            except json.JSONDecodeError:\n",
    "                logger.error(f\"Réponse non-JSON:\\n{resp.text[:200]}\")\n",
    "                continue\n",
    "            \n",
    "            # Affichage d’un extrait du JSON (en DEBUG, car potentiellement verbeux)\n",
    "            dumped_json = json.dumps(resp_json, indent=2)\n",
    "            logger.debug(f\"Réponse (début): {dumped_json}\")\n",
    "\n",
    "            # Nombre de tokens si success\n",
    "            tokens_used = None\n",
    "            if resp.status_code == 200:\n",
    "                if \"usage\" in resp_json:\n",
    "                    tokens_used = resp_json[\"usage\"].get(\"total_tokens\")\n",
    "                logger.info(\n",
    "                    f\"[OK] Réponse HTTP 200 en {elapsed_time:.2f}s, \"\n",
    "                    f\"Tokens utilisés={tokens_used if tokens_used else 'N/A'}\"\n",
    "                )\n",
    "            else:\n",
    "                # On log au niveau ERROR pour signifier un souci\n",
    "                logger.error(\n",
    "                    f\"[ERREUR] HTTP {resp.status_code}, texte={resp_json.get('message', resp.text)}\"\n",
    "                )\n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.error(\"Timeout après 30s.\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Exception lors de la requête: {str(e)}\")\n",
    "\n",
    "# On exécute le test brut\n",
    "test_brut_endpoints()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24f5e2",
   "metadata": {
    "papermill": {
     "duration": 0.01102,
     "end_time": "2026-02-18T08:13:45.231428",
     "exception": false,
     "start_time": "2026-02-18T08:13:45.220408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du test HTTP brut\n",
    "\n",
    "Ce test vérifie la **connectivité de base** avec chaque endpoint :\n",
    "\n",
    "**Points testés** :\n",
    "\n",
    "1. **Authentification** : Bearer token accepté\n",
    "2. **Format requête** : JSON `messages` + `model` + `max_completion_tokens`\n",
    "3. **Statut HTTP** : 200 OK vs erreurs (401, 404, 500, timeout)\n",
    "4. **Parsing JSON** : Réponse structurée correctement\n",
    "\n",
    "**Résultats attendus** :\n",
    "\n",
    "| Endpoint | Statut | Tokens utilisés | Latence |\n",
    "|----------|--------|-----------------|---------|\n",
    "| **OpenAI cloud** | 200 OK | ~50-100 | 1-2s |\n",
    "| **Local vLLM** | 200 OK | ~50-100 | 0.5-1.5s |\n",
    "| **Local Ollama** | 200 OK | ~50-100 | 1-3s |\n",
    "\n",
    "**Diagnostics des erreurs** :\n",
    "\n",
    "- **Timeout 30s** : Modèle non chargé en VRAM, serveur surchargé, ou réseau lent\n",
    "- **HTTP 401** : API key invalide ou Bearer token manquant\n",
    "- **HTTP 404** : URL incorrecte ou modèle introuvable\n",
    "- **JSON parsing error** : Réponse HTML (erreur serveur) au lieu de JSON\n",
    "\n",
    "**Métriques de santé** :\n",
    "\n",
    "- **Latence < 3s** : Bon (modèle déjà en VRAM)\n",
    "- **Latence > 10s** : Problème (cold start ou charge élevée)\n",
    "- **Tokens utilisés** : Valide que le modèle génère bien une réponse\n",
    "\n",
    "> **Note** : Ce test utilise `requests.post` brut pour éviter toute dépendance à une bibliothèque OpenAI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae14b897",
   "metadata": {
    "papermill": {
     "duration": 0.008534,
     "end_time": "2026-02-18T08:13:45.248869",
     "exception": false,
     "start_time": "2026-02-18T08:13:45.240335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokens et logits\n",
    "\n",
    "### Tests de Tokenizers\n",
    "\n",
    "Mise en place des fonctions utilitaires de **tokenization** (via `tiktoken` ou l'API `/tokenize` de vLLM), ainsi que quelques helpers pour **dé/retokenizer** en local (via l'API `/detokenize`) et pour **debugger** la segmentation et la correspondance de chaque token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac68009",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:13:45.280323Z",
     "iopub.status.busy": "2026-02-18T08:13:45.279590Z",
     "iopub.status.idle": "2026-02-18T08:13:46.050823Z",
     "shell.execute_reply": "2026-02-18T08:13:46.050029Z"
    },
    "papermill": {
     "duration": 0.791795,
     "end_time": "2026-02-18T08:13:46.051994",
     "exception": false,
     "start_time": "2026-02-18T08:13:45.260199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:45 [INFO] Local Llama - === Test Tokenizer API (ou local tiktoken) pour endpoint 'OpenAI' ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:45 [INFO] Local Llama - Endpoint='https://api.openai.com/v1' => API OpenAI OFFICIELLE => usage local de tiktoken.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:45 [INFO] Local Llama - Nombre de tokens (tiktoken): 30\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:45 [INFO] Local Llama - Tokens générés: [45751, 1073, 4678, 15058, 537, 29186, 5824, 4619, 11, 109492, 1930, 53201, 1221, 10458, 5359, 859, 3500, 41770, 3937, 73470, 33951, 13, 15406, 18766, 74553, 3500, 41770, 32226, 43820, 1423]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:45 [INFO] Local Llama - \n",
      "--- Endpoint: OpenAI ---\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== sample text: '</think>Wait,Alternatively,Hmm,But.\n",
      "But wait,  But let me think again. Wait, Alternatively, Hmm, ' ===\n",
      "\n",
      "=== Test sur endpoint 'OpenAI' ===\n",
      "[tokenize_vllm] Erreur 404 => \n",
      "Pas de tokens ou échec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Endpoint='https://api.openai.com/v1' => API OpenAI OFFICIELLE => usage local de tiktoken.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Nombre de tokens (tiktoken): 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Endpoint='https://api.openai.com/v1' => API OpenAI OFFICIELLE => usage local de tiktoken.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Nombre de tokens (tiktoken): 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Endpoint='https://api.openai.com/v1' => API OpenAI OFFICIELLE => usage local de tiktoken.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Nombre de tokens (tiktoken): 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Endpoint='https://api.openai.com/v1' => API OpenAI OFFICIELLE => usage local de tiktoken.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Nombre de tokens (tiktoken): 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Pour le mot 'je', variantes ['je', ' je', 'Je', ' Je'] -> token IDs: {1264, 4678, 1587, 11302}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Token IDs pour 'je' sur 'OpenAI': {1264, 4678, 1587, 11302}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tokenize_vllm] Erreur 404 => \n",
      "Pas de tokens ou échec.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import tiktoken\n",
    "import time\n",
    "import logging\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Helpers pour Tokenization\n",
    "# ========================\n",
    "def tokenize_sentence(api_base, api_key, sentence, model_fallback=\"gpt-5-mini\"):\n",
    "    \"\"\"\n",
    "    Tokenise 'sentence' en utilisant l'API /tokenize d'un backend vLLM ou, si l'API\n",
    "    semble être celle d'OpenAI/Azure, utilise la librairie tiktoken.\n",
    "    \"\"\"\n",
    "    official_providers = (\"openai.com\", \"azure.com\")\n",
    "    if any(provider in api_base for provider in official_providers):\n",
    "        logger.info(\"Endpoint='%s' => API OpenAI OFFICIELLE => usage local de tiktoken.\", api_base)\n",
    "        try:\n",
    "            enc = tiktoken.encoding_for_model(model_fallback)\n",
    "        except Exception:\n",
    "            logger.warning(\"Modèle tiktoken inconnu (%s), fallback sur 'cl100k_base'.\", model_fallback)\n",
    "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        token_ids = enc.encode(sentence)\n",
    "        logger.info(f\"Nombre de tokens (tiktoken): {len(token_ids)}\")\n",
    "        return token_ids\n",
    "\n",
    "    logger.info(\"Endpoint='%s' => vLLM local => on utilise POST /tokenize\", api_base)\n",
    "    api_base_sanitized = re.sub(r\"/v1/?$\", \"\", api_base.rstrip(\"/\"))\n",
    "    url = f\"{api_base_sanitized}/tokenize\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\" if api_key else \"\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\"prompt\": sentence}\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=10)\n",
    "        if resp.status_code == 200:\n",
    "            data = resp.json()\n",
    "            tokens = data.get(\"tokens\", [])\n",
    "            count = data.get(\"count\", 0)\n",
    "            logger.info(f\"Nombre de tokens (vLLM): {count}\")\n",
    "            return tokens\n",
    "        else:\n",
    "            logger.error(f\"Erreur Tokenizer API: {resp.status_code}, {resp.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception lors de l'appel à /tokenize: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def tokenize_vllm(api_base, api_key, text):\n",
    "    \"\"\"\n",
    "    Appelle POST /tokenize sur un endpoint vLLM local.\n",
    "    Retourne la liste d'IDs ou None en cas d'erreur.\n",
    "    \"\"\"\n",
    "    base = re.sub(r\"/v1/?$\", \"\", api_base.rstrip(\"/\"))\n",
    "    url = f\"{base}/tokenize\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\" if api_key else \"\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\"prompt\": text}\n",
    "    resp = requests.post(url, headers=headers, json=payload, timeout=10)\n",
    "    if resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        return data.get(\"tokens\", None)\n",
    "    else:\n",
    "        print(f\"[tokenize_vllm] Erreur {resp.status_code} => {resp.text}\")\n",
    "        return None\n",
    "\n",
    "def detokenize_vllm(api_base, api_key, token_ids):\n",
    "    \"\"\"\n",
    "    Appelle POST /detokenize sur un endpoint vLLM local.\n",
    "    Retourne la chaîne correspondant à la liste de tokens.\n",
    "    \"\"\"\n",
    "    base = re.sub(r\"/v1/?$\", \"\", api_base.rstrip(\"/\"))\n",
    "    url = f\"{base}/detokenize\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\" if api_key else \"\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\"tokens\": token_ids}\n",
    "    resp = requests.post(url, headers=headers, json=payload, timeout=10)\n",
    "    if resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        return data.get(\"prompt\", \"\")\n",
    "    else:\n",
    "        print(f\"[detokenize_vllm] Erreur {resp.status_code} => {resp.text}\")\n",
    "        return None\n",
    "\n",
    "def debug_token_mapping(api_base, api_key, text):\n",
    "    \"\"\"\n",
    "    Tokenise 'text' et affiche pour chaque token son fragment via /detokenize.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_vllm(api_base, api_key, text)\n",
    "    if not tokens:\n",
    "        print(\"Pas de tokens ou échec.\")\n",
    "        return\n",
    "    print(f\"Liste des token_ids: {tokens}\")\n",
    "    print(f\"Nombre de tokens = {len(tokens)}\")\n",
    "    for idx, tid in enumerate(tokens):\n",
    "        sub = detokenize_vllm(api_base, api_key, [tid])\n",
    "        print(f\"[{idx}] token_id={tid} => {repr(sub)}\")\n",
    "\n",
    "def get_token_id_for_word(api_base, api_key, word, model_fallback=\"gpt-5-mini\"):\n",
    "    \"\"\"\n",
    "    Retourne le premier token ID obtenu en tokenisant 'word' sur l'endpoint donné.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_sentence(api_base, api_key, word, model_fallback)\n",
    "    if tokens and len(tokens) > 0:\n",
    "        return tokens[0]\n",
    "    else:\n",
    "        logger.warning(f\"Aucun token obtenu pour le mot '{word}' sur l'endpoint '{api_base}'.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============= Exemples d'utilisation =============\n",
    "# 1) Test rapide de la \"tokenize_sentence\" sur tous endpoints\n",
    "sampleSentence = \"Bonjour ! Je suis un assistant virtuel, conçu pour répondre à vos questions et vous aider avec diverses informations. Comment puis-je vous aider aujourd'hui ?\"\n",
    "for ep in endpoints:\n",
    "    logger.info(f\"=== Test Tokenizer API (ou local tiktoken) pour endpoint '{ep['name']}' ===\")\n",
    "    tokens = tokenize_sentence(ep[\"api_base\"], ep[\"api_key\"], sampleSentence)\n",
    "    if tokens:\n",
    "        logger.info(f\"Tokens générés: {tokens}\")\n",
    "    else:\n",
    "        logger.warning(f\"Échec de la tokenisation pour l'endpoint '{ep['name']}'.\")\n",
    "\n",
    "# 2) Test plus avancé: debug_token_mapping\n",
    "sample_text = \"</think>Wait,Alternatively,Hmm,But.\\nBut wait,  But let me think again. Wait, Alternatively, Hmm, \"\n",
    "print(f\"\\n=== sample text: '{sample_text}' ===\")\n",
    "for ep in endpoints:\n",
    "    print(f\"\\n=== Test sur endpoint '{ep['name']}' ===\")\n",
    "    debug_token_mapping(ep[\"api_base\"], ep[\"api_key\"], sample_text)\n",
    "\n",
    "\n",
    "\n",
    "def get_token_ids_for_variants(api_base, api_key, word, model_fallback=\"gpt-5-mini\"):\n",
    "    \"\"\"\n",
    "    Pour un mot donné, retourne un ensemble des token IDs correspondant aux variantes\n",
    "    possibles : sans espace, avec espace en préfixe, et avec majuscule/minuscule.\n",
    "    Exemple pour \"je\": [\"je\", \" Je\", \"Je\", \" je\"].\n",
    "    \"\"\"\n",
    "    variants = [word, \" \" + word, word.capitalize(), \" \" + word.capitalize()]\n",
    "    token_ids = set()\n",
    "    for variant in variants:\n",
    "        tokens = tokenize_sentence(api_base, api_key, variant, model_fallback)\n",
    "        if tokens and len(tokens) > 0:\n",
    "            token_ids.add(tokens[0])\n",
    "    if not token_ids:\n",
    "        logger.warning(f\"Aucun token obtenu pour les variantes du mot '{word}' sur l'endpoint '{api_base}'.\")\n",
    "    else:\n",
    "        logger.info(f\"Pour le mot '{word}', variantes {variants} -> token IDs: {token_ids}\")\n",
    "    return token_ids\n",
    "\n",
    "# Exemple d'utilisation pour vérifier la tokenisation de \"je\"\n",
    "sample_word = \"je\"\n",
    "for ep in endpoints:\n",
    "    logger.info(f\"\\n--- Endpoint: {ep['name']} ---\")\n",
    "    debug_token_mapping(ep[\"api_base\"], ep[\"api_key\"], sample_word)\n",
    "    # Affiche les token IDs pour les variantes de \"je\"\n",
    "    ids = get_token_ids_for_variants(ep[\"api_base\"], ep[\"api_key\"], sample_word)\n",
    "    logger.info(f\"Token IDs pour '{sample_word}' sur '{ep['name']}': {ids}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabb8dd",
   "metadata": {
    "papermill": {
     "duration": 0.0102,
     "end_time": "2026-02-18T08:13:46.071326",
     "exception": false,
     "start_time": "2026-02-18T08:13:46.061126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du debug de tokenisation\n",
    "\n",
    "La fonction `debug_token_mapping` affiche la correspondance exacte **token_id ↔ fragment de texte** :\n",
    "\n",
    "**Exemple de sortie attendu** :\n",
    "```\n",
    "[0] token_id=1234 => '</think>'\n",
    "[1] token_id=5678 => 'Wait'\n",
    "[2] token_id=9012 => ','\n",
    "[3] token_id=3456 => 'Alternatively'\n",
    "...\n",
    "```\n",
    "\n",
    "**Enseignements** :\n",
    "\n",
    "1. **Tokens spéciaux** : `</think>`, `Wait`, `Hmm` peuvent être des tokens uniques ou décomposés selon le modèle\n",
    "2. **Espaces et ponctuation** : Notez comment les espaces sont traités (parfois fusionnés avec les mots, parfois séparés)\n",
    "3. **Variabilité** : Le même texte peut produire des tokens différents selon l'endpoint (Llama vs Qwen vs GPT)\n",
    "\n",
    "**Pourquoi c'est important ?**\n",
    "\n",
    "- **Logit bias** : Pour biaiser un token, il faut connaître son ID exact\n",
    "- **Optimization** : Certains modèles tokenisent plus efficacement (moins de tokens = moins de coût)\n",
    "- **Debugging** : Comprendre pourquoi un prompt produit un nombre de tokens inattendu\n",
    "\n",
    "**Cas d'usage** :\n",
    "- Forcer ou interdire certains mots dans la génération (via `logit_bias`)\n",
    "- Analyser les différences de tokenisation entre modèles\n",
    "- Optimiser les prompts pour réduire la consommation de tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28408d",
   "metadata": {
    "papermill": {
     "duration": 0.009483,
     "end_time": "2026-02-18T08:13:46.090269",
     "exception": false,
     "start_time": "2026-02-18T08:13:46.080786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse des résultats logit_bias\n",
    "\n",
    "Le test `logit_bias_consistency` évalue **3 aspects critiques** :\n",
    "\n",
    "**1. Impact du logit_bias**\n",
    "- Un `logit_bias` négatif (-100.0) sur les tokens de \"je\" devrait **fortement pénaliser** leur apparition\n",
    "- La réponse AVEC bias devrait éviter d'utiliser \"je\" (ou ses variantes)\n",
    "- Exemple attendu : \"Bonjour ! En tant qu'assistant...\" au lieu de \"Bonjour, je suis...\"\n",
    "\n",
    "**2. Cohérence avec seed fixée**\n",
    "- Deux appels identiques (`temperature=1.0`, `seed=42`) SANS bias devraient retourner la **même réponse**\n",
    "- Si les réponses diffèrent, cela indique un problème de reproductibilité du serveur\n",
    "\n",
    "**3. Validation du mécanisme**\n",
    "- Si AVEC bias = SANS bias, le logit_bias n'a pas fonctionné (bug serveur ou modèle non compatible)\n",
    "- Si AVEC bias ≠ SANS bias ET cohérence OK, le test est réussi\n",
    "\n",
    "**Codes couleur des logs** :\n",
    "- **Vert (INFO)** : Cohérence vérifiée, différence détectée → succès\n",
    "- **Rouge (ERROR)** : Incohérence entre deux appels sans bias → problème\n",
    "- **Orange (WARNING)** : Pas de différence détectée → logit_bias inefficace\n",
    "\n",
    "**Implications** :\n",
    "- Certains modèles locaux (Llama, Qwen) peuvent ne pas supporter `logit_bias`\n",
    "- OpenAI officiel supporte bien ce paramètre\n",
    "- Utile pour contrôler le vocabulaire (ex: interdire des mots sensibles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b15b5a",
   "metadata": {
    "papermill": {
     "duration": 0.008751,
     "end_time": "2026-02-18T08:13:46.107892",
     "exception": false,
     "start_time": "2026-02-18T08:13:46.099141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Test de requête avec des logit_bias\n",
    "\n",
    "Nous présentons à présent un **exemple de test** pour le paramètre [`logit_bias`](https://platform.openai.com/docs/api-reference/chat/create#chat-create-logit_bias). Le principe : envoyer deux requêtes identiques vers chaque endpoint :\n",
    "\n",
    "1. L’une comporte un champ `logit_bias` qui favorise ou pénalise un certain token ID.  \n",
    "2. L’autre n’a pas de `logit_bias`.\n",
    "\n",
    "On compare ensuite les réponses pour voir si l’application du biais a un effet tangible sur la génération. Dans la pratique, vous devrez adapter la valeur du token ID ciblé (`\"13824\"` ci-dessous) à la sortie que vous aurez obtenue dans la cellule précédente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "340a11c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:13:46.126460Z",
     "iopub.status.busy": "2026-02-18T08:13:46.126013Z",
     "iopub.status.idle": "2026-02-18T08:13:49.176842Z",
     "shell.execute_reply": "2026-02-18T08:13:49.175831Z"
    },
    "papermill": {
     "duration": 3.061994,
     "end_time": "2026-02-18T08:13:49.178376",
     "exception": false,
     "start_time": "2026-02-18T08:13:46.116382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - === Test de logit_bias avec vérification de cohérence et concaténation ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - \n",
      "=== Test logit_bias sur endpoint 'OpenAI' ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Endpoint='https://api.openai.com/v1' => API OpenAI OFFICIELLE => usage local de tiktoken.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Nombre de tokens (tiktoken): 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Endpoint='https://api.openai.com/v1' => API OpenAI OFFICIELLE => usage local de tiktoken.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Nombre de tokens (tiktoken): 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Endpoint='https://api.openai.com/v1' => API OpenAI OFFICIELLE => usage local de tiktoken.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Nombre de tokens (tiktoken): 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Endpoint='https://api.openai.com/v1' => API OpenAI OFFICIELLE => usage local de tiktoken.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Nombre de tokens (tiktoken): 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Endpoint='https://api.openai.com/v1' => API OpenAI OFFICIELLE => usage local de tiktoken.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Nombre de tokens (tiktoken): 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Pour le mot 'je', variantes ['je', ' je', 'Je', ' Je', 'Bonjour'] -> token IDs: {4678, 11302, 1264, 1587, 45751}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:46 [INFO] Local Llama - Logit_bias appliqué: {'4678': -100.0, '11302': -100.0, '1264': -100.0, '1587': -100.0, '45751': -100.0}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:46 [DEBUG] Local Llama - Envoi de la requête AVEC logit_bias...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:47 [DEBUG] Local Llama - Statut HTTP avec logit_bias: 200\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:47 [INFO] Local Llama - Réponse avec logit_bias: Bonjour ! J suis un assistant virtuel, ici pour répondre à vos questions et vous aider dans la mesure du possible. Comment puis-je vous aider aujourd'hui ?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:47 [DEBUG] Local Llama - Envoi de la première requête SANS logit_bias...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:48 [DEBUG] Local Llama - Statut HTTP sans logit_bias (1): 200\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:48 [INFO] Local Llama - Réponse sans logit_bias (1): Bonjour ! Je suis un assistant virtuel, ici pour répondre à vos questions et vous aider dans la mesure du possible. Que puis-je faire pour vous aujourd'hui ?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:48 [DEBUG] Local Llama - Envoi de la deuxième requête SANS logit_bias...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:49 [DEBUG] Local Llama - Statut HTTP sans logit_bias (2): 200\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:49 [INFO] Local Llama - Réponse sans logit_bias (2): Bonjour ! Je suis un assistant virtuel, ici pour répondre à vos questions et vous aider dans divers sujets. Comment puis-je vous assister aujourd'hui ?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91m09:13:49 [ERROR] Local Llama - => ERREUR : Les deux appels sans logit_bias avec la même seed renvoient des réponses différentes!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:49 [INFO] Local Llama - => Différence détectée entre AVEC et SANS logit_bias.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def combine_message(msg):\n",
    "    \"\"\"\n",
    "    Concatène 'reasoning_content' et 'content' d'un message pour obtenir\n",
    "    une réponse complète.\n",
    "    Si l'un des champs est None, il est remplacé par une chaîne vide.\n",
    "    \"\"\"\n",
    "    if not msg:\n",
    "        return \"\"\n",
    "    reasoning = msg.get(\"reasoning_content\") or \"\"\n",
    "    content = msg.get(\"content\") or \"\"\n",
    "    return (reasoning + \" \" + content).strip()\n",
    "\n",
    "def test_logit_bias_consistency():\n",
    "    \"\"\"\n",
    "    Pour chaque endpoint, cette fonction effectue :\n",
    "    \n",
    "    1. Le calcul des token IDs pour le mot \"je\" en considérant les variantes\n",
    "       [\"je\", \" je\", \"Je\", \" Je\"], en passant le nom du modèle pour le tokenizer.\n",
    "    2. Un appel à l'API avec un logit_bias négatif (pour biaiser ces tokens).\n",
    "    3. Deux appels identiques (même prompt, température, seed) sans logit_bias pour vérifier la cohérence.\n",
    "    4. Si le message retourné contient \"reasoning_content\", celui-ci est concaténé avec \"content\".\n",
    "    \n",
    "    Les résultats sont loggués :\n",
    "      - Si les deux appels sans logit_bias renvoient la même réponse, la cohérence est vérifiée.\n",
    "      - Sinon, une erreur est logguée en rouge.\n",
    "      - Une différence entre la réponse avec logit_bias et sans est également signalée.\n",
    "    \"\"\"\n",
    "    logger.info(\"=== Test de logit_bias avec vérification de cohérence et concaténation ===\")\n",
    "    \n",
    "    url_suffix = \"/chat/completions\"\n",
    "    for ep in endpoints:\n",
    "        logger.info(f\"\\n=== Test logit_bias sur endpoint '{ep['name']}' ===\")\n",
    "        url = f\"{ep['api_base']}{url_suffix}\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {ep['api_key']}\"\n",
    "        }\n",
    "        prompt = \"Bonjour, qui es-tu?\"\n",
    "        \n",
    "        # Calcul des token IDs pour les variantes de \"je\" en passant le modèle\n",
    "        token_variants = [\"je\", \" je\", \"Je\", \" Je\", \"Bonjour\"]\n",
    "        token_ids_set = set()\n",
    "        model_name = ep.get(\"model\", \"gpt-5-mini\")\n",
    "        for variant in token_variants:\n",
    "            tokens = tokenize_sentence(ep[\"api_base\"], ep[\"api_key\"], variant, model_fallback=model_name)\n",
    "            if tokens:\n",
    "                token_ids_set.update(tokens)\n",
    "        if token_ids_set:\n",
    "            logger.info(f\"Pour le mot 'je', variantes {token_variants} -> token IDs: {token_ids_set}\")\n",
    "        else:\n",
    "            logger.warning(f\"Aucun token obtenu pour le mot 'je' sur l'endpoint '{ep['name']}'. Passage à l'endpoint suivant.\")\n",
    "            continue\n",
    "        \n",
    "        # Construction du logit_bias négatif sur ces tokens\n",
    "        logit_bias = {str(tid): -100.0 for tid in token_ids_set}\n",
    "        logger.info(f\"Logit_bias appliqué: {logit_bias}\")\n",
    "        max_completion_tokens = 50\n",
    "        temperature = 1.0\n",
    "        \n",
    "        payload_bias = {\n",
    "            \"model\": ep[\"model\"],\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_completion_tokens\": max_completion_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"seed\": 42,\n",
    "            \"logit_bias\": logit_bias\n",
    "        }\n",
    "        payload_no_bias = {\n",
    "            \"model\": ep[\"model\"],\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_completion_tokens\": max_completion_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"seed\": 42\n",
    "        }\n",
    "        \n",
    "        # Appel avec logit_bias\n",
    "        logger.debug(\"Envoi de la requête AVEC logit_bias...\")\n",
    "        try:\n",
    "            resp_bias = requests.post(url, headers=headers, json=payload_bias, timeout=30)\n",
    "            logger.debug(f\"Statut HTTP avec logit_bias: {resp_bias.status_code}\")\n",
    "            data_bias = resp_bias.json()\n",
    "            message_bias = data_bias.get(\"choices\", [{}])[0].get(\"message\", {})\n",
    "            result_bias = combine_message(message_bias)\n",
    "        except Exception as e:\n",
    "            result_bias = f\"Erreur lors de la lecture de la réponse avec logit_bias: {e}\"\n",
    "        logger.info(f\"Réponse avec logit_bias: {result_bias}\")\n",
    "        \n",
    "        # Deux appels sans logit_bias pour vérifier la cohérence\n",
    "        logger.debug(\"Envoi de la première requête SANS logit_bias...\")\n",
    "        try:\n",
    "            resp_no_bias1 = requests.post(url, headers=headers, json=payload_no_bias, timeout=30)\n",
    "            logger.debug(f\"Statut HTTP sans logit_bias (1): {resp_no_bias1.status_code}\")\n",
    "            data_no_bias1 = resp_no_bias1.json()\n",
    "            message_no_bias1 = data_no_bias1.get(\"choices\", [{}])[0].get(\"message\", {})\n",
    "            result_no_bias1 = combine_message(message_no_bias1)\n",
    "        except Exception as e:\n",
    "            result_no_bias1 = f\"Erreur lors de la première réponse sans logit_bias: {e}\"\n",
    "        logger.info(f\"Réponse sans logit_bias (1): {result_no_bias1}\")\n",
    "        \n",
    "        logger.debug(\"Envoi de la deuxième requête SANS logit_bias...\")\n",
    "        try:\n",
    "            resp_no_bias2 = requests.post(url, headers=headers, json=payload_no_bias, timeout=30)\n",
    "            logger.debug(f\"Statut HTTP sans logit_bias (2): {resp_no_bias2.status_code}\")\n",
    "            data_no_bias2 = resp_no_bias2.json()\n",
    "            message_no_bias2 = data_no_bias2.get(\"choices\", [{}])[0].get(\"message\", {})\n",
    "            result_no_bias2 = combine_message(message_no_bias2)\n",
    "        except Exception as e:\n",
    "            result_no_bias2 = f\"Erreur lors de la deuxième réponse sans logit_bias: {e}\"\n",
    "        logger.info(f\"Réponse sans logit_bias (2): {result_no_bias2}\")\n",
    "        \n",
    "        # Vérification de la cohérence\n",
    "        if result_no_bias1 == result_no_bias2:\n",
    "            logger.info(\"=> Cohérence vérifiée : les deux appels sans logit_bias avec la même seed renvoient la même réponse.\")\n",
    "        else:\n",
    "            logger.error(\"=> ERREUR : Les deux appels sans logit_bias avec la même seed renvoient des réponses différentes!\")\n",
    "        \n",
    "        # Comparaison entre la réponse avec logit_bias et sans\n",
    "        if result_bias != result_no_bias1:\n",
    "            logger.info(\"=> Différence détectée entre AVEC et SANS logit_bias.\")\n",
    "        else:\n",
    "            logger.warning(\"=> Aucune différence détectée. Le logit_bias n'a pas eu d'effet apparent.\")\n",
    "\n",
    "# Exécute le test\n",
    "test_logit_bias_consistency()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c5b68",
   "metadata": {
    "papermill": {
     "duration": 0.01509,
     "end_time": "2026-02-18T08:13:49.206183",
     "exception": false,
     "start_time": "2026-02-18T08:13:49.191093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation de la fonction combine_message\n",
    "\n",
    "Cette fonction utilitaire **fusionne le contenu reasoning et content** :\n",
    "\n",
    "**Cas d'usage** :\n",
    "\n",
    "Certains modèles (DeepSeek R1, o1) génèrent deux champs séparés :\n",
    "\n",
    "- `reasoning_content` : Le raisonnement intermédiaire (balises `<think>`)\n",
    "- `content` : La réponse finale\n",
    "\n",
    "**Comportement** :\n",
    "\n",
    "```python\n",
    "msg = {\"reasoning_content\": \"Calcul: 2+2=4\", \"content\": \"La réponse est 4\"}\n",
    "combine_message(msg)  # Retourne: \"Calcul: 2+2=4\\n\\nLa réponse est 4\"\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "\n",
    "- **Affichage complet** : Voir le raisonnement ET la conclusion\n",
    "- **Debugging** : Identifier les erreurs dans le raisonnement\n",
    "- **Audit** : Tracer le cheminement du modèle\n",
    "\n",
    "**Cas où reasoning_content est None** :\n",
    "\n",
    "- Modèles standard (GPT-5, Llama 3.3) : Pas de mode reasoning\n",
    "- DeepSeek R1 sans `--enable-reasoning` : Champ vide\n",
    "\n",
    "> **Note** : La fonction gère gracieusement le cas où `reasoning_content` est absent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014154ff",
   "metadata": {
    "papermill": {
     "duration": 0.016924,
     "end_time": "2026-02-18T08:13:49.235599",
     "exception": false,
     "start_time": "2026-02-18T08:13:49.218675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test avec la librairie `openai`\n",
    "\n",
    "On reproduit un appel classique OpenAI, \n",
    "mais en changeant `openai.api_base` et `openai.api_key` \n",
    "pour chaque endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "439169e2",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:13:49.272606Z",
     "iopub.status.busy": "2026-02-18T08:13:49.272037Z",
     "iopub.status.idle": "2026-02-18T08:13:54.225792Z",
     "shell.execute_reply": "2026-02-18T08:13:54.224943Z"
    },
    "papermill": {
     "duration": 4.971752,
     "end_time": "2026-02-18T08:13:54.227304",
     "exception": false,
     "start_time": "2026-02-18T08:13:49.255552",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:49 [INFO] Local Llama - \n",
      "=== Test openai pour gpt-4o-mini (OpenAI) ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:50 [DEBUG] Local Llama - Appel de client.chat.completions.create()...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:54 [INFO] Local Llama -  -> Réponse (début): La philosophie stoïcienne, fondée à Athènes par Zénon de Citium, est une école de pensée qui enseigne l'importance de la raison, de la vertu et de l'acceptation des événements extérieurs. Les stoïcien...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:54 [INFO] Local Llama -  -> Nb tokens: 185\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:54 [INFO] Local Llama -  -> Temps écoulé: 4.94 sec\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def test_openai_chat(api_base, api_key, prompt, model):\n",
    "    \"\"\"\n",
    "    Appel classique OpenAI, en utilisant la classe `OpenAI`\n",
    "    et en gérant la journalisation via logger.\n",
    "    \"\"\"\n",
    "    # Création du client OpenAI-compatible\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=api_base\n",
    "    )\n",
    "\n",
    "    if not model:\n",
    "        logger.error(\"[!] Modèle non défini.\")\n",
    "        raise ValueError(\"Modèle non défini\")\n",
    "\n",
    "    try:\n",
    "        # Appel chat.completions\n",
    "        logger.debug(\"Appel de client.chat.completions.create()...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_completion_tokens=500\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        tokens_used = response.usage.total_tokens if response.usage else None\n",
    "        return content, tokens_used\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Exception lors de l'appel OpenAI: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def test_openai_endpoints():\n",
    "    \"\"\"Itère sur tous les endpoints et lance un prompt 'philosophie stoïcienne'.\"\"\"\n",
    "    for ep in endpoints:\n",
    "        label_model = ep.get(\"model\", \"<aucun>\")\n",
    "        logger.info(f\"\\n=== Test openai pour {label_model} ({ep['name']}) ===\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        prompt = \"Peux-tu résumer la philosophie stoïcienne en quelques lignes ?\"\n",
    "        content, tks = test_openai_chat(\n",
    "            ep[\"api_base\"],\n",
    "            ep[\"api_key\"],\n",
    "            prompt,\n",
    "            ep[\"model\"]\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if content:\n",
    "            logger.info(f\" -> Réponse (début): {content[:200]}...\")\n",
    "            logger.info(f\" -> Nb tokens: {tks}\")\n",
    "            logger.info(f\" -> Temps écoulé: {elapsed_time:.2f} sec\")\n",
    "        else:\n",
    "            logger.warning(\" -> Pas de contenu (erreur ou exception).\")\n",
    "\n",
    "# On exécute le test\n",
    "test_openai_endpoints()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9935e8",
   "metadata": {
    "papermill": {
     "duration": 0.010929,
     "end_time": "2026-02-18T08:13:54.249952",
     "exception": false,
     "start_time": "2026-02-18T08:13:54.239023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du test avec la bibliothèque OpenAI\n",
    "\n",
    "Ce test utilise la **bibliothèque officielle `openai`** plutôt que `requests` :\n",
    "\n",
    "**Avantages de la bibliothèque** :\n",
    "\n",
    "- **Gestion automatique** : Retry, timeouts, streaming\n",
    "- **Type hints** : Autocomplétion IDE et validation\n",
    "- **Parsing JSON** : Conversion automatique des réponses\n",
    "- **Compatibilité** : Fonctionne avec OpenAI cloud ET endpoints locaux (vLLM/Ollama)\n",
    "\n",
    "**Résultats attendus** :\n",
    "\n",
    "| Endpoint | Statut | Réponse |\n",
    "|----------|--------|---------|\n",
    "| **OpenAI cloud** | 200 OK | \"Je suis ChatGPT, un modèle d'IA...\" |\n",
    "| **Local vLLM/Ollama** | 200 OK | \"Je suis un assistant IA...\" |\n",
    "\n",
    "**Diagnostic des erreurs** :\n",
    "\n",
    "- **401 Unauthorized** : API key invalide ou manquante\n",
    "- **404 Not Found** : URL incorrecte ou modèle introuvable\n",
    "- **Timeout** : Serveur surchargé ou prompt trop long\n",
    "\n",
    "**Comparaison avec requests.post** :\n",
    "\n",
    "- `requests.post` : Plus bas niveau, contrôle total, code verbeux\n",
    "- `openai.OpenAI` : Haut niveau, simple, recommandé pour production\n",
    "\n",
    "> **Best practice** : Utiliser `openai.OpenAI` pour le code production, `requests` uniquement pour debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24f0e3",
   "metadata": {
    "papermill": {
     "duration": 0.011087,
     "end_time": "2026-02-18T08:13:54.272917",
     "exception": false,
     "start_time": "2026-02-18T08:13:54.261830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test avec Semantic Kernel (optionnel)\n",
    "\n",
    "Exemple d'intégration avec [Semantic Kernel](https://github.com/microsoft/semantic-kernel).\n",
    "On crée un Kernel, on y ajoute un service chat OpenAI-like avec l'endpoint souhaité, \n",
    "et on exécute un prompt simple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9144c6e",
   "metadata": {
    "papermill": {
     "duration": 0.017407,
     "end_time": "2026-02-18T08:13:54.306249",
     "exception": false,
     "start_time": "2026-02-18T08:13:54.288842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse du Function/Tool Calling\n",
    "\n",
    "Le Function Calling permet au LLM de **déclencher des actions structurées** au lieu de générer du texte libre :\n",
    "\n",
    "**Déroulement observé** :\n",
    "\n",
    "1. **Déclaration de l'outil** : On définit `get_weather` avec son schéma JSON (paramètres location, unit)\n",
    "2. **Prompt utilisateur** : \"Donne-moi la météo pour Marseille en celsius\"\n",
    "3. **Réponse du modèle** : \n",
    "   - Le modèle détecte qu'il doit appeler `get_weather`\n",
    "   - Il extrait automatiquement les arguments : `{\"location\": \"Marseille\", \"unit\": \"celsius\"}`\n",
    "4. **Exécution côté client** : Notre fonction Python `get_weather()` est appelée avec ces arguments\n",
    "5. **Résultat** : \"Simulation: Météo à Marseille, unité=celsius, ciel dégagé.\"\n",
    "\n",
    "**Avantages majeurs** :\n",
    "\n",
    "- **Structuration** : Pas besoin de parser du texte libre (ex: \"Il fait 20 degrés\")\n",
    "- **Fiabilité** : Les arguments sont validés par le schéma JSON\n",
    "- **Automatisation** : Connexion directe à des APIs réelles (météo, calendrier, base de données)\n",
    "\n",
    "**Endpoints compatibles** :\n",
    "- OpenAI (natif)\n",
    "- vLLM avec `--enable-auto-tool-choice` et `--tool-call-parser`\n",
    "- Certains modèles locaux (Llama 3.1+, Qwen 2.5+)\n",
    "\n",
    "**Warning possible** : Si un endpoint ne supporte pas le tool calling, il renverra du texte au lieu d'un `function_call`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb84ddf",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:13:54.365160Z",
     "iopub.status.busy": "2026-02-18T08:13:54.364132Z",
     "iopub.status.idle": "2026-02-18T08:14:08.884742Z",
     "shell.execute_reply": "2026-02-18T08:14:08.883835Z"
    },
    "papermill": {
     "duration": 14.561756,
     "end_time": "2026-02-18T08:14:08.886145",
     "exception": false,
     "start_time": "2026-02-18T08:13:54.324389",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:56 [INFO] Local Llama - === Test Semantic Kernel pour endpoint='OpenAI', model='gpt-4o-mini' ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:13:57 [DEBUG] Local Llama - Service OpenAI ajouté au Kernel.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:13:57 [INFO] Local Llama -   -> Exécution en cours (Semantic Kernel)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:08 [INFO] Local Llama -   -> Résultat (début): L'apprentissage profond, ou deep learning en anglais, est une sous-catégorie de l'intelligence artificielle et plus particulièrement d'apprentissage automatique (machine learning). Il se distingue par l'utilisation de réseaux de neurones artificiels, qui s'inspirent du fonctionnement du cerveau humain. Ces réseaux sont conçus pour analyser des données, apprendre des représentations complexes et fa...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:08 [INFO] Local Llama -   -> Durée: 11.76s, Tokens=348, speed=29.60 tok/s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    OpenAIChatPromptExecutionSettings\n",
    ")\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "async def test_semantic_kernel():\n",
    "    \"\"\"\n",
    "    Exécute un prompt via Semantic Kernel pour chaque endpoint,\n",
    "    et journalise les résultats en couleur via `logger`.\n",
    "    \"\"\"\n",
    "    for ep in endpoints:\n",
    "        model_id = ep.get(\"model\")\n",
    "        api_key = ep[\"api_key\"]\n",
    "\n",
    "        logger.info(f\"=== Test Semantic Kernel pour endpoint='{ep['name']}', model='{model_id}' ===\")\n",
    "\n",
    "        kernel = sk.Kernel()\n",
    "        async_client = AsyncOpenAI(api_key=api_key, base_url=ep[\"api_base\"])\n",
    "\n",
    "        kernel.add_service(\n",
    "            OpenAIChatCompletion(\n",
    "                service_id=\"default\",\n",
    "                ai_model_id=model_id,\n",
    "                async_client=async_client\n",
    "            )\n",
    "        )\n",
    "        logger.debug(\"Service OpenAI ajouté au Kernel.\")\n",
    "\n",
    "        prompt_template = \"Explique ce qu'est l'apprentissage profond (deep learning) en 500 mots.\"\n",
    "\n",
    "        exec_settings = OpenAIChatPromptExecutionSettings(\n",
    "            service_id=\"default\",\n",
    "            ai_model_id=model_id,\n",
    "            max_completion_tokens=500,\n",
    "        )\n",
    "\n",
    "        pt_config = PromptTemplateConfig(\n",
    "            template=prompt_template,\n",
    "            name=\"deepLearningFunction\",\n",
    "            template_format=\"semantic-kernel\",\n",
    "            input_variables=[],\n",
    "            execution_settings=exec_settings,\n",
    "        )\n",
    "\n",
    "        func = kernel.add_function(\n",
    "            function_name=\"deepLearningFunction\",\n",
    "            plugin_name=\"defaultPlugin\",\n",
    "            prompt_template_config=pt_config\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            logger.info(\"  -> Exécution en cours (Semantic Kernel)...\")\n",
    "            start_time = time.time()\n",
    "            result = await kernel.invoke(func, KernelArguments())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Comptage approximatif de tokens\n",
    "            tokens_count = len(str(result).split())\n",
    "            speed = tokens_count / elapsed if elapsed > 0 else 0\n",
    "\n",
    "            logger.info(f\"  -> Résultat (début): {str(result)[:400]}...\")\n",
    "            logger.info(f\"  -> Durée: {elapsed:.2f}s, Tokens={tokens_count}, speed={speed:.2f} tok/s\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"  [!] Erreur SK sur endpoint='{ep['name']}': {str(e)}\")\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "await test_semantic_kernel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0317ed",
   "metadata": {
    "papermill": {
     "duration": 0.011155,
     "end_time": "2026-02-18T08:14:08.909095",
     "exception": false,
     "start_time": "2026-02-18T08:14:08.897940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation de la configuration Semantic Kernel\n",
    "\n",
    "Ce code initialise **Semantic Kernel** avec les endpoints locaux/cloud :\n",
    "\n",
    "**Configuration créée** :\n",
    "\n",
    "- **Kernel** : Instance principale de Semantic Kernel\n",
    "- **Service chat** : `OpenAIChatCompletion` configuré pour chaque endpoint\n",
    "- **Arguments** : `max_tokens=200` pour limiter la longueur des réponses\n",
    "\n",
    "**Points clés** :\n",
    "\n",
    "| Élément | Valeur | Signification |\n",
    "|---------|--------|---------------|\n",
    "| `api_key` | Bearer token | Authentification endpoint |\n",
    "| `base_url` | URL OpenAI-compatible | Pointe vers vLLM/Ollama local |\n",
    "| `ai_model_id` | Nom du modèle | Ex: `unsloth/DeepSeek-R1-Distill-Llama-8B` |\n",
    "\n",
    "**Avantages de Semantic Kernel** :\n",
    "\n",
    "- **Abstraction** : Même code pour OpenAI/vLLM/Ollama\n",
    "- **Plugins** : Support des fonctions/tools\n",
    "- **Chaining** : Enchaînement de prompts et fonctions\n",
    "- **Enterprise-ready** : Logging, telemetry, error handling intégrés\n",
    "\n",
    "> **Note** : Semantic Kernel détecte automatiquement le format de réponse (reasoning, function_call, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add94c0",
   "metadata": {
    "papermill": {
     "duration": 0.019342,
     "end_time": "2026-02-18T08:14:08.944347",
     "exception": false,
     "start_time": "2026-02-18T08:14:08.925005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test du Function/Tool Calling sur chaque endpoint\n",
    "\n",
    "Avec vLLM, lorsqu’on démarre les containers avec `--enable-auto-tool-choice` et un `--tool-call-parser` adéquat,\n",
    "le modèle peut déclencher automatiquement un « tool call » s’il juge qu’un outil est pertinent.  \n",
    "On doit alors inclure un paramètre `tools` dans la requête, et indiquer `tool_choice=\"auto\"` (ou un nom de fonction précis).\n",
    "\n",
    "**Note** : Pour exécuter concrètement la fonction côté client Python, on doit définir une fonction Python qui correspond,\n",
    "et réinjecter manuellement le résultat dans la conversation.  \n",
    "Voici un exemple simplifié : on va appeler un `get_weather(location, unit)` sur *tous* les endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8d88396",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:14:08.997995Z",
     "iopub.status.busy": "2026-02-18T08:14:08.997371Z",
     "iopub.status.idle": "2026-02-18T08:14:10.404110Z",
     "shell.execute_reply": "2026-02-18T08:14:10.402809Z"
    },
    "papermill": {
     "duration": 1.449554,
     "end_time": "2026-02-18T08:14:10.406058",
     "exception": false,
     "start_time": "2026-02-18T08:14:08.956504",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:09 [INFO] Local Llama - === Test Tool Calling sur endpoint 'OpenAI' ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:14:09 [DEBUG] Local Llama - Appel chat.completions avec tool_choice='auto'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:14:10 [DEBUG] Local Llama - Réponse textuelle (début): ''\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:14:10 [DEBUG] Local Llama - Contenu complet de la réponse: {\n",
      "  \"id\": \"chatcmpl-DAWzCdqteNu6wIj22blUiNffl2Kz8\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": [],\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_N6owxONH7x2cgBVX8vPNxKD0\",\n",
      "            \"function\": {\n",
      "              \"arguments\": \"{\\\"location\\\":\\\"Marseille, France\\\",\\\"unit\\\":\\\"celsius\\\"}\",\n",
      "              \"name\": \"get_weather\"\n",
      "            },\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1771402450,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": \"default\",\n",
      "  \"system_fingerprint\": \"fp_373a14eb6f\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 22,\n",
      "    \"prompt_tokens\": 82,\n",
      "    \"total_tokens\": 104,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  }\n",
      "}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:10 [INFO] Local Llama -   -> 1 tool_call(s) détecté(s) sur endpoint='OpenAI'.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:10 [INFO] Local Llama -      Fonction appelée: get_weather | arguments={'location': 'Marseille, France', 'unit': 'celsius'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:10 [INFO] Local Llama -      => Résultat simulé: Simulation: Météo à Marseille, France, unité=celsius, ciel dégagé.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_weather(location: str, unit: str):\n",
    "    \"\"\"Exemple de fonction locale pour la météo.\"\"\"\n",
    "    return f\"Simulation: Météo à {location}, unité={unit}, ciel dégagé.\"\n",
    "\n",
    "def test_tool_calling():\n",
    "    \"\"\"\n",
    "    Test du Function/Tool Calling pour chaque endpoint, en mode auto (tool_choice='auto').\n",
    "    On journalise les étapes et le résultat.\n",
    "    \"\"\"\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Obtenir la météo pour un lieu donné\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\"type\": \"string\", \"description\": \"Ex: 'Paris, France'\"},\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
    "                    },\n",
    "                    \"required\": [\"location\", \"unit\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    user_message = \"Bonjour, est-ce que tu peux me donner la météo pour Marseille en celsius ?\"\n",
    "\n",
    "    for ep in endpoints:\n",
    "        logger.info(f\"=== Test Tool Calling sur endpoint '{ep['name']}' ===\")\n",
    "\n",
    "        openai.api_base = ep[\"api_base\"]\n",
    "        openai.api_key  = ep[\"api_key\"]\n",
    "\n",
    "        client = OpenAI(\n",
    "            api_key=ep[\"api_key\"],\n",
    "            base_url=ep[\"api_base\"]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            logger.debug(\"Appel chat.completions avec tool_choice='auto'\")\n",
    "            response = client.chat.completions.create(\n",
    "                model=ep.get(\"model\"),\n",
    "                messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "                tools=tools,\n",
    "                tool_choice=\"auto\",\n",
    "            )\n",
    "        except Exception as ex:\n",
    "            logger.error(f\"[!] Erreur lors de l'appel sur endpoint='{ep['name']}': {ex}\")\n",
    "            continue\n",
    "\n",
    "        choice = response.choices[0]\n",
    "        msg = choice.message\n",
    "\n",
    "        text_content = msg.content if msg.content else \"\"\n",
    "        tool_calls = msg.tool_calls\n",
    "\n",
    "        logger.debug(f\"Réponse textuelle (début): {text_content[:400]!r}\")\n",
    "        logger.debug(f\"Contenu complet de la réponse: {json.dumps(response.to_dict(), indent=2)}\")\n",
    "\n",
    "        if tool_calls:\n",
    "            logger.info(f\"  -> {len(tool_calls)} tool_call(s) détecté(s) sur endpoint='{ep['name']}'.\")\n",
    "            for call in tool_calls:\n",
    "                func_name = call.function.name\n",
    "                args_str  = call.function.arguments or \"{}\"\n",
    "                args_dict = json.loads(args_str)\n",
    "\n",
    "                logger.info(f\"     Fonction appelée: {func_name} | arguments={args_dict}\")\n",
    "\n",
    "                # Exécution locale simulée\n",
    "                if func_name == \"get_weather\":\n",
    "                    result = get_weather(**args_dict)\n",
    "                    logger.info(f\"     => Résultat simulé: {result}\")\n",
    "                else:\n",
    "                    logger.warning(f\"     => Fonction inconnue: {func_name}\")\n",
    "        else:\n",
    "            logger.warning(\"  -> Aucun tool_call détecté dans la réponse.\")\n",
    "\n",
    "# On l’exécute\n",
    "test_tool_calling()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690e955",
   "metadata": {
    "papermill": {
     "duration": 0.014087,
     "end_time": "2026-02-18T08:14:10.436993",
     "exception": false,
     "start_time": "2026-02-18T08:14:10.422906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du test de function calling\n",
    "\n",
    "Ce test vérifie le **support natif du function/tool calling** :\n",
    "\n",
    "**Workflow observé** :\n",
    "\n",
    "1. **Définition** : Schéma JSON de la fonction `get_weather(location, unit)`\n",
    "2. **Prompt** : \"Donne-moi la météo pour Marseille en celsius\"\n",
    "3. **Détection** : Le modèle identifie qu'il doit appeler `get_weather`\n",
    "4. **Extraction** : Arguments extraits : `{\"location\": \"Marseille\", \"unit\": \"celsius\"}`\n",
    "5. **Exécution** : Fonction Python appelée côté client\n",
    "6. **Résultat** : \"Simulation: Météo à Marseille, unité=celsius, ciel dégagé.\"\n",
    "\n",
    "**Avantages** :\n",
    "\n",
    "- **Structuration** : Pas de parsing de texte libre nécessaire\n",
    "- **Validation** : Arguments validés par le schéma JSON\n",
    "- **Automatisation** : Connexion directe à des APIs (météo, calendrier, DB)\n",
    "- **Fiabilité** : Format standardisé (compatibilité multi-modèles)\n",
    "\n",
    "**Endpoints compatibles** :\n",
    "\n",
    "| Endpoint | Support | Configuration requise |\n",
    "|----------|---------|----------------------|\n",
    "| **OpenAI (gpt-5)** | Natif | Aucune |\n",
    "| **vLLM (Llama 3.1+, Qwen 2.5+)** | Oui | `--enable-auto-tool-choice --tool-call-parser hermes` |\n",
    "| **Ollama** | Partiel | Dépend du modèle |\n",
    "\n",
    "**Warning possible** :\n",
    "\n",
    "Si un endpoint ne supporte pas le tool calling, il renverra du **texte libre** au lieu d'un `function_call` structuré.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668c512",
   "metadata": {
    "papermill": {
     "duration": 0.015784,
     "end_time": "2026-02-18T08:14:10.465189",
     "exception": false,
     "start_time": "2026-02-18T08:14:10.449405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test du Function/Tool Calling via semantic-kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89036075",
   "metadata": {
    "papermill": {
     "duration": 0.013184,
     "end_time": "2026-02-18T08:14:10.490854",
     "exception": false,
     "start_time": "2026-02-18T08:14:10.477670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse des résultats du benchmark\n",
    "\n",
    "Le benchmark séquentiel évalue les **performances individuelles** de chaque endpoint :\n",
    "\n",
    "**Métriques clés** :\n",
    "\n",
    "1. **Temps moyen par requête** (`avg_time_s`) :\n",
    "   - Indicateur de la latence du modèle\n",
    "   - Dépend de : taille du modèle, quantization, charge serveur, bande passante\n",
    "\n",
    "2. **Tokens par seconde** (`tokens_per_sec`) :\n",
    "   - **Métrique de vitesse de génération**\n",
    "   - Formule : `total_tokens / total_time`\n",
    "   - Valeurs typiques :\n",
    "     - Modèles 7B-8B (4-bit) : 20-50 tok/s\n",
    "     - Modèles 14B (4-bit) : 10-25 tok/s\n",
    "     - GPU puissant (RTX 4090) : 50-100+ tok/s\n",
    "\n",
    "3. **Success rate** :\n",
    "   - Nombre de requêtes réussies / total de requêtes\n",
    "   - Un taux < 100% indique des timeouts ou erreurs\n",
    "\n",
    "**Facteurs influençant les performances** :\n",
    "- **Quantization** : 4-bit (bnb) vs 8-bit (fp8) vs 16-bit (fp16)\n",
    "- **VRAM disponible** : Plus de VRAM = batch size plus grand\n",
    "- **Context length** : Prompts longs = génération plus lente\n",
    "- **KV cache** : Optimisation pour les conversations longues\n",
    "\n",
    "**Comparaison typique** :\n",
    "```\n",
    "OpenAI (gpt-5-mini)    : ~15-20 tok/s (cloud, partagé)\n",
    "Local mini (8B 4-bit)   : ~30-40 tok/s (dédié, RTX 3090)\n",
    "Local medium (14B 4-bit): ~15-20 tok/s (dédié, RTX 3090)\n",
    "```\n",
    "\n",
    "**Conclusion** : Les modèles locaux peuvent être **plus rapides** que le cloud si bien configurés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f115692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:14:10.520506Z",
     "iopub.status.busy": "2026-02-18T08:14:10.519873Z",
     "iopub.status.idle": "2026-02-18T08:14:14.618692Z",
     "shell.execute_reply": "2026-02-18T08:14:14.617619Z"
    },
    "papermill": {
     "duration": 4.116436,
     "end_time": "2026-02-18T08:14:14.620370",
     "exception": false,
     "start_time": "2026-02-18T08:14:10.503934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:10 [INFO] Local Llama - === Test Semantic Kernel pour endpoint='OpenAI', model='gpt-4o-mini' ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:14:11 [DEBUG] Local Llama - Service OpenAI ajouté au Kernel.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:14:11 [DEBUG] Local Llama - Appel d'agent semantickernel avec plugin et tool_choice='auto'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Hello'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91m09:14:11 [ERROR] Local Llama - [!] Erreur lors de l'appel sur endpoint='OpenAI': 'StreamingChatMessageContent' object has no attribute 'strip'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:14:11 [DEBUG] Local Llama - Appel d'agent semantickernel avec plugin et tool_choice='auto'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\opentelemetry\\trace\\__init__.py\", line 587, in use_span\n",
      "    yield span\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py\", line 271, in get_streaming_chat_message_contents\n",
      "    yield messages\n",
      "GeneratorExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\opentelemetry\\context\\__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\opentelemetry\\context\\contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x00000233001E42C0> at 0x000002330087A040> was created in a different Context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: '# User: 'What is the special soup?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91m09:14:12 [ERROR] Local Llama - [!] Erreur lors de l'appel sur endpoint='OpenAI': 'StreamingChatMessageContent' object has no attribute 'strip'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:14:12 [DEBUG] Local Llama - Appel d'agent semantickernel avec plugin et tool_choice='auto'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\opentelemetry\\trace\\__init__.py\", line 587, in use_span\n",
      "    yield span\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py\", line 271, in get_streaming_chat_message_contents\n",
      "    yield messages\n",
      "GeneratorExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\opentelemetry\\context\\__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\opentelemetry\\context\\contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x00000233001E42C0> at 0x0000023300592580> was created in a different Context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: '# User: 'What does it cost?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91m09:14:13 [ERROR] Local Llama - [!] Erreur lors de l'appel sur endpoint='OpenAI': 'StreamingChatMessageContent' object has no attribute 'strip'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:14:13 [DEBUG] Local Llama - Appel d'agent semantickernel avec plugin et tool_choice='auto'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\opentelemetry\\trace\\__init__.py\", line 587, in use_span\n",
      "    yield span\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py\", line 271, in get_streaming_chat_message_contents\n",
      "    yield messages\n",
      "GeneratorExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\opentelemetry\\context\\__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\opentelemetry\\context\\contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x00000233001E42C0> at 0x0000023300819300> was created in a different Context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: '# User: 'Thanks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91m09:14:14 [ERROR] Local Llama - [!] Erreur lors de l'appel sur endpoint='OpenAI': 'StreamingChatMessageContent' object has no attribute 'strip'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: '"
     ]
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    OpenAIChatPromptExecutionSettings\n",
    ")\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.functions import KernelArguments, kernel_function\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "from typing import TYPE_CHECKING, Annotated\n",
    "\n",
    "\n",
    "class MenuPlugin:\n",
    "    \"\"\"Plugin pour gérer un menu\"\"\"\n",
    "    @kernel_function(description=\"Liste les specials\")\n",
    "    def get_specials(self) -> Annotated[str, \"Describes specials\"]:\n",
    "        # print function call\n",
    "        print(\"get_specials called\")\n",
    "        return \"Special Soup: Clam Chowder\\nSpecial Salad: Cobb Salad\\nSpecial Drink: Chai Tea\"\n",
    "    @kernel_function(description=\"Donne le prix d'un item\")\n",
    "    def get_item_price(self, menu_item: Annotated[str, \"nom de l'item\"]) -> str:\n",
    "         # print function call\n",
    "        print(\"get_item_price called\")\n",
    "        return \"$9.99\"\n",
    "    \n",
    "\n",
    "\n",
    "async def test_semantic_kernel_plugin():\n",
    "    \"\"\"\n",
    "    Exécute un prompt via Semantic Kernel pour chaque endpoint,\n",
    "    et journalise les résultats en couleur via `logger`.\n",
    "    \"\"\"\n",
    "    for ep in endpoints:\n",
    "        model_id = ep.get(\"model\")\n",
    "        api_key = ep[\"api_key\"]\n",
    "\n",
    "        logger.info(f\"=== Test Semantic Kernel pour endpoint='{ep['name']}', model='{model_id}' ===\")\n",
    "\n",
    "        kernel = sk.Kernel()\n",
    "        kernel.add_plugin(MenuPlugin(), plugin_name=\"menu\")\n",
    "        async_client = AsyncOpenAI(api_key=api_key, base_url=ep[\"api_base\"])\n",
    "\n",
    "        kernel.add_service(\n",
    "            OpenAIChatCompletion(\n",
    "                service_id=\"default\",\n",
    "                ai_model_id=model_id,\n",
    "                async_client=async_client\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        settings2 = kernel.get_prompt_execution_settings_from_service_id(service_id=\"default\")\n",
    "        settings2.function_choice_behavior = FunctionChoiceBehavior.Auto()     \n",
    "        \n",
    "        logger.debug(\"Service OpenAI ajouté au Kernel.\")\n",
    "\n",
    "        AGENT2_NAME = \"Host\"\n",
    "        AGENT2_INSTRUCTIONS = \"Answer questions about the menu.\"\n",
    "        agent2 = ChatCompletionAgent(\n",
    "            kernel=kernel,\n",
    "            name=AGENT2_NAME,\n",
    "            instructions=AGENT2_INSTRUCTIONS,\n",
    "            arguments=KernelArguments(settings=settings2),\n",
    "        )\n",
    "        chat_history = ChatHistory()\n",
    "        user_msgs = [\n",
    "            \"Hello\",\n",
    "            \"What is the special soup?\",\n",
    "            \"What does it cost?\",\n",
    "            \"Thanks\",\n",
    "        ]\n",
    "        for user_input in user_msgs:\n",
    "            chat_history.add_user_message(user_input)\n",
    "            print(f\"# User: '{user_input}'\")\n",
    "            agent_name = None\n",
    "            try:\n",
    "                logger.debug(\"Appel d'agent semantickernel avec plugin et tool_choice='auto'\")\n",
    "                async for content in agent2.invoke_stream(chat_history):\n",
    "                    if not agent_name:\n",
    "                        agent_name = content.name or AGENT2_NAME\n",
    "                        print(f\"# {agent_name}: '\", end=\"\")\n",
    "                    if content.content.strip():\n",
    "                        print(content.content, end=\"\", flush=True)\n",
    "            except Exception as ex:\n",
    "                logger.error(f\"[!] Erreur lors de l'appel sur endpoint='{ep['name']}': {ex}\")\n",
    "                continue\n",
    "            \n",
    " \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "await test_semantic_kernel_plugin()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd758d30",
   "metadata": {
    "papermill": {
     "duration": 0.012645,
     "end_time": "2026-02-18T08:14:14.647071",
     "exception": false,
     "start_time": "2026-02-18T08:14:14.634426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du benchmark Semantic Kernel\n",
    "\n",
    "Ce test évalue l'**intégration avec Semantic Kernel** et la fonctionnalité de function calling :\n",
    "\n",
    "**Points testés** :\n",
    "\n",
    "1. **Chat completion** : Dialogue simple avec Semantic Kernel\n",
    "2. **Function calling** : Capacité du modèle à appeler `get_weather()`\n",
    "3. **Arguments extraction** : Parsing correct de `location` et `unit`\n",
    "\n",
    "**Résultats attendus** :\n",
    "\n",
    "| Test | Endpoint compatible | Endpoint incompatible |\n",
    "|------|--------------------|-----------------------|\n",
    "| **Chat simple** | Réponse texte correcte | Réponse texte correcte |\n",
    "| **Function call** | `function_call` structuré | Texte libre (pas de JSON) |\n",
    "| **Arguments** | `{\"location\": \"Paris\", \"unit\": \"celsius\"}` | N/A |\n",
    "\n",
    "**Diagnostics** :\n",
    "\n",
    "- **Success** : Le modèle renvoie un `function_call` avec les bons arguments\n",
    "- **Partial** : Le modèle mentionne la météo mais ne structure pas la réponse\n",
    "- **Fail** : Erreur parsing ou pas de support tool calling\n",
    "\n",
    "> **Note** : vLLM nécessite `--enable-auto-tool-choice` et `--tool-call-parser` pour supporter le function calling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a14a9a",
   "metadata": {
    "papermill": {
     "duration": 0.03319,
     "end_time": "2026-02-18T08:14:14.697065",
     "exception": false,
     "start_time": "2026-02-18T08:14:14.663875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test du mode « Reasoning Outputs »\n",
    "\n",
    "Certains modèles (p. ex. DeepSeek R1) sont lancés avec `--enable-reasoning --reasoning-parser deepseek_r1`.\n",
    "Cela permet de renvoyer, en plus du `content` final, un champ `reasoning_content` qui détaille la chaîne de raisonnement.\n",
    "\n",
    "Voici un exemple d’appel sur *tous* les endpoints (certains n’auront pas de champ `reasoning_content` si le modèle ne supporte pas le raisonnement).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04d3b457",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:14:14.727448Z",
     "iopub.status.busy": "2026-02-18T08:14:14.727048Z",
     "iopub.status.idle": "2026-02-18T08:14:17.454148Z",
     "shell.execute_reply": "2026-02-18T08:14:17.452791Z"
    },
    "papermill": {
     "duration": 2.746983,
     "end_time": "2026-02-18T08:14:17.458595",
     "exception": false,
     "start_time": "2026-02-18T08:14:14.711612",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:14 [INFO] Local Llama - === Test Reasoning Output sur endpoint='OpenAI' ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:14:17 [DEBUG] Local Llama -   -> Réponse (finale) : Pour résoudre l'expression \\( 253 \\times 73 - 287 \\), commençons par calculer le produit :\n",
      "\n",
      "\\[\n",
      "253 \\times 73 = 18469\n",
      "\\]\n",
      "\n",
      "Ensuite, soustrayons 287 :\n",
      "\n",
      "\\[\n",
      "18469 - 287 = 18182\n",
      "\\]\n",
      "\n",
      "Donc, \\( 253 \\times 73 - 287 = 18182 \\).\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93m09:14:17 [WARNING] Local Llama -   -> Pas de 'reasoning_content' pour ce modèle/parsing.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def test_reasoning():\n",
    "    \"\"\"\n",
    "    Test du mode reasoning_output (champ 'reasoning_content') \n",
    "    s'il est activé sur certains modèles. \n",
    "    \"\"\"\n",
    "    question = \"Combien font 253 * 73 - 287 ?\"\n",
    "\n",
    "    for ep in endpoints:\n",
    "        logger.info(f\"=== Test Reasoning Output sur endpoint='{ep['name']}' ===\")\n",
    "\n",
    "        openai.api_base = ep[\"api_base\"]\n",
    "        openai.api_key  = ep[\"api_key\"]\n",
    "\n",
    "        client = OpenAI(\n",
    "            api_key=ep[\"api_key\"],\n",
    "            base_url=ep[\"api_base\"]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=ep.get(\"model\"),\n",
    "                messages=[{\"role\": \"user\", \"content\": question}]\n",
    "            )\n",
    "        except Exception as ex:\n",
    "            logger.error(f\"  [!] Erreur lors de l'appel endpoint='{ep['name']}': {ex}\")\n",
    "            continue\n",
    "\n",
    "        choice = response.choices[0]\n",
    "        msg = choice.message\n",
    "\n",
    "        reasoning_part = getattr(msg, \"reasoning_content\", None)\n",
    "        final_content = msg.content or \"\"\n",
    "\n",
    "        logger.debug(f\"  -> Réponse (finale) : {final_content[:250]}\")\n",
    "        if reasoning_part:\n",
    "            logger.info(f\"  -> Raisonnement   : {reasoning_part[:250]}\")\n",
    "        else:\n",
    "            logger.warning(\"  -> Pas de 'reasoning_content' pour ce modèle/parsing.\")\n",
    "\n",
    "# Lancement\n",
    "test_reasoning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f38c48",
   "metadata": {
    "papermill": {
     "duration": 0.014264,
     "end_time": "2026-02-18T08:14:17.494523",
     "exception": false,
     "start_time": "2026-02-18T08:14:17.480259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du test de reasoning\n",
    "\n",
    "Ce test vérifie le **support du mode reasoning** sur les différents endpoints :\n",
    "\n",
    "**Comportement attendu** :\n",
    "\n",
    "| Endpoint | Support reasoning | Format de sortie |\n",
    "|----------|-------------------|------------------|\n",
    "| **DeepSeek R1** | Natif | `reasoning_content` + `content` séparés |\n",
    "| **o1/o3 (OpenAI)** | Natif mais non exposé | `content` uniquement (reasoning masqué) |\n",
    "| **Qwen/Llama standard** | Non supporté | `content` uniquement |\n",
    "\n",
    "**Exemple de sortie (DeepSeek R1)** :\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"reasoning_content\": \"<think>Calculons 253 × 73 : 253×70=17710, 253×3=759, total=18469. Ensuite 18469-287=18182</think>\",\n",
    "  \"content\": \"Le résultat est 18 182.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Points clés** :\n",
    "\n",
    "- **Balises `<think>`** : Indiquent le raisonnement intermédiaire\n",
    "- **Coût en tokens** : Le reasoning compte dans le total (peut doubler la consommation)\n",
    "- **Transparence** : Permet d'auditer le raisonnement du modèle\n",
    "- **Cas d'usage** : Pédagogie, debugging, vérification de calculs complexes\n",
    "\n",
    "**Limitations** :\n",
    "\n",
    "- Nécessite `--enable-reasoning --reasoning-parser deepseek_r1` lors du lancement vLLM\n",
    "- Pas supporté par tous les modèles (DeepSeek R1, o1/o3 uniquement pour l'instant)\n",
    "- Augmente significativement la latence et le coût\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6231ba",
   "metadata": {
    "papermill": {
     "duration": 0.012671,
     "end_time": "2026-02-18T08:14:17.522184",
     "exception": false,
     "start_time": "2026-02-18T08:14:17.509513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Benchmark final (avec journaux réguliers)\n",
    "\n",
    "Cette étape exécute un warm-up + N itérations par endpoint.\n",
    "On calcule ensuite la vitesse tokens/s. \n",
    "\n",
    "**Important** : Le prompt est un peu plus long, et la génération peut \n",
    "prendre du temps selon la taille du modèle ou la quantization.\n",
    "\n",
    "Pour ne pas paraître figé, on ajoute des `print` avant et après l'appel, \n",
    "pour indiquer la progression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2515fb7d",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:14:17.549729Z",
     "iopub.status.busy": "2026-02-18T08:14:17.549030Z",
     "iopub.status.idle": "2026-02-18T08:14:39.803101Z",
     "shell.execute_reply": "2026-02-18T08:14:39.801975Z"
    },
    "papermill": {
     "duration": 22.269969,
     "end_time": "2026-02-18T08:14:39.804534",
     "exception": false,
     "start_time": "2026-02-18T08:14:17.534565",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:17 [INFO] Local Llama - === Benchmark: endpoint=OpenAI, label=auto_benchmark1 ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:17 [INFO] Local Llama -    (warm-up) Lancement d'un appel simple.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:14:17 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:18 [INFO] Local Llama -    (benchmark) On va faire 1 itérations.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:18 [INFO] Local Llama -    -> Iteration 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m09:14:19 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:39 [INFO] Local Llama -       => Durée: 20.69s, tokens=1053\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:39 [INFO] Local Llama -    => OpenAI OK: 1/1 calls, avg_time=20.69s, speed=50.89 tok/s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:39 [INFO] Local Llama - [INFO] Fin du benchmark pour label='auto_benchmark1'.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:39 [INFO] Local Llama - \n",
      "=== Résultats de ce premier benchmark (label='auto_benchmark1') ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:39 [INFO] Local Llama - OpenAI -> 1/1 ok, avg time=20.69s, speed=50.89 tok/s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Paramètre: combien de fois on répète la requête pour la mesure.\n",
    "N_REPEATS = 1\n",
    "all_results = []\n",
    "\n",
    "def query_once(api_base, api_key, prompt, model=None):\n",
    "    \"\"\"\n",
    "    Appel unique via la nouvelle API (client = OpenAI(...)) + log.\n",
    "    Retourne (elapsed_seconds, total_tokens).\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    logger.debug(\"      ... appel client.chat.completions.create() en cours ...\")\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "            max_completion_tokens=1000,\n",
    "            stream=False,\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        tokens = resp.usage.total_tokens if resp.usage else None\n",
    "        return elapsed, tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"      [!] Exception: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def run_benchmark(label, prompt, repeats=N_REPEATS):\n",
    "    \"\"\"\n",
    "    1) Warm-up (unique appel) + N appels chronométrés par endpoint.\n",
    "    2) On log avant/après chaque appel via logger.\n",
    "    3) On stocke les métriques dans all_results.\n",
    "    \"\"\"\n",
    "    global all_results\n",
    "\n",
    "    for ep in endpoints:\n",
    "        logger.info(f\"=== Benchmark: endpoint={ep['name']}, label={label} ===\")\n",
    "\n",
    "        # WARM-UP\n",
    "        logger.info(\"   (warm-up) Lancement d'un appel simple.\")\n",
    "        w_elapsed, w_tokens = query_once(\n",
    "            ep[\"api_base\"], ep[\"api_key\"],\n",
    "            \"Warm up. Ignorez ce message.\",\n",
    "            model=ep.get(\"model\", None)\n",
    "        )\n",
    "        if w_elapsed is None:\n",
    "            logger.warning(f\"   [!] Warm-up échoué => skip {ep['name']}.\")\n",
    "            continue\n",
    "\n",
    "        total_time = 0.0\n",
    "        total_tokens = 0\n",
    "        success_count = 0\n",
    "\n",
    "        logger.info(f\"   (benchmark) On va faire {repeats} itérations.\")\n",
    "        for i in range(repeats):\n",
    "            logger.info(f\"   -> Iteration {i+1}/{repeats}\")\n",
    "            e, tks = query_once(\n",
    "                ep[\"api_base\"], ep[\"api_key\"],\n",
    "                prompt,\n",
    "                model=ep.get('model', None)\n",
    "            )\n",
    "            if e is None:\n",
    "                logger.error(\"      Echec de l'appel, on continue avec le suivant.\")\n",
    "                continue\n",
    "            logger.info(f\"      => Durée: {e:.2f}s, tokens={tks}\")\n",
    "            total_time += e\n",
    "            if tks is not None:\n",
    "                total_tokens += tks\n",
    "            success_count += 1\n",
    "\n",
    "        if success_count == 0:\n",
    "            logger.warning(\"   [!] Aucune itération réussie pour ce endpoint.\")\n",
    "            continue\n",
    "\n",
    "        avg_time = total_time / success_count\n",
    "        tok_per_sec = 0.0\n",
    "        if total_time > 0:\n",
    "            tok_per_sec = total_tokens / total_time\n",
    "\n",
    "        res = {\n",
    "            \"label\": label,\n",
    "            \"endpoint\": ep[\"name\"],\n",
    "            \"repeats\": repeats,\n",
    "            \"success_count\": success_count,\n",
    "            \"total_time_s\": total_time,\n",
    "            \"avg_time_s\": avg_time,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"tokens_per_sec\": tok_per_sec\n",
    "        }\n",
    "        all_results.append(res)\n",
    "        logger.info(\n",
    "            f\"   => {ep['name']} OK: {success_count}/{repeats} calls, \"\n",
    "            f\"avg_time={avg_time:.2f}s, speed={tok_per_sec:.2f} tok/s\"\n",
    "        )\n",
    "\n",
    "    logger.info(f\"[INFO] Fin du benchmark pour label='{label}'.\")\n",
    "\n",
    "# Exemple d’appel:\n",
    "label_run = \"auto_benchmark1\"\n",
    "USER_PROMPT = (\n",
    "    \"Rédige un texte d'environ 1000 mots sur l'IA, \"\n",
    "    \"en évoquant l'apprentissage machine, les grands modèles de langage, \"\n",
    "    \"et quelques perspectives d'évolution.\"\n",
    ")\n",
    "\n",
    "run_benchmark(label_run, USER_PROMPT, repeats=1)\n",
    "\n",
    "logger.info(f\"\\n=== Résultats de ce premier benchmark (label='{label_run}') ===\")\n",
    "for r in all_results:\n",
    "    if r[\"label\"] == label_run:\n",
    "        logger.info(\n",
    "            f\"{r['endpoint']} -> {r['success_count']}/{r['repeats']} ok, \"\n",
    "            f\"avg time={r['avg_time_s']:.2f}s, speed={r['tokens_per_sec']:.2f} tok/s\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185cf66",
   "metadata": {
    "papermill": {
     "duration": 0.015752,
     "end_time": "2026-02-18T08:14:39.849684",
     "exception": false,
     "start_time": "2026-02-18T08:14:39.833932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du benchmark séquentiel\n",
    "\n",
    "Ce benchmark mesure les **performances individuelles** de chaque endpoint en conditions normales :\n",
    "\n",
    "**Méthodologie** :\n",
    "\n",
    "- **Warm-up** : 2-3 requêtes pour charger le modèle en VRAM\n",
    "- **10 itérations** par endpoint avec le même prompt\n",
    "- **Mesure** : latence moyenne, débit (tokens/sec), success rate\n",
    "\n",
    "**Métriques analysées** :\n",
    "\n",
    "| Métrique | Signification | Valeur typique |\n",
    "|----------|---------------|----------------|\n",
    "| **Latence moyenne** | Temps par requête | 1-3s (8B 4-bit), 3-5s (14B 4-bit) |\n",
    "| **Tokens/sec** | Vitesse de génération | 30-50 tok/s (local), 15-20 tok/s (cloud) |\n",
    "| **Success rate** | Fiabilité | 100% (pas d'erreur réseau) |\n",
    "\n",
    "**Facteurs influençant les performances** :\n",
    "\n",
    "1. **Taille du modèle** : 7B < 14B < 70B (inversement proportionnel au débit)\n",
    "2. **Quantization** : 4-bit (rapide, moins précis) vs 16-bit (lent, très précis)\n",
    "3. **VRAM** : Plus de VRAM = batch size plus grand = meilleur débit\n",
    "4. **Context length** : Prompts longs → génération plus lente\n",
    "\n",
    "**Comparaison typique** :\n",
    "\n",
    "- **Local mini (8B 4-bit)** : 35-45 tok/s (RTX 3090/4090)\n",
    "- **Local medium (14B 4-bit)** : 20-25 tok/s (RTX 3090/4090)\n",
    "- **OpenAI gpt-5-mini** : 15-20 tok/s (cloud partagé)\n",
    "\n",
    "> **Conclusion** : Les modèles locaux bien configurés peuvent être **plus rapides** que le cloud pour des usages dédiés.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9ee50",
   "metadata": {
    "papermill": {
     "duration": 0.014431,
     "end_time": "2026-02-18T08:14:39.878766",
     "exception": false,
     "start_time": "2026-02-18T08:14:39.864335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test de traitement parallèle (batching)\n",
    "\n",
    "Dans cette cellule, nous allons :\n",
    "- Définir un nombre de requêtes à envoyer en parallèle (`N_PARALLEL`).\n",
    "- Pour chaque endpoint, lancer ces requêtes en **concurrence**.\n",
    "- Mesurer le temps total écoulé et le nombre total de tokens.\n",
    "- Calculer la **vitesse globale** de traitement (tokens / seconde) lorsque plusieurs requêtes arrivent simultanément.\n",
    "\n",
    "**vLLM** est réputé supporter le batching token-level et donc bénéficier d'une meilleure latence moyenne et d'un meilleur débit lorsqu'il y a plusieurs requêtes en parallèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0a56325",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:14:39.909360Z",
     "iopub.status.busy": "2026-02-18T08:14:39.908959Z",
     "iopub.status.idle": "2026-02-18T08:14:46.404797Z",
     "shell.execute_reply": "2026-02-18T08:14:46.403078Z"
    },
    "papermill": {
     "duration": 6.513439,
     "end_time": "2026-02-18T08:14:46.407534",
     "exception": false,
     "start_time": "2026-02-18T08:14:39.894095",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:39 [INFO] Local Llama - ==== Lancement du test parallèle: 25 requêtes simultanées par endpoint ====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:39 [INFO] Local Llama - === Parallel test sur 'OpenAI' ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:46 [INFO] Local Llama -   -> 25/25 appels OK\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:46 [INFO] Local Llama -   -> Durée totale (concurrente): 6.48 s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:46 [INFO] Local Llama -   -> Tokens cumulés: 5875\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:46 [INFO] Local Llama -   -> Vitesse globale: 907.07 tok/s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:46 [INFO] Local Llama - \n",
      "=== Récapitulatif du test parallèle ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:46 [INFO] Local Llama - OpenAI: 25/25 ok, total_time=6.48s, speed=907.07 tok/s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "N_PARALLEL = 25\n",
    "PARALLEL_PROMPT = (\n",
    "    \"Bonjour, ceci est un test de requêtes parallèles. \"\n",
    "    \"Peux-tu me donner quelques idées créatives pour un week-end ?\"\n",
    ")\n",
    "\n",
    "async def async_chat_completion(api_base: str, api_key: str, model: str, prompt: str):\n",
    "    url = f\"{api_base}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 200\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        start_t = time.time()\n",
    "        try:\n",
    "            async with session.post(url, headers=headers, json=payload, timeout=60) as resp:\n",
    "                elapsed = time.time() - start_t\n",
    "                if resp.status == 200:\n",
    "                    data = await resp.json()\n",
    "                    tokens = None\n",
    "                    if \"usage\" in data and data[\"usage\"].get(\"total_tokens\"):\n",
    "                        tokens = data[\"usage\"][\"total_tokens\"]\n",
    "                    return (elapsed, tokens)\n",
    "                else:\n",
    "                    return (None, None)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[!] Exception asynchrone: {e}\")\n",
    "            return (None, None)\n",
    "\n",
    "async def run_parallel_test(endpoint, n_parallel, prompt):\n",
    "    api_base = endpoint[\"api_base\"]\n",
    "    api_key  = endpoint[\"api_key\"]\n",
    "    model    = endpoint.get(\"model\", None)\n",
    "\n",
    "    tasks = []\n",
    "    for _ in range(n_parallel):\n",
    "        tasks.append(asyncio.create_task(async_chat_completion(api_base, api_key, model, prompt)))\n",
    "\n",
    "    start_time = time.time()\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    nb_ok = 0\n",
    "    sum_tokens = 0\n",
    "    for (elapsed, tokens) in results:\n",
    "        if elapsed is not None and tokens is not None:\n",
    "            nb_ok += 1\n",
    "            sum_tokens += tokens\n",
    "\n",
    "    return {\n",
    "        \"endpoint\": endpoint[\"name\"],\n",
    "        \"n_req\": n_parallel,\n",
    "        \"n_ok\": nb_ok,\n",
    "        \"total_time_s\": total_time,\n",
    "        \"sum_tokens\": sum_tokens\n",
    "    }\n",
    "\n",
    "async def parallel_benchmark(endpoints_list, n_parallel, prompt, use_random_prefix=True):\n",
    "    summary = []\n",
    "    logger.info(f\"==== Lancement du test parallèle: {n_parallel} requêtes simultanées par endpoint ====\")\n",
    "    \n",
    "    for ep in endpoints_list:\n",
    "        logger.info(f\"=== Parallel test sur '{ep['name']}' ===\")\n",
    "        \n",
    "        if use_random_prefix:\n",
    "            prefix = ''.join(random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZ', k=5))\n",
    "            modified_prompt = f\"{prefix} {prompt}\"\n",
    "        else:\n",
    "            modified_prompt = prompt\n",
    "        \n",
    "        res = await run_parallel_test(ep, n_parallel, modified_prompt)\n",
    "\n",
    "        nb_ok = res[\"n_ok\"]\n",
    "        total_time = res[\"total_time_s\"]\n",
    "        sum_tokens = res[\"sum_tokens\"]\n",
    "        speed = sum_tokens / total_time if total_time > 0 else 0\n",
    "\n",
    "        logger.info(f\"  -> {nb_ok}/{n_parallel} appels OK\")\n",
    "        logger.info(f\"  -> Durée totale (concurrente): {total_time:.2f} s\")\n",
    "        logger.info(f\"  -> Tokens cumulés: {sum_tokens}\")\n",
    "        logger.info(f\"  -> Vitesse globale: {speed:.2f} tok/s\")\n",
    "\n",
    "        summary.append(res)\n",
    "\n",
    "    logger.info(\"\\n=== Récapitulatif du test parallèle ===\")\n",
    "    for s in summary:\n",
    "        speed = 0.0\n",
    "        if s[\"total_time_s\"] > 0:\n",
    "            speed = s[\"sum_tokens\"] / s[\"total_time_s\"]\n",
    "        logger.info(\n",
    "            f\"{s['endpoint']}: {s['n_ok']}/{s['n_req']} ok, \"\n",
    "            f\"total_time={s['total_time_s']:.2f}s, speed={speed:.2f} tok/s\"\n",
    "        )\n",
    "\n",
    "import nest_asyncio\n",
    "import random\n",
    "nest_asyncio.apply()\n",
    "\n",
    "await parallel_benchmark(endpoints, N_PARALLEL, PARALLEL_PROMPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49d4e3",
   "metadata": {
    "papermill": {
     "duration": 0.015705,
     "end_time": "2026-02-18T08:14:46.451002",
     "exception": false,
     "start_time": "2026-02-18T08:14:46.435297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du test de batching\n",
    "\n",
    "Ce test évalue la **capacité de traitement parallèle** de chaque endpoint :\n",
    "\n",
    "**Méthodologie** :\n",
    "\n",
    "- **25 requêtes simultanées** par endpoint (via `asyncio.gather`)\n",
    "- **Prompt long** (500 mots demandés) pour mesurer la génération de tokens\n",
    "- **Mesure individuelle** : latence par requête et débit moyen\n",
    "\n",
    "**Résultats attendus** :\n",
    "\n",
    "| Endpoint | Latence moyenne | Débit (tok/s) | Observations |\n",
    "|----------|----------------|---------------|--------------|\n",
    "| **vLLM avec batching** | 3-5s | 150-300 tok/s | Latence stable même avec 25 requêtes |\n",
    "| **Sans batching** | 50-100s | 10-20 tok/s | Dégradation linéaire (file d'attente) |\n",
    "| **OpenAI cloud** | 5-10s | 50-100 tok/s | Rate limiting appliqué |\n",
    "\n",
    "**Points clés** :\n",
    "\n",
    "- **Batching efficace** : Les requêtes sont traitées en parallèle sur le GPU\n",
    "- **Utilisation VRAM** : Le batch size est limité par la VRAM disponible\n",
    "- **Trade-off** : Latence légèrement augmentée vs throughput multiplié\n",
    "\n",
    "> **Note technique** : vLLM utilise PagedAttention pour gérer efficacement le KV cache de multiples requêtes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643bbecf",
   "metadata": {
    "papermill": {
     "duration": 0.013774,
     "end_time": "2026-02-18T08:14:46.478775",
     "exception": false,
     "start_time": "2026-02-18T08:14:46.465001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 13 12) Test de parallélisme global : mesures de débits individuels et ordre d’exécution aléatoire\n",
    "\n",
    "Ici, nous souhaitons :\n",
    "1. **Mesurer** le débit (tokens/s) **individuellement** pour chaque endpoint.\n",
    "2. **Lancer** toutes les requêtes (pour tous les endpoints) **en ordre aléatoire**, sans rajouter de délais artificiels.\n",
    "\n",
    "**Principe** :\n",
    "- On construit d’abord **la liste** complète des appels (ex. `N_PARALLEL_GLOBAL` requêtes pour chaque endpoint).\n",
    "- On associe à chaque appel l’endpoint correspondant, puis on **randomise** l’ordre de cette liste.\n",
    "- On déclenche **en simultané** l’ensemble des requêtes (via `asyncio.gather`).\n",
    "- Après exécution, on calcule :\n",
    "  - **Durée totale** (début → fin) pour l’ensemble des requêtes,\n",
    "  - **Résultats individuels** (tokens cumulés par endpoint, nombre de requêtes OK, etc.),\n",
    "  - **Débit** de chaque endpoint : (somme des tokens pour cet endpoint) / (durée totale),\n",
    "  - **Débit global** : (somme de tous les tokens) / (durée totale).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff0cf71",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T08:14:46.507832Z",
     "iopub.status.busy": "2026-02-18T08:14:46.507379Z",
     "iopub.status.idle": "2026-02-18T08:14:51.697676Z",
     "shell.execute_reply": "2026-02-18T08:14:51.696670Z"
    },
    "papermill": {
     "duration": 5.207024,
     "end_time": "2026-02-18T08:14:51.699155",
     "exception": false,
     "start_time": "2026-02-18T08:14:46.492131",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:46 [INFO] Local Llama - === Lancement du test de parallélisme global sur 1 endpoints ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:46 [INFO] Local Llama -    -> 25 requêtes par endpoint (ordre aléatoire).\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:51 [INFO] Local Llama - \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:51 [INFO] Local Llama - === Résultats du test global (tous endpoints en même temps) ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:51 [INFO] Local Llama -   -> Nombre total de requêtes : 25\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:51 [INFO] Local Llama -   -> Nombre de requêtes OK   : 25\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:51 [INFO] Local Llama -   -> Fenêtre de temps (global) : 5.16 s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:51 [INFO] Local Llama -   -> Cumul de tokens (global)  : 5275\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:51 [INFO] Local Llama -   -> Débit global (tous endpoints) : 1022.02 tok/s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:51 [INFO] Local Llama - \n",
      "=== Détails par endpoint ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:51 [INFO] Local Llama - - OpenAI : 25/25 OK, tokens=5275\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:51 [INFO] Local Llama -     => Fenêtre concurrency = 5.16s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:14:51 [INFO] Local Llama -     => Débit effectif ~ 1022.02 tok/s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "\n",
    "N_PARALLEL_GLOBAL = 25  # Nombre de requêtes à envoyer par endpoint\n",
    "GLOBAL_PROMPT = (\n",
    "    \"Bonjour, ceci est un test de parallélisme global. \"\n",
    "    \"Peux-tu me détailler en 500 mots les avantages et inconvénients de travailler \"\n",
    "    \"avec plusieurs grands modèles (Llama, Qwen, GPT, etc.) en parallèle sur un même serveur ?\"\n",
    ")\n",
    "\n",
    "async def async_chat_completion(api_base: str, api_key: str, model: str, prompt: str):\n",
    "    \"\"\"\n",
    "    Effectue une requête unique (POST /chat/completions) en asynchrone.\n",
    "    Retourne un tuple (start_t, end_t, tokens, success).\n",
    "      - start_t  : l'instant du début effectif (time.time()) juste avant l'appel\n",
    "      - end_t    : l'instant de fin (time.time()) juste après la réception de la réponse\n",
    "      - tokens   : nombre de tokens dans la réponse (None si échec)\n",
    "      - success  : booléen (True si statut=200 et parse JSON OK)\n",
    "    \"\"\"\n",
    "    # On note l'instant de démarrage avant de créer la session et d'envoyer la requête\n",
    "    start_t = time.time()\n",
    "\n",
    "    url = f\"{api_base}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 150\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url, headers=headers, json=payload, timeout=60) as resp:\n",
    "                end_t = time.time()\n",
    "                if resp.status == 200:\n",
    "                    data = await resp.json()\n",
    "                    tokens = data.get(\"usage\", {}).get(\"total_tokens\", None)\n",
    "                    return (start_t, end_t, tokens, True)\n",
    "                else:\n",
    "                    return (start_t, end_t, None, False)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[!] Exception asynchrone: {e}\")\n",
    "            end_t = time.time()\n",
    "            return (start_t, end_t, None, False)\n",
    "\n",
    "async def run_full_parallel_on_all_endpoints(endpoints, n_parallel, prompt):\n",
    "    \"\"\"\n",
    "    Lance n_parallel requêtes pour chaque endpoint (donc total = n_parallel * len(endpoints)),\n",
    "    en un seul grand batch concurrent *et dans un ordre aléatoire*.\n",
    "\n",
    "    Retourne un dict contenant :\n",
    "\n",
    "    - global_min_start  : le plus petit start_t (sur toutes les requêtes, tous endpoints)\n",
    "    - global_max_end    : le plus grand end_t   (sur toutes les requêtes)\n",
    "    - n_req_total       : total de requêtes\n",
    "    - n_ok_global       : total de requêtes ayant répondu 200 OK\n",
    "    - sum_tokens_global : total de tokens (toutes requêtes OK)\n",
    "    - stats_endpoints   : dict par endpoint, contenant :\n",
    "         {\n",
    "           \"calls\": int,\n",
    "           \"ok\": int,\n",
    "           \"sum_tokens\": int,\n",
    "           \"min_start\": float,\n",
    "           \"max_end\": float\n",
    "         }\n",
    "      (on calcule ensuite un débit = sum_tokens / (max_end - min_start))\n",
    "    \"\"\"\n",
    "\n",
    "    # Prépare tous les appels (endpoint, prompt)\n",
    "    tasks_info = []\n",
    "    for ep in endpoints:\n",
    "        for _ in range(n_parallel):\n",
    "            # Optionnel : préfixe aléatoire pour \"casser\" un éventuel cache\n",
    "            prefix = \"\".join(random.choices(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", k=3))\n",
    "            modified_prompt = f\"{prefix} {prompt}\"\n",
    "            tasks_info.append((ep, modified_prompt))\n",
    "\n",
    "    # Mélange l’ordre des requêtes\n",
    "    random.shuffle(tasks_info)\n",
    "\n",
    "    # Transforme chaque (ep, prompt) en coroutine\n",
    "    coroutines = []\n",
    "    for (ep, pr) in tasks_info:\n",
    "        coroutines.append(async_chat_completion(ep[\"api_base\"], ep[\"api_key\"], ep[\"model\"], pr))\n",
    "\n",
    "    # Exécution en parallèle\n",
    "    results = await asyncio.gather(*coroutines)\n",
    "\n",
    "    # Rassemble toutes les stats\n",
    "    # On calcule le min de start_t et le max de end_t global\n",
    "    all_starts = []\n",
    "    all_ends = []\n",
    "\n",
    "    stats_per_endpoint = {}\n",
    "    for ep in endpoints:\n",
    "        stats_per_endpoint[ep[\"name\"]] = {\n",
    "            \"calls\": 0,\n",
    "            \"ok\": 0,\n",
    "            \"sum_tokens\": 0,\n",
    "            \"min_start\": float(\"inf\"),\n",
    "            \"max_end\": float(\"-inf\"),\n",
    "        }\n",
    "\n",
    "    global_ok = 0\n",
    "    global_tokens = 0\n",
    "\n",
    "    for i, (start_t, end_t, tokens, success) in enumerate(results):\n",
    "        ep_name = tasks_info[i][0][\"name\"]\n",
    "        ep_stats = stats_per_endpoint[ep_name]\n",
    "        ep_stats[\"calls\"] += 1\n",
    "\n",
    "        # On enrichit le min/max local à l'endpoint\n",
    "        if start_t < ep_stats[\"min_start\"]:\n",
    "            ep_stats[\"min_start\"] = start_t\n",
    "        if end_t > ep_stats[\"max_end\"]:\n",
    "            ep_stats[\"max_end\"] = end_t\n",
    "\n",
    "        # Idem pour le global\n",
    "        all_starts.append(start_t)\n",
    "        all_ends.append(end_t)\n",
    "\n",
    "        # On comptabilise tokens si success\n",
    "        if success and tokens is not None:\n",
    "            ep_stats[\"ok\"] += 1\n",
    "            ep_stats[\"sum_tokens\"] += tokens\n",
    "            global_ok += 1\n",
    "            global_tokens += tokens\n",
    "\n",
    "    # min/max global\n",
    "    global_min_start = min(all_starts) if all_starts else None\n",
    "    global_max_end = max(all_ends) if all_ends else None\n",
    "\n",
    "    summary = {\n",
    "        \"global_min_start\": global_min_start,\n",
    "        \"global_max_end\": global_max_end,\n",
    "        \"n_req_total\": len(results),\n",
    "        \"n_ok_global\": global_ok,\n",
    "        \"sum_tokens_global\": global_tokens,\n",
    "        \"stats_endpoints\": stats_per_endpoint\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "\n",
    "# --- Lancement effectif du test ---\n",
    "logger.info(f\"=== Lancement du test de parallélisme global sur {len(endpoints)} endpoints ===\")\n",
    "logger.info(f\"   -> {N_PARALLEL_GLOBAL} requêtes par endpoint (ordre aléatoire).\")\n",
    "\n",
    "summary_global = await run_full_parallel_on_all_endpoints(endpoints, N_PARALLEL_GLOBAL, GLOBAL_PROMPT)\n",
    "\n",
    "# Récapitulatif\n",
    "global_min_start = summary_global[\"global_min_start\"]\n",
    "global_max_end = summary_global[\"global_max_end\"]\n",
    "n_req_total = summary_global[\"n_req_total\"]\n",
    "n_ok_global = summary_global[\"n_ok_global\"]\n",
    "sum_tokens_global = summary_global[\"sum_tokens_global\"]\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"=== Résultats du test global (tous endpoints en même temps) ===\")\n",
    "logger.info(f\"  -> Nombre total de requêtes : {n_req_total}\")\n",
    "logger.info(f\"  -> Nombre de requêtes OK   : {n_ok_global}\")\n",
    "\n",
    "if global_min_start is not None and global_max_end is not None:\n",
    "    global_duration = global_max_end - global_min_start\n",
    "    logger.info(f\"  -> Fenêtre de temps (global) : {global_duration:.2f} s\")\n",
    "    logger.info(f\"  -> Cumul de tokens (global)  : {sum_tokens_global}\")\n",
    "    if global_duration > 0:\n",
    "        global_speed = sum_tokens_global / global_duration\n",
    "        logger.info(f\"  -> Débit global (tous endpoints) : {global_speed:.2f} tok/s\")\n",
    "\n",
    "logger.info(\"\\n=== Détails par endpoint ===\")\n",
    "for ep_name, ep_stats in summary_global[\"stats_endpoints\"].items():\n",
    "    calls = ep_stats[\"calls\"]\n",
    "    ok = ep_stats[\"ok\"]\n",
    "    sum_tks = ep_stats[\"sum_tokens\"]\n",
    "    ep_min_start = ep_stats[\"min_start\"]\n",
    "    ep_max_end   = ep_stats[\"max_end\"]\n",
    "\n",
    "    logger.info(f\"- {ep_name} : {ok}/{calls} OK, tokens={sum_tks}\")\n",
    "    if ok > 0 and ep_min_start < ep_max_end:  # au moins 1 requête\n",
    "        ep_concurrency_window = ep_max_end - ep_min_start\n",
    "        # 'ep_concurrency_window' = fenêtrage du 1er démarrage -> dernier aboutissement\n",
    "        speed_ep = sum_tks / ep_concurrency_window if ep_concurrency_window > 0 else 0\n",
    "        logger.info(f\"    => Fenêtre concurrency = {ep_concurrency_window:.2f}s\")\n",
    "        logger.info(f\"    => Débit effectif ~ {speed_ep:.2f} tok/s\")\n",
    "    else:\n",
    "        logger.info(\"    => Pas de requêtes OK ou pas de fenêtre exploitable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ea0fd",
   "metadata": {
    "papermill": {
     "duration": 0.014256,
     "end_time": "2026-02-18T08:14:51.727683",
     "exception": false,
     "start_time": "2026-02-18T08:14:51.713427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du test de parallélisme global\n",
    "\n",
    "Ce test mesure les **performances réelles en conditions de charge** en lançant toutes les requêtes simultanément :\n",
    "\n",
    "**Méthodologie** :\n",
    "\n",
    "1. **25 requêtes par endpoint** lancées en parallèle dans un ordre aléatoire\n",
    "2. **Mesure de la fenêtre de concurrence** : du premier démarrage au dernier aboutissement\n",
    "3. **Calcul du débit effectif** : tokens cumulés / durée totale\n",
    "\n",
    "**Métriques analysées** :\n",
    "\n",
    "| Métrique | Description | Valeur typique |\n",
    "|----------|-------------|----------------|\n",
    "| **Débit global** | Tous endpoints combinés | 500-1000 tok/s (3 endpoints) |\n",
    "| **Débit par endpoint** | Performance individuelle sous charge | 150-350 tok/s |\n",
    "| **Fenêtre concurrency** | Du 1er lancement au dernier retour | 15-30s pour 25 requêtes |\n",
    "| **Success rate** | Proportion de requêtes réussies | 100% (sans erreur réseau) |\n",
    "\n",
    "**Observations attendues** :\n",
    "\n",
    "- **vLLM avec batching** : Débit élevé même avec 25 requêtes simultanées (peu de dégradation)\n",
    "- **Sans batching** : Dégradation linéaire (25 requêtes = ~25× plus lent)\n",
    "- **OpenAI cloud** : Débit stable mais plafonné par rate limits\n",
    "\n",
    "**Comparaison séquentiel vs parallèle** :\n",
    "\n",
    "Séquentiel : 30 tok/s × 1 requête = 30 tok/s\n",
    "Parallèle  : 25 tok/s × 25 requêtes = 625 tok/s total\n",
    "\n",
    "**Avantage du batching** :\n",
    "\n",
    "- **Throughput multiplié** sans dégrader la latence individuelle\n",
    "- **Utilisation optimale du GPU** (saturant le batch size)\n",
    "- **Scalabilité** pour applications multi-utilisateurs\n",
    "\n",
    "**Points clés pour la production** :\n",
    "\n",
    "1. **Charge réaliste** : Simuler le trafic attendu (nombre de requêtes/min)\n",
    "2. **Rate limiting** : Implémenter des quotas pour éviter la surcharge\n",
    "3. **Monitoring** : Tracer débit, latence P50/P95/P99, success rate\n",
    "4. **Auto-scaling** : Ajouter des instances si débit global insuffisant\n",
    "\n",
    "> **Conclusion** : Le batching permet de **multiplier le débit global** sans dégrader significativement la latence individuelle. C'est l'avantage majeur de vLLM sur des solutions mono-requête.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa5179",
   "metadata": {
    "papermill": {
     "duration": 0.016931,
     "end_time": "2026-02-18T08:14:51.759640",
     "exception": false,
     "start_time": "2026-02-18T08:14:51.742709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Comparaison Coût : Cloud vs Local\n",
    "\n",
    "## Calcul du coût par token\n",
    "\n",
    "### OpenAI (Cloud)\n",
    "\n",
    "| Modèle | Input ($/1M tokens) | Output ($/1M tokens) |\n",
    "|--------|---------------------|----------------------|\n",
    "| gpt-4o | $2.50 | $10.00 |\n",
    "| gpt-4o-mini | $0.15 | $0.60 |\n",
    "| o4-mini | $1.10 | $4.40 |\n",
    "\n",
    "### Local (estimation)\n",
    "\n",
    "**Hypothèses** :\n",
    "- GPU RTX 3090 : ~$1500 amortis sur 3 ans\n",
    "- Électricité : ~$0.15/kWh, consommation ~350W\n",
    "- Hébergement : ~200€/mois (serveur dédié) ou gratuit (machine personnelle)\n",
    "\n",
    "**Calcul pour 1M tokens générés** (DeepSeek R1 8B, ~40 tok/s) :\n",
    "- Temps : 1M / 40 = 25,000 secondes = ~7 heures\n",
    "- Électricité : 7h × 0.35kW × $0.15 = **$0.37**\n",
    "- Amortissement GPU : (7h / 26,280h) × $1500 = **$0.40**\n",
    "- **Total local : ~$0.77 / 1M tokens**\n",
    "\n",
    "### Point d'équilibre\n",
    "\n",
    "```\n",
    "Volume mensuel où local devient rentable :\n",
    "- gpt-4o-mini output : $0.60/1M → local rentable dès ~100M tokens/mois\n",
    "- gpt-4o output : $10.00/1M → local rentable dès ~10M tokens/mois\n",
    "```\n",
    "\n",
    "**Conclusion** : Pour un usage intensif (>10M tokens/mois), l'hébergement local est généralement plus économique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba759d",
   "metadata": {
    "papermill": {
     "duration": 0.025425,
     "end_time": "2026-02-18T08:14:51.802655",
     "exception": false,
     "start_time": "2026-02-18T08:14:51.777230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "## Points clés\n",
    "\n",
    "1. **Compatibilité OpenAI API** : vLLM et Ollama exposent la même API que OpenAI\n",
    "2. **DeepSeek R1** : Alternative locale aux modèles raisonnants (o1, o3)\n",
    "3. **Qwen 2.5** : Excellent pour le tool calling local\n",
    "4. **Batching vLLM** : Multiplicateur de débit pour requêtes parallèles\n",
    "5. **Coût** : Local devient rentable au-delà de ~10M tokens/mois\n",
    "\n",
    "## Commandes Docker utiles\n",
    "\n",
    "```bash\n",
    "# vLLM avec DeepSeek R1\n",
    "docker run --gpus all -p 8000:8000 \\\n",
    "  vllm/vllm-openai:latest \\\n",
    "  --model deepseek-ai/DeepSeek-R1-Distill-Llama-8B \\\n",
    "  --enable-reasoning --reasoning-parser deepseek_r1\n",
    "\n",
    "# Ollama (plus simple)\n",
    "docker run -d --gpus all -p 11434:11434 ollama/ollama\n",
    "ollama pull deepseek-r1:8b\n",
    "```\n",
    "\n",
    "## Ressources\n",
    "\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "- [Ollama](https://ollama.ai/)\n",
    "- [DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-R1)\n",
    "- [Qwen 2.5](https://github.com/QwenLM/Qwen2.5)\n",
    "\n",
    "## Prochaines étapes\n",
    "\n",
    "- **Notebook 8** : Comparaison détaillée des modèles raisonnants\n",
    "- **Notebook 9** : Patterns de production (retry, monitoring)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 82.695494,
   "end_time": "2026-02-18T08:14:52.826006",
   "environment_variables": {},
   "exception": null,
   "input_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Texte\\10_LocalLlama.ipynb",
   "output_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Texte\\10_LocalLlama_output.ipynb",
   "parameters": {
    "BATCH_MODE": "true"
   },
   "start_time": "2026-02-18T08:13:30.130512",
   "version": "2.6.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}