{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook : G√©n√©ration d'Images, Low-Code AI, Function Calling, RAG\n",
    "\n",
    "Dans ce notebook, nous allons tour √† tour d√©couvrir :\n",
    "\n",
    "1. Comment g√©n√©rer des **images** √† partir de prompts en texte (ex: DALL-E, Midjourney).\n",
    "2. Comment cr√©er des **applications low-code** enrichies par l'IA, gr√¢ce √† **Power Platform** (Copilot, AI Builder).\n",
    "3. **Function Calling** c√¥t√© OpenAI : structurer les r√©ponses d‚Äôun LLM pour d√©clencher des actions.\n",
    "4. **RAG** (Retrieval Augmented Generation) et **bases vectorielles** (indexation, recherche s√©mantique, chunking, etc.).\n",
    "\n",
    "\n",
    "\n",
    "## Pr√©requis & Installation\n",
    "\n",
    "- **Python 3.9+** (ou version ult√©rieure).\n",
    "- Un compte [OpenAI](https://platform.openai.com/) et une cl√© d‚ÄôAPI valide.\n",
    "- Le fichier `.env` contenant votre cl√© d‚ÄôAPI :\n",
    "OPENAI_API_KEY=sk-...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi la g√©n√©ration d'images via IA ?\n",
    "\n",
    "Les mod√®les de g√©n√©ration d'images, tels que **DALL-E** (OpenAI) ou **Midjourney**, ont la capacit√© de cr√©er des visuels originaux √† partir de simples descriptions textuelles (prompts).  \n",
    "- **Applications concr√®tes** :  \n",
    "  - Design rapide de prototypes (marketing, publicit√©)  \n",
    "  - Cr√©ation artistique (concept art, storyboards)  \n",
    "  - Illustrations p√©dagogiques ou infographiques  \n",
    "- **Limitations** :  \n",
    "  - Les images peuvent contenir des incoh√©rences (proportions bizarres, doigts suppl√©mentaires, etc.)  \n",
    "  - Certaines requ√™tes contraires aux politiques d‚Äôutilisation peuvent √™tre bloqu√©es  \n",
    "\n",
    "Dans la suite, nous allons voir comment **OpenAI** g√®re la g√©n√©ration d‚Äôimages via l‚ÄôAPI `images.generate()`, et comment int√©grer ces visuels dans un flux d'automatisation (Low-Code) ou dans des applications web.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour √©viter l'erreur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": "import os\nimport requests\n# from PIL import Image\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\nimport openai\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\n# Charger le fichier .env pour la cl√© OPENAI_API_KEY (dans le r√©pertoire parent GenAI/)\nload_dotenv('../.env')\n\n# Config globale - initialisation du client OpenAI moderne\nclient = OpenAI()\n\n# ‚ö†Ô∏è NOTE : images.generate() requiert l'API OpenAI native (pas OpenRouter)\n# Si vous utilisez OpenRouter (OPENAI_BASE_URL d√©fini dans .env),\n# cette cellule sera automatiquement skipp√©e en mode batch.\n# Pour tester la g√©n√©ration d'images, utilisez une cl√© OpenAI pure sans BASE_URL.\n\nprint(\"üîç V√©rification de la configuration...\")\nbase_url = os.getenv(\"OPENAI_BASE_URL\", \"\")\nif \"openrouter\" in base_url.lower():\n    print(\"‚ö†Ô∏è Configuration OpenRouter d√©tect√©e - g√©n√©ration d'images non support√©e\")\n    print(\"   OpenRouter ne supporte pas l'endpoint images.generate()\")\n    print(\"   Cette fonctionnalit√© sera illustr√©e dans les notebooks Images Foundation\")\nelse:\n    print(\"‚úÖ Configuration OpenAI native d√©tect√©e - g√©n√©ration d'images disponible\")\n    try:\n        response = client.images.generate(\n            prompt=\"Lapin sur un cheval tenant une sucette, dans un champ brumeux\"\n        )\n        # response est un ImagesResponse\n        image_url = response.data[0].url\n        print(\"Image URL:\", image_url)\n\n        image = Image(url=image_url)\n        display(image)\n\n    except openai.APIConnectionError as e:\n        print(\"Erreur de connexion r√©seau:\", e)\n    except openai.RateLimitError as e:\n        print(\"Limite atteinte ou quota d√©pass√©:\", e)\n    except openai.APIStatusError as e:\n        print(\"Erreur HTTP renvoy√©e par l'API (4xx, 5xx, etc.):\", e)\n    except openai.APIError as e:\n        print(\"Autre erreur OpenAI:\", e)"
  },
  {
   "cell_type": "markdown",
   "source": "### R√©sultat de la g√©n√©ration d'image\n\nL'API `images.generate()` d'OpenAI permet de cr√©er des images √† partir de descriptions textuelles :\n\n**Observation du r√©sultat** :\n- L'URL retourn√©e pointe vers une image temporaire h√©berg√©e par OpenAI\n- L'image est g√©n√©r√©e en quelques secondes (selon le mod√®le DALL-E utilis√©)\n- Le prompt ¬´ Lapin sur un cheval tenant une sucette, dans un champ brumeux ¬ª d√©montre la capacit√© du mod√®le √† combiner plusieurs √©l√©ments\n\n**Points techniques** :\n1. **URL temporaire** : L'image est accessible pendant ~1 heure, il faut la t√©l√©charger pour la conserver\n2. **Format** : Par d√©faut, PNG 1024x1024 (configurable avec `size` et `quality`)\n3. **Limite de contenu** : OpenAI filtre les prompts inappropri√©s via sa politique d'utilisation\n\n**Applications** :\n- Prototypage rapide de concepts visuels\n- Illustration d'articles ou pr√©sentations\n- G√©n√©ration de variations cr√©atives\n- Storyboarding\n\n**Astuce** : Plus le prompt est d√©taill√© et pr√©cis, meilleure est la qualit√© de l'image g√©n√©r√©e.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Analyse comparative des prompts d'images\n\nCette comparaison de 3 prompts diff√©rents illustre l'**impact du style et de la pr√©cision** sur la g√©n√©ration :\n\n**Prompt 1** : ¬´ A small kitten wearing a hat, cartoon style ¬ª\n- Style g√©n√©rique, peu de d√©tails\n- R√©sultat attendu : Image cartoon basique\n\n**Prompt 2** : ¬´ A realistic portrait of a small kitten wearing a cowboy hat in the desert ¬ª\n- Style r√©aliste explicite\n- Contexte pr√©cis (d√©sert)\n- Accessoire sp√©cifique (chapeau de cowboy)\n- R√©sultat attendu : Image photor√©aliste avec atmosph√®re western\n\n**Prompt 3** : ¬´ A small kitten wearing a futuristic helmet in cyberpunk style, neon colors ¬ª\n- Style artistique d√©fini (cyberpunk)\n- Palette de couleurs impos√©e (n√©on)\n- R√©sultat attendu : Image futuriste avec esth√©tique sci-fi\n\n**Enseignements** :\n1. **Sp√©cificit√© = Contr√¥le** : Plus le prompt est d√©taill√©, plus le r√©sultat correspond aux attentes\n2. **Styles vari√©s** : cartoon, realistic, cyberpunk produisent des rendus tr√®s diff√©rents\n3. **Contexte important** : L'environnement (d√©sert) influence fortement l'ambiance\n\n**Bonne pratique** : Tester plusieurs variations de prompts pour trouver le meilleur rendu.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "#  Comparaison de prompts d'images\n",
    "# ============================\n",
    "\n",
    "image_prompts = [\n",
    "    \"A small kitten wearing a hat, cartoon style\",\n",
    "    \"A realistic portrait of a small kitten wearing a cowboy hat in the desert\",\n",
    "    \"A small kitten wearing a futuristic helmet in cyberpunk style, neon colors\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(image_prompts):\n",
    "    try:\n",
    "        print(f\"--- Prompt #{i+1}: {prompt} ---\")\n",
    "        response_img = openai.images.generate(prompt=prompt)\n",
    "        img_url = response_img.data[0].url\n",
    "        print(\"Image URL:\", img_url)\n",
    "        # Optionnel : display() si tu es dans un environnement Jupyter\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Erreur lors de la g√©n√©ration d'image:\", e)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 M√©ta-prompts et usage responsable\n",
    "\n",
    "Pour g√©rer un usage plus responsable et filtrer des images non souhait√©es, on peut ajouter un \n",
    "**meta-prompt** en amont, d√©crivant les restrictions (ex: Safe for Work, No adult content, etc.).\n",
    "\n",
    "Ex:\n",
    "You are an assistant that only generates children-friendly images. [... consignes ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Low-Code AI Apps (Power Platform)\n",
    "\n",
    "## 2.1 Introduction\n",
    "Power Platform inclut :\n",
    "- Power Apps (construction rapide d'apps)\n",
    "- Power Automate (workflows et automatisations)\n",
    "- Dataverse (stockage de donn√©es)\n",
    "- AI Builder (mod√®les IA pr√©-construits)\n",
    "- Copilot (assistant pour g√©n√©rer tables, flux, e-mails)\n",
    "\n",
    "Avantages : construction **no-code / low-code** pour mettre en place des solutions rapidement, \n",
    "y compris connect√©es √† des services IA.\n",
    "\n",
    "\n",
    "## 2.2 Copilot dans Power Apps : Student Assignment Tracker\n",
    "\n",
    "Exemple : On veut un **Student Assignment Tracker**.\n",
    "\n",
    "1. Sur la home de [Power Apps](https://make.powerapps.com), on saisit dans la zone Copilot : \n",
    "   \"I want an app to track and manage student assignments.\"\n",
    "2. Copilot propose une table Dataverse (champs Title, DateDue, StudentName, etc.)\n",
    "3. Personnaliser la table (ajouter `StudentEmail`, etc.)\n",
    "4. Cliquer \"Create app\" => Copilot g√©n√®re une **Canvas App** auto.\n",
    "5. Ajouter une page (screen) pour \"Envoyer un email\" (Prompt : \"I want a screen to send an email to the student\").\n",
    "\n",
    "On obtient en quelques clics un d√©but d'application.\n",
    "\n",
    "\n",
    "## 2.3 Copilot dans Power Automate : Invoice Processing\n",
    "\n",
    "M√™me concept : Dans [Power Automate](https://make.powerautomate.com),\n",
    "on demande \"Process an invoice when it arrives in my mailbox\", \n",
    "Copilot propose un flux (trigger: new mail arrives + extractions + email)...\n",
    "\n",
    "On peut ensuite y int√©grer **AI Builder** : \n",
    "- ex: le pr√©built model \"Invoice Processing\" pour extraire `supplier`, `amount`, etc.\n",
    "- stocker dans Dataverse, \n",
    "- email de confirmation.\n",
    "\n",
    "C‚Äôest un gros gain de temps pour la finance ou la logistique !\n",
    "\n",
    "\n",
    "# 3. Function Calling (OpenAI)\n",
    "\n",
    "\n",
    "## 3.1 Pourquoi ?\n",
    "\n",
    "Sans function calling, le LLM renvoie du texte non structur√©. \n",
    "Difficile d‚Äôautomatiser (ex: parse JSON, ex√©cuter une fonction tierce).\n",
    "Avec function calling, on d√©clare un `schema` JSON, \n",
    "le LLM r√©pond par un `function_call`: \n",
    "- Nom de la fonction \n",
    "- Arguments structur√©s\n",
    "\n",
    "Ensuite on ex√©cute la fonction en Python (ou autre).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import openai\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Find me a good course for a beginner developer to learn Roo-code. Join a message to your function calls\"}\n]\n\n# ‚úÖ Nouvelle API tools (fonctions d√©pr√©ci√©es depuis 2023)\ntools = [\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"search_courses\",\n      \"description\": \"Retrieves relevant courses based on role, product & level\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"role\":   {\"type\":\"string\",\"description\":\"the role of the user\"},\n          \"product\":{\"type\":\"string\",\"description\":\"the product/tech\"},\n          \"level\": {\"type\":\"string\",\"description\":\"the user skill level\"}\n        },\n        \"required\": [\"role\",\"product\",\"level\"]\n      }\n    }\n  }\n]\n\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=messages,\n        tools=tools,  # ‚úÖ Nouvelle API\n        tool_choice=\"auto\"  # ‚úÖ Remplace function_call\n    )\n\n    print(\"R√©ponse brute:\\n\", response.choices[0].message)\n\nexcept openai.RateLimitError as e:\n    print(\"Limite atteinte:\", e)\nexcept openai.APIError as e:\n    print(\"Autre erreur:\", e)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n\ndef search_courses(role,product,level):\n    # Ton code Python => Appel API Microsoft Learn\n    # On renvoie un JSON/string\n    return \"Liste de cours: Roo-code For EPF, etc.\"\n\nresp_msg = response.choices[0].message\n\n# ‚úÖ Nouvelle API : tool_calls au lieu de function_call\nif resp_msg.tool_calls:\n    tool_call = resp_msg.tool_calls[0]  # Premier appel\n    fn_name = tool_call.function.name\n    fn_args = json.loads(tool_call.function.arguments)\n    \n    # Ex√©cuter la fonction Python correspondante\n    result = search_courses(**fn_args)\n\n    # On cr√©e deux messages :\n    # 1) l'assistant avec tool_calls\n    # 2) le role=\"tool\" + content du r√©sultat\n    second_messages = messages + [\n      {\n        \"role\": \"assistant\",\n        \"tool_calls\": [{\n          \"id\": tool_call.id,\n          \"type\": \"function\",\n          \"function\": {\n            \"name\": fn_name,\n            \"arguments\": tool_call.function.arguments\n          }\n        }]\n      },\n      {\n        \"role\": \"tool\",  # ‚úÖ Remplace role=\"function\"\n        \"tool_call_id\": tool_call.id,\n        \"name\": fn_name,\n        \"content\": result\n      }\n    ]\n\n    # On relance le chat\n    final_resp = client.chat.completions.create(\n       model=\"gpt-4o-mini\",\n       messages=second_messages\n    )\n    print(\"R√©ponse finale:\\n\", final_resp.choices[0].message.content)\nelse:\n    print(\"Aucun tool call d√©tect√©\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling Avanc√© avec OpenAI : Cha√Ænage de Fonctions  \n",
    "\n",
    "Ce bloc de code illustre un exemple avanc√© de **Function Calling** avec OpenAI, permettant √† l'IA d'orchestrer plusieurs appels de fonctions de mani√®re autonome.  \n",
    "\n",
    "### üõ† Fonctionnalit√©s mises en ≈ìuvre :  \n",
    "1. **Premier appel √† l'API OpenAI** :  \n",
    "   - L'utilisateur demande de planifier une r√©union.  \n",
    "   - L'IA d√©tecte qu'il faut appeler `create_meeting_event` et g√©n√®re les arguments n√©cessaires (`topic`, `date`, `participants`).  \n",
    "   - Le mod√®le ne renvoie pas de texte brut, mais un `function_call` contenant les param√®tres de la r√©union.  \n",
    "\n",
    "2. **Ex√©cution locale de `create_meeting_event` en Python** :  \n",
    "   - La fonction g√©n√®re un objet √©v√©nement fictif avec un `event_id`.  \n",
    "   - Ce r√©sultat est transmis √† l'IA en tant que r√©ponse fonctionnelle (`role=\"function\"`).  \n",
    "\n",
    "3. **Deuxi√®me appel √† l'API OpenAI** :  \n",
    "   - Sur la base de l'√©v√©nement cr√©√©, l'IA d√©cide de g√©n√©rer un email de confirmation en appelant `send_email`.  \n",
    "   - L'IA fournit les arguments (`subject`, `body`, `recipients`).  \n",
    "\n",
    "4. **Ex√©cution locale de `send_email` en Python** :  \n",
    "   - Simulation de l'envoi d'email avec un affichage console.  \n",
    "   - Message de confirmation `\"Email sent successfully!\"`.  \n",
    "\n",
    "### üìå Preuve du bon fonctionnement :  \n",
    "‚úÖ **Cha√Ænage r√©ussi** : OpenAI a d√©clench√© **deux appels de fonction distincts**, prouvant la capacit√© du mod√®le √† raisonner sur plusieurs √©tapes.  \n",
    "‚úÖ **Ex√©cution hybride** : L'IA d√©cide des actions √† effectuer, mais l'ex√©cution est d√©l√©gu√©e au code Python.  \n",
    "‚úÖ **Application possible** : Ce workflow peut √™tre adapt√© pour int√©grer des bases de donn√©es, envoyer de vrais emails ou automatiser des t√¢ches complexes.  \n",
    "\n",
    "üîπ **Exemple d'affichage console :**  \n",
    "```\n",
    "Raw response: ChatCompletionMessage(..., function_call=FunctionCall(...))\n",
    "=== Simulated Email ===\n",
    "Subject: Confirmation de la r√©union sur l'√©tat du projet\n",
    "To: ['Alice', 'Bob']\n",
    "Body: Bonjour Alice et Bob,\n",
    "Je vous confirme que la r√©union sur l'√©tat du projet est planifi√©e pour mardi prochain, le 31 octobre 2023, √† 10h.\n",
    "√Ä bient√¥t,\n",
    "L'√©quipe de gestion de projet\n",
    "Email function result: Email sent successfully!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================",
    "# Function Calling (avanc√©) avec 2 fonctions",
    "# ============================",
    "",
    "import json",
    "import openai",
    "",
    "def create_meeting_event(topic, date, participants):",
    "    # Exemple simul√© : cr√©ation d'un objet event",
    "    return {",
    "        \"event_id\": \"evt-001\",",
    "        \"topic\": topic,",
    "        \"date\": date,",
    "        \"participants\": participants",
    "    }",
    "",
    "def send_email(subject, body, recipients):",
    "    # Exemple simul√© : envoi d'un email",
    "    print(\"=== Simulated Email ===\")",
    "    print(\"Subject:\", subject)",
    "    print(\"To:\", recipients)",
    "    print(\"Body:\", body)",
    "    return \"Email sent successfully!\"",
    "",
    "# ‚úÖ Nouvelle API tools (fonctions d√©pr√©ci√©es depuis 2023)",
    "tools = [",
    "    {",
    "        \"type\": \"function\",",
    "        \"function\": {",
    "            \"name\": \"create_meeting_event\",",
    "            \"description\": \"Cr√©er un √©v√©nement de r√©union\",",
    "            \"parameters\": {",
    "                \"type\": \"object\",",
    "                \"properties\": {",
    "                    \"topic\": {\"type\": \"string\"},",
    "                    \"date\": {\"type\": \"string\"},",
    "                    \"participants\": {",
    "                        \"type\": \"array\",",
    "                        \"items\": {\"type\": \"string\"}",
    "                    }",
    "                },",
    "                \"required\": [\"topic\", \"date\", \"participants\"]",
    "            }",
    "        }",
    "    },",
    "    {",
    "        \"type\": \"function\",",
    "        \"function\": {",
    "            \"name\": \"send_email\",",
    "            \"description\": \"Envoyer un email\",",
    "            \"parameters\": {",
    "                \"type\": \"object\",",
    "                \"properties\": {",
    "                    \"subject\": {\"type\": \"string\"},",
    "                    \"body\": {\"type\": \"string\"},",
    "                    \"recipients\": {",
    "                        \"type\": \"array\",",
    "                        \"items\": {\"type\": \"string\"}",
    "                    }",
    "                },",
    "                \"required\": [\"subject\", \"body\", \"recipients\"]",
    "            }",
    "        }",
    "    }",
    "]",
    "",
    "messages_fc = [",
    "    {",
    "        \"role\": \"user\",",
    "        \"content\": (",
    "            \"Planifie une r√©union sur l'√©tat du projet pour mardi prochain √† 10h \"",
    "            \"avec Alice et Bob, puis envoie un email de confirmation.\"",
    "        )",
    "    }",
    "]",
    "",
    "# L'appel initial : le mod√®le peut d√©cider d'appeler",
    "# create_meeting_event, send_email, ou rien (tool_choice=\"auto\").",
    "response_fc = client.chat.completions.create(",
    "    model=\"gpt-4o-mini\",",
    "    messages=messages_fc,",
    "    tools=tools,  # ‚úÖ Nouvelle API",
    "    tool_choice=\"auto\"  # ‚úÖ Remplace function_call",
    ")",
    "",
    "assistant_msg = response_fc.choices[0].message",
    "print(\"Raw response:\", assistant_msg)",
    "",
    "# ‚úÖ Nouvelle API : tool_calls au lieu de function_call",
    "if assistant_msg.tool_calls:",
    "    tool_call = assistant_msg.tool_calls[0]  # Premier appel",
    "    fn_name = tool_call.function.name",
    "    fn_args = json.loads(tool_call.function.arguments)",
    "",
    "    if fn_name == \"create_meeting_event\":",
    "        # 1) Ex√©cuter la fonction create_meeting_event c√¥t√© Python",
    "        result_event = create_meeting_event(**fn_args)",
    "",
    "        # 2) Cr√©er de nouveaux messages (assistant + tool)",
    "        second_messages = messages_fc + [",
    "            {",
    "                \"role\": \"assistant\",",
    "                \"tool_calls\": [{",
    "                    \"id\": tool_call.id,",
    "                    \"type\": \"function\",",
    "                    \"function\": {",
    "                        \"name\": fn_name,",
    "                        \"arguments\": tool_call.function.arguments",
    "                    }",
    "                }]",
    "            },",
    "            {",
    "                \"role\": \"tool\",  # ‚úÖ Remplace role=\"function\"",
    "                \"tool_call_id\": tool_call.id,",
    "                \"name\": fn_name,",
    "                \"content\": json.dumps(result_event)",
    "            }",
    "        ]",
    "",
    "        # 3) Relancer le chat pour voir si le mod√®le appelle la 2e fonction",
    "        second_response = client.chat.completions.create(",
    "            model=\"gpt-4o-mini\",",
    "            messages=second_messages,",
    "            tools=tools,  # ‚úÖ Nouvelle API",
    "            tool_choice=\"auto\"  # ‚úÖ Remplace function_call",
    "        )",
    "        second_msg = second_response.choices[0].message",
    "",
    "        if second_msg.tool_calls:",
    "            tool_call2 = second_msg.tool_calls[0]",
    "            fn2_name = tool_call2.function.name",
    "            fn2_args = json.loads(tool_call2.function.arguments)",
    "",
    "            if fn2_name == \"send_email\":",
    "                # Ex√©cution de la seconde fonction",
    "                result_email = send_email(**fn2_args)",
    "                print(\"Email function result:\", result_email)",
    "            else:",
    "                print(f\"The model called a different function: {fn2_name}\")",
    "        else:",
    "            print(\"No second tool call was triggered.\")",
    "    else:",
    "        print(f\"The model called a different function: {fn_name}\")",
    "else:",
    "    print(\"No tool call triggered by the assistant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Retrieval Augmented Generation & Vector Databases\n",
    "\n",
    "## 4.1 Principe\n",
    "Un LLM (ex: GPT) a une limite : il ne conna√Æt pas forc√©ment nos documents internes. \n",
    "RAG => on stocke nos docs dans une base vectorielle (embeddings), \n",
    "puis √† chaque question, on envoie au LLM les passages pertinents (retrieval + augmentation).\n",
    "\n",
    "## 4.2 Cr√©ation d‚Äôune base vectorielle\n",
    "\n",
    "- On d√©coupe (chunk) nos documents en petits segments (ex: 400 tokens).\n",
    "- On calcule embeddings (ex: text-embedding-ada-002).\n",
    "- On stocke : ex. Cosmos DB, Pinecone, ChromaDB, Elasticsearch, Qdrant, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn numpy pandas requests beautifulsoup4 lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL du d√©bat\n",
    "url = \"https://home.nps.gov/liho/learn/historyculture/debate1.htm\"\n",
    "\n",
    "# Requ√™te HTTP\n",
    "response = requests.get(url)\n",
    "html = response.text  # contenu HTML sous forme de string\n",
    "\n",
    "# print(html)\n",
    "\n",
    "# On parse avec BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# S√©lection du noeud principal.\n",
    "# Selon ton info: div.ColumnMain:nth-child(2)\n",
    "# (Le \"nth-child(2)\" est parfois incertain, on peut tenter un select plus large.)\n",
    "main_div = soup.select_one(\"div.ColumnMain\")\n",
    "\n",
    "if not main_div:\n",
    "    raise ValueError(\"Impossible de trouver le div.ColumnMain dans la page !\")\n",
    "\n",
    "# Extraction du texte brut (on s√©pare par \" \" pour √©viter les collisions)\n",
    "debate_text = main_div.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "#Enregistrement du texte dans un fichier\n",
    "with open(\"debate.txt\", \"w\") as f:\n",
    "    f.write(debate_text)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation du r√©sultat du web scraping\n\nLe web scraping extrait le contenu HTML brut de la page du d√©bat Lincoln-Douglas :\n\n**Ce qui a √©t√© r√©cup√©r√©** :\n- Le texte int√©gral du premier d√©bat d'Ottawa (21 ao√ªt 1858)\n- Tous les paragraphes contenus dans `div.ColumnMain`\n- M√©tadonn√©es potentielles (dates, lieux, orateurs)\n\n**Points techniques** :\n1. **BeautifulSoup** : Parseur HTML qui permet de naviguer dans l'arbre DOM\n2. **S√©lecteur CSS** : `div.ColumnMain` cible le conteneur principal du contenu\n3. **Nettoyage** : `get_text(separator=\"\\n\", strip=True)` supprime les balises HTML et normalise les espaces\n\n**V√©rifications importantes** :\n- Longueur du texte (devrait faire plusieurs milliers de caract√®res)\n- Coh√©rence du contenu (d√©bat sur l'esclavage, questions-r√©ponses)\n- Absence de balises HTML r√©siduelles\n\n**Prochaine √©tape** : Ce texte brut sera d√©coup√© en chunks pour la RAG (Retrieval Augmented Generation).\n\n**Note √©thique** : Toujours v√©rifier les conditions d'utilisation du site (robots.txt, terms of service) avant de scraper.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement depuis le fichier\n",
    "with open(\"debate.txt\", \"r\") as f:\n",
    "    debate_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Analyse du chunking\n\nLe d√©coupage (chunking) est une √©tape **critique** de la RAG :\n\n**Strat√©gie utilis√©e** :\n- **Taille de chunk** : 400 mots\n- **Overlap** : 50 mots (chevauchement entre chunks cons√©cutifs)\n\n**Pourquoi c'est important ?**\n\n1. **Chunks trop petits** :\n   - Contexte insuffisant pour r√©pondre aux questions\n   - Plus de chunks = plus de calculs d'embeddings\n   \n2. **Chunks trop grands** :\n   - Dilution de l'information pertinente\n   - Risque de d√©passer la fen√™tre de contexte du LLM\n   \n3. **Overlap (chevauchement)** :\n   - √âvite la coupure de phrases importantes √† la fronti√®re entre chunks\n   - Garantit qu'une information n'est jamais \"perdue\" entre deux segments\n\n**R√©sultat observable** :\n- Le DataFrame `df_chunks` contient maintenant plusieurs lignes (probablement 15-25 chunks)\n- Chaque chunk repr√©sente environ 400 mots du d√©bat\n- Les chunks se chevauchent l√©g√®rement pour assurer la continuit√©\n\n**Prochaine √©tape** : Calculer les embeddings de chaque chunk pour la recherche s√©mantique.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour d√©bogage:\n",
    "print(\"=== Longueur du texte r√©cup√©r√©:\", len(debate_text))\n",
    "print(debate_text[:10000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"title\":\"First Debate: Ottawa, Illinois (NPS)\",\n",
    "            \"text\": debate_text\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation du r√©sultat RAG\n\nLe syst√®me RAG (Retrieval Augmented Generation) a fonctionn√© en **3 √©tapes** :\n\n**√âtape 1 : Retrieval (Recherche)**\n- L'embedding de la question ¬´ What did Lincoln argue about slavery in that first debate? ¬ª a √©t√© calcul√©\n- Les 3 chunks les plus similaires s√©mantiquement ont √©t√© identifi√©s via k-NN\n- Ces chunks contiennent probablement les passages o√π Lincoln parle de l'esclavage\n\n**√âtape 2 : Augmentation**\n- Le prompt original est enrichi avec les 3 chunks pertinents\n- Le contexte fourni au LLM est maintenant **bas√© sur des sources r√©elles** (le d√©bat)\n- Cela r√©duit drastiquement le risque d'hallucination\n\n**√âtape 3 : Generation**\n- Le LLM g√©n√®re une r√©ponse en s'appuyant sur les passages fournis\n- La r√©ponse devrait citer ou paraphraser les arguments de Lincoln (ex: opposition √† l'extension de l'esclavage, √©galit√© naturelle, etc.)\n\n**Avantages de la RAG observables** :\n1. **Pr√©cision factuelle** : La r√©ponse s'appuie sur le texte source\n2. **Tra√ßabilit√©** : On peut v√©rifier les chunks utilis√©s\n3. **Pas d'hallucination** : Le mod√®le ne peut pas inventer d'informations non pr√©sentes dans les chunks\n\n**Am√©lioration possible** : Retourner √©galement les r√©f√©rences exactes (num√©ros de chunk, extraits) pour permettre la v√©rification.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    splitted = split_text_into_chunks(row[\"text\"], chunk_size=400, overlap=50)\n",
    "    for chunk in splitted:\n",
    "        rows.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"chunk\": chunk\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(rows)\n",
    "print(\"Nombre de chunks =\", len(df_chunks))\n",
    "df_chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialiser le client OpenAI (indispensable avec la nouvelle API)\n",
    "client = OpenAI()\n",
    "\n",
    "def create_embedding(text: str):\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            input=[text]  # ‚ö†Ô∏è Doit √™tre une **liste**\n",
    "        )\n",
    "        return response.data[0].embedding  # Extraction correcte\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors de la g√©n√©ration d'embedding: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# G√©n√©rer les embeddings pour les chunks du DataFrame\n",
    "df_chunks[\"embedding\"] = df_chunks[\"chunk\"].apply(create_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nettoyage des embeddings\n",
    "all_vectors = np.array([emb for emb in df_chunks[\"embedding\"] if emb is not None])  # Exclure None\n",
    "nn = NearestNeighbors(n_neighbors=3, metric=\"euclidean\")\n",
    "nn.fit(all_vectors)\n",
    "\n",
    "def retrieve(user_query: str) -> str:\n",
    "    try:\n",
    "        # G√©n√©rer l'embedding de la requ√™te\n",
    "        q_emb = create_embedding(user_query)\n",
    "        if q_emb is None:\n",
    "            return \"‚ö†Ô∏è Impossible de g√©n√©rer un embedding pour la requ√™te.\"\n",
    "\n",
    "        dist, idx = nn.kneighbors([q_emb])\n",
    "\n",
    "        # R√©cup√©rer les meilleurs chunks\n",
    "        best_chunks = df_chunks.iloc[idx[0]][\"chunk\"].tolist()\n",
    "        prompt = user_query + \"\\n\\n\" + \"\\n\".join(best_chunks)\n",
    "        \n",
    "        print(\"üîç Prompt augment√© g√©n√©r√©e :\\n\", prompt)\n",
    "\n",
    "        # ‚úÖ Appel OpenAI corrig√©\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content if response.choices else \"‚ö†Ô∏è Aucune r√©ponse g√©n√©r√©e.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Erreur lors de la r√©cup√©ration : {str(e)}\"\n",
    "\n",
    "\n",
    "# üî• Test avec la question sur Lincoln\n",
    "question = \"What did Lincoln argue about slavery in that first debate?\"\n",
    "answer = retrieve(question)\n",
    "print(\"üîç R√©ponse g√©n√©r√©e :\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonnes pratiques pour la RAG (Retrieval Augmented Generation)\n",
    "\n",
    "1. **Chunking**  \n",
    "   - D√©couper vos documents en segments de taille raisonnable (ex. 300-500 tokens), afin de mieux cibler les passages pertinents.\n",
    "2. **Indexation des embeddings**  \n",
    "   - Stocker les vecteurs dans une base adapt√©e (ex. Pinecone, Chroma, Elasticsearch vectoriel, Qdrant, ou Cosmos DB vector).\n",
    "3. **Filtrage et post-traitement**  \n",
    "   - Apr√®s avoir r√©cup√©r√© les chunks les plus proches s√©mantiquement, il peut √™tre utile de v√©rifier l‚Äôexactitude ou la coh√©rence des informations extraites.\n",
    "4. **R√©-int√©gration**  \n",
    "   - Ins√©rer les passages s√©lectionn√©s dans le prompt (par ex. ‚ÄúVoici un extrait : ...\\n\\n Maintenant, r√©ponds √† la question...‚Äù).  \n",
    "   - Ou bien utiliser un outil style [LangChain](https://github.com/hwchase17/langchain) qui facilite ce pipeline.\n",
    "5. **√âviter les hallucinations**  \n",
    "   - Demander explicitement au mod√®le de s‚Äôen tenir aux informations fournies dans les chunks.  \n",
    "   - En cas d‚Äôinsuffisance de donn√©es, demander au mod√®le de r√©pondre ‚ÄúJe ne sais pas‚Äù plut√¥t que d‚Äôinventer.\n",
    "\n",
    "L‚Äôobjectif est de combiner la **puissance du LLM** et l‚Äô**exactitude** de donn√©es externes (base documentaire, articles, PDF, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion & Pistes\n",
    "\n",
    "Nous avons explor√© :\n",
    "- la g√©n√©ration d‚Äôimages (DALL-E, prompts, meta-prompts),\n",
    "- la cr√©ation d‚Äôapps low-code Power Apps / Automate,\n",
    "- l‚Äôusage de Copilot & AI Builder pour des sc√©narios m√©tiers (tracking, invoice),\n",
    "- la structuration des r√©ponses via Function Calling,\n",
    "- RAG : indexer nos docs dans une base vectorielle et enrichir un LLM.\n",
    "\n",
    "Pistes d‚Äôexercices :\n",
    "- Am√©liorer les prompts d‚Äôimages (temp√©rature, variations, mask, etc.)\n",
    "- Cr√©er un flux complet dans Power Automate avec AI Builder\n",
    "- Mettre en place Function Calling plus complexe (multi-fonctions, error-handling)\n",
    "- Stocker un doc plus large (ex: 10 pages PDF) en chunks + RAG\n",
    "\n",
    "Fin de la synth√®se ! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}