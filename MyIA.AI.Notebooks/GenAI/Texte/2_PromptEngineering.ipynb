{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering : Advanced Prompting avec OpenAI\n",
    "\n",
    "Dans ce notebook, nous allons tester différentes techniques avancées de **prompt engineering**:\n",
    "- **Zero-shot prompting**\n",
    "- **Few-shot prompting**\n",
    "- **Chain-of-thought** (CoT)\n",
    "- **Self-refine** (ou auto-amélioration)\n",
    "\n",
    "Nous utiliserons la **nouvelle API** de la bibliothèque `openai` (>=1.0.0) via la classe `OpenAI` et ses méthodes de chat (`client.chat.completions.create`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.109.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (0.11.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.11.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2025.9.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 1 : Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": "# ============================\n# Cellule 2 : Configuration\n# ============================\n\nimport os\nimport sys\nfrom dotenv import load_dotenv\n\n# Charger la configuration depuis le fichier .env (dans le répertoire parent GenAI/)\nload_dotenv('../.env')\n\n# On suppose que ton .env contient :\n# OPENAI_API_KEY=sk-xxxxxx\n# (ou autre variable si tu utilises Azure)\n#\n# Récupère la clé d'API\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"Clé API introuvable. Vérifie ton fichier .env.\")\n\n# Mode batch pour exécution non-interactive (Papermill, tests automatisés)\n# Détection automatique si exécution via Papermill ou si stdin non disponible\ndef is_interactive():\n    \"\"\"Détecte si l'exécution est interactive (terminal) ou batch (Papermill)\"\"\"\n    try:\n        # Check if running in Papermill\n        import __main__\n        if hasattr(__main__, '__file__') and 'papermill' in str(getattr(__main__, '__file__', '')).lower():\n            return False\n        # Check if stdin is available\n        if not sys.stdin.isatty():\n            return False\n        return True\n    except:\n        return False\n\nBATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\" or not is_interactive()\nprint(f\"Mode: {'BATCH' if BATCH_MODE else 'INTERACTIF'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": "# ============================\n# Cellule 3 : Client OpenAI\n# ============================\n\nimport openai\nfrom openai import OpenAI\n\n# Charger le modèle depuis .env ou utiliser gpt-4o par défaut\n# Note: gpt-4o-mini est conservé pour les exemples avec temperature/max_tokens\nDEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\nMODEL_NAME = DEFAULT_MODEL\n\n# Instanciation du client\nclient = OpenAI(\n    api_key=api_key,\n    # Tu peux configurer d'autres options si besoin\n)\n\nprint(\"Client OpenAI initialisé avec succès !\")\nprint(f\"Modèle par défaut: {MODEL_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappel des différences entre Zero-shot, Few-shot, Chain-of-thought et Self-refine\n",
    "\n",
    "1. **Zero-shot Prompting**  \n",
    "   - Aucune instruction ou exemple préalable (à part la demande de l'utilisateur).  \n",
    "   - Simple et direct, mais parfois moins précis ou cohérent.\n",
    "\n",
    "2. **Few-shot Prompting**  \n",
    "   - Fournir quelques exemples “input → output” pour guider la réponse.  \n",
    "   - Permet de **spécifier le format**, le style, ou le contenu souhaité.  \n",
    "   - Améliore significativement la qualité des réponses sur des tâches complexes.\n",
    "\n",
    "3. **Chain-of-thought (CoT)**  \n",
    "   - On **incite** le modèle à détailler son raisonnement étape par étape.  \n",
    "   - Souvent utile pour des questions de logique, mathématiques, programmation ou raisonnement complexe.  \n",
    "   - Peut **augmenter** la cohérence et la justesse de la réponse (mais attention à ne pas divulguer ces “étapes” si elles sont confidentielles).\n",
    "\n",
    "4. **Self-refine**  \n",
    "   - Demander au modèle de s’auto-critiquer puis de proposer une réponse améliorée.  \n",
    "   - Mise en œuvre en plusieurs appels (réponse initiale, re-demande d’analyse, ré-énoncé final).  \n",
    "   - Intéressant pour du code, des textes longs, ou des situations nécessitant un contrôle qualité.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zero-shot Prompt ===\n",
      "Prompt: Donne-moi 3 idées de recettes végétariennes à base de tomates.\n",
      "\n",
      "Réponse du modèle :\n",
      "\n",
      "Bien sûr ! Voici trois idées de recettes végétariennes à base de tomates :\n",
      "\n",
      "### 1. **Tarte Tomates et Féta**\n",
      "**Ingrédients :**\n",
      "- Pâte brisée\n",
      "- Tomates (cerises ou en tranches)\n",
      "- Féta\n",
      "- Olives noires\n",
      "- Herbes de Provence\n",
      "- Huile d'olive\n",
      "- Sel et poivre\n",
      "\n",
      "**Instructions :**\n",
      "1. Préchauffez le four à 180°C.\n",
      "2. Étalez la pâte brisée dans un moule à tarte.\n",
      "3. Disposez les tomates sur la pâte, puis émiettez la féta par-dessus.\n",
      "4. Ajoutez les olives, assaisonnez avec les herbes de Provence, le sel et le poivre.\n",
      "5. Arrosez d'un filet d'huile d'olive et enfournez pendant 30-35 minutes, jusqu'à ce que les tomates soient bien rôties.\n",
      "\n",
      "### 2. **Ratatouille**\n",
      "**Ingrédients :**\n",
      "- 2 tomates\n",
      "- 1 courgette\n",
      "- 1 aubergine\n",
      "- 1 poivron (rouge ou jaune)\n",
      "- 1 oignon\n",
      "- 2 gousses d'ail\n",
      "- Herbes de Provence\n",
      "- Huile d'olive\n",
      "- Sel et poivre\n",
      "\n",
      "**Instructions :**\n",
      "1. Coupez tous les légumes en dés.\n",
      "2. Dans une grande poêle, faites chauffer l'huile d'olive et ajoutez l'oignon et l'ail émincés. Faites revenir jusqu'à ce qu'ils soient translucides.\n",
      "3. Ajoutez les autres légumes et faites cuire à feu moyen pendant environ 20-25 minutes, en remuant régulièrement.\n",
      "4. Assaisonnez avec les herbes de Provence, le sel et le poivre. Servez chaud ou tiède, en accompagnement ou en plat principal.\n",
      "\n",
      "### 3. **Salade\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Zero-shot\n",
    "# ============================\n",
    "\n",
    "prompt_1 = \"Donne-moi 3 idées de recettes végétariennes à base de tomates.\"\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt_1}\n",
    "    ],\n",
    "    # Contrôle du style\n",
    "    max_tokens=400,\n",
    "    temperature=0.7  # plus la température est haute, plus c'est créatif\n",
    ")\n",
    "\n",
    "print(\"=== Zero-shot Prompt ===\")\n",
    "print(f\"Prompt: {prompt_1}\\n\")\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_1.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultat du Zero-shot prompting\n",
    "\n",
    "Le zero-shot prompting est la technique la plus simple : **aucun exemple préalable**, juste une instruction directe.\n",
    "\n",
    "**Avantages** :\n",
    "- Rapide à mettre en œuvre\n",
    "- Fonctionne bien pour des tâches courantes (résumés, traductions, questions générales)\n",
    "- Économique en tokens\n",
    "\n",
    "**Limites** :\n",
    "- Moins précis sur des tâches complexes ou spécialisées\n",
    "- Le format de sortie peut être imprévisible\n",
    "- Nécessite des prompts très clairs et bien formulés\n",
    "\n",
    "Dans cet exemple, le modèle génère 3 recettes végétariennes à base de tomates sans aucun exemple préalable. La qualité dépend fortement de la clarté du prompt et de la capacité du modèle à comprendre le domaine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, pas d’exemples ni d’instructions détaillées, on se contente d’un prompt direct.\n",
    "\n",
    "\n",
    "\n",
    "###  Exemple Few-shot prompting (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du Few-shot prompting\n",
    "\n",
    "Le few-shot prompting apporte une **amélioration significative** par rapport au zero-shot :\n",
    "\n",
    "**Mécanisme** :\n",
    "1. On fournit 2-3 exemples de la tâche souhaitée (paires question/réponse)\n",
    "2. Le modèle apprend le **pattern** et le **format** attendu\n",
    "3. Il applique ensuite ce pattern à la nouvelle question\n",
    "\n",
    "**Avantages observables** :\n",
    "- **Format cohérent** : Le modèle reproduit la structure des exemples (sujet, salutation, corps, signature)\n",
    "- **Ton approprié** : Le style professionnel est maintenu\n",
    "- **Contenu pertinent** : La réponse suit les conventions des exemples fournis\n",
    "\n",
    "**Quand utiliser Few-shot ?**\n",
    "- Tâches avec un format spécifique (emails, rapports, analyses structurées)\n",
    "- Cas où le zero-shot donne des résultats trop variables\n",
    "- Besoin de cohérence stylistique\n",
    "\n",
    "**Compromis** : Chaque exemple consomme des tokens supplémentaires, donc à utiliser avec modération pour des contextes très longs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exemple Few-shot (e-mail professionnel) ===\n",
      "Sujet: Changement de planning et invitation à une réunion de suivi\n",
      "\n",
      "Bonjour [Nom du Collaborateur],\n",
      "\n",
      "Je souhaite vous informer d'un changement concernant notre planning initial. En raison de [mentionner la raison, si possible], nous devons ajuster certaines dates.\n",
      "\n",
      "Pour discuter de ces modifications et de leur impact sur nos projets, je vous invite à une réunion de suivi qui se tiendra le [date] à [heure]. Nous pourrons ainsi aborder les nouvelles échéances et répondre à toutes vos questions.\n",
      "\n",
      "Merci de me confirmer votre disponibilité pour cette réunion.\n",
      "\n",
      "Cordialement,\n",
      "\n",
      "[Votre Nom]  \n",
      "[Votre Poste]  \n",
      "[Votre Société]  \n",
      "[Vos Coordonnées]\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Few-shot supplémentaire\n",
    "# ============================\n",
    "\n",
    "few_shot_prompt_2 = \"\"\"\n",
    "Tu es un assistant spécialisé en rédaction d'e-mails professionnels.\n",
    "Voici quelques exemples de style :\n",
    "\n",
    "Exemple 1:\n",
    "Q: Rédige un e-mail pour informer un client d'un retard de livraison\n",
    "A: \n",
    "Sujet: Information concernant le retard de votre livraison\n",
    "\n",
    "Bonjour [Nom du Client],\n",
    "\n",
    "Nous tenions à vous informer que votre commande #1234 a pris du retard...\n",
    "[...suite du mail...]\n",
    "\n",
    "Exemple 2:\n",
    "Q: Envoie un e-mail de remerciement pour un entretien d'embauche\n",
    "A:\n",
    "Sujet: Remerciements suite à notre entretien\n",
    "\n",
    "Bonjour [Nom du Contact],\n",
    "\n",
    "Je tiens à vous remercier pour le temps que vous m'avez accordé...\n",
    "[...suite du mail...]\n",
    "\n",
    "Maintenant, voici ma demande:\n",
    "\n",
    "Q: Écris un e-mail pour informer un collaborateur d'un changement de planning et l'inviter à une réunion de suivi.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response_few_shot_2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,  # ex. \"gpt-4o-mini\"\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt_2}\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "print(\"=== Exemple Few-shot (e-mail professionnel) ===\")\n",
    "print(response_few_shot_2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation du Chain-of-thought\n",
    "\n",
    "Le Chain-of-thought (CoT) est particulièrement efficace pour les **problèmes de raisonnement** :\n",
    "\n",
    "**Analyse du résultat** :\n",
    "Le modèle devrait avoir détaillé :\n",
    "1. État initial : Alice a 5 pommes\n",
    "2. Étape 1 : Elle en jette 2 → 5 - 2 = 3 pommes\n",
    "3. Étape 2 : Elle en donne 1 à Bob → 3 - 1 = 2 pommes\n",
    "4. Étape 3 : Bob lui rend 1 pomme → 2 + 1 = 3 pommes\n",
    "5. **Réponse finale : 3 pommes**\n",
    "\n",
    "**Pourquoi c'est important ?**\n",
    "- **Transparence** : On peut vérifier le raisonnement étape par étape\n",
    "- **Détection d'erreurs** : Si le résultat est faux, on peut identifier où le modèle s'est trompé\n",
    "- **Confiance** : Le raisonnement explicite augmente la crédibilité\n",
    "- **Debugging** : Facilite la correction du prompt si nécessaire\n",
    "\n",
    "**Applications** :\n",
    "- Calculs mathématiques\n",
    "- Raisonnement logique\n",
    "- Résolution de problèmes complexes\n",
    "- Analyse de cas juridiques ou médicaux\n",
    "\n",
    "**Note** : Temperature=0.2 garantit un raisonnement cohérent et reproductible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous donnons au modèle **deux exemples** de questions/réponses avant la **véritable question**. Cela oriente le style et le contexte.\n",
    "\n",
    "\n",
    "\n",
    "###  7 : Exemple Chain-of-thought (Code)\n",
    "\n",
    "On va demander un **calcul** simple, en guidant le modèle à réfléchir pas à pas :\n",
    "\n",
    "\n",
    "\n",
    "- **Masquer le raisonnement si nécessaire** :  \n",
    "  Parfois, on ne souhaite pas afficher au client final les étapes du raisonnement. Il existe des techniques pour “cacher” ce raisonnement ou n’afficher qu’un résumé. Par exemple :  \n",
    "  1. Dans le prompt, on peut demander : “Don’t reveal your chain-of-thought. Provide only the final concise answer to the user.”  \n",
    "  2. Ou effectuer un 2e appel API où l’on transmet l’enchaînement de pensées, mais on n'affiche que la conclusion.\n",
    "\n",
    "- **Adapter la température** :  \n",
    "  Pour un problème logique ou mathématique, une température trop élevée peut introduire des dérives ou des incohérences. Une température entre 0.0 et 0.3 est souvent recommandée pour les questions de calcul.\n",
    "\n",
    "- **Chain-of-thought partiel** :  \n",
    "  On peut encourager un raisonnement **intermédiaire** (quelques étapes clés) au lieu d’un raisonnement hyper détaillé, afin d’éviter que le texte devienne trop long ou difficile à comprendre.\n",
    "\n",
    "- **Expliquer la démarche** :  \n",
    "  On peut terminer par un résumé du raisonnement en 2-3 phrases, pour rendre la solution plus lisible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chain-of-thought Prompt ===\n",
      "Réponse du modèle (avec raisonnement) :\n",
      "\n",
      "Alice a 3 pommes à la fin.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Chain-of-thought\n",
    "# ============================\n",
    "\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "Donne directement la réponse sans étape intermédiaire.\n",
    "\"\"\"\n",
    "\n",
    "# Explique ton raisonnement étape par étape, puis donne la réponse finale.\n",
    "response_3 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": cot_prompt}\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.2  # on réduit la température pour moins de fantaisie\n",
    ")\n",
    "\n",
    "print(\"=== Chain-of-thought Prompt ===\")\n",
    "print(\"Réponse du modèle (avec raisonnement) :\\n\")\n",
    "print(response_3.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On demande explicitement « explique ton raisonnement ». Cela **n’oblige** pas le modèle à le faire, mais en pratique, GPT-4o-mini (ou tout modèle qui gère le CoT) fournit souvent une solution pas-à-pas.\n",
    "\n",
    "---\n",
    "\n",
    "###  8 : Exemple Self-refine (Code)\n",
    "\n",
    "L’idée : on fait **une première demande** (première réponse) et ensuite **on redemande** au modèle de s’auto-corriger.\n",
    "\n",
    "#### 8a. Premier prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilan du Self-refine\n",
    "\n",
    "Le processus Self-refine en deux étapes démontre la capacité du modèle à s'**auto-améliorer** :\n",
    "\n",
    "**Première étape** : Génération intentionnellement bugguée\n",
    "- Le modèle crée du code avec un bug volontaire (par exemple, division par zéro, mauvaise initialisation, etc.)\n",
    "\n",
    "**Deuxième étape** : Critique et correction\n",
    "- Le modèle analyse son propre code\n",
    "- Identifie le bug\n",
    "- Propose une version corrigée\n",
    "- Explique la nature du problème\n",
    "\n",
    "**Enseignements** :\n",
    "1. **Méta-cognition** : Le LLM peut raisonner sur ses propres productions\n",
    "2. **Amélioration itérative** : Chaque passe peut affiner la qualité\n",
    "3. **Détection de bugs** : Utile pour du code review automatisé\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Génération de code robuste (plusieurs passes de correction)\n",
    "- Rédaction de documents (brouillon → révision → version finale)\n",
    "- Traduction (première traduction → révision → amélioration)\n",
    "\n",
    "**Limite** : Chaque itération consomme des tokens et du temps. À utiliser pour des tâches critiques nécessitant haute qualité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (1) : Code buggy ===\n",
      "\n",
      "Voici une fonction Python qui calcule la somme d'une liste mais qui contient un bug intentionnel :\n",
      "\n",
      "```python\n",
      "def somme_liste(ma_liste):\n",
      "    total = 0\n",
      "    for i in range(len(ma_liste)):\n",
      "        total += ma_liste[i]  # Bug : on ajoute 1 au total à chaque itération\n",
      "    return total + 1  # Bug : correction erronée\n",
      "```\n",
      "\n",
      "Ce code a un bug car il ajoute 1 au `total` à chaque itération de la boucle, ce qui fausse le résultat de la somme. De plus, il ajoute également 1 à la fin, ce qui rend le résultat encore plus incorrect.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8a : Self-refine Step 1\n",
    "# ============================\n",
    "\n",
    "prompt_sr1 = \"\"\"\n",
    "Ecris une courte fonction Python pour calculer la somme d'une liste. \n",
    "Ajoute un bug volontaire dans le code. \n",
    "\"\"\"\n",
    "\n",
    "response_sr1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr1}],\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "buggy_code = response_sr1.choices[0].message.content\n",
    "\n",
    "print(\"=== Self-refine (1) : Code buggy ===\\n\")\n",
    "print(buggy_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8b. Self-critique et amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (2) : Correction ===\n",
      "\n",
      "Analysons le code fourni :\n",
      "\n",
      "```python\n",
      "def somme_liste(ma_liste):\n",
      "    total = 0\n",
      "    for i in range(len(ma_liste)):\n",
      "        total += ma_liste[i]  # Bug : on ajoute 1 au total à chaque itération\n",
      "    return total + 1  # Bug : correction erronée\n",
      "```\n",
      "\n",
      "### Analyse du code\n",
      "\n",
      "1. **Erreur dans l'addition** : Le commentaire indique qu'il y a un bug où 1 est ajouté à chaque itération. Cependant, le code ne fait pas cela. Il additionne simplement les éléments de `ma_liste` à `total`. Donc, le commentaire est trompeur. Le vrai problème est que la ligne `return total + 1` ajoute 1 au total final, ce qui est incorrect.\n",
      "\n",
      "2. **Correction nécessaire** : Pour corriger le code, il suffit de retirer l'ajout de 1 lors du retour de la fonction. La somme des éléments de la liste doit être renvoyée telle quelle.\n",
      "\n",
      "### Correctif proposé\n",
      "\n",
      "Voici la version corrigée du code :\n",
      "\n",
      "```python\n",
      "def somme_liste(ma_liste):\n",
      "    total = 0\n",
      "    for i in range(len(ma_liste)):\n",
      "        total += ma_liste[i]  # Ajoute chaque élément de la liste à total\n",
      "    return total  # Retourne le total sans ajout supplémentaire\n",
      "```\n",
      "\n",
      "### Version améliorée\n",
      "\n",
      "Nous pouvons également améliorer le code en utilisant la fonction intégrée `sum()`, qui est plus concise et plus efficace :\n",
      "\n",
      "```python\n",
      "def somme_liste(ma_liste):\n",
      "    return sum(ma_liste)  # Utilise la fonction intégrée sum pour calculer la somme\n",
      "```\n",
      "\n",
      "### Explication de la correction\n",
      "\n",
      "- **Correction de l'erreur de retour** : En supprimant l'ajout de 1 dans la ligne de retour, nous garantissons que la fonction renvoie la somme correcte des éléments de la liste\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8b : Self-refine Step 2\n",
    "# ============================\n",
    "\n",
    "prompt_sr2 = f\"\"\"\n",
    "Voici un code Python qui contient un bug:\n",
    "\n",
    "{buggy_code}\n",
    "\n",
    "Peux-tu l'analyser, détecter le bug, proposer un correctif et une version améliorée du code ? \n",
    "Explique la correction.\n",
    "\"\"\"\n",
    "\n",
    "response_sr2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr2}],\n",
    "    max_tokens=400,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine (2) : Correction ===\\n\")\n",
    "print(response_sr2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine avec 'developer role' ===\n",
      "Voici une fonction Python qui calcule la factorielle d'un nombre entier. Je vais également vérifier le code pour d'éventuels bugs ou améliorations.\n",
      "\n",
      "```python\n",
      "def factorielle(n):\n",
      "    if not isinstance(n, int):\n",
      "        raise ValueError(\"L'argument doit être un entier.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"La factorielle n'est pas définie pour les nombres négatifs.\")\n",
      "    \n",
      "    resultat = 1\n",
      "    for i in range(2, n + 1):\n",
      "        resultat *= i\n",
      "    return resultat\n",
      "```\n",
      "\n",
      "### Vérifications et améliorations :\n",
      "\n",
      "1. **Type de l'argument** : La fonction vérifie si l'argument est un entier, ce qui est une bonne pratique.\n",
      "2. **Gestion des nombres négatifs** : La fonction lève une exception si `n` est négatif, ce qui est correct.\n",
      "3. **Utilisation de `range`** : L'utilisation de `range(2, n + 1)` est efficace pour calculer la factorielle.\n",
      "4. **Performance** : Pour des valeurs de `n` très élevées, il serait plus efficace d'utiliser une approche récursive ou d'utiliser la fonction intégrée `math.factorial`.\n",
      "\n",
      "### Version améliorée :\n",
      "\n",
      "Je vais également ajouter une version récursive et utiliser la bibliothèque `math` pour montrer une alternative.\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "def factorielle(n):\n",
      "    if not isinstance(n, int):\n",
      "        raise\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Self-refine avec developer role\n",
    "# ============================\n",
    "\n",
    "messages_sr = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": (\n",
    "            \"You are a self-improving coding assistant. Whenever you provide code, \"\n",
    "            \"you will automatically search for potential bugs or improvements \"\n",
    "            \"and refine your output.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Écris une fonction Python qui calcule la factorielle d'un nombre entier. \"\n",
    "            \"Ensuite, relis-toi et corrige d'éventuels bugs.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "response_self_refine = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages_sr,\n",
    "    max_tokens=300,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine avec 'developer role' ===\")\n",
    "print(response_self_refine.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on utilise la première réponse pour nourrir le second prompt, demandant au modèle de **critiquer** et **améliorer** la réponse initiale.\n",
    "\n",
    "---\n",
    "\n",
    "### Cellule 9 : Interactive Prompt (Code)\n",
    "\n",
    "Enfin, on peut proposer une **cellule interactive** : l’utilisateur peut saisir un prompt, et on envoie la requête au modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": "# ============================\n# Cellule 9 : Prompt interactif\n# ============================\n\n# En mode batch (BATCH_MODE=true dans .env), cette cellule utilise des exemples prédéfinis\n# En mode interactif, elle permet de tester le modèle en boucle\n\nif BATCH_MODE:\n    # Mode batch: exécuter des exemples simples\n    test_prompts = [\"Bonjour!\", \"Quelle est la capitale de la France?\"]\n    for prompt in test_prompts:\n        print(f\"[BATCH] Prompt: {prompt}\")\n        resp = client.chat.completions.create(\n            model=MODEL_NAME,\n            messages=[{\"role\":\"user\",\"content\":prompt}],\n            max_tokens=200,\n            temperature=0.7\n        )\n        print(f\"Réponse: {resp.choices[0].message.content}\\n\")\n    print(\"Mode batch terminé.\")\nelse:\n    while True:\n        user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n            print(\"Fin de l'interaction.\")\n            break\n\n        resp = client.chat.completions.create(\n            model=MODEL_NAME,\n            messages=[{\"role\":\"user\",\"content\":user_input}],\n            max_tokens=200,\n            temperature=0.7\n        )\n\n        print(\"\\n=== Réponse du modèle ===\")\n        print(resp.choices[0].message.content)\n        print(\"---------------------------------------------------\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================\n# Cellule 9 : Prompt interactif avec mémoire de chat\n# ============================\n\n# En mode batch, cette cellule utilise un dialogue prédéfini\n# En mode interactif, elle permet un échange avec mémoire\n\nif BATCH_MODE:\n    # Mode batch: simuler une conversation avec mémoire\n    batch_conversation = [\n        \"Je m'appelle Alice\",\n        \"Comment je m'appelle?\",\n    ]\n    current_messages = []\n\n    for user_input in batch_conversation:\n        print(f\"[BATCH] User: {user_input}\")\n        resp = client.chat.completions.create(\n            model=MODEL_NAME,\n            messages=current_messages + [{\"role\":\"user\",\"content\":user_input}],\n            max_tokens=200,\n            temperature=0.7,\n        )\n        assistant_message = resp.choices[0].message.content\n        print(f\"Assistant: {assistant_message}\\n\")\n        current_messages.append({\"role\":\"user\",\"content\":user_input})\n        current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})\n\n    print(\"Mode batch (mémoire) terminé.\")\nelse:\n    user_input = \"\"\n    current_messages = []\n\n    while True:\n        user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n            print(\"Fin de l'interaction.\")\n            break\n        print(\"\\n=== message de l'utilisateur ===\")\n        resp = client.chat.completions.create(\n            model=MODEL_NAME,\n            messages = current_messages + [{\"role\":\"user\",\"content\":user_input}],\n            max_tokens=200,\n            temperature=0.7,\n        )\n\n        print(\"\\n=== Réponse du modèle ===\")\n        assistant_message = resp.choices[0].message.content\n        print(assistant_message)\n        print(\"---------------------------------------------------\\n\")\n        current_messages.append({\"role\":\"user\",\"content\":user_input})\n        current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting pour Modèles de Raisonnement (2025)\n",
    "\n",
    "Les modèles de raisonnement (o4-mini, gpt-5-thinking) représentent une évolution majeure. Contrairement aux modèles de chat, ils prennent le temps de \"réfléchir\" avant de répondre.\n",
    "\n",
    "## Différences clés avec les modèles chat\n",
    "\n",
    "| Aspect | Modèles Chat (gpt-4o) | Modèles Raisonnement (o4-mini) |\n",
    "|--------|----------------------|-------------------------------|\n",
    "| Temps de réponse | Rapide | Plus lent (réflexion) |\n",
    "| Prompts | Détaillés, structurés | **Simples et directs** |\n",
    "| Chain-of-thought | Demandé explicitement | **Intégré nativement** |\n",
    "| Messages | system, user, assistant | **developer**, user, assistant |\n",
    "| Paramètre spécial | temperature | **reasoning_effort** |\n",
    "\n",
    "## Règle d'or : Simplifier les prompts\n",
    "\n",
    "**Pour les modèles de raisonnement, des prompts simples fonctionnent mieux !**\n",
    "\n",
    "Les modèles raisonnants sont capables de :\n",
    "- Comprendre l'intention sans instructions détaillées\n",
    "- Gérer les ambiguïtés intelligemment\n",
    "- Demander des clarifications si nécessaire\n",
    "\n",
    "**Éviter** : \"Analyse ce problème en détaillant chaque étape de ton raisonnement...\"\n",
    "**Préférer** : \"Résous ce problème.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gpt-4o-mini (Chat Model) ===\n",
      "Temps: 5.36s\n",
      "Le fermier peut transporter le loup, la chèvre et le chou de l'autre côté de la rivière en suivant ces étapes :\n",
      "\n",
      "1. **Prendre la chèvre** et la transporter de l'autre côté de la rivière. Il laisse le loup et le chou ensemble, car ils ne se mangent pas.\n",
      "   \n",
      "2. **Retourner seul** de l'autre côté de la rivière.\n",
      "\n",
      "3. **Prendre le loup** et le transporter de l'autre côté de la rivière.\n",
      "\n",
      "4. **Laisser le ...\n",
      "\n",
      "=== o4-mini (Reasoning Model) ===\n",
      "Temps: 7.56s\n",
      "Voici une façon de procéder en 7 traversées, sans jamais laisser ensemble le loup et la chèvre, ni la chèvre et le chou :\n",
      "\n",
      "1. Le fermier prend la chèvre et la traverse sur l’autre rive.  \n",
      "2. Il revient seul sur la rive de départ.  \n",
      "3. Il prend le loup et le traverse sur l’autre rive.  \n",
      "4. Il laisse le loup, reprend la chèvre et revient sur la rive de départ.  \n",
      "5. Il prend le chou et le traverse su...\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Comparaison Chat vs Reasoning\n",
    "# ============================\n",
    "\n",
    "import time\n",
    "\n",
    "probleme_complexe = \"\"\"\n",
    "Un fermier veut traverser une rivière avec un loup, une chèvre et un chou.\n",
    "Son bateau ne peut transporter que lui et un objet à la fois.\n",
    "Si le loup est laissé seul avec la chèvre, il la mange.\n",
    "Si la chèvre est laissée seule avec le chou, elle le mange.\n",
    "Comment le fermier peut-il tout transporter de l'autre côté?\n",
    "\"\"\"\n",
    "\n",
    "# Test avec gpt-4o-mini (chat model)\n",
    "print(\"=== gpt-4o-mini (Chat Model) ===\")\n",
    "start = time.time()\n",
    "response_chat = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": probleme_complexe}],\n",
    "    max_tokens=500,\n",
    "    temperature=0.2\n",
    ")\n",
    "print(f\"Temps: {time.time() - start:.2f}s\")\n",
    "print(response_chat.choices[0].message.content[:400] + \"...\")\n",
    "\n",
    "# Test avec o4-mini (reasoning model) - si disponible\n",
    "print(\"\\n=== o4-mini (Reasoning Model) ===\")\n",
    "try:\n",
    "    start = time.time()\n",
    "    response_reason = client.chat.completions.create(\n",
    "        model=\"o4-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "            {\"role\": \"user\", \"content\": probleme_complexe}\n",
    "        ],\n",
    "        reasoning_effort=\"medium\"\n",
    "    )\n",
    "    print(f\"Temps: {time.time() - start:.2f}s\")\n",
    "    print(response_reason.choices[0].message.content[:400] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"o4-mini non disponible: {type(e).__name__}\")\n",
    "    print(\"Les modèles de raisonnement nécessitent un accès spécifique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, tu peux saisir n’importe quel prompt, et tu verras la réponse du modèle.  \n",
    "Tape `exit` pour quitter la boucle.\n",
    "\n",
    "----\n",
    "\n",
    "## Conclusion et Ressources Supplémentaires\n",
    "\n",
    "Dans ce notebook, nous avons approfondi diverses techniques de **prompt engineering** : \n",
    "- Zero-shot  \n",
    "- Few-shot  \n",
    "- Chain-of-thought  \n",
    "- Self-refine  \n",
    "- Interactions multi-messages (avec les rôles `system`, `developer`, `user`, `assistant`)  \n",
    "\n",
    "### Ressources conseillées\n",
    "- [**OpenAI Cookbook**](https://github.com/openai/openai-cookbook) : Recettes et astuces pour résoudre des problèmes concrets (prompt engineering, RAG, etc.).  \n",
    "- [**Prompt Engineering Guide**](https://www.promptingguide.ai/) : Conseils de rédaction de prompts, cas d’usages, bonnes pratiques.  \n",
    "- [**Chaine d’outils** (LangChain, LlamaIndex, etc.)](https://github.com/hwchase17/langchain) : Facilite la création de pipelines complexes (RAG, function calling, mémoire de conversation).  \n",
    "\n",
    "> **Idées d’exercices**  \n",
    "> 1. Adapter la technique Few-shot à d’autres cas (e.g., Q&R sur la finance, la santé ou le marketing).  \n",
    "> 2. Utiliser la Self-refine pour générer un texte marketing, puis le réécrire en style “plus formel” ou “plus humoristique”.  \n",
    "> 3. Tester la **combinaison** de techniques : un prompt Few-shot + Chain-of-thought + un re-run Self-refine.\n",
    "\n",
    "---\n",
    "\n",
    "Merci d'avoir suivi ce notebook sur le **prompt engineering avancé** !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}