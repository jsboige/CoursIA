{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63a5e62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T20:09:46.609429Z",
     "iopub.status.busy": "2026-02-25T20:09:46.608958Z",
     "iopub.status.idle": "2026-02-25T20:09:46.614527Z",
     "shell.execute_reply": "2026-02-25T20:09:46.613700Z"
    },
    "papermill": {
     "duration": 0.02771,
     "end_time": "2026-02-25T20:09:46.615693",
     "exception": false,
     "start_time": "2026-02-25T20:09:46.587983",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_MODE = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f26e6",
   "metadata": {
    "papermill": {
     "duration": 0.003444,
     "end_time": "2026-02-25T20:09:46.624639",
     "exception": false,
     "start_time": "2026-02-25T20:09:46.621195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt Engineering : Advanced Prompting avec OpenAI\n",
    "\n",
    "Dans ce notebook, nous allons tester différentes techniques avancées de **prompt engineering**:\n",
    "- **Zero-shot prompting**\n",
    "- **Few-shot prompting**\n",
    "- **Chain-of-thought** (CoT)\n",
    "- **Self-refine** (ou auto-amélioration)\n",
    "\n",
    "Nous utiliserons la **nouvelle API** de la bibliothèque `openai` (>=1.0.0) via la classe `OpenAI` et ses méthodes de chat (`client.chat.completions.create`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4b75c",
   "metadata": {
    "papermill": {
     "duration": 0.003355,
     "end_time": "2026-02-25T20:09:46.631497",
     "exception": false,
     "start_time": "2026-02-25T20:09:46.628142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Prompt Engineering : Techniques Avancées\n",
    "\n",
    "**Navigation** : [<< Precedent](1_OpenAI_Intro.ipynb) | [Index](../../README.md) | [Suivant >>](3_Structured_Outputs.ipynb)\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Maîtriser les techniques de zero-shot et few-shot prompting\n",
    "2. Comprendre et appliquer le chain-of-thought (CoT)\n",
    "3. Implémenter le self-refine pour l'amélioration itérative\n",
    "4. Distinguer modèles chat et modèles de raisonnement\n",
    "\n",
    "### Prerequis\n",
    "- Notebook 1 (Introduction a l'IA generative)\n",
    "- Python 3.10+\n",
    "- Cle API OpenAI configuree\n",
    "\n",
    "### Duree estimee : 60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "# Prompt Engineering : Advanced Prompting avec OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e31e04",
   "metadata": {
    "papermill": {
     "duration": 0.003461,
     "end_time": "2026-02-25T20:09:46.638369",
     "exception": false,
     "start_time": "2026-02-25T20:09:46.634908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Installation des dépendances\n",
    "\n",
    "Avant de commencer, nous devons installer les bibliothèques Python nécessaires.\n",
    "\n",
    "**Packages requis** :\n",
    "- **openai** : Bibliothèque officielle pour interagir avec l'API OpenAI (>=1.0.0)\n",
    "- **tiktoken** : Encodeur de tokens pour compter et gérer les tokens GPT\n",
    "- **python-dotenv** : Gestion sécurisée des clés API via fichiers .env\n",
    "\n",
    "> **Note de sécurité** : Ne jamais inclure vos clés API directement dans le code. Toujours utiliser un fichier  exclu du contrôle de version (.gitignore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331404b4",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T20:09:46.647434Z",
     "iopub.status.busy": "2026-02-25T20:09:46.647227Z",
     "iopub.status.idle": "2026-02-25T20:09:48.226666Z",
     "shell.execute_reply": "2026-02-25T20:09:48.226172Z"
    },
    "papermill": {
     "duration": 1.585019,
     "end_time": "2026-02-25T20:09:48.227334",
     "exception": false,
     "start_time": "2026-02-25T20:09:46.642315",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.109.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python313\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\python313\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: colorama in c:\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 1 : Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85933556",
   "metadata": {
    "papermill": {
     "duration": 0.003636,
     "end_time": "2026-02-25T20:09:48.234920",
     "exception": false,
     "start_time": "2026-02-25T20:09:48.231284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Pourquoi le Prompt Engineering ?\n",
    "\n",
    "Le **prompt engineering** est l'art de formuler des instructions efficaces pour obtenir les meilleures reponses des modeles de langage. C'est une competence essentielle car :\n",
    "\n",
    "1. **Impact direct sur la qualite** : Un bon prompt peut transformer une reponse mediocre en resultat excellent\n",
    "2. **Economie de tokens** : Des prompts bien concus reduisent les iterations et donc les couts\n",
    "3. **Reproductibilite** : Des techniques structurees permettent des resultats coherents\n",
    "\n",
    "### Progression de ce notebook\n",
    "\n",
    "| Technique | Complexite | Cas d'usage |\n",
    "|-----------|------------|-------------|\n",
    "| Zero-shot | Simple | Questions generales, taches courantes |\n",
    "| Few-shot | Moyenne | Format specifique, style personnalise |\n",
    "| Chain-of-thought | Moyenne | Raisonnement, mathematiques, logique |\n",
    "| Self-refine | Avancee | Code, textes critiques, haute qualite |\n",
    "\n",
    "> **Documentation officielle** : [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b9f43da",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T20:09:48.242704Z",
     "iopub.status.busy": "2026-02-25T20:09:48.242544Z",
     "iopub.status.idle": "2026-02-25T20:09:48.253034Z",
     "shell.execute_reply": "2026-02-25T20:09:48.252501Z"
    },
    "papermill": {
     "duration": 0.015456,
     "end_time": "2026-02-25T20:09:48.253859",
     "exception": false,
     "start_time": "2026-02-25T20:09:48.238403",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: BATCH\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 2 : Configuration\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger la configuration depuis le fichier .env (dans le répertoire parent GenAI/)\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# On suppose que ton .env contient :\n",
    "# OPENAI_API_KEY=sk-xxxxxx\n",
    "# (ou autre variable si tu utilises Azure)\n",
    "#\n",
    "# Récupère la clé d'API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Clé API introuvable. Vérifie ton fichier .env.\")\n",
    "\n",
    "# Mode batch pour exécution non-interactive (Papermill, tests automatisés)\n",
    "# Détection automatique si exécution via Papermill ou si stdin non disponible\n",
    "def is_interactive():\n",
    "    \"\"\"Détecte si l'exécution est interactive (terminal) ou batch (Papermill)\"\"\"\n",
    "    try:\n",
    "        # Check if running in Papermill\n",
    "        import __main__\n",
    "        if hasattr(__main__, '__file__') and 'papermill' in str(getattr(__main__, '__file__', '')).lower():\n",
    "            return False\n",
    "        # Check if stdin is available\n",
    "        if not sys.stdin.isatty():\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "BATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\" or not is_interactive()\n",
    "print(f\"Mode: {'BATCH' if BATCH_MODE else 'INTERACTIF'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a87bf8",
   "metadata": {
    "papermill": {
     "duration": 0.003579,
     "end_time": "2026-02-25T20:09:48.261241",
     "exception": false,
     "start_time": "2026-02-25T20:09:48.257662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Configuration de l'environnement\n",
    "\n",
    "Avant de commencer les expérimentations, nous devons configurer l'accès à l'API OpenAI et gérer les modes d'exécution.\n",
    "\n",
    "### Mode batch vs mode interactif\n",
    "\n",
    "Le notebook supporte deux modes d'exécution :\n",
    "\n",
    "- **Mode interactif** : Pour l'apprentissage, avec saisie utilisateur et expérimentation libre\n",
    "- **Mode batch** : Pour l'exécution automatisée (Papermill, tests, CI/CD), sans interaction\n",
    "\n",
    "La détection est automatique, mais vous pouvez forcer le mode batch via la variable d'environnement `BATCH_MODE=true` dans le fichier `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abdf011",
   "metadata": {
    "papermill": {
     "duration": 0.003392,
     "end_time": "2026-02-25T20:09:48.268046",
     "exception": false,
     "start_time": "2026-02-25T20:09:48.264654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Initialisation du client OpenAI\n",
    "\n",
    "Le client OpenAI moderne (>=1.0.0) utilise une API orientée objet :\n",
    "\n",
    "```python\n",
    "client = OpenAI(api_key=\"...\")\n",
    "response = client.chat.completions.create(...)\n",
    "```\n",
    "\n",
    "**Paramètres importants** :\n",
    "- `model` : Le modèle à utiliser (gpt-4o-mini, o4-mini, etc.)\n",
    "- `max_tokens` : Longueur maximale de la réponse (évite les réponses trop longues et coûteuses)\n",
    "- `temperature` : Contrôle la créativité (0.0 = déterministe, 2.0 = très créatif)\n",
    "\n",
    "Par défaut, nous utilisons `gpt-4o-mini` configuré dans le fichier `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830e31c",
   "metadata": {
    "papermill": {
     "duration": 0.003928,
     "end_time": "2026-02-25T20:09:48.275911",
     "exception": false,
     "start_time": "2026-02-25T20:09:48.271983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Partie 1 : Zero-shot Prompting\n",
    "\n",
    "Le **zero-shot prompting** est la technique la plus directe : on pose une question sans fournir d'exemples préalables.\n",
    "\n",
    "### Quand utiliser Zero-shot ?\n",
    "\n",
    "- Questions générales ou conversationnelles\n",
    "- Tâches courantes bien comprises par le modèle (résumés, traductions simples)\n",
    "- Prototypage rapide\n",
    "- Budget tokens limité\n",
    "\n",
    "Testons avec une demande simple : générer des idées de recettes végétariennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddcaa015",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T20:09:48.284562Z",
     "iopub.status.busy": "2026-02-25T20:09:48.284350Z",
     "iopub.status.idle": "2026-02-25T20:09:49.153732Z",
     "shell.execute_reply": "2026-02-25T20:09:49.152959Z"
    },
    "papermill": {
     "duration": 0.874948,
     "end_time": "2026-02-25T20:09:49.154765",
     "exception": false,
     "start_time": "2026-02-25T20:09:48.279817",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client OpenAI initialisé avec succès !\n",
      "Modèle par défaut: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Client OpenAI\n",
    "# ============================\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Charger le modèle depuis .env ou utiliser gpt-4o-mini par défaut\n",
    "DEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "MODEL_NAME = DEFAULT_MODEL\n",
    "\n",
    "# Instanciation du client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "print(\"Client OpenAI initialisé avec succès !\")\n",
    "print(f\"Modèle par défaut: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4437657",
   "metadata": {
    "papermill": {
     "duration": 0.003508,
     "end_time": "2026-02-25T20:09:49.162201",
     "exception": false,
     "start_time": "2026-02-25T20:09:49.158693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Rappel des différences entre Zero-shot, Few-shot, Chain-of-thought et Self-refine\n",
    "\n",
    "1. **Zero-shot Prompting**  \n",
    "   - Aucune instruction ou exemple préalable (à part la demande de l'utilisateur).  \n",
    "   - Simple et direct, mais parfois moins précis ou cohérent.\n",
    "\n",
    "2. **Few-shot Prompting**  \n",
    "   - Fournir quelques exemples “input → output” pour guider la réponse.  \n",
    "   - Permet de **spécifier le format**, le style, ou le contenu souhaité.  \n",
    "   - Améliore significativement la qualité des réponses sur des tâches complexes.\n",
    "\n",
    "3. **Chain-of-thought (CoT)**  \n",
    "   - On **incite** le modèle à détailler son raisonnement étape par étape.  \n",
    "   - Souvent utile pour des questions de logique, mathématiques, programmation ou raisonnement complexe.  \n",
    "   - Peut **augmenter** la cohérence et la justesse de la réponse (mais attention à ne pas divulguer ces “étapes” si elles sont confidentielles).\n",
    "\n",
    "4. **Self-refine**  \n",
    "   - Demander au modèle de s’auto-critiquer puis de proposer une réponse améliorée.  \n",
    "   - Mise en œuvre en plusieurs appels (réponse initiale, re-demande d’analyse, ré-énoncé final).  \n",
    "   - Intéressant pour du code, des textes longs, ou des situations nécessitant un contrôle qualité.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9619dc4",
   "metadata": {
    "papermill": {
     "duration": 0.003337,
     "end_time": "2026-02-25T20:09:49.168960",
     "exception": false,
     "start_time": "2026-02-25T20:09:49.165623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Partie 2 : Few-shot Prompting\n",
    "\n",
    "Le **few-shot prompting** consiste à fournir 2-3 exemples de la tâche souhaitée avant la vraie question.\n",
    "\n",
    "### Mécanisme d'apprentissage en contexte\n",
    "\n",
    "Le modèle :\n",
    "1. Analyse les exemples fournis\n",
    "2. Détecte le **pattern** (format, style, structure)\n",
    "3. Applique ce pattern à la nouvelle question\n",
    "\n",
    "C'est ce qu'on appelle **l'apprentissage en contexte** (in-context learning) : le modèle s'adapte sans modifier ses poids.\n",
    "\n",
    "### Premier exemple : Rédaction d'emails professionnels\n",
    "\n",
    "Nous allons guider le modèle à produire un email avec un format et un ton spécifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a60165",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T20:09:49.178085Z",
     "iopub.status.busy": "2026-02-25T20:09:49.177865Z",
     "iopub.status.idle": "2026-02-25T20:09:57.297911Z",
     "shell.execute_reply": "2026-02-25T20:09:57.297497Z"
    },
    "papermill": {
     "duration": 8.126075,
     "end_time": "2026-02-25T20:09:57.298904",
     "exception": false,
     "start_time": "2026-02-25T20:09:49.172829",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zero-shot Prompt ===\n",
      "Prompt: Donne-moi 3 idées de recettes végétariennes à base de tomates.\n",
      "\n",
      "Réponse du modèle :\n",
      "\n",
      "Bien sûr ! Voici trois idées de recettes végétariennes à base de tomates :\n",
      "\n",
      "### 1. **Tarte aux tomates et au fromage de chèvre**\n",
      "**Ingrédients :**\n",
      "- Pâte brisée\n",
      "- Tomates cerises ou tomates classiques\n",
      "- Fromage de chèvre\n",
      "- Huile d'olive\n",
      "- Herbes de Provence\n",
      "- Sel et poivre\n",
      "\n",
      "**Instructions :**\n",
      "1. Préchauffez le four à 180°C (350°F).\n",
      "2. Étalez la pâte brisée dans un moule à tarte et piquez le fond avec une fourchette.\n",
      "3. Coupez les tomates en rondelles et disposez-les sur la pâte.\n",
      "4. Émiettez le fromage de chèvre par-dessus, puis arrosez d'un filet d'huile d'olive.\n",
      "5. Saupoudrez d'herbes de Provence, de sel et de poivre.\n",
      "6. Enfournez pendant environ 25-30 minutes, jusqu'à ce que la pâte soit dorée et les tomates légèrement caramélisées.\n",
      "7. Servez chaud ou tiède, accompagné d'une salade verte.\n",
      "\n",
      "### 2. **Ratatouille méditerranéenne**\n",
      "**Ingrédients :**\n",
      "- 2 tomates\n",
      "- 1 courgette\n",
      "- 1 aubergine\n",
      "- 1 poivron (de la couleur de votre choix)\n",
      "- 1 oignon\n",
      "- 2 gousses d'ail\n",
      "- Huile d'olive\n",
      "- Sel, poivre et herbes de Provence\n",
      "\n",
      "**Instructions :**\n",
      "1. Dans une grande poêle, faites chauffer l'huile d'olive et ajoutez l'oignon et l'ail hachés. Faites revenir jusqu'à ce qu'ils soient translucides.\n",
      "2. Ajoutez les tomates coupées en dés, la courgette, l'aubergine et le poivron coupés en morceaux.\n",
      "3. Ass\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Zero-shot\n",
    "# ============================\n",
    "\n",
    "prompt_1 = \"Donne-moi 3 idées de recettes végétariennes à base de tomates.\"\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt_1}\n",
    "    ],\n",
    "    # Contrôle du style\n",
    "    max_completion_tokens=400,\n",
    "    temperature=0.7  # plus la température est haute, plus c'est créatif\n",
    ")\n",
    "\n",
    "print(\"=== Zero-shot Prompt ===\")\n",
    "print(f\"Prompt: {prompt_1}\\n\")\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_1.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7616a0d",
   "metadata": {
    "papermill": {
     "duration": 0.003925,
     "end_time": "2026-02-25T20:09:57.311006",
     "exception": false,
     "start_time": "2026-02-25T20:09:57.307081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Résultat du Zero-shot prompting\n",
    "\n",
    "Le zero-shot prompting est la technique la plus simple : **aucun exemple préalable**, juste une instruction directe.\n",
    "\n",
    "**Avantages** :\n",
    "- Rapide à mettre en œuvre\n",
    "- Fonctionne bien pour des tâches courantes (résumés, traductions, questions générales)\n",
    "- Économique en tokens\n",
    "\n",
    "**Limites** :\n",
    "- Moins précis sur des tâches complexes ou spécialisées\n",
    "- Le format de sortie peut être imprévisible\n",
    "- Nécessite des prompts très clairs et bien formulés\n",
    "\n",
    "Dans cet exemple, le modèle génère 3 recettes végétariennes à base de tomates sans aucun exemple préalable. La qualité dépend fortement de la clarté du prompt et de la capacité du modèle à comprendre le domaine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767fb79",
   "metadata": {
    "papermill": {
     "duration": 0.00444,
     "end_time": "2026-02-25T20:09:57.320472",
     "exception": false,
     "start_time": "2026-02-25T20:09:57.316032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Partie 3 : Chain-of-thought (CoT)\n",
    "\n",
    "Le **Chain-of-thought** demande explicitement au modèle de détailler son raisonnement étape par étape.\n",
    "\n",
    "### Pourquoi le CoT fonctionne ?\n",
    "\n",
    "Les modèles de langage sont entraînés sur des textes où les raisonnements sont explicités. En demandant les étapes intermédiaires, on active ce pattern et on améliore la précision.\n",
    "\n",
    "### Applications du CoT\n",
    "\n",
    "| Domaine | Exemple |\n",
    "|---------|---------|\n",
    "| Mathématiques | Résolution d'équations, problèmes de mots |\n",
    "| Logique | Syllogismes, déductions |\n",
    "| Programmation | Debugging, conception d'algorithmes |\n",
    "| Analyse | Cas juridiques, diagnostics médicaux |\n",
    "\n",
    "### Exemple : Problème arithmétique simple\n",
    "\n",
    "Testons avec un calcul impliquant plusieurs étapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb36ea",
   "metadata": {
    "papermill": {
     "duration": 0.003659,
     "end_time": "2026-02-25T20:09:57.328945",
     "exception": false,
     "start_time": "2026-02-25T20:09:57.325286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exemple Few-shot (e-mail professionnel) ===\n",
      "Sujet: Changement de planning et invitation à une réunion de suivi\n",
      "\n",
      "Bonjour [Nom du Collaborateur],\n",
      "\n",
      "Je souhaite vous informer d'un changement dans notre planning concernant le projet [Nom du Projet]. En raison de [raison du changement], nous avons dû ajuster certaines échéances.\n",
      "\n",
      "Pour discuter de ces modifications et nous assurer que nous restons alignés, je vous invite à une réunion de suivi qui se tiendra le [date] à [heure] dans [lieu ou lien de visioconférence].\n",
      "\n",
      "Merci de me confirmer votre disponibilité. \n",
      "\n",
      "Cordialement,\n",
      "\n",
      "[Votre Nom]  \n",
      "[Votre Poste]  \n",
      "[Votre Entreprise]  \n",
      "[Votre Numéro de Téléphone]  \n",
      "[Votre Adresse E-mail]  \n"
     ]
    }
   ],
   "source": [
    "## Partie 4 : Self-refine (Auto-amélioration)\n",
    "\n",
    "Le **Self-refine** exploite la capacité du modèle à critiquer et améliorer ses propres productions.\n",
    "\n",
    "### Processus en deux étapes\n",
    "\n",
    "1. **Génération initiale** : Produire une première version (potentiellement bugguée ou imparfaite)\n",
    "2. **Critique et correction** : Analyser la première version, identifier les problèmes, proposer une amélioration\n",
    "\n",
    "### Avantages du Self-refine\n",
    "\n",
    "- **Qualité supérieure** : Deux passes donnent généralement de meilleurs résultats\n",
    "- **Détection de bugs** : Le modèle peut repérer ses propres erreurs\n",
    "- **Amélioration itérative** : Peut être répété plusieurs fois si nécessaire\n",
    "\n",
    "### Compromis\n",
    "\n",
    "- **Coût** : Double les tokens consommés (deux appels API)\n",
    "- **Temps** : Latence multipliée\n",
    "- **À utiliser pour** : Code critique, documents importants, tâches complexes\n",
    "\n",
    "### Exemple : Code Python avec bug volontaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52374bc",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T20:09:57.338260Z",
     "iopub.status.busy": "2026-02-25T20:09:57.337945Z",
     "iopub.status.idle": "2026-02-25T20:10:01.099461Z",
     "shell.execute_reply": "2026-02-25T20:10:01.098750Z"
    },
    "papermill": {
     "duration": 3.767759,
     "end_time": "2026-02-25T20:10:01.100438",
     "exception": false,
     "start_time": "2026-02-25T20:09:57.332679",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exemple Few-shot (e-mail professionnel) ===\n",
      "Sujet: Changement de planning et invitation à une réunion de suivi\n",
      "\n",
      "Bonjour [Nom du Collaborateur],\n",
      "\n",
      "Je souhaite vous informer d'un changement concernant notre planning initial. En raison de [préciser la raison, si nécessaire], nous avons dû ajuster certaines échéances.\n",
      "\n",
      "Afin de discuter des impacts de ce changement et de nous assurer que nous restons alignés sur nos objectifs, je vous invite à une réunion de suivi. Voici les détails :\n",
      "\n",
      "**Date :** [Date de la réunion]  \n",
      "**Heure :** [Heure de la réunion]  \n",
      "**Lieu :** [Lieu de la réunion ou lien de la visioconférence]\n",
      "\n",
      "Merci de me confirmer votre disponibilité pour cette réunion. Si vous avez des questions ou des points que vous souhaitez aborder, n'hésitez pas à me le faire savoir.\n",
      "\n",
      "Cordialement,\n",
      "\n",
      "[Votre Nom]  \n",
      "[Votre Poste]  \n",
      "[Votre Entreprise]  \n",
      "[Votre Numéro de Téléphone]  \n",
      "[Votre Adresse e-mail]  \n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Few-shot supplémentaire\n",
    "# ============================\n",
    "\n",
    "few_shot_prompt_2 = \"\"\"\n",
    "Tu es un assistant spécialisé en rédaction d'e-mails professionnels.\n",
    "Voici quelques exemples de style :\n",
    "\n",
    "Exemple 1:\n",
    "Q: Rédige un e-mail pour informer un client d'un retard de livraison\n",
    "A: \n",
    "Sujet: Information concernant le retard de votre livraison\n",
    "\n",
    "Bonjour [Nom du Client],\n",
    "\n",
    "Nous tenions à vous informer que votre commande #1234 a pris du retard...\n",
    "[...suite du mail...]\n",
    "\n",
    "Exemple 2:\n",
    "Q: Envoie un e-mail de remerciement pour un entretien d'embauche\n",
    "A:\n",
    "Sujet: Remerciements suite à notre entretien\n",
    "\n",
    "Bonjour [Nom du Contact],\n",
    "\n",
    "Je tiens à vous remercier pour le temps que vous m'avez accordé...\n",
    "[...suite du mail...]\n",
    "\n",
    "Maintenant, voici ma demande:\n",
    "\n",
    "Q: Écris un e-mail pour informer un collaborateur d'un changement de planning et l'inviter à une réunion de suivi.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response_few_shot_2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,  # ex. \"gpt-4o-mini\"\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt_2}\n",
    "    ],\n",
    "    max_completion_tokens=300,\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "print(\"=== Exemple Few-shot (e-mail professionnel) ===\")\n",
    "print(response_few_shot_2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb1a44",
   "metadata": {
    "papermill": {
     "duration": 0.004087,
     "end_time": "2026-02-25T20:10:01.109498",
     "exception": false,
     "start_time": "2026-02-25T20:10:01.105411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Résultat du Few-shot : Format cohérent\n",
    "\n",
    "Le modèle a reproduit fidèlement la structure des exemples :\n",
    "\n",
    "1. **Sujet** : Clair et professionnel\n",
    "2. **Salutation** : Formule de politesse appropriée\n",
    "3. **Corps** : Structure logique (contexte → action → conclusion)\n",
    "4. **Signature** : Complète avec coordonnées\n",
    "\n",
    "**Observation clé** : Sans les exemples, le modèle aurait pu générer un email plus informel ou moins structuré. Le few-shot garantit la cohérence du format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6294e",
   "metadata": {
    "papermill": {
     "duration": 0.003734,
     "end_time": "2026-02-25T20:10:01.117159",
     "exception": false,
     "start_time": "2026-02-25T20:10:01.113425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Analyse du Few-shot prompting\n",
    "\n",
    "Le few-shot prompting apporte une **amélioration significative** par rapport au zero-shot :\n",
    "\n",
    "**Mécanisme** :\n",
    "1. On fournit 2-3 exemples de la tâche souhaitée (paires question/réponse)\n",
    "2. Le modèle apprend le **pattern** et le **format** attendu\n",
    "3. Il applique ensuite ce pattern à la nouvelle question\n",
    "\n",
    "**Avantages observables** :\n",
    "- **Format cohérent** : Le modèle reproduit la structure des exemples (sujet, salutation, corps, signature)\n",
    "- **Ton approprié** : Le style professionnel est maintenu\n",
    "- **Contenu pertinent** : La réponse suit les conventions des exemples fournis\n",
    "\n",
    "**Quand utiliser Few-shot ?**\n",
    "- Tâches avec un format spécifique (emails, rapports, analyses structurées)\n",
    "- Cas où le zero-shot donne des résultats trop variables\n",
    "- Besoin de cohérence stylistique\n",
    "\n",
    "**Compromis** : Chaque exemple consomme des tokens supplémentaires, donc à utiliser avec modération pour des contextes très longs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4164e",
   "metadata": {
    "papermill": {
     "duration": 0.00408,
     "end_time": "2026-02-25T20:10:01.125164",
     "exception": false,
     "start_time": "2026-02-25T20:10:01.121084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Partie 5 : Interactions conversationnelles\n",
    "\n",
    "Au-delà des techniques de prompting, la structure des conversations avec le modèle est cruciale pour des applications interactives.\n",
    "\n",
    "### Prompt simple sans mémoire\n",
    "\n",
    "La première cellule interactive montre un échange **stateless** (sans état) : chaque prompt est indépendant, le modèle n'a aucune mémoire des interactions précédentes.\n",
    "\n",
    "**Cas d'usage** :\n",
    "- Tests rapides d'un modèle\n",
    "- Questions indépendantes\n",
    "- Prototypage de prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff357bdc",
   "metadata": {
    "papermill": {
     "duration": 0.004471,
     "end_time": "2026-02-25T20:10:01.133933",
     "exception": false,
     "start_time": "2026-02-25T20:10:01.129462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Conversation avec mémoire\n",
    "\n",
    "La cellule suivante introduit un mécanisme de **mémoire de conversation** essentiel pour les chatbots et assistants.\n",
    "\n",
    "**Principe** :\n",
    "1. Accumuler les messages dans une liste `current_messages`\n",
    "2. À chaque tour, envoyer **tout l'historique** + le nouveau message\n",
    "3. Ajouter la réponse du modèle à l'historique\n",
    "\n",
    "**Structure des messages** :\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"Je m'appelle Alice\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Enchanté Alice !\"},\n",
    "    {\"role\": \"user\", \"content\": \"Comment je m'appelle ?\"},\n",
    "    # Le modèle peut répondre \"Vous vous appelez Alice\" grâce à l'historique\n",
    "]\n",
    "```\n",
    "\n",
    "**Attention** : L'historique consomme des tokens à chaque appel. Pour de longues conversations, il faut :\n",
    "- Tronquer les messages anciens\n",
    "- Résumer l'historique périodiquement\n",
    "- Utiliser des techniques de compression (embeddings, résumés automatiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2c7b0",
   "metadata": {
    "papermill": {
     "duration": 0.005438,
     "end_time": "2026-02-25T20:10:01.143507",
     "exception": false,
     "start_time": "2026-02-25T20:10:01.138069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chain-of-thought Prompt ===\n",
      "Réponse du modèle (avec raisonnement) :\n",
      "\n",
      "Alice a 3 pommes à la fin.\n"
     ]
    }
   ],
   "source": [
    "## Partie 6 : Modèles de raisonnement (2025)\n",
    "\n",
    "Les **modèles de raisonnement** (reasoning models) comme `o4-mini` et `gpt-5-thinking` représentent une évolution majeure des LLMs.\n",
    "\n",
    "### Différence fondamentale\n",
    "\n",
    "Les modèles chat génèrent token par token en temps réel. Les modèles de raisonnement :\n",
    "1. **Réfléchissent** en interne avant de répondre (pensées non visibles)\n",
    "2. **Explorent** plusieurs pistes de réflexion\n",
    "3. **Valident** leur raisonnement avant de produire la réponse finale\n",
    "\n",
    "### Paramètre clé : reasoning_effort\n",
    "\n",
    "Remplace le paramètre `temperature` pour les modèles de raisonnement :\n",
    "\n",
    "| Valeur | Durée | Cas d'usage |\n",
    "|--------|-------|-------------|\n",
    "| `low` | Rapide | Questions simples, prototypage |\n",
    "| `medium` | Modéré | Problèmes standards |\n",
    "| `high` | Lent | Problèmes complexes, mathématiques avancées |\n",
    "\n",
    "### Role \"developer\" obligatoire\n",
    "\n",
    "Les modèles de raisonnement n'utilisent plus le role `system`. Le role `developer` configure le comportement global, et peut activer/désactiver le formatage de la pensée interne.\n",
    "\n",
    "### Comparaison pratique\n",
    "\n",
    "Testons le même problème avec un modèle chat (gpt-4o-mini) et un modèle de raisonnement (o4-mini)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d1544b",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T20:10:01.152383Z",
     "iopub.status.busy": "2026-02-25T20:10:01.152150Z",
     "iopub.status.idle": "2026-02-25T20:10:01.691244Z",
     "shell.execute_reply": "2026-02-25T20:10:01.689753Z"
    },
    "papermill": {
     "duration": 0.547471,
     "end_time": "2026-02-25T20:10:01.694674",
     "exception": false,
     "start_time": "2026-02-25T20:10:01.147203",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chain-of-thought Prompt ===\n",
      "Réponse du modèle (avec raisonnement) :\n",
      "\n",
      "Alice a 3 pommes à la fin.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Chain-of-thought\n",
    "# ============================\n",
    "\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "Donne directement la réponse sans étape intermédiaire.\n",
    "\"\"\"\n",
    "\n",
    "# Explique ton raisonnement étape par étape, puis donne la réponse finale.\n",
    "response_3 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": cot_prompt}\n",
    "    ],\n",
    "    max_completion_tokens=200,\n",
    "    temperature=0.2  # on réduit la température pour moins de fantaisie\n",
    ")\n",
    "\n",
    "print(\"=== Chain-of-thought Prompt ===\")\n",
    "print(\"Réponse du modèle (avec raisonnement) :\\n\")\n",
    "print(response_3.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5eb015",
   "metadata": {
    "papermill": {
     "duration": 0.012762,
     "end_time": "2026-02-25T20:10:01.721123",
     "exception": false,
     "start_time": "2026-02-25T20:10:01.708361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Analyse du résultat CoT\n",
    "\n",
    "**Observation** : Le prompt demandait une réponse directe SANS étapes intermédiaires, et le modèle a bien obéi en donnant simplement \"3 pommes\".\n",
    "\n",
    "**Expérimentation recommandée** : Modifiez le prompt pour demander explicitement le raisonnement :\n",
    "\n",
    "```python\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "Explique ton raisonnement étape par étape avant de donner la réponse finale.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Vous devriez alors obtenir :\n",
    "- Étape 1 : 5 - 2 = 3\n",
    "- Étape 2 : 3 - 1 = 2\n",
    "- Étape 3 : 2 + 1 = 3\n",
    "- **Réponse : 3 pommes**\n",
    "\n",
    "### Bonnes pratiques CoT\n",
    "\n",
    "1. **Temperature basse** (0.0-0.3) pour les calculs et la logique\n",
    "2. **Prompt explicite** : \"Explique ton raisonnement étape par étape\"\n",
    "3. **Vérification** : Le raisonnement explicite permet de détecter les erreurs\n",
    "4. **Masquage optionnel** : Pour les applications de production, on peut demander au modèle de ne pas révéler son raisonnement à l'utilisateur final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a6f88",
   "metadata": {
    "papermill": {
     "duration": 0.004197,
     "end_time": "2026-02-25T20:10:01.730179",
     "exception": false,
     "start_time": "2026-02-25T20:10:01.725982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (1) : Code buggy ===\n",
      "\n",
      "Voici une fonction Python qui calcule la somme d'une liste, avec un bug intentionnel :\n",
      "\n",
      "```python\n",
      "def somme_liste(ma_liste):\n",
      "    total = 0\n",
      "    for nombre in ma_liste:\n",
      "        total += nombre\n",
      "    return total\n",
      "\n",
      "# Bug intentionnel : On oublie de convertir les éléments en nombres, ce qui provoquera une erreur si la liste contient des chaînes de caractères\n",
      "liste_exemple = [1, 2, '3', 4]\n",
      "resultat = somme_liste(liste_exemple)\n",
      "print(resultat)\n",
      "```\n",
      "\n",
      "Dans ce code, le bug est que l'élément '3' (une chaîne de caractères) dans `liste_exemple` va provoquer une erreur lors de l'addition, car on essaie d'additionner une chaîne avec des entiers.\n"
     ]
    }
   ],
   "source": [
    "### Interprétation du Chain-of-thought\n",
    "\n",
    "Le Chain-of-thought (CoT) est particulièrement efficace pour les **problèmes de raisonnement** :\n",
    "\n",
    "**Analyse du résultat** :\n",
    "Le modèle devrait avoir détaillé :\n",
    "1. État initial : Alice a 5 pommes\n",
    "2. Étape 1 : Elle en jette 2 → 5 - 2 = 3 pommes\n",
    "3. Étape 2 : Elle en donne 1 à Bob → 3 - 1 = 2 pommes\n",
    "4. Étape 3 : Bob lui rend 1 pomme → 2 + 1 = 3 pommes\n",
    "5. **Réponse finale : 3 pommes**\n",
    "\n",
    "**Pourquoi c'est important ?**\n",
    "- **Transparence** : On peut vérifier le raisonnement étape par étape\n",
    "- **Détection d'erreurs** : Si le résultat est faux, on peut identifier où le modèle s'est trompé\n",
    "- **Confiance** : Le raisonnement explicite augmente la crédibilité\n",
    "- **Debugging** : Facilite la correction du prompt si nécessaire\n",
    "\n",
    "**Applications** :\n",
    "- Calculs mathématiques\n",
    "- Raisonnement logique\n",
    "- Résolution de problèmes complexes\n",
    "- Analyse de cas juridiques ou médicaux\n",
    "\n",
    "**Note** : Temperature=0.2 garantit un raisonnement cohérent et reproductible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30edbb7c",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T20:10:01.741355Z",
     "iopub.status.busy": "2026-02-25T20:10:01.740821Z",
     "iopub.status.idle": "2026-02-25T20:10:06.084091Z",
     "shell.execute_reply": "2026-02-25T20:10:06.083417Z"
    },
    "papermill": {
     "duration": 4.35078,
     "end_time": "2026-02-25T20:10:06.085026",
     "exception": false,
     "start_time": "2026-02-25T20:10:01.734246",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (1) : Code buggy ===\n",
      "\n",
      "Voici une courte fonction Python pour calculer la somme d'une liste, avec un bug introduit volontairement :\n",
      "\n",
      "```python\n",
      "def somme_liste(liste):\n",
      "    total = 0\n",
      "    for i in liste:\n",
      "        total += i\n",
      "    return total + 1  # Bug : On ajoute 1 au total, ce qui fausse le résultat.\n",
      "\n",
      "# Exemple d'utilisation\n",
      "ma_liste = [1, 2, 3, 4, 5]\n",
      "print(somme_liste(ma_liste))  # Cela affichera 16 au lieu de 15.\n",
      "```\n",
      "\n",
      "Dans cette fonction, un 1 est ajouté au total final, ce qui introduit une erreur dans le calcul de la somme.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8a : Self-refine Step 1\n",
    "# ============================\n",
    "\n",
    "prompt_sr1 = \"\"\"\n",
    "Ecris une courte fonction Python pour calculer la somme d'une liste. \n",
    "Ajoute un bug volontaire dans le code. \n",
    "\"\"\"\n",
    "\n",
    "response_sr1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr1}],\n",
    "    max_completion_tokens=300\n",
    ")\n",
    "\n",
    "buggy_code = response_sr1.choices[0].message.content\n",
    "\n",
    "print(\"=== Self-refine (1) : Code buggy ===\\n\")\n",
    "print(buggy_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75b428",
   "metadata": {
    "papermill": {
     "duration": 0.003613,
     "end_time": "2026-02-25T20:10:06.098027",
     "exception": false,
     "start_time": "2026-02-25T20:10:06.094414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (2) : Correction ===\n",
      "\n",
      "Bien sûr ! Analysons le code que vous avez fourni.\n",
      "\n",
      "### Analyse du code\n",
      "\n",
      "La fonction `somme_liste` est censée calculer la somme des éléments d'une liste. Cependant, si la liste contient des chaînes de caractères (comme dans l'exemple avec `'3'`), cela provoquera une erreur de type lors de l'opération d'addition. En Python, vous ne pouvez pas additionner un entier et une chaîne de caractères, ce qui entraînera une exception `TypeError`.\n",
      "\n",
      "### Bug identifié\n",
      "\n",
      "Le bug se trouve dans la tentative d'addition d'éléments de types différents (entiers et chaînes de caractères). \n",
      "\n",
      "### Correctif\n",
      "\n",
      "Pour corriger ce bug, nous devons nous assurer que tous les éléments de la liste sont des nombres avant de les additionner. Nous pouvons le faire en convertissant chaque élément en entier (ou en flottant) si cela est possible. Si un élément ne peut pas être converti, nous pouvons choisir de l'ignorer ou de lever une exception.\n",
      "\n",
      "### Version améliorée du code\n",
      "\n",
      "Voici une version améliorée de la fonction qui gère les erreurs de conversion et ignore les éléments non numériques :\n",
      "\n",
      "```python\n",
      "def somme_liste(ma_liste):\n",
      "    total = 0\n",
      "    for element in ma_liste:\n",
      "        try:\n",
      "            # Convertir l'élément en nombre (entier ou flottant)\n",
      "            nombre = float(element)  # Utilisation de float pour gérer les nombres décimaux\n",
      "            total += nombre\n",
      "        except (ValueError, TypeError):\n",
      "            # Ignorer les éléments qui ne peuvent pas être convertis en nombre\n",
      "            print(f\"Élément ignoré : {element} (non convertible en nombre)\")\n",
      "    return total\n",
      "\n",
      "# Liste d'exemple avec un élément non numérique\n",
      "liste_exemple = [1, 2, '3', 4, 'abc', None]\n",
      "resultat = somme_liste(liste_exemple)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 8b. Self-critique et amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d54be4e",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T20:10:06.106323Z",
     "iopub.status.busy": "2026-02-25T20:10:06.106128Z",
     "iopub.status.idle": "2026-02-25T20:10:13.576280Z",
     "shell.execute_reply": "2026-02-25T20:10:13.575315Z"
    },
    "papermill": {
     "duration": 7.476167,
     "end_time": "2026-02-25T20:10:13.577864",
     "exception": false,
     "start_time": "2026-02-25T20:10:06.101697",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (2) : Correction ===\n",
      "\n",
      "Bien sûr ! Analysons le code fourni et identifions le bug.\n",
      "\n",
      "### Analyse du code\n",
      "\n",
      "La fonction `somme_liste` est censée calculer la somme des éléments d'une liste. Voici le code :\n",
      "\n",
      "```python\n",
      "def somme_liste(liste):\n",
      "    total = 0\n",
      "    for i in liste:\n",
      "        total += i\n",
      "    return total + 1  # Bug : On ajoute 1 au total, ce qui fausse le résultat.\n",
      "```\n",
      "\n",
      "#### Bug identifié\n",
      "Le bug dans ce code est que la ligne `return total + 1` ajoute 1 au total calculé, ce qui fausse le résultat. Par exemple, pour la liste `[1, 2, 3, 4, 5]`, la somme correcte est 15, mais le code renvoie 16 à cause de cette addition.\n",
      "\n",
      "### Correctif\n",
      "\n",
      "Pour corriger ce bug, il suffit de modifier la ligne de retour pour qu'elle renvoie simplement `total` sans ajouter 1 :\n",
      "\n",
      "```python\n",
      "def somme_liste(liste):\n",
      "    total = 0\n",
      "    for i in liste:\n",
      "        total += i\n",
      "    return total  # Correction : On renvoie simplement le total.\n",
      "```\n",
      "\n",
      "### Version améliorée\n",
      "\n",
      "Bien que la fonction corrigée soit maintenant correcte, nous pouvons l'améliorer en utilisant la fonction intégrée `sum()` de Python, qui est plus concise et optimisée. Voici la version améliorée :\n",
      "\n",
      "```python\n",
      "def somme_liste(liste):\n",
      "    return sum(liste)  # Utilisation de la fonction intégrée sum pour simplifier le code.\n",
      "\n",
      "# Exemple d'utilisation\n",
      "ma_liste = [1, 2, 3, 4, 5]\n",
      "print(somme_liste(ma_liste))  # Cela affichera 15 comme attendu.\n",
      "```\n",
      "\n",
      "### Explication de la correction\n",
      "\n",
      "1. **Correction du bug** : En supprimant l'ajout de 1 dans la\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8b : Self-refine Step 2\n",
    "# ============================\n",
    "\n",
    "prompt_sr2 = f\"\"\"\n",
    "Voici un code Python qui contient un bug:\n",
    "\n",
    "{buggy_code}\n",
    "\n",
    "Peux-tu l'analyser, détecter le bug, proposer un correctif et une version améliorée du code ? \n",
    "Explique la correction.\n",
    "\"\"\"\n",
    "\n",
    "response_sr2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr2}],\n",
    "    max_completion_tokens=400,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine (2) : Correction ===\\n\")\n",
    "print(response_sr2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa422e",
   "metadata": {
    "papermill": {
     "duration": 0.006046,
     "end_time": "2026-02-25T20:10:13.592250",
     "exception": false,
     "start_time": "2026-02-25T20:10:13.586204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Résultat du Self-refine : Amélioration itérative\n",
    "\n",
    "Le modèle a :\n",
    "1. **Identifié le bug** : L'ajout de `+ 1` dans le `return` fausse le résultat\n",
    "2. **Proposé une correction** : Retirer le `+ 1`\n",
    "3. **Suggéré une amélioration** : Utiliser la fonction native `sum()` pour plus de simplicité\n",
    "\n",
    "**Enseignement** : Le modèle possède une capacité de **méta-cognition** - il peut raisonner sur ses propres productions et les améliorer.\n",
    "\n",
    "### Variante avancée : Self-refine avec role \"developer\"\n",
    "\n",
    "Au lieu d'un processus en deux appels API, on peut utiliser le rôle `developer` pour configurer le comportement d'auto-amélioration directement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c8a9c",
   "metadata": {
    "papermill": {
     "duration": 0.003902,
     "end_time": "2026-02-25T20:10:13.600516",
     "exception": false,
     "start_time": "2026-02-25T20:10:13.596614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine avec 'developer role' ===\n",
      "Voici une fonction Python qui calcule la factorielle d'un nombre entier. Je vais également m'assurer qu'il n'y a pas de bugs et que le code est optimisé.\n",
      "\n",
      "```python\n",
      "def factorielle(n):\n",
      "    \"\"\"Calcule la factorielle d'un nombre entier n.\"\"\"\n",
      "    if not isinstance(n, int):\n",
      "        raise ValueError(\"L'argument doit être un entier.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"La factorielle n'est pas définie pour les nombres négatifs.\")\n",
      "    \n",
      "    resultat = 1\n",
      "    for i in range(2, n + 1):\n",
      "        resultat *= i\n",
      "    return resultat\n",
      "```\n",
      "\n",
      "### Améliorations et vérifications :\n",
      "\n",
      "1. **Validation de l'entrée** : La fonction vérifie maintenant si l'argument est un entier et s'il est positif. Cela évite des erreurs lors de l'exécution.\n",
      "2. **Utilisation d'une boucle** : La méthode itérative est utilisée pour éviter les problèmes de récursion qui peuvent survenir avec des nombres très élevés.\n",
      "3. **Documentation** : J'ai ajouté une docstring pour expliquer ce que fait la fonction.\n",
      "\n",
      "### Test de la fonction\n",
      "\n",
      "Pour s'assurer que la fonction fonctionne correctement, voici quelques tests :\n",
      "\n",
      "```python\n",
      "print(factorielle(5))  # Devrait afficher 120\n",
      "print(factorielle(0))  # Devrait afficher 1\n",
      "print(factorielle(1))  # Devrait afficher\n"
     ]
    }
   ],
   "source": [
    "### Bilan du Self-refine\n",
    "\n",
    "Le processus Self-refine en deux étapes démontre la capacité du modèle à s'**auto-améliorer** :\n",
    "\n",
    "**Première étape** : Génération intentionnellement bugguée\n",
    "- Le modèle crée du code avec un bug volontaire (par exemple, division par zéro, mauvaise initialisation, etc.)\n",
    "\n",
    "**Deuxième étape** : Critique et correction\n",
    "- Le modèle analyse son propre code\n",
    "- Identifie le bug\n",
    "- Propose une version corrigée\n",
    "- Explique la nature du problème\n",
    "\n",
    "**Enseignements** :\n",
    "1. **Méta-cognition** : Le LLM peut raisonner sur ses propres productions\n",
    "2. **Amélioration itérative** : Chaque passe peut affiner la qualité\n",
    "3. **Détection de bugs** : Utile pour du code review automatisé\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Génération de code robuste (plusieurs passes de correction)\n",
    "- Rédaction de documents (brouillon → révision → version finale)\n",
    "- Traduction (première traduction → révision → amélioration)\n",
    "\n",
    "**Limite** : Chaque itération consomme des tokens et du temps. À utiliser pour des tâches critiques nécessitant haute qualité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b94379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T20:10:13.611479Z",
     "iopub.status.busy": "2026-02-25T20:10:13.611255Z",
     "iopub.status.idle": "2026-02-25T20:10:21.280566Z",
     "shell.execute_reply": "2026-02-25T20:10:21.278390Z"
    },
    "papermill": {
     "duration": 7.678002,
     "end_time": "2026-02-25T20:10:21.283222",
     "exception": false,
     "start_time": "2026-02-25T20:10:13.605220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine avec 'developer role' ===\n",
      "Voici une fonction Python qui calcule la factorielle d'un nombre entier. Je vais m'assurer qu'elle est correcte et qu'elle gère les cas particuliers.\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    # Vérification que n est un entier\n",
      "    if not isinstance(n, int):\n",
      "        raise ValueError(\"L'argument doit être un entier.\")\n",
      "    \n",
      "    # Gestion des cas particuliers\n",
      "    if n < 0:\n",
      "        raise ValueError(\"La factorielle n'est pas définie pour les entiers négatifs.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    \n",
      "    # Calcul de la factorielle\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    \n",
      "    return result\n",
      "```\n",
      "\n",
      "### Améliorations et vérifications :\n",
      "1. **Validation de l'entrée** : La fonction vérifie maintenant que l'argument est un entier et qu'il n'est pas négatif.\n",
      "2. **Cas particuliers** : Elle gère correctement les cas où `n` est 0 ou 1, en retournant 1.\n",
      "3. **Boucle de calcul** : La boucle commence à 2, ce qui est optimal pour éviter des multiplications inutiles.\n",
      "\n",
      "### Test de la fonction\n",
      "Voici quelques tests pour s'assurer que la fonction fonctionne correctement :\n",
      "\n",
      "```python\n",
      "print(factorial(5))  # Devrait afficher 120\n",
      "print(factorial(0))\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Self-refine avec developer role\n",
    "# ============================\n",
    "\n",
    "messages_sr = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": (\n",
    "            \"You are a self-improving coding assistant. Whenever you provide code, \"\n",
    "            \"you will automatically search for potential bugs or improvements \"\n",
    "            \"and refine your output.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Écris une fonction Python qui calcule la factorielle d'un nombre entier. \"\n",
    "            \"Ensuite, relis-toi et corrige d'éventuels bugs.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "response_self_refine = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages_sr,\n",
    "    max_completion_tokens=300,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine avec 'developer role' ===\")\n",
    "print(response_self_refine.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad7e6c4",
   "metadata": {
    "papermill": {
     "duration": 0.003944,
     "end_time": "2026-02-25T20:10:21.294305",
     "exception": false,
     "start_time": "2026-02-25T20:10:21.290361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] Prompt: Bonjour!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse: Bonjour ! Comment puis-je vous aider aujourd'hui ?\n",
      "\n",
      "[BATCH] Prompt: Quelle est la capitale de la France?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse: La capitale de la France est Paris.\n",
      "\n",
      "Mode batch terminé.\n"
     ]
    }
   ],
   "source": [
    "### Interprétation du Self-refine avec role \"developer\"\n",
    "\n",
    "Le rôle `developer` (introduit avec les modèles de raisonnement) permet de définir le comportement global du modèle, contrairement au rôle `system` qui était utilisé auparavant.\n",
    "\n",
    "**Différence clé** :\n",
    "- **Role \"system\"** (ancienne API) : Instructions générales, parfois ignorées par le modèle\n",
    "- **Role \"developer\"** (nouvelle API) : Instructions prioritaires, mieux respectées\n",
    "\n",
    "Dans cet exemple :\n",
    "- Le modèle génère la fonction factorielle\n",
    "- Il vérifie automatiquement les bugs potentiels\n",
    "- Il propose des améliorations (gestion d'erreurs, performance, alternatives)\n",
    "\n",
    "**Observation** : La fonction générée inclut directement :\n",
    "- Vérification du type (`isinstance`)\n",
    "- Gestion des nombres négatifs\n",
    "- Version alternative avec `math.factorial`\n",
    "\n",
    "C'est plus efficace que le Self-refine en deux passes, mais nécessite un modèle récent supportant le rôle `developer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe87023",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T20:10:21.303378Z",
     "iopub.status.busy": "2026-02-25T20:10:21.303184Z",
     "iopub.status.idle": "2026-02-25T20:10:23.456211Z",
     "shell.execute_reply": "2026-02-25T20:10:23.455533Z"
    },
    "papermill": {
     "duration": 2.159382,
     "end_time": "2026-02-25T20:10:23.457712",
     "exception": false,
     "start_time": "2026-02-25T20:10:21.298330",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] Prompt: Bonjour!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse: Bonjour ! Comment puis-je vous aider aujourd'hui ?\n",
      "\n",
      "[BATCH] Prompt: Quelle est la capitale de la France?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse: La capitale de la France est Paris.\n",
      "\n",
      "Mode batch terminé.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif\n",
    "# ============================\n",
    "\n",
    "# En mode batch (BATCH_MODE=true dans .env), cette cellule utilise des exemples prédéfinis\n",
    "# En mode interactif, elle permet de tester le modèle en boucle\n",
    "\n",
    "if BATCH_MODE:\n",
    "    # Mode batch: exécuter des exemples simples\n",
    "    test_prompts = [\"Bonjour!\", \"Quelle est la capitale de la France?\"]\n",
    "    for prompt in test_prompts:\n",
    "        print(f\"[BATCH] Prompt: {prompt}\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            max_completion_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        print(f\"Réponse: {resp.choices[0].message.content}\\n\")\n",
    "    print(\"Mode batch terminé.\")\n",
    "else:\n",
    "    while True:\n",
    "        user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Fin de l'interaction.\")\n",
    "            break\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_completion_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== Réponse du modèle ===\")\n",
    "        print(resp.choices[0].message.content)\n",
    "        print(\"---------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9baccb",
   "metadata": {
    "papermill": {
     "duration": 0.004129,
     "end_time": "2026-02-25T20:10:23.469325",
     "exception": false,
     "start_time": "2026-02-25T20:10:23.465196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] User: Je m'appelle Alice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Bonjour Alice ! Comment puis-je vous aider aujourd'hui ?\n",
      "\n",
      "[BATCH] User: Comment je m'appelle?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Vous vous appelez Alice. Comment puis-je vous aider aujourd'hui ?\n",
      "\n",
      "Mode batch (mémoire) terminé.\n"
     ]
    }
   ],
   "source": [
    "### Interprétation des tests interactifs\n",
    "\n",
    "Cette cellule démontre le **mode batch** pour tester le modèle sans interaction manuelle.\n",
    "\n",
    "**Résultats observés** :\n",
    "- **Prompt simple** : \"Bonjour!\" → Réponse polie et ouverture de dialogue\n",
    "- **Question factuelle** : \"Quelle est la capitale de la France?\" → Réponse directe et précise\n",
    "\n",
    "**Points clés** :\n",
    "1. **Température 0.7** : Équilibre entre cohérence et créativité\n",
    "2. **Max tokens 200** : Limite les réponses pour réduire les coûts\n",
    "3. **Messages stateless** : Chaque prompt est indépendant, pas de mémoire entre appels\n",
    "\n",
    "**Différence avec la cellule suivante** : La prochaine cellule introduit la **mémoire de conversation** (accumulation de l'historique), essentielle pour les chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd01567c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T20:10:23.477884Z",
     "iopub.status.busy": "2026-02-25T20:10:23.477696Z",
     "iopub.status.idle": "2026-02-25T20:10:24.963443Z",
     "shell.execute_reply": "2026-02-25T20:10:24.962605Z"
    },
    "papermill": {
     "duration": 1.491732,
     "end_time": "2026-02-25T20:10:24.964919",
     "exception": false,
     "start_time": "2026-02-25T20:10:23.473187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] User: Je m'appelle Alice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Bonjour Alice ! Comment puis-je vous aider aujourd'hui ?\n",
      "\n",
      "[BATCH] User: Comment je m'appelle?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Vous vous appelez Alice. Comment puis-je vous aider aujourd'hui, Alice ?\n",
      "\n",
      "Mode batch (mémoire) terminé.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif avec mémoire de chat\n",
    "# ============================\n",
    "\n",
    "# En mode batch, cette cellule utilise un dialogue prédéfini\n",
    "# En mode interactif, elle permet un échange avec mémoire\n",
    "\n",
    "if BATCH_MODE:\n",
    "    # Mode batch: simuler une conversation avec mémoire\n",
    "    batch_conversation = [\n",
    "        \"Je m'appelle Alice\",\n",
    "        \"Comment je m'appelle?\",\n",
    "    ]\n",
    "    current_messages = []\n",
    "\n",
    "    for user_input in batch_conversation:\n",
    "        print(f\"[BATCH] User: {user_input}\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=current_messages + [{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_completion_tokens=200,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        assistant_message = resp.choices[0].message.content\n",
    "        print(f\"Assistant: {assistant_message}\\n\")\n",
    "        current_messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "        current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})\n",
    "\n",
    "    print(\"Mode batch (mémoire) terminé.\")\n",
    "else:\n",
    "    user_input = \"\"\n",
    "    current_messages = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Fin de l'interaction.\")\n",
    "            break\n",
    "        print(\"\\n=== message de l'utilisateur ===\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages = current_messages + [{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_completion_tokens=200,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== Réponse du modèle ===\")\n",
    "        assistant_message = resp.choices[0].message.content\n",
    "        print(assistant_message)\n",
    "        print(\"---------------------------------------------------\\n\")\n",
    "        current_messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "        current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1caa5f9",
   "metadata": {
    "papermill": {
     "duration": 0.006354,
     "end_time": "2026-02-25T20:10:24.977325",
     "exception": false,
     "start_time": "2026-02-25T20:10:24.970971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Interprétation de la conversation avec mémoire\n",
    "\n",
    "Cette cellule démontre la **mémoire conversationnelle** - le modèle se souvient du contexte précédent.\n",
    "\n",
    "**Résultats observés** :\n",
    "1. **Tour 1** : \"Je m'appelle Alice\" → Le modèle enregistre cette information dans `current_messages`\n",
    "2. **Tour 2** : \"Comment je m'appelle?\" → Le modèle répond \"Vous vous appelez Alice\" grâce à l'historique\n",
    "\n",
    "**Mécanisme technique** :\n",
    "```python\n",
    "current_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Je m'appelle Alice\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Bonjour, Alice...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Comment je m'appelle?\"},\n",
    "    # Le modèle peut répondre grâce à l'historique complet\n",
    "]\n",
    "```\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Chatbots conversationnels\n",
    "- Assistants personnels\n",
    "- Support client automatisé\n",
    "\n",
    "**Gestion de l'historique** :\n",
    "- **Court terme** : Garder tout l'historique (comme ici)\n",
    "- **Long terme** : Tronquer les messages anciens ou résumer périodiquement\n",
    "- **Tokens** : Attention au coût - chaque tour envoie TOUT l'historique à l'API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35585119",
   "metadata": {
    "papermill": {
     "duration": 0.005985,
     "end_time": "2026-02-25T20:10:24.990122",
     "exception": false,
     "start_time": "2026-02-25T20:10:24.984137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gpt-4o-mini (Chat Model) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps: 4.67s\n",
      "Le fermier peut transporter le loup, la chèvre et le chou de l'autre côté de la rivière en suivant ces étapes :\n",
      "\n",
      "1. Le fermier prend la chèvre et la transporte de l'autre côté de la rivière.\n",
      "2. Il laisse la chèvre de l'autre côté et retourne seul.\n",
      "3. Il prend le loup et le transporte de l'autre côté.\n",
      "4. Il laisse le loup de l'autre côté, mais il prend la chèvre avec lui pour la ramener de l'autre ...\n",
      "\n",
      "=== o4-mini (Reasoning Model) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps: 5.24s\n",
      "Voici une façon de procéder en 7 déplacements :\n",
      "\n",
      "1. Le fermier prend la chèvre et la traverse sur l’autre rive.  \n",
      "   Rive de départ : loup + chou  \n",
      "   Rive d’arrivée : chèvre\n",
      "\n",
      "2. Il revient seul à la rive de départ.  \n",
      "   Rive de départ : loup + chou  \n",
      "   Rive d’arrivée : chèvre\n",
      "\n",
      "3. Il prend le loup et le traverse sur l’autre rive.  \n",
      "   Rive de départ : chou  \n",
      "   Rive d’arrivée : chèvre + loup\n",
      "\n",
      "4. ...\n"
     ]
    }
   ],
   "source": [
    "# Prompting pour Modèles de Raisonnement (2025)\n",
    "\n",
    "Les modèles de raisonnement (o4-mini, gpt-5-thinking) représentent une évolution majeure. Contrairement aux modèles de chat, ils prennent le temps de \"réfléchir\" avant de répondre.\n",
    "\n",
    "## Différences clés avec les modèles chat\n",
    "\n",
    "| Aspect | Modèles Chat (gpt-4o) | Modèles Raisonnement (o4-mini) |\n",
    "|--------|----------------------|-------------------------------|\n",
    "| Temps de réponse | Rapide | Plus lent (réflexion) |\n",
    "| Prompts | Détaillés, structurés | **Simples et directs** |\n",
    "| Chain-of-thought | Demandé explicitement | **Intégré nativement** |\n",
    "| Messages | system, user, assistant | **developer**, user, assistant |\n",
    "| Paramètre spécial | temperature | **reasoning_effort** |\n",
    "\n",
    "## Règle d'or : Simplifier les prompts\n",
    "\n",
    "**Pour les modèles de raisonnement, des prompts simples fonctionnent mieux !**\n",
    "\n",
    "Les modèles raisonnants sont capables de :\n",
    "- Comprendre l'intention sans instructions détaillées\n",
    "- Gérer les ambiguïtés intelligemment\n",
    "- Demander des clarifications si nécessaire\n",
    "\n",
    "**Éviter** : \"Analyse ce problème en détaillant chaque étape de ton raisonnement...\"\n",
    "**Préférer** : \"Résous ce problème.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbc4a0e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T20:10:25.000478Z",
     "iopub.status.busy": "2026-02-25T20:10:24.999864Z",
     "iopub.status.idle": "2026-02-25T20:10:33.077234Z",
     "shell.execute_reply": "2026-02-25T20:10:33.076403Z"
    },
    "papermill": {
     "duration": 8.083853,
     "end_time": "2026-02-25T20:10:33.078460",
     "exception": false,
     "start_time": "2026-02-25T20:10:24.994607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gpt-4o-mini (Chat Model) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps: 3.69s\n",
      "Le fermier peut traverser la rivière en suivant ces étapes :\n",
      "\n",
      "1. Le fermier prend la chèvre et la transporte de l'autre côté de la rivière.\n",
      "2. Il laisse la chèvre de l'autre côté et retourne seul de l'autre côté.\n",
      "3. Il prend le loup et le transporte de l'autre côté.\n",
      "4. Il laisse le loup de l'autre côté, mais il prend la chèvre avec lui pour la ramener de l'autre côté.\n",
      "5. Il laisse la chèvre de ce ...\n",
      "\n",
      "=== o4-mini (Reasoning Model) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps: 4.38s\n",
      "Voici la suite de manœuvres qui permet au fermier de tout faire passer sans perte :\n",
      "\n",
      "1. Le fermier traverse la rivière avec la chèvre et la laisse de l’autre côté.  \n",
      "2. Il revient seul sur la rive de départ.  \n",
      "3. Il traverse avec le loup et le dépose de l’autre côté.  \n",
      "4. Il reprend la chèvre dans le bateau et retourne sur la rive de départ.  \n",
      "5. Il laisse la chèvre, traverse avec le chou et le dé...\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Comparaison Chat vs Reasoning\n",
    "# ============================\n",
    "\n",
    "import time\n",
    "\n",
    "probleme_complexe = \"\"\"\n",
    "Un fermier veut traverser une rivière avec un loup, une chèvre et un chou.\n",
    "Son bateau ne peut transporter que lui et un objet à la fois.\n",
    "Si le loup est laissé seul avec la chèvre, il la mange.\n",
    "Si la chèvre est laissée seule avec le chou, elle le mange.\n",
    "Comment le fermier peut-il tout transporter de l'autre côté?\n",
    "\"\"\"\n",
    "\n",
    "# Test avec gpt-4o-mini (chat model)\n",
    "print(\"=== gpt-4o-mini (Chat Model) ===\")\n",
    "start = time.time()\n",
    "response_chat = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": probleme_complexe}],\n",
    "    max_completion_tokens=500,\n",
    "    temperature=0.2\n",
    ")\n",
    "print(f\"Temps: {time.time() - start:.2f}s\")\n",
    "print(response_chat.choices[0].message.content[:400] + \"...\")\n",
    "\n",
    "# Test avec o4-mini (reasoning model) - si disponible\n",
    "print(\"\\n=== o4-mini (Reasoning Model) ===\")\n",
    "try:\n",
    "    start = time.time()\n",
    "    response_reason = client.chat.completions.create(\n",
    "        model=\"o4-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "            {\"role\": \"user\", \"content\": probleme_complexe}\n",
    "        ],\n",
    "        reasoning_effort=\"medium\"\n",
    "    )\n",
    "    print(f\"Temps: {time.time() - start:.2f}s\")\n",
    "    print(response_reason.choices[0].message.content[:400] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"o4-mini non disponible: {type(e).__name__}\")\n",
    "    print(\"Les modèles de raisonnement nécessitent un accès spécifique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e7835",
   "metadata": {
    "papermill": {
     "duration": 0.004393,
     "end_time": "2026-02-25T20:10:33.088014",
     "exception": false,
     "start_time": "2026-02-25T20:10:33.083621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse des résultats : Chat vs Reasoning\n",
    "\n",
    "**Observations attendues** :\n",
    "\n",
    "1. **Temps de réponse** :\n",
    "   - Chat (gpt-4o-mini) : ~2-5 secondes\n",
    "   - Reasoning (o4-mini) : ~10-30 secondes (réflexion interne)\n",
    "\n",
    "2. **Qualité de la solution** :\n",
    "   - Chat : Solution généralement correcte, mais peut manquer des optimisations\n",
    "   - Reasoning : Solution optimale, avec explication détaillée\n",
    "\n",
    "3. **Robustesse** :\n",
    "   - Chat : Peut parfois donner des solutions incorrectes sur des variantes complexes\n",
    "   - Reasoning : Vérifie sa solution avant de répondre, taux d'erreur plus faible\n",
    "\n",
    "### Quand utiliser les modèles de raisonnement ?\n",
    "\n",
    "**Préférer Chat** (gpt-4o, gpt-4o-mini) :\n",
    "- Applications temps réel (chatbots)\n",
    "- Tâches simples et courantes\n",
    "- Budget temps/coût limité\n",
    "\n",
    "**Préférer Reasoning** (o4-mini, gpt-5-thinking) :\n",
    "- Mathématiques avancées\n",
    "- Problèmes de logique complexes\n",
    "- Code critique nécessitant validation\n",
    "- Analyse approfondie\n",
    "\n",
    "**Note** : Les modèles de raisonnement sont plus coûteux en tokens et en temps, mais offrent une qualité supérieure pour les tâches complexes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 48.27091,
   "end_time": "2026-02-25T20:10:33.227996",
   "environment_variables": {},
   "exception": null,
   "input_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Texte\\2_PromptEngineering.ipynb",
   "output_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Texte\\2_PromptEngineering_output.ipynb",
   "parameters": {
    "BATCH_MODE": "true"
   },
   "start_time": "2026-02-25T20:09:44.957086",
   "version": "2.6.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}