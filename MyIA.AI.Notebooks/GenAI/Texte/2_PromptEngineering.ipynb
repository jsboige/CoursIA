{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering : Advanced Prompting avec OpenAI\n",
    "\n",
    "Dans ce notebook, nous allons tester différentes techniques avancées de **prompt engineering**:\n",
    "- **Zero-shot prompting**\n",
    "- **Few-shot prompting**\n",
    "- **Chain-of-thought** (CoT)\n",
    "- **Self-refine** (ou auto-amélioration)\n",
    "\n",
    "Nous utiliserons la **nouvelle API** de la bibliothèque `openai` (>=1.0.0) via la classe `OpenAI` et ses méthodes de chat (`client.chat.completions.create`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 2. Prompt Engineering : Techniques Avancées\n\n**Navigation** : [<< Precedent](1_OpenAI_Intro.ipynb) | [Index](../../README.md) | [Suivant >>](3_Structured_Outputs.ipynb)\n\n## Objectifs d'apprentissage\n\nA la fin de ce notebook, vous saurez :\n1. Maîtriser les techniques de zero-shot et few-shot prompting\n2. Comprendre et appliquer le chain-of-thought (CoT)\n3. Implémenter le self-refine pour l'amélioration itérative\n4. Distinguer modèles chat et modèles de raisonnement\n\n### Prerequis\n- Notebook 1 (Introduction a l'IA generative)\n- Python 3.10+\n- Cle API OpenAI configuree\n\n### Duree estimee : 60 minutes\n\n---\n\n# Prompt Engineering : Advanced Prompting avec OpenAI",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation des dépendances\n",
    "\n",
    "Avant de commencer, nous devons installer les bibliothèques Python nécessaires.\n",
    "\n",
    "**Packages requis** :\n",
    "- **openai** : Bibliothèque officielle pour interagir avec l'API OpenAI (>=1.0.0)\n",
    "- **tiktoken** : Encodeur de tokens pour compter et gérer les tokens GPT\n",
    "- **python-dotenv** : Gestion sécurisée des clés API via fichiers .env\n",
    "\n",
    "> **Note de sécurité** : Ne jamais inclure vos clés API directement dans le code. Toujours utiliser un fichier  exclu du contrôle de version (.gitignore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.109.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (0.11.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.11.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2025.9.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 1 : Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi le Prompt Engineering ?\n",
    "\n",
    "Le **prompt engineering** est l'art de formuler des instructions efficaces pour obtenir les meilleures reponses des modeles de langage. C'est une competence essentielle car :\n",
    "\n",
    "1. **Impact direct sur la qualite** : Un bon prompt peut transformer une reponse mediocre en resultat excellent\n",
    "2. **Economie de tokens** : Des prompts bien concus reduisent les iterations et donc les couts\n",
    "3. **Reproductibilite** : Des techniques structurees permettent des resultats coherents\n",
    "\n",
    "### Progression de ce notebook\n",
    "\n",
    "| Technique | Complexite | Cas d'usage |\n",
    "|-----------|------------|-------------|\n",
    "| Zero-shot | Simple | Questions generales, taches courantes |\n",
    "| Few-shot | Moyenne | Format specifique, style personnalise |\n",
    "| Chain-of-thought | Moyenne | Raisonnement, mathematiques, logique |\n",
    "| Self-refine | Avancee | Code, textes critiques, haute qualite |\n",
    "\n",
    "> **Documentation officielle** : [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: BATCH\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 2 : Configuration\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger la configuration depuis le fichier .env (dans le répertoire parent GenAI/)\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# On suppose que ton .env contient :\n",
    "# OPENAI_API_KEY=sk-xxxxxx\n",
    "# (ou autre variable si tu utilises Azure)\n",
    "#\n",
    "# Récupère la clé d'API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Clé API introuvable. Vérifie ton fichier .env.\")\n",
    "\n",
    "# Mode batch pour exécution non-interactive (Papermill, tests automatisés)\n",
    "# Détection automatique si exécution via Papermill ou si stdin non disponible\n",
    "def is_interactive():\n",
    "    \"\"\"Détecte si l'exécution est interactive (terminal) ou batch (Papermill)\"\"\"\n",
    "    try:\n",
    "        # Check if running in Papermill\n",
    "        import __main__\n",
    "        if hasattr(__main__, '__file__') and 'papermill' in str(getattr(__main__, '__file__', '')).lower():\n",
    "            return False\n",
    "        # Check if stdin is available\n",
    "        if not sys.stdin.isatty():\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "BATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\" or not is_interactive()\n",
    "print(f\"Mode: {'BATCH' if BATCH_MODE else 'INTERACTIF'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration de l'environnement\n",
    "\n",
    "Avant de commencer les expérimentations, nous devons configurer l'accès à l'API OpenAI et gérer les modes d'exécution.\n",
    "\n",
    "### Mode batch vs mode interactif\n",
    "\n",
    "Le notebook supporte deux modes d'exécution :\n",
    "\n",
    "- **Mode interactif** : Pour l'apprentissage, avec saisie utilisateur et expérimentation libre\n",
    "- **Mode batch** : Pour l'exécution automatisée (Papermill, tests, CI/CD), sans interaction\n",
    "\n",
    "La détection est automatique, mais vous pouvez forcer le mode batch via la variable d'environnement `BATCH_MODE=true` dans le fichier `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Initialisation du client OpenAI\n\nLe client OpenAI moderne (>=1.0.0) utilise une API orientée objet :\n\n```python\nclient = OpenAI(api_key=\"...\")\nresponse = client.chat.completions.create(...)\n```\n\n**Paramètres importants** :\n- `model` : Le modèle à utiliser (gpt-5-mini, o4-mini, etc.)\n- `max_tokens` : Longueur maximale de la réponse (évite les réponses trop longues et coûteuses)\n- `temperature` : Contrôle la créativité (0.0 = déterministe, 2.0 = très créatif)\n\nPar défaut, nous utilisons `gpt-5-mini` configuré dans le fichier `.env`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Zero-shot Prompting\n",
    "\n",
    "Le **zero-shot prompting** est la technique la plus directe : on pose une question sans fournir d'exemples préalables.\n",
    "\n",
    "### Quand utiliser Zero-shot ?\n",
    "\n",
    "- Questions générales ou conversationnelles\n",
    "- Tâches courantes bien comprises par le modèle (résumés, traductions simples)\n",
    "- Prototypage rapide\n",
    "- Budget tokens limité\n",
    "\n",
    "Testons avec une demande simple : générer des idées de recettes végétariennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": "# ============================\n# Cellule 3 : Client OpenAI\n# ============================\n\nimport openai\nfrom openai import OpenAI\n\n# Charger le modèle depuis .env ou utiliser gpt-5-mini par défaut\nDEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\nMODEL_NAME = DEFAULT_MODEL\n\n# Instanciation du client\nclient = OpenAI(\n    api_key=api_key,\n)\n\nprint(\"Client OpenAI initialisé avec succès !\")\nprint(f\"Modèle par défaut: {MODEL_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappel des différences entre Zero-shot, Few-shot, Chain-of-thought et Self-refine\n",
    "\n",
    "1. **Zero-shot Prompting**  \n",
    "   - Aucune instruction ou exemple préalable (à part la demande de l'utilisateur).  \n",
    "   - Simple et direct, mais parfois moins précis ou cohérent.\n",
    "\n",
    "2. **Few-shot Prompting**  \n",
    "   - Fournir quelques exemples “input → output” pour guider la réponse.  \n",
    "   - Permet de **spécifier le format**, le style, ou le contenu souhaité.  \n",
    "   - Améliore significativement la qualité des réponses sur des tâches complexes.\n",
    "\n",
    "3. **Chain-of-thought (CoT)**  \n",
    "   - On **incite** le modèle à détailler son raisonnement étape par étape.  \n",
    "   - Souvent utile pour des questions de logique, mathématiques, programmation ou raisonnement complexe.  \n",
    "   - Peut **augmenter** la cohérence et la justesse de la réponse (mais attention à ne pas divulguer ces “étapes” si elles sont confidentielles).\n",
    "\n",
    "4. **Self-refine**  \n",
    "   - Demander au modèle de s’auto-critiquer puis de proposer une réponse améliorée.  \n",
    "   - Mise en œuvre en plusieurs appels (réponse initiale, re-demande d’analyse, ré-énoncé final).  \n",
    "   - Intéressant pour du code, des textes longs, ou des situations nécessitant un contrôle qualité.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Few-shot Prompting\n",
    "\n",
    "Le **few-shot prompting** consiste à fournir 2-3 exemples de la tâche souhaitée avant la vraie question.\n",
    "\n",
    "### Mécanisme d'apprentissage en contexte\n",
    "\n",
    "Le modèle :\n",
    "1. Analyse les exemples fournis\n",
    "2. Détecte le **pattern** (format, style, structure)\n",
    "3. Applique ce pattern à la nouvelle question\n",
    "\n",
    "C'est ce qu'on appelle **l'apprentissage en contexte** (in-context learning) : le modèle s'adapte sans modifier ses poids.\n",
    "\n",
    "### Premier exemple : Rédaction d'emails professionnels\n",
    "\n",
    "Nous allons guider le modèle à produire un email avec un format et un ton spécifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zero-shot Prompt ===\n",
      "Prompt: Donne-moi 3 idées de recettes végétariennes à base de tomates.\n",
      "\n",
      "Réponse du modèle :\n",
      "\n",
      "Voici trois idées de recettes végétariennes à base de tomates :\n",
      "\n",
      "1. **Tarte à la tomate et à la moutarde** :\n",
      "   - **Ingrédients** : Pâte feuilletée, tomates mûres, moutarde à l'ancienne, fromage râpé (comme le gruyère ou le comté), herbes de Provence, sel, poivre.\n",
      "   - **Préparation** : Préchauffez le four à 200°C. Étalez la pâte feuilletée dans un moule à tarte. Badigeonnez le fond de tarte avec de la moutarde. Disposez les tranches de tomates uniformément sur la pâte. Saupoudrez de fromage râpé, d'herbes de Provence, de sel et de poivre. Enfournez pendant 25 à 30 minutes jusqu'à ce que la tarte soit dorée.\n",
      "\n",
      "2. **Salade de tomates, mozzarella, et basilic (Caprese)** :\n",
      "   - **Ingrédients** : Tomates mûres, mozzarella de bufflonne, feuilles de basilic frais, huile d'olive extra-vierge, vinaigre balsamique, sel, poivre.\n",
      "   - **Préparation** : Coupez les tomates et la mozzarella en tranches. Alternez les tranches de tomates et de mozzarella sur une assiette. Ajoutez des feuilles de basilic entre les tranches. Arrosez d'huile d'olive et de quelques gouttes de vinaigre balsamique. Assaisonnez avec du sel et du poivre selon votre goût.\n",
      "\n",
      "3. **Pasta alla Puttanesca végétarienne** :\n",
      "   - **Ingrédients** : Spaghetti, tomates concassées, olives noires, câpres, ail, piment rouge (facultatif), persil frais, huile d'olive, sel, poivre.\n",
      "   - **Préparation** : Faites cuire les spaghetti selon les instructions du paquet\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Zero-shot\n",
    "# ============================\n",
    "\n",
    "prompt_1 = \"Donne-moi 3 idées de recettes végétariennes à base de tomates.\"\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt_1}\n",
    "    ],\n",
    "    # Contrôle du style\n",
    "    max_tokens=400,\n",
    "    temperature=0.7  # plus la température est haute, plus c'est créatif\n",
    ")\n",
    "\n",
    "print(\"=== Zero-shot Prompt ===\")\n",
    "print(f\"Prompt: {prompt_1}\\n\")\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_1.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultat du Zero-shot prompting\n",
    "\n",
    "Le zero-shot prompting est la technique la plus simple : **aucun exemple préalable**, juste une instruction directe.\n",
    "\n",
    "**Avantages** :\n",
    "- Rapide à mettre en œuvre\n",
    "- Fonctionne bien pour des tâches courantes (résumés, traductions, questions générales)\n",
    "- Économique en tokens\n",
    "\n",
    "**Limites** :\n",
    "- Moins précis sur des tâches complexes ou spécialisées\n",
    "- Le format de sortie peut être imprévisible\n",
    "- Nécessite des prompts très clairs et bien formulés\n",
    "\n",
    "Dans cet exemple, le modèle génère 3 recettes végétariennes à base de tomates sans aucun exemple préalable. La qualité dépend fortement de la clarté du prompt et de la capacité du modèle à comprendre le domaine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 : Chain-of-thought (CoT)\n",
    "\n",
    "Le **Chain-of-thought** demande explicitement au modèle de détailler son raisonnement étape par étape.\n",
    "\n",
    "### Pourquoi le CoT fonctionne ?\n",
    "\n",
    "Les modèles de langage sont entraînés sur des textes où les raisonnements sont explicités. En demandant les étapes intermédiaires, on active ce pattern et on améliore la précision.\n",
    "\n",
    "### Applications du CoT\n",
    "\n",
    "| Domaine | Exemple |\n",
    "|---------|---------|\n",
    "| Mathématiques | Résolution d'équations, problèmes de mots |\n",
    "| Logique | Syllogismes, déductions |\n",
    "| Programmation | Debugging, conception d'algorithmes |\n",
    "| Analyse | Cas juridiques, diagnostics médicaux |\n",
    "\n",
    "### Exemple : Problème arithmétique simple\n",
    "\n",
    "Testons avec un calcul impliquant plusieurs étapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 4 : Self-refine (Auto-amélioration)\n",
    "\n",
    "Le **Self-refine** exploite la capacité du modèle à critiquer et améliorer ses propres productions.\n",
    "\n",
    "### Processus en deux étapes\n",
    "\n",
    "1. **Génération initiale** : Produire une première version (potentiellement bugguée ou imparfaite)\n",
    "2. **Critique et correction** : Analyser la première version, identifier les problèmes, proposer une amélioration\n",
    "\n",
    "### Avantages du Self-refine\n",
    "\n",
    "- **Qualité supérieure** : Deux passes donnent généralement de meilleurs résultats\n",
    "- **Détection de bugs** : Le modèle peut repérer ses propres erreurs\n",
    "- **Amélioration itérative** : Peut être répété plusieurs fois si nécessaire\n",
    "\n",
    "### Compromis\n",
    "\n",
    "- **Coût** : Double les tokens consommés (deux appels API)\n",
    "- **Temps** : Latence multipliée\n",
    "- **À utiliser pour** : Code critique, documents importants, tâches complexes\n",
    "\n",
    "### Exemple : Code Python avec bug volontaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exemple Few-shot (e-mail professionnel) ===\n",
      "Sujet: Changement de planning et invitation à une réunion de suivi\n",
      "\n",
      "Bonjour [Nom du Collaborateur],\n",
      "\n",
      "Je souhaite vous informer d'un changement récent dans notre planning initial. Afin de discuter des ajustements nécessaires et de nous assurer que tout le monde est aligné, je vous invite à participer à une réunion de suivi.\n",
      "\n",
      "La réunion aura lieu le [Date] à [Heure], dans [Lieu/Plateforme pour une réunion en ligne]. Nous aborderons les points suivants : [énumérer les sujets principaux]. Votre présence et vos contributions seront précieuses pour garantir le succès de ce projet.\n",
      "\n",
      "Merci de bien vouloir confirmer votre disponibilité à cette date. N'hésitez pas à me contacter si vous avez des questions ou des préoccupations avant la réunion.\n",
      "\n",
      "Cordialement,\n",
      "\n",
      "[Votre Nom]  \n",
      "[Votre Position]  \n",
      "[Votre Entreprise]  \n",
      "[Vos Coordonnées]\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Few-shot supplémentaire\n",
    "# ============================\n",
    "\n",
    "few_shot_prompt_2 = \"\"\"\n",
    "Tu es un assistant spécialisé en rédaction d'e-mails professionnels.\n",
    "Voici quelques exemples de style :\n",
    "\n",
    "Exemple 1:\n",
    "Q: Rédige un e-mail pour informer un client d'un retard de livraison\n",
    "A: \n",
    "Sujet: Information concernant le retard de votre livraison\n",
    "\n",
    "Bonjour [Nom du Client],\n",
    "\n",
    "Nous tenions à vous informer que votre commande #1234 a pris du retard...\n",
    "[...suite du mail...]\n",
    "\n",
    "Exemple 2:\n",
    "Q: Envoie un e-mail de remerciement pour un entretien d'embauche\n",
    "A:\n",
    "Sujet: Remerciements suite à notre entretien\n",
    "\n",
    "Bonjour [Nom du Contact],\n",
    "\n",
    "Je tiens à vous remercier pour le temps que vous m'avez accordé...\n",
    "[...suite du mail...]\n",
    "\n",
    "Maintenant, voici ma demande:\n",
    "\n",
    "Q: Écris un e-mail pour informer un collaborateur d'un changement de planning et l'inviter à une réunion de suivi.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response_few_shot_2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,  # ex. \"gpt-4o-mini\"\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt_2}\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "print(\"=== Exemple Few-shot (e-mail professionnel) ===\")\n",
    "print(response_few_shot_2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultat du Few-shot : Format cohérent\n",
    "\n",
    "Le modèle a reproduit fidèlement la structure des exemples :\n",
    "\n",
    "1. **Sujet** : Clair et professionnel\n",
    "2. **Salutation** : Formule de politesse appropriée\n",
    "3. **Corps** : Structure logique (contexte → action → conclusion)\n",
    "4. **Signature** : Complète avec coordonnées\n",
    "\n",
    "**Observation clé** : Sans les exemples, le modèle aurait pu générer un email plus informel ou moins structuré. Le few-shot garantit la cohérence du format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du Few-shot prompting\n",
    "\n",
    "Le few-shot prompting apporte une **amélioration significative** par rapport au zero-shot :\n",
    "\n",
    "**Mécanisme** :\n",
    "1. On fournit 2-3 exemples de la tâche souhaitée (paires question/réponse)\n",
    "2. Le modèle apprend le **pattern** et le **format** attendu\n",
    "3. Il applique ensuite ce pattern à la nouvelle question\n",
    "\n",
    "**Avantages observables** :\n",
    "- **Format cohérent** : Le modèle reproduit la structure des exemples (sujet, salutation, corps, signature)\n",
    "- **Ton approprié** : Le style professionnel est maintenu\n",
    "- **Contenu pertinent** : La réponse suit les conventions des exemples fournis\n",
    "\n",
    "**Quand utiliser Few-shot ?**\n",
    "- Tâches avec un format spécifique (emails, rapports, analyses structurées)\n",
    "- Cas où le zero-shot donne des résultats trop variables\n",
    "- Besoin de cohérence stylistique\n",
    "\n",
    "**Compromis** : Chaque exemple consomme des tokens supplémentaires, donc à utiliser avec modération pour des contextes très longs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 5 : Interactions conversationnelles\n",
    "\n",
    "Au-delà des techniques de prompting, la structure des conversations avec le modèle est cruciale pour des applications interactives.\n",
    "\n",
    "### Prompt simple sans mémoire\n",
    "\n",
    "La première cellule interactive montre un échange **stateless** (sans état) : chaque prompt est indépendant, le modèle n'a aucune mémoire des interactions précédentes.\n",
    "\n",
    "**Cas d'usage** :\n",
    "- Tests rapides d'un modèle\n",
    "- Questions indépendantes\n",
    "- Prototypage de prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation avec mémoire\n",
    "\n",
    "La cellule suivante introduit un mécanisme de **mémoire de conversation** essentiel pour les chatbots et assistants.\n",
    "\n",
    "**Principe** :\n",
    "1. Accumuler les messages dans une liste `current_messages`\n",
    "2. À chaque tour, envoyer **tout l'historique** + le nouveau message\n",
    "3. Ajouter la réponse du modèle à l'historique\n",
    "\n",
    "**Structure des messages** :\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"Je m'appelle Alice\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Enchanté Alice !\"},\n",
    "    {\"role\": \"user\", \"content\": \"Comment je m'appelle ?\"},\n",
    "    # Le modèle peut répondre \"Vous vous appelez Alice\" grâce à l'historique\n",
    "]\n",
    "```\n",
    "\n",
    "**Attention** : L'historique consomme des tokens à chaque appel. Pour de longues conversations, il faut :\n",
    "- Tronquer les messages anciens\n",
    "- Résumer l'historique périodiquement\n",
    "- Utiliser des techniques de compression (embeddings, résumés automatiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 6 : Modèles de raisonnement (2025)\n",
    "\n",
    "Les **modèles de raisonnement** (reasoning models) comme `o4-mini` et `gpt-5-thinking` représentent une évolution majeure des LLMs.\n",
    "\n",
    "### Différence fondamentale\n",
    "\n",
    "Les modèles chat génèrent token par token en temps réel. Les modèles de raisonnement :\n",
    "1. **Réfléchissent** en interne avant de répondre (pensées non visibles)\n",
    "2. **Explorent** plusieurs pistes de réflexion\n",
    "3. **Valident** leur raisonnement avant de produire la réponse finale\n",
    "\n",
    "### Paramètre clé : reasoning_effort\n",
    "\n",
    "Remplace le paramètre `temperature` pour les modèles de raisonnement :\n",
    "\n",
    "| Valeur | Durée | Cas d'usage |\n",
    "|--------|-------|-------------|\n",
    "| `low` | Rapide | Questions simples, prototypage |\n",
    "| `medium` | Modéré | Problèmes standards |\n",
    "| `high` | Lent | Problèmes complexes, mathématiques avancées |\n",
    "\n",
    "### Role \"developer\" obligatoire\n",
    "\n",
    "Les modèles de raisonnement n'utilisent plus le role `system`. Le role `developer` configure le comportement global, et peut activer/désactiver le formatage de la pensée interne.\n",
    "\n",
    "### Comparaison pratique\n",
    "\n",
    "Testons le même problème avec un modèle chat (gpt-4o-mini) et un modèle de raisonnement (o4-mini)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": "# ============================\n# Cellule 7 : Chain-of-thought\n# ============================\n\ncot_prompt = \"\"\"\nAlice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\nBob lui rend ensuite 1 pomme.\nCombien de pommes Alice a-t-elle à la fin ?\nDonne directement la réponse sans étape intermédiaire.\n\"\"\"\n\n# Explique ton raisonnement étape par étape, puis donne la réponse finale.\nresponse_3 = client.chat.completions.create(\n    model=\"gpt-5-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": cot_prompt}\n    ],\n    max_tokens=200,\n    temperature=0.2  # on réduit la température pour moins de fantaisie\n)\n\nprint(\"=== Chain-of-thought Prompt ===\")\nprint(\"Réponse du modèle (avec raisonnement) :\\n\")\nprint(response_3.choices[0].message.content)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du résultat CoT\n",
    "\n",
    "**Observation** : Le prompt demandait une réponse directe SANS étapes intermédiaires, et le modèle a bien obéi en donnant simplement \"3 pommes\".\n",
    "\n",
    "**Expérimentation recommandée** : Modifiez le prompt pour demander explicitement le raisonnement :\n",
    "\n",
    "```python\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "Explique ton raisonnement étape par étape avant de donner la réponse finale.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Vous devriez alors obtenir :\n",
    "- Étape 1 : 5 - 2 = 3\n",
    "- Étape 2 : 3 - 1 = 2\n",
    "- Étape 3 : 2 + 1 = 3\n",
    "- **Réponse : 3 pommes**\n",
    "\n",
    "### Bonnes pratiques CoT\n",
    "\n",
    "1. **Temperature basse** (0.0-0.3) pour les calculs et la logique\n",
    "2. **Prompt explicite** : \"Explique ton raisonnement étape par étape\"\n",
    "3. **Vérification** : Le raisonnement explicite permet de détecter les erreurs\n",
    "4. **Masquage optionnel** : Pour les applications de production, on peut demander au modèle de ne pas révéler son raisonnement à l'utilisateur final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation du Chain-of-thought\n",
    "\n",
    "Le Chain-of-thought (CoT) est particulièrement efficace pour les **problèmes de raisonnement** :\n",
    "\n",
    "**Analyse du résultat** :\n",
    "Le modèle devrait avoir détaillé :\n",
    "1. État initial : Alice a 5 pommes\n",
    "2. Étape 1 : Elle en jette 2 → 5 - 2 = 3 pommes\n",
    "3. Étape 2 : Elle en donne 1 à Bob → 3 - 1 = 2 pommes\n",
    "4. Étape 3 : Bob lui rend 1 pomme → 2 + 1 = 3 pommes\n",
    "5. **Réponse finale : 3 pommes**\n",
    "\n",
    "**Pourquoi c'est important ?**\n",
    "- **Transparence** : On peut vérifier le raisonnement étape par étape\n",
    "- **Détection d'erreurs** : Si le résultat est faux, on peut identifier où le modèle s'est trompé\n",
    "- **Confiance** : Le raisonnement explicite augmente la crédibilité\n",
    "- **Debugging** : Facilite la correction du prompt si nécessaire\n",
    "\n",
    "**Applications** :\n",
    "- Calculs mathématiques\n",
    "- Raisonnement logique\n",
    "- Résolution de problèmes complexes\n",
    "- Analyse de cas juridiques ou médicaux\n",
    "\n",
    "**Note** : Temperature=0.2 garantit un raisonnement cohérent et reproductible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (1) : Code buggy ===\n",
      "\n",
      "Voici une fonction Python pour calculer la somme des éléments d'une liste, avec un bug volontaire inclus :\n",
      "\n",
      "```python\n",
      "def somme_liste(liste):\n",
      "    total = 0\n",
      "    for i in range(len(liste)):\n",
      "        total += liste[i]\n",
      "    # Bug volontaire : ici, on décrémente le total au lieu de l'incrémenter\n",
      "    total -= liste[0]\n",
      "    return total\n",
      "\n",
      "# Exemple d'utilisation\n",
      "ma_liste = [1, 2, 3, 4]\n",
      "print(somme_liste(ma_liste))  # La sortie sera incorrecte à cause du bug\n",
      "```\n",
      "\n",
      "Le bug volontaire ici est la décrémentation de `total` par le premier élément de la liste après avoir accumulé la somme de tous les éléments, ce qui conduit à un résultat incorrect.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8a : Self-refine Step 1\n",
    "# ============================\n",
    "\n",
    "prompt_sr1 = \"\"\"\n",
    "Ecris une courte fonction Python pour calculer la somme d'une liste. \n",
    "Ajoute un bug volontaire dans le code. \n",
    "\"\"\"\n",
    "\n",
    "response_sr1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr1}],\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "buggy_code = response_sr1.choices[0].message.content\n",
    "\n",
    "print(\"=== Self-refine (1) : Code buggy ===\\n\")\n",
    "print(buggy_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8b. Self-critique et amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (2) : Correction ===\n",
      "\n",
      "Le code donné contient un bug dans la fonction `somme_liste`. Le problème se situe à la ligne où le total est décrémenté par le premier élément de la liste après avoir calculé la somme de tous les éléments. Cela entraîne un résultat incorrect, car la somme totale est réduite de manière inappropriée.\n",
      "\n",
      "### Analyse du bug\n",
      "\n",
      "Dans la fonction `somme_liste`, après avoir correctement additionné tous les éléments de la liste, le code soustrait le premier élément de la liste du total. Cela signifie que le résultat final sera toujours inférieur à la somme réelle des éléments de la liste par la valeur du premier élément.\n",
      "\n",
      "### Correction\n",
      "\n",
      "Pour corriger ce bug, il suffit de supprimer la ligne qui décrémente le total par le premier élément de la liste. Voici la version corrigée du code :\n",
      "\n",
      "```python\n",
      "def somme_liste(liste):\n",
      "    total = 0\n",
      "    for i in range(len(liste)):\n",
      "        total += liste[i]\n",
      "    return total\n",
      "\n",
      "# Exemple d'utilisation\n",
      "ma_liste = [1, 2, 3, 4]\n",
      "print(somme_liste(ma_liste))  # La sortie sera maintenant correcte : 10\n",
      "```\n",
      "\n",
      "### Explication de la correction\n",
      "\n",
      "- **Suppression de la ligne incorrecte** : En supprimant la ligne `total -= liste[0]`, nous évitons de soustraire le premier élément de la liste après avoir calculé la somme totale. Cela garantit que le total retourné est bien la somme de tous les éléments de la liste.\n",
      "\n",
      "### Amélioration du code\n",
      "\n",
      "On peut également améliorer le code en utilisant une approche plus pythonique. Au lieu de parcourir la liste avec un index, on peut utiliser une boucle `for` directe sur les éléments de la liste. Voici la version améliorée :\n",
      "\n",
      "```python\n",
      "def somme_liste(liste):\n",
      "    total = 0\n",
      "    for element in liste:\n",
      "        total += element\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8b : Self-refine Step 2\n",
    "# ============================\n",
    "\n",
    "prompt_sr2 = f\"\"\"\n",
    "Voici un code Python qui contient un bug:\n",
    "\n",
    "{buggy_code}\n",
    "\n",
    "Peux-tu l'analyser, détecter le bug, proposer un correctif et une version améliorée du code ? \n",
    "Explique la correction.\n",
    "\"\"\"\n",
    "\n",
    "response_sr2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr2}],\n",
    "    max_tokens=400,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine (2) : Correction ===\\n\")\n",
    "print(response_sr2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultat du Self-refine : Amélioration itérative\n",
    "\n",
    "Le modèle a :\n",
    "1. **Identifié le bug** : L'ajout de `+ 1` dans le `return` fausse le résultat\n",
    "2. **Proposé une correction** : Retirer le `+ 1`\n",
    "3. **Suggéré une amélioration** : Utiliser la fonction native `sum()` pour plus de simplicité\n",
    "\n",
    "**Enseignement** : Le modèle possède une capacité de **méta-cognition** - il peut raisonner sur ses propres productions et les améliorer.\n",
    "\n",
    "### Variante avancée : Self-refine avec role \"developer\"\n",
    "\n",
    "Au lieu d'un processus en deux appels API, on peut utiliser le rôle `developer` pour configurer le comportement d'auto-amélioration directement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilan du Self-refine\n",
    "\n",
    "Le processus Self-refine en deux étapes démontre la capacité du modèle à s'**auto-améliorer** :\n",
    "\n",
    "**Première étape** : Génération intentionnellement bugguée\n",
    "- Le modèle crée du code avec un bug volontaire (par exemple, division par zéro, mauvaise initialisation, etc.)\n",
    "\n",
    "**Deuxième étape** : Critique et correction\n",
    "- Le modèle analyse son propre code\n",
    "- Identifie le bug\n",
    "- Propose une version corrigée\n",
    "- Explique la nature du problème\n",
    "\n",
    "**Enseignements** :\n",
    "1. **Méta-cognition** : Le LLM peut raisonner sur ses propres productions\n",
    "2. **Amélioration itérative** : Chaque passe peut affiner la qualité\n",
    "3. **Détection de bugs** : Utile pour du code review automatisé\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Génération de code robuste (plusieurs passes de correction)\n",
    "- Rédaction de documents (brouillon → révision → version finale)\n",
    "- Traduction (première traduction → révision → amélioration)\n",
    "\n",
    "**Limite** : Chaque itération consomme des tokens et du temps. À utiliser pour des tâches critiques nécessitant haute qualité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine avec 'developer role' ===\n",
      "Voici une fonction Python qui calcule la factorielle d'un nombre entier :\n",
      "\n",
      "```python\n",
      "def factorielle(n):\n",
      "    if not isinstance(n, int) or n < 0:\n",
      "        raise ValueError(\"Le nombre doit être un entier non négatif.\")\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    resultat = 1\n",
      "    for i in range(2, n + 1):\n",
      "        resultat *= i\n",
      "    return resultat\n",
      "```\n",
      "\n",
      "### Vérification et améliorations :\n",
      "\n",
      "1. **Validation des entrées** : La fonction vérifie si l'entrée est un entier non négatif. Si ce n'est pas le cas, elle lève une exception `ValueError`.\n",
      "\n",
      "2. **Cas de base** : La factorielle de 0 et 1 est bien gérée en retournant 1.\n",
      "\n",
      "3. **Boucle de calcul** : La boucle commence à 2, ce qui est correct, car multiplier par 1 est redondant.\n",
      "\n",
      "4. **Performance** : Pour les très grands nombres, cette approche pourrait être améliorée en utilisant des techniques avancées comme la récursion avec mémorisation ou des bibliothèques spécialisées, mais pour des usages courants, cette implémentation est suffisante.\n",
      "\n",
      "La fonction semble correcte et efficace pour la plupart des cas d'utilisation standard.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Self-refine avec developer role\n",
    "# ============================\n",
    "\n",
    "messages_sr = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": (\n",
    "            \"You are a self-improving coding assistant. Whenever you provide code, \"\n",
    "            \"you will automatically search for potential bugs or improvements \"\n",
    "            \"and refine your output.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Écris une fonction Python qui calcule la factorielle d'un nombre entier. \"\n",
    "            \"Ensuite, relis-toi et corrige d'éventuels bugs.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "response_self_refine = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages_sr,\n",
    "    max_tokens=300,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine avec 'developer role' ===\")\n",
    "print(response_self_refine.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation du Self-refine avec role \"developer\"\n",
    "\n",
    "Le rôle `developer` (introduit avec les modèles de raisonnement) permet de définir le comportement global du modèle, contrairement au rôle `system` qui était utilisé auparavant.\n",
    "\n",
    "**Différence clé** :\n",
    "- **Role \"system\"** (ancienne API) : Instructions générales, parfois ignorées par le modèle\n",
    "- **Role \"developer\"** (nouvelle API) : Instructions prioritaires, mieux respectées\n",
    "\n",
    "Dans cet exemple :\n",
    "- Le modèle génère la fonction factorielle\n",
    "- Il vérifie automatiquement les bugs potentiels\n",
    "- Il propose des améliorations (gestion d'erreurs, performance, alternatives)\n",
    "\n",
    "**Observation** : La fonction générée inclut directement :\n",
    "- Vérification du type (`isinstance`)\n",
    "- Gestion des nombres négatifs\n",
    "- Version alternative avec `math.factorial`\n",
    "\n",
    "C'est plus efficace que le Self-refine en deux passes, mais nécessite un modèle récent supportant le rôle `developer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] Prompt: Bonjour!\n",
      "Réponse: Bonjour! Comment puis-je vous aider aujourd'hui?\n",
      "\n",
      "[BATCH] Prompt: Quelle est la capitale de la France?\n",
      "Réponse: La capitale de la France est Paris.\n",
      "\n",
      "Mode batch terminé.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif\n",
    "# ============================\n",
    "\n",
    "# En mode batch (BATCH_MODE=true dans .env), cette cellule utilise des exemples prédéfinis\n",
    "# En mode interactif, elle permet de tester le modèle en boucle\n",
    "\n",
    "if BATCH_MODE:\n",
    "    # Mode batch: exécuter des exemples simples\n",
    "    test_prompts = [\"Bonjour!\", \"Quelle est la capitale de la France?\"]\n",
    "    for prompt in test_prompts:\n",
    "        print(f\"[BATCH] Prompt: {prompt}\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            max_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        print(f\"Réponse: {resp.choices[0].message.content}\\n\")\n",
    "    print(\"Mode batch terminé.\")\n",
    "else:\n",
    "    while True:\n",
    "        user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Fin de l'interaction.\")\n",
    "            break\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== Réponse du modèle ===\")\n",
    "        print(resp.choices[0].message.content)\n",
    "        print(\"---------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation des tests interactifs\n",
    "\n",
    "Cette cellule démontre le **mode batch** pour tester le modèle sans interaction manuelle.\n",
    "\n",
    "**Résultats observés** :\n",
    "- **Prompt simple** : \"Bonjour!\" → Réponse polie et ouverture de dialogue\n",
    "- **Question factuelle** : \"Quelle est la capitale de la France?\" → Réponse directe et précise\n",
    "\n",
    "**Points clés** :\n",
    "1. **Température 0.7** : Équilibre entre cohérence et créativité\n",
    "2. **Max tokens 200** : Limite les réponses pour réduire les coûts\n",
    "3. **Messages stateless** : Chaque prompt est indépendant, pas de mémoire entre appels\n",
    "\n",
    "**Différence avec la cellule suivante** : La prochaine cellule introduit la **mémoire de conversation** (accumulation de l'historique), essentielle pour les chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] User: Je m'appelle Alice\n",
      "Assistant: Bonjour, Alice ! Comment puis-je vous aider aujourd'hui ?\n",
      "\n",
      "[BATCH] User: Comment je m'appelle?\n",
      "Assistant: Vous avez dit que vous vous appelez Alice.\n",
      "\n",
      "Mode batch (mémoire) terminé.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif avec mémoire de chat\n",
    "# ============================\n",
    "\n",
    "# En mode batch, cette cellule utilise un dialogue prédéfini\n",
    "# En mode interactif, elle permet un échange avec mémoire\n",
    "\n",
    "if BATCH_MODE:\n",
    "    # Mode batch: simuler une conversation avec mémoire\n",
    "    batch_conversation = [\n",
    "        \"Je m'appelle Alice\",\n",
    "        \"Comment je m'appelle?\",\n",
    "    ]\n",
    "    current_messages = []\n",
    "\n",
    "    for user_input in batch_conversation:\n",
    "        print(f\"[BATCH] User: {user_input}\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=current_messages + [{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_tokens=200,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        assistant_message = resp.choices[0].message.content\n",
    "        print(f\"Assistant: {assistant_message}\\n\")\n",
    "        current_messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "        current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})\n",
    "\n",
    "    print(\"Mode batch (mémoire) terminé.\")\n",
    "else:\n",
    "    user_input = \"\"\n",
    "    current_messages = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Fin de l'interaction.\")\n",
    "            break\n",
    "        print(\"\\n=== message de l'utilisateur ===\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages = current_messages + [{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_tokens=200,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== Réponse du modèle ===\")\n",
    "        assistant_message = resp.choices[0].message.content\n",
    "        print(assistant_message)\n",
    "        print(\"---------------------------------------------------\\n\")\n",
    "        current_messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "        current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation de la conversation avec mémoire\n",
    "\n",
    "Cette cellule démontre la **mémoire conversationnelle** - le modèle se souvient du contexte précédent.\n",
    "\n",
    "**Résultats observés** :\n",
    "1. **Tour 1** : \"Je m'appelle Alice\" → Le modèle enregistre cette information dans `current_messages`\n",
    "2. **Tour 2** : \"Comment je m'appelle?\" → Le modèle répond \"Vous vous appelez Alice\" grâce à l'historique\n",
    "\n",
    "**Mécanisme technique** :\n",
    "```python\n",
    "current_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Je m'appelle Alice\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Bonjour, Alice...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Comment je m'appelle?\"},\n",
    "    # Le modèle peut répondre grâce à l'historique complet\n",
    "]\n",
    "```\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Chatbots conversationnels\n",
    "- Assistants personnels\n",
    "- Support client automatisé\n",
    "\n",
    "**Gestion de l'historique** :\n",
    "- **Court terme** : Garder tout l'historique (comme ici)\n",
    "- **Long terme** : Tronquer les messages anciens ou résumer périodiquement\n",
    "- **Tokens** : Attention au coût - chaque tour envoie TOUT l'historique à l'API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting pour Modèles de Raisonnement (2025)\n",
    "\n",
    "Les modèles de raisonnement (o4-mini, gpt-5-thinking) représentent une évolution majeure. Contrairement aux modèles de chat, ils prennent le temps de \"réfléchir\" avant de répondre.\n",
    "\n",
    "## Différences clés avec les modèles chat\n",
    "\n",
    "| Aspect | Modèles Chat (gpt-4o) | Modèles Raisonnement (o4-mini) |\n",
    "|--------|----------------------|-------------------------------|\n",
    "| Temps de réponse | Rapide | Plus lent (réflexion) |\n",
    "| Prompts | Détaillés, structurés | **Simples et directs** |\n",
    "| Chain-of-thought | Demandé explicitement | **Intégré nativement** |\n",
    "| Messages | system, user, assistant | **developer**, user, assistant |\n",
    "| Paramètre spécial | temperature | **reasoning_effort** |\n",
    "\n",
    "## Règle d'or : Simplifier les prompts\n",
    "\n",
    "**Pour les modèles de raisonnement, des prompts simples fonctionnent mieux !**\n",
    "\n",
    "Les modèles raisonnants sont capables de :\n",
    "- Comprendre l'intention sans instructions détaillées\n",
    "- Gérer les ambiguïtés intelligemment\n",
    "- Demander des clarifications si nécessaire\n",
    "\n",
    "**Éviter** : \"Analyse ce problème en détaillant chaque étape de ton raisonnement...\"\n",
    "**Préférer** : \"Résous ce problème.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================\n# Cellule : Comparaison Chat vs Reasoning\n# ============================\n\nimport time\n\nprobleme_complexe = \"\"\"\nUn fermier veut traverser une rivière avec un loup, une chèvre et un chou.\nSon bateau ne peut transporter que lui et un objet à la fois.\nSi le loup est laissé seul avec la chèvre, il la mange.\nSi la chèvre est laissée seule avec le chou, elle le mange.\nComment le fermier peut-il tout transporter de l'autre côté?\n\"\"\"\n\n# Test avec gpt-5-mini (chat model)\nprint(\"=== gpt-5-mini (Chat Model) ===\")\nstart = time.time()\nresponse_chat = client.chat.completions.create(\n    model=\"gpt-5-mini\",\n    messages=[{\"role\": \"user\", \"content\": probleme_complexe}],\n    max_tokens=500,\n    temperature=0.2\n)\nprint(f\"Temps: {time.time() - start:.2f}s\")\nprint(response_chat.choices[0].message.content[:400] + \"...\")\n\n# Test avec o4-mini (reasoning model) - si disponible\nprint(\"\\n=== o4-mini (Reasoning Model) ===\")\ntry:\n    start = time.time()\n    response_reason = client.chat.completions.create(\n        model=\"o4-mini\",\n        messages=[\n            {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n            {\"role\": \"user\", \"content\": probleme_complexe}\n        ],\n        reasoning_effort=\"medium\"\n    )\n    print(f\"Temps: {time.time() - start:.2f}s\")\n    print(response_reason.choices[0].message.content[:400] + \"...\")\nexcept Exception as e:\n    print(f\"o4-mini non disponible: {type(e).__name__}\")\n    print(\"Les modèles de raisonnement nécessitent un accès spécifique.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats : Chat vs Reasoning\n",
    "\n",
    "**Observations attendues** :\n",
    "\n",
    "1. **Temps de réponse** :\n",
    "   - Chat (gpt-4o-mini) : ~2-5 secondes\n",
    "   - Reasoning (o4-mini) : ~10-30 secondes (réflexion interne)\n",
    "\n",
    "2. **Qualité de la solution** :\n",
    "   - Chat : Solution généralement correcte, mais peut manquer des optimisations\n",
    "   - Reasoning : Solution optimale, avec explication détaillée\n",
    "\n",
    "3. **Robustesse** :\n",
    "   - Chat : Peut parfois donner des solutions incorrectes sur des variantes complexes\n",
    "   - Reasoning : Vérifie sa solution avant de répondre, taux d'erreur plus faible\n",
    "\n",
    "### Quand utiliser les modèles de raisonnement ?\n",
    "\n",
    "**Préférer Chat** (gpt-4o, gpt-4o-mini) :\n",
    "- Applications temps réel (chatbots)\n",
    "- Tâches simples et courantes\n",
    "- Budget temps/coût limité\n",
    "\n",
    "**Préférer Reasoning** (o4-mini, gpt-5-thinking) :\n",
    "- Mathématiques avancées\n",
    "- Problèmes de logique complexes\n",
    "- Code critique nécessitant validation\n",
    "- Analyse approfondie\n",
    "\n",
    "**Note** : Les modèles de raisonnement sont plus coûteux en tokens et en temps, mais offrent une qualité supérieure pour les tâches complexes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}