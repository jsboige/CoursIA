{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a3f26e6",
   "metadata": {
    "papermill": {
     "duration": 0.003858,
     "end_time": "2026-02-25T23:00:10.847945",
     "exception": false,
     "start_time": "2026-02-25T23:00:10.844087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prompt Engineering : Advanced Prompting avec OpenAI\n",
    "\n",
    "Dans ce notebook, nous allons tester différentes techniques avancées de **prompt engineering**:\n",
    "- **Zero-shot prompting**\n",
    "- **Few-shot prompting**\n",
    "- **Chain-of-thought** (CoT)\n",
    "- **Self-refine** (ou auto-amélioration)\n",
    "\n",
    "Nous utiliserons la **nouvelle API** de la bibliothèque `openai` (>=1.0.0) via la classe `OpenAI` et ses méthodes de chat (`client.chat.completions.create`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4b75c",
   "metadata": {
    "papermill": {
     "duration": 0.004337,
     "end_time": "2026-02-25T23:00:10.855928",
     "exception": false,
     "start_time": "2026-02-25T23:00:10.851591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Prompt Engineering : Techniques Avancées\n",
    "\n",
    "**Navigation** : [<< Precedent](1_OpenAI_Intro.ipynb) | [Index](../../README.md) | [Suivant >>](3_Structured_Outputs.ipynb)\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Maîtriser les techniques de zero-shot et few-shot prompting\n",
    "2. Comprendre et appliquer le chain-of-thought (CoT)\n",
    "3. Implémenter le self-refine pour l'amélioration itérative\n",
    "4. Distinguer modèles chat et modèles de raisonnement\n",
    "\n",
    "### Prerequis\n",
    "- Notebook 1 (Introduction a l'IA generative)\n",
    "- Python 3.10+\n",
    "- Cle API OpenAI configuree\n",
    "\n",
    "### Duree estimee : 60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "# Prompt Engineering : Advanced Prompting avec OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e31e04",
   "metadata": {
    "papermill": {
     "duration": 0.003766,
     "end_time": "2026-02-25T23:00:10.864742",
     "exception": false,
     "start_time": "2026-02-25T23:00:10.860976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Installation des dépendances\n",
    "\n",
    "Avant de commencer, nous devons installer les bibliothèques Python nécessaires.\n",
    "\n",
    "**Packages requis** :\n",
    "- **openai** : Bibliothèque officielle pour interagir avec l'API OpenAI (>=1.0.0)\n",
    "- **tiktoken** : Encodeur de tokens pour compter et gérer les tokens GPT\n",
    "- **python-dotenv** : Gestion sécurisée des clés API via fichiers .env\n",
    "\n",
    "> **Note de sécurité** : Ne jamais inclure vos clés API directement dans le code. Toujours utiliser un fichier  exclu du contrôle de version (.gitignore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "331404b4",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:10.875464Z",
     "iopub.status.busy": "2026-02-25T23:00:10.875230Z",
     "iopub.status.idle": "2026-02-25T23:00:12.629918Z",
     "shell.execute_reply": "2026-02-25T23:00:12.629046Z"
    },
    "papermill": {
     "duration": 1.760309,
     "end_time": "2026-02-25T23:00:12.630909",
     "exception": false,
     "start_time": "2026-02-25T23:00:10.870600",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.109.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python313\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\python313\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: colorama in c:\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 1 : Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85933556",
   "metadata": {
    "papermill": {
     "duration": 0.003071,
     "end_time": "2026-02-25T23:00:12.637406",
     "exception": false,
     "start_time": "2026-02-25T23:00:12.634335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pourquoi le Prompt Engineering ?\n",
    "\n",
    "Le **prompt engineering** est l'art de formuler des instructions efficaces pour obtenir les meilleures reponses des modeles de langage. C'est une competence essentielle car :\n",
    "\n",
    "1. **Impact direct sur la qualite** : Un bon prompt peut transformer une reponse mediocre en resultat excellent\n",
    "2. **Economie de tokens** : Des prompts bien concus reduisent les iterations et donc les couts\n",
    "3. **Reproductibilite** : Des techniques structurees permettent des resultats coherents\n",
    "\n",
    "### Progression de ce notebook\n",
    "\n",
    "| Technique | Complexite | Cas d'usage |\n",
    "|-----------|------------|-------------|\n",
    "| Zero-shot | Simple | Questions generales, taches courantes |\n",
    "| Few-shot | Moyenne | Format specifique, style personnalise |\n",
    "| Chain-of-thought | Moyenne | Raisonnement, mathematiques, logique |\n",
    "| Self-refine | Avancee | Code, textes critiques, haute qualite |\n",
    "\n",
    "> **Documentation officielle** : [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b9f43da",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:12.644411Z",
     "iopub.status.busy": "2026-02-25T23:00:12.644200Z",
     "iopub.status.idle": "2026-02-25T23:00:12.656053Z",
     "shell.execute_reply": "2026-02-25T23:00:12.655679Z"
    },
    "papermill": {
     "duration": 0.016409,
     "end_time": "2026-02-25T23:00:12.656819",
     "exception": false,
     "start_time": "2026-02-25T23:00:12.640410",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: BATCH\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 2 : Configuration\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger la configuration depuis le fichier .env (dans le répertoire parent GenAI/)\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# On suppose que ton .env contient :\n",
    "# OPENAI_API_KEY=sk-xxxxxx\n",
    "# (ou autre variable si tu utilises Azure)\n",
    "#\n",
    "# Récupère la clé d'API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Clé API introuvable. Vérifie ton fichier .env.\")\n",
    "\n",
    "# Mode batch pour exécution non-interactive (Papermill, tests automatisés)\n",
    "# Détection automatique si exécution via Papermill ou si stdin non disponible\n",
    "def is_interactive():\n",
    "    \"\"\"Détecte si l'exécution est interactive (terminal) ou batch (Papermill)\"\"\"\n",
    "    try:\n",
    "        # Check if running in Papermill\n",
    "        import __main__\n",
    "        if hasattr(__main__, '__file__') and 'papermill' in str(getattr(__main__, '__file__', '')).lower():\n",
    "            return False\n",
    "        # Check if stdin is available\n",
    "        if not sys.stdin.isatty():\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "BATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\" or not is_interactive()\n",
    "print(f\"Mode: {'BATCH' if BATCH_MODE else 'INTERACTIF'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a87bf8",
   "metadata": {
    "papermill": {
     "duration": 0.003114,
     "end_time": "2026-02-25T23:00:12.665651",
     "exception": false,
     "start_time": "2026-02-25T23:00:12.662537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration de l'environnement\n",
    "\n",
    "Avant de commencer les expérimentations, nous devons configurer l'accès à l'API OpenAI et gérer les modes d'exécution.\n",
    "\n",
    "### Mode batch vs mode interactif\n",
    "\n",
    "Le notebook supporte deux modes d'exécution :\n",
    "\n",
    "- **Mode interactif** : Pour l'apprentissage, avec saisie utilisateur et expérimentation libre\n",
    "- **Mode batch** : Pour l'exécution automatisée (Papermill, tests, CI/CD), sans interaction\n",
    "\n",
    "La détection est automatique, mais vous pouvez forcer le mode batch via la variable d'environnement `BATCH_MODE=true` dans le fichier `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abdf011",
   "metadata": {
    "papermill": {
     "duration": 0.002936,
     "end_time": "2026-02-25T23:00:12.671586",
     "exception": false,
     "start_time": "2026-02-25T23:00:12.668650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialisation du client OpenAI\n",
    "\n",
    "Le client OpenAI moderne (>=1.0.0) utilise une API orientée objet :\n",
    "\n",
    "```python\n",
    "client = OpenAI(api_key=\"...\")\n",
    "response = client.chat.completions.create(...)\n",
    "```\n",
    "\n",
    "**Paramètres importants** :\n",
    "- `model` : Le modèle à utiliser (gpt-5-mini, o4-mini, etc.)\n",
    "- `max_tokens` : Longueur maximale de la réponse (évite les réponses trop longues et coûteuses)\n",
    "- `temperature` : Contrôle la créativité (0.0 = déterministe, 2.0 = très créatif)\n",
    "\n",
    "Par défaut, nous utilisons `gpt-5-mini` configuré dans le fichier `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830e31c",
   "metadata": {
    "papermill": {
     "duration": 0.003131,
     "end_time": "2026-02-25T23:00:12.677588",
     "exception": false,
     "start_time": "2026-02-25T23:00:12.674457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 1 : Zero-shot Prompting\n",
    "\n",
    "Le **zero-shot prompting** est la technique la plus directe : on pose une question sans fournir d'exemples préalables.\n",
    "\n",
    "### Quand utiliser Zero-shot ?\n",
    "\n",
    "- Questions générales ou conversationnelles\n",
    "- Tâches courantes bien comprises par le modèle (résumés, traductions simples)\n",
    "- Prototypage rapide\n",
    "- Budget tokens limité\n",
    "\n",
    "Testons avec une demande simple : générer des idées de recettes végétariennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddcaa015",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:12.684308Z",
     "iopub.status.busy": "2026-02-25T23:00:12.684107Z",
     "iopub.status.idle": "2026-02-25T23:00:13.644699Z",
     "shell.execute_reply": "2026-02-25T23:00:13.644077Z"
    },
    "papermill": {
     "duration": 0.965445,
     "end_time": "2026-02-25T23:00:13.645923",
     "exception": false,
     "start_time": "2026-02-25T23:00:12.680478",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client OpenAI initialisé avec succès !\n",
      "Modèle par défaut: gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Client OpenAI\n",
    "# ============================\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Charger le modèle depuis .env ou utiliser gpt-5-mini par défaut\n",
    "DEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\n",
    "MODEL_NAME = DEFAULT_MODEL\n",
    "\n",
    "# Instanciation du client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "print(\"Client OpenAI initialisé avec succès !\")\n",
    "print(f\"Modèle par défaut: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4437657",
   "metadata": {
    "papermill": {
     "duration": 0.003679,
     "end_time": "2026-02-25T23:00:13.654121",
     "exception": false,
     "start_time": "2026-02-25T23:00:13.650442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Rappel des différences entre Zero-shot, Few-shot, Chain-of-thought et Self-refine\n",
    "\n",
    "1. **Zero-shot Prompting**  \n",
    "   - Aucune instruction ou exemple préalable (à part la demande de l'utilisateur).  \n",
    "   - Simple et direct, mais parfois moins précis ou cohérent.\n",
    "\n",
    "2. **Few-shot Prompting**  \n",
    "   - Fournir quelques exemples “input → output” pour guider la réponse.  \n",
    "   - Permet de **spécifier le format**, le style, ou le contenu souhaité.  \n",
    "   - Améliore significativement la qualité des réponses sur des tâches complexes.\n",
    "\n",
    "3. **Chain-of-thought (CoT)**  \n",
    "   - On **incite** le modèle à détailler son raisonnement étape par étape.  \n",
    "   - Souvent utile pour des questions de logique, mathématiques, programmation ou raisonnement complexe.  \n",
    "   - Peut **augmenter** la cohérence et la justesse de la réponse (mais attention à ne pas divulguer ces “étapes” si elles sont confidentielles).\n",
    "\n",
    "4. **Self-refine**  \n",
    "   - Demander au modèle de s’auto-critiquer puis de proposer une réponse améliorée.  \n",
    "   - Mise en œuvre en plusieurs appels (réponse initiale, re-demande d’analyse, ré-énoncé final).  \n",
    "   - Intéressant pour du code, des textes longs, ou des situations nécessitant un contrôle qualité.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9619dc4",
   "metadata": {
    "papermill": {
     "duration": 0.004361,
     "end_time": "2026-02-25T23:00:13.662318",
     "exception": false,
     "start_time": "2026-02-25T23:00:13.657957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 2 : Few-shot Prompting\n",
    "\n",
    "Le **few-shot prompting** consiste à fournir 2-3 exemples de la tâche souhaitée avant la vraie question.\n",
    "\n",
    "### Mécanisme d'apprentissage en contexte\n",
    "\n",
    "Le modèle :\n",
    "1. Analyse les exemples fournis\n",
    "2. Détecte le **pattern** (format, style, structure)\n",
    "3. Applique ce pattern à la nouvelle question\n",
    "\n",
    "C'est ce qu'on appelle **l'apprentissage en contexte** (in-context learning) : le modèle s'adapte sans modifier ses poids.\n",
    "\n",
    "### Premier exemple : Rédaction d'emails professionnels\n",
    "\n",
    "Nous allons guider le modèle à produire un email avec un format et un ton spécifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a60165",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:13.671924Z",
     "iopub.status.busy": "2026-02-25T23:00:13.671464Z",
     "iopub.status.idle": "2026-02-25T23:00:21.109154Z",
     "shell.execute_reply": "2026-02-25T23:00:21.108514Z"
    },
    "papermill": {
     "duration": 7.443812,
     "end_time": "2026-02-25T23:00:21.110102",
     "exception": false,
     "start_time": "2026-02-25T23:00:13.666290",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zero-shot Prompt ===\n",
      "Prompt: Donne-moi 3 idées de recettes végétariennes à base de tomates.\n",
      "\n",
      "Réponse du modèle :\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Zero-shot\n",
    "# ============================\n",
    "\n",
    "prompt_1 = \"Donne-moi 3 idées de recettes végétariennes à base de tomates.\"\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt_1}\n",
    "    ],\n",
    "    # Contrôle du style\n",
    "    max_completion_tokens=400,\n",
    "                    # temperature non supporte par gpt-5-mini (defaut=1.0)\n",
    ")\n",
    "\n",
    "print(\"=== Zero-shot Prompt ===\")\n",
    "print(f\"Prompt: {prompt_1}\\n\")\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_1.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7616a0d",
   "metadata": {
    "papermill": {
     "duration": 0.003734,
     "end_time": "2026-02-25T23:00:21.117539",
     "exception": false,
     "start_time": "2026-02-25T23:00:21.113805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Résultat du Zero-shot prompting\n",
    "\n",
    "Le zero-shot prompting est la technique la plus simple : **aucun exemple préalable**, juste une instruction directe.\n",
    "\n",
    "**Avantages** :\n",
    "- Rapide à mettre en œuvre\n",
    "- Fonctionne bien pour des tâches courantes (résumés, traductions, questions générales)\n",
    "- Économique en tokens\n",
    "\n",
    "**Limites** :\n",
    "- Moins précis sur des tâches complexes ou spécialisées\n",
    "- Le format de sortie peut être imprévisible\n",
    "- Nécessite des prompts très clairs et bien formulés\n",
    "\n",
    "Dans cet exemple, le modèle génère 3 recettes végétariennes à base de tomates sans aucun exemple préalable. La qualité dépend fortement de la clarté du prompt et de la capacité du modèle à comprendre le domaine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767fb79",
   "metadata": {
    "papermill": {
     "duration": 0.003501,
     "end_time": "2026-02-25T23:00:21.124544",
     "exception": false,
     "start_time": "2026-02-25T23:00:21.121043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 3 : Chain-of-thought (CoT)\n",
    "\n",
    "Le **Chain-of-thought** demande explicitement au modèle de détailler son raisonnement étape par étape.\n",
    "\n",
    "### Pourquoi le CoT fonctionne ?\n",
    "\n",
    "Les modèles de langage sont entraînés sur des textes où les raisonnements sont explicités. En demandant les étapes intermédiaires, on active ce pattern et on améliore la précision.\n",
    "\n",
    "### Applications du CoT\n",
    "\n",
    "| Domaine | Exemple |\n",
    "|---------|---------|\n",
    "| Mathématiques | Résolution d'équations, problèmes de mots |\n",
    "| Logique | Syllogismes, déductions |\n",
    "| Programmation | Debugging, conception d'algorithmes |\n",
    "| Analyse | Cas juridiques, diagnostics médicaux |\n",
    "\n",
    "### Exemple : Problème arithmétique simple\n",
    "\n",
    "Testons avec un calcul impliquant plusieurs étapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb36ea",
   "metadata": {
    "papermill": {
     "duration": 0.004715,
     "end_time": "2026-02-25T23:00:21.133365",
     "exception": false,
     "start_time": "2026-02-25T23:00:21.128650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 4 : Self-refine (Auto-amélioration)\n",
    "\n",
    "Le **Self-refine** exploite la capacité du modèle à critiquer et améliorer ses propres productions.\n",
    "\n",
    "### Processus en deux étapes\n",
    "\n",
    "1. **Génération initiale** : Produire une première version (potentiellement bugguée ou imparfaite)\n",
    "2. **Critique et correction** : Analyser la première version, identifier les problèmes, proposer une amélioration\n",
    "\n",
    "### Avantages du Self-refine\n",
    "\n",
    "- **Qualité supérieure** : Deux passes donnent généralement de meilleurs résultats\n",
    "- **Détection de bugs** : Le modèle peut repérer ses propres erreurs\n",
    "- **Amélioration itérative** : Peut être répété plusieurs fois si nécessaire\n",
    "\n",
    "### Compromis\n",
    "\n",
    "- **Coût** : Double les tokens consommés (deux appels API)\n",
    "- **Temps** : Latence multipliée\n",
    "- **À utiliser pour** : Code critique, documents importants, tâches complexes\n",
    "\n",
    "### Exemple : Code Python avec bug volontaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d52374bc",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:21.142185Z",
     "iopub.status.busy": "2026-02-25T23:00:21.141610Z",
     "iopub.status.idle": "2026-02-25T23:00:25.451186Z",
     "shell.execute_reply": "2026-02-25T23:00:25.450319Z"
    },
    "papermill": {
     "duration": 4.315537,
     "end_time": "2026-02-25T23:00:25.452537",
     "exception": false,
     "start_time": "2026-02-25T23:00:21.137000",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exemple Few-shot (e-mail professionnel) ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Few-shot supplémentaire\n",
    "# ============================\n",
    "\n",
    "few_shot_prompt_2 = \"\"\"\n",
    "Tu es un assistant spécialisé en rédaction d'e-mails professionnels.\n",
    "Voici quelques exemples de style :\n",
    "\n",
    "Exemple 1:\n",
    "Q: Rédige un e-mail pour informer un client d'un retard de livraison\n",
    "A: \n",
    "Sujet: Information concernant le retard de votre livraison\n",
    "\n",
    "Bonjour [Nom du Client],\n",
    "\n",
    "Nous tenions à vous informer que votre commande #1234 a pris du retard...\n",
    "[...suite du mail...]\n",
    "\n",
    "Exemple 2:\n",
    "Q: Envoie un e-mail de remerciement pour un entretien d'embauche\n",
    "A:\n",
    "Sujet: Remerciements suite à notre entretien\n",
    "\n",
    "Bonjour [Nom du Contact],\n",
    "\n",
    "Je tiens à vous remercier pour le temps que vous m'avez accordé...\n",
    "[...suite du mail...]\n",
    "\n",
    "Maintenant, voici ma demande:\n",
    "\n",
    "Q: Écris un e-mail pour informer un collaborateur d'un changement de planning et l'inviter à une réunion de suivi.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response_few_shot_2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,  # ex. \"gpt-5-mini\"\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt_2}\n",
    "    ],\n",
    "    max_completion_tokens=300,\n",
    "                    # temperature=0.7  # gpt-5-mini ne supporte que temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"=== Exemple Few-shot (e-mail professionnel) ===\")\n",
    "print(response_few_shot_2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb1a44",
   "metadata": {
    "papermill": {
     "duration": 0.003253,
     "end_time": "2026-02-25T23:00:25.459724",
     "exception": false,
     "start_time": "2026-02-25T23:00:25.456471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Résultat du Few-shot : Format cohérent\n",
    "\n",
    "Le modèle a reproduit fidèlement la structure des exemples :\n",
    "\n",
    "1. **Sujet** : Clair et professionnel\n",
    "2. **Salutation** : Formule de politesse appropriée\n",
    "3. **Corps** : Structure logique (contexte → action → conclusion)\n",
    "4. **Signature** : Complète avec coordonnées\n",
    "\n",
    "**Observation clé** : Sans les exemples, le modèle aurait pu générer un email plus informel ou moins structuré. Le few-shot garantit la cohérence du format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6294e",
   "metadata": {
    "papermill": {
     "duration": 0.003048,
     "end_time": "2026-02-25T23:00:25.465932",
     "exception": false,
     "start_time": "2026-02-25T23:00:25.462884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse du Few-shot prompting\n",
    "\n",
    "Le few-shot prompting apporte une **amélioration significative** par rapport au zero-shot :\n",
    "\n",
    "**Mécanisme** :\n",
    "1. On fournit 2-3 exemples de la tâche souhaitée (paires question/réponse)\n",
    "2. Le modèle apprend le **pattern** et le **format** attendu\n",
    "3. Il applique ensuite ce pattern à la nouvelle question\n",
    "\n",
    "**Avantages observables** :\n",
    "- **Format cohérent** : Le modèle reproduit la structure des exemples (sujet, salutation, corps, signature)\n",
    "- **Ton approprié** : Le style professionnel est maintenu\n",
    "- **Contenu pertinent** : La réponse suit les conventions des exemples fournis\n",
    "\n",
    "**Quand utiliser Few-shot ?**\n",
    "- Tâches avec un format spécifique (emails, rapports, analyses structurées)\n",
    "- Cas où le zero-shot donne des résultats trop variables\n",
    "- Besoin de cohérence stylistique\n",
    "\n",
    "**Compromis** : Chaque exemple consomme des tokens supplémentaires, donc à utiliser avec modération pour des contextes très longs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4164e",
   "metadata": {
    "papermill": {
     "duration": 0.003332,
     "end_time": "2026-02-25T23:00:25.472372",
     "exception": false,
     "start_time": "2026-02-25T23:00:25.469040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 5 : Interactions conversationnelles\n",
    "\n",
    "Au-delà des techniques de prompting, la structure des conversations avec le modèle est cruciale pour des applications interactives.\n",
    "\n",
    "### Prompt simple sans mémoire\n",
    "\n",
    "La première cellule interactive montre un échange **stateless** (sans état) : chaque prompt est indépendant, le modèle n'a aucune mémoire des interactions précédentes.\n",
    "\n",
    "**Cas d'usage** :\n",
    "- Tests rapides d'un modèle\n",
    "- Questions indépendantes\n",
    "- Prototypage de prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff357bdc",
   "metadata": {
    "papermill": {
     "duration": 0.002958,
     "end_time": "2026-02-25T23:00:25.478229",
     "exception": false,
     "start_time": "2026-02-25T23:00:25.475271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Conversation avec mémoire\n",
    "\n",
    "La cellule suivante introduit un mécanisme de **mémoire de conversation** essentiel pour les chatbots et assistants.\n",
    "\n",
    "**Principe** :\n",
    "1. Accumuler les messages dans une liste `current_messages`\n",
    "2. À chaque tour, envoyer **tout l'historique** + le nouveau message\n",
    "3. Ajouter la réponse du modèle à l'historique\n",
    "\n",
    "**Structure des messages** :\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"Je m'appelle Alice\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Enchanté Alice !\"},\n",
    "    {\"role\": \"user\", \"content\": \"Comment je m'appelle ?\"},\n",
    "    # Le modèle peut répondre \"Vous vous appelez Alice\" grâce à l'historique\n",
    "]\n",
    "```\n",
    "\n",
    "**Attention** : L'historique consomme des tokens à chaque appel. Pour de longues conversations, il faut :\n",
    "- Tronquer les messages anciens\n",
    "- Résumer l'historique périodiquement\n",
    "- Utiliser des techniques de compression (embeddings, résumés automatiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2c7b0",
   "metadata": {
    "papermill": {
     "duration": 0.002925,
     "end_time": "2026-02-25T23:00:25.484085",
     "exception": false,
     "start_time": "2026-02-25T23:00:25.481160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 6 : Modèles de raisonnement (2025)\n",
    "\n",
    "Les **modèles de raisonnement** (reasoning models) comme `o4-mini` et `o1` représentent une évolution majeure des LLMs.\n",
    "\n",
    "### Différence fondamentale\n",
    "\n",
    "Les modèles chat génèrent token par token en temps réel. Les modèles de raisonnement :\n",
    "1. **Réfléchissent** en interne avant de répondre (pensées non visibles)\n",
    "2. **Explorent** plusieurs pistes de réflexion\n",
    "3. **Valident** leur raisonnement avant de produire la réponse finale\n",
    "\n",
    "### Paramètre clé : reasoning_effort\n",
    "\n",
    "Remplace le paramètre `temperature` pour les modèles de raisonnement :\n",
    "\n",
    "| Valeur | Durée | Cas d'usage |\n",
    "|--------|-------|-------------|\n",
    "| `low` | Rapide | Questions simples, prototypage |\n",
    "| `medium` | Modéré | Problèmes standards |\n",
    "| `high` | Lent | Problèmes complexes, mathématiques avancées |\n",
    "\n",
    "### Role \"developer\" obligatoire\n",
    "\n",
    "Les modèles de raisonnement n'utilisent plus le role `system`. Le role `developer` configure le comportement global, et peut activer/désactiver le formatage de la pensée interne.\n",
    "\n",
    "### Comparaison pratique\n",
    "\n",
    "Testons le même problème avec un modèle chat (gpt-5-mini) et un modèle de raisonnement (o4-mini)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d1544b",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:25.491190Z",
     "iopub.status.busy": "2026-02-25T23:00:25.490717Z",
     "iopub.status.idle": "2026-02-25T23:00:27.869705Z",
     "shell.execute_reply": "2026-02-25T23:00:27.868981Z"
    },
    "papermill": {
     "duration": 2.384142,
     "end_time": "2026-02-25T23:00:27.871087",
     "exception": false,
     "start_time": "2026-02-25T23:00:25.486945",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chain-of-thought Prompt ===\n",
      "Réponse du modèle (avec raisonnement) :\n",
      "\n",
      "3 pommes\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Chain-of-thought\n",
    "# ============================\n",
    "\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "Donne directement la réponse sans étape intermédiaire.\n",
    "\"\"\"\n",
    "\n",
    "# Explique ton raisonnement étape par étape, puis donne la réponse finale.\n",
    "response_3 = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": cot_prompt}\n",
    "    ],\n",
    "    max_completion_tokens=200,\n",
    "                    # temperature=0.7  # gpt-5-mini ne supporte que temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"=== Chain-of-thought Prompt ===\")\n",
    "print(\"Réponse du modèle (avec raisonnement) :\\n\")\n",
    "print(response_3.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5eb015",
   "metadata": {
    "papermill": {
     "duration": 0.003943,
     "end_time": "2026-02-25T23:00:27.881711",
     "exception": false,
     "start_time": "2026-02-25T23:00:27.877768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse du résultat CoT\n",
    "\n",
    "**Observation** : Le prompt demandait une réponse directe SANS étapes intermédiaires, et le modèle a bien obéi en donnant simplement \"3 pommes\".\n",
    "\n",
    "**Expérimentation recommandée** : Modifiez le prompt pour demander explicitement le raisonnement :\n",
    "\n",
    "```python\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "Explique ton raisonnement étape par étape avant de donner la réponse finale.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Vous devriez alors obtenir :\n",
    "- Étape 1 : 5 - 2 = 3\n",
    "- Étape 2 : 3 - 1 = 2\n",
    "- Étape 3 : 2 + 1 = 3\n",
    "- **Réponse : 3 pommes**\n",
    "\n",
    "### Bonnes pratiques CoT\n",
    "\n",
    "1. **Temperature basse** (0.0-0.3) pour les calculs et la logique\n",
    "2. **Prompt explicite** : \"Explique ton raisonnement étape par étape\"\n",
    "3. **Vérification** : Le raisonnement explicite permet de détecter les erreurs\n",
    "4. **Masquage optionnel** : Pour les applications de production, on peut demander au modèle de ne pas révéler son raisonnement à l'utilisateur final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a6f88",
   "metadata": {
    "papermill": {
     "duration": 0.002885,
     "end_time": "2026-02-25T23:00:27.887873",
     "exception": false,
     "start_time": "2026-02-25T23:00:27.884988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du Chain-of-thought\n",
    "\n",
    "Le Chain-of-thought (CoT) est particulièrement efficace pour les **problèmes de raisonnement** :\n",
    "\n",
    "**Analyse du résultat** :\n",
    "Le modèle devrait avoir détaillé :\n",
    "1. État initial : Alice a 5 pommes\n",
    "2. Étape 1 : Elle en jette 2 → 5 - 2 = 3 pommes\n",
    "3. Étape 2 : Elle en donne 1 à Bob → 3 - 1 = 2 pommes\n",
    "4. Étape 3 : Bob lui rend 1 pomme → 2 + 1 = 3 pommes\n",
    "5. **Réponse finale : 3 pommes**\n",
    "\n",
    "**Pourquoi c'est important ?**\n",
    "- **Transparence** : On peut vérifier le raisonnement étape par étape\n",
    "- **Détection d'erreurs** : Si le résultat est faux, on peut identifier où le modèle s'est trompé\n",
    "- **Confiance** : Le raisonnement explicite augmente la crédibilité\n",
    "- **Debugging** : Facilite la correction du prompt si nécessaire\n",
    "\n",
    "**Applications** :\n",
    "- Calculs mathématiques\n",
    "- Raisonnement logique\n",
    "- Résolution de problèmes complexes\n",
    "- Analyse de cas juridiques ou médicaux\n",
    "\n",
    "**Note** : Temperature=0.2 garantit un raisonnement cohérent et reproductible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30edbb7c",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:27.895224Z",
     "iopub.status.busy": "2026-02-25T23:00:27.894913Z",
     "iopub.status.idle": "2026-02-25T23:00:31.782024Z",
     "shell.execute_reply": "2026-02-25T23:00:31.781174Z"
    },
    "papermill": {
     "duration": 3.892731,
     "end_time": "2026-02-25T23:00:31.783455",
     "exception": false,
     "start_time": "2026-02-25T23:00:27.890724",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (1) : Code buggy ===\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8a : Self-refine Step 1\n",
    "# ============================\n",
    "\n",
    "prompt_sr1 = \"\"\"\n",
    "Ecris une courte fonction Python pour calculer la somme d'une liste. \n",
    "Ajoute un bug volontaire dans le code. \n",
    "\"\"\"\n",
    "\n",
    "response_sr1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr1}],\n",
    "    max_completion_tokens=300\n",
    ")\n",
    "\n",
    "buggy_code = response_sr1.choices[0].message.content\n",
    "\n",
    "print(\"=== Self-refine (1) : Code buggy ===\\n\")\n",
    "print(buggy_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75b428",
   "metadata": {
    "papermill": {
     "duration": 0.003693,
     "end_time": "2026-02-25T23:00:31.792528",
     "exception": false,
     "start_time": "2026-02-25T23:00:31.788835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 8b. Self-critique et amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d54be4e",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:31.800869Z",
     "iopub.status.busy": "2026-02-25T23:00:31.800675Z",
     "iopub.status.idle": "2026-02-25T23:00:36.398347Z",
     "shell.execute_reply": "2026-02-25T23:00:36.397422Z"
    },
    "papermill": {
     "duration": 4.602792,
     "end_time": "2026-02-25T23:00:36.399205",
     "exception": false,
     "start_time": "2026-02-25T23:00:31.796413",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (2) : Correction ===\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8b : Self-refine Step 2\n",
    "# ============================\n",
    "\n",
    "prompt_sr2 = f\"\"\"\n",
    "Voici un code Python qui contient un bug:\n",
    "\n",
    "{buggy_code}\n",
    "\n",
    "Peux-tu l'analyser, détecter le bug, proposer un correctif et une version améliorée du code ? \n",
    "Explique la correction.\n",
    "\"\"\"\n",
    "\n",
    "response_sr2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr2}],\n",
    "    max_completion_tokens=400,\n",
    "                    # temperature non supporte par gpt-5-mini (defaut=1.0)\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine (2) : Correction ===\\n\")\n",
    "print(response_sr2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa422e",
   "metadata": {
    "papermill": {
     "duration": 0.003685,
     "end_time": "2026-02-25T23:00:36.406481",
     "exception": false,
     "start_time": "2026-02-25T23:00:36.402796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Résultat du Self-refine : Amélioration itérative\n",
    "\n",
    "Le modèle a :\n",
    "1. **Identifié le bug** : L'ajout de `+ 1` dans le `return` fausse le résultat\n",
    "2. **Proposé une correction** : Retirer le `+ 1`\n",
    "3. **Suggéré une amélioration** : Utiliser la fonction native `sum()` pour plus de simplicité\n",
    "\n",
    "**Enseignement** : Le modèle possède une capacité de **méta-cognition** - il peut raisonner sur ses propres productions et les améliorer.\n",
    "\n",
    "### Variante avancée : Self-refine avec role \"developer\"\n",
    "\n",
    "Au lieu d'un processus en deux appels API, on peut utiliser le rôle `developer` pour configurer le comportement d'auto-amélioration directement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c8a9c",
   "metadata": {
    "papermill": {
     "duration": 0.004542,
     "end_time": "2026-02-25T23:00:36.416423",
     "exception": false,
     "start_time": "2026-02-25T23:00:36.411881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Bilan du Self-refine\n",
    "\n",
    "Le processus Self-refine en deux étapes démontre la capacité du modèle à s'**auto-améliorer** :\n",
    "\n",
    "**Première étape** : Génération intentionnellement bugguée\n",
    "- Le modèle crée du code avec un bug volontaire (par exemple, division par zéro, mauvaise initialisation, etc.)\n",
    "\n",
    "**Deuxième étape** : Critique et correction\n",
    "- Le modèle analyse son propre code\n",
    "- Identifie le bug\n",
    "- Propose une version corrigée\n",
    "- Explique la nature du problème\n",
    "\n",
    "**Enseignements** :\n",
    "1. **Méta-cognition** : Le LLM peut raisonner sur ses propres productions\n",
    "2. **Amélioration itérative** : Chaque passe peut affiner la qualité\n",
    "3. **Détection de bugs** : Utile pour du code review automatisé\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Génération de code robuste (plusieurs passes de correction)\n",
    "- Rédaction de documents (brouillon → révision → version finale)\n",
    "- Traduction (première traduction → révision → amélioration)\n",
    "\n",
    "**Limite** : Chaque itération consomme des tokens et du temps. À utiliser pour des tâches critiques nécessitant haute qualité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b94379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:36.425074Z",
     "iopub.status.busy": "2026-02-25T23:00:36.424738Z",
     "iopub.status.idle": "2026-02-25T23:00:40.747812Z",
     "shell.execute_reply": "2026-02-25T23:00:40.747230Z"
    },
    "papermill": {
     "duration": 4.328073,
     "end_time": "2026-02-25T23:00:40.748781",
     "exception": false,
     "start_time": "2026-02-25T23:00:36.420708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine avec 'developer role' ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Self-refine avec developer role\n",
    "# ============================\n",
    "\n",
    "messages_sr = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": (\n",
    "            \"You are a self-improving coding assistant. Whenever you provide code, \"\n",
    "            \"you will automatically search for potential bugs or improvements \"\n",
    "            \"and refine your output.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Écris une fonction Python qui calcule la factorielle d'un nombre entier. \"\n",
    "            \"Ensuite, relis-toi et corrige d'éventuels bugs.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "response_self_refine = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages_sr,\n",
    "    max_completion_tokens=300,\n",
    "                    # temperature non supporte par gpt-5-mini (defaut=1.0)\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine avec 'developer role' ===\")\n",
    "print(response_self_refine.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad7e6c4",
   "metadata": {
    "papermill": {
     "duration": 0.005482,
     "end_time": "2026-02-25T23:00:40.759062",
     "exception": false,
     "start_time": "2026-02-25T23:00:40.753580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du Self-refine avec role \"developer\"\n",
    "\n",
    "Le rôle `developer` (introduit avec les modèles de raisonnement) permet de définir le comportement global du modèle, contrairement au rôle `system` qui était utilisé auparavant.\n",
    "\n",
    "**Différence clé** :\n",
    "- **Role \"system\"** (ancienne API) : Instructions générales, parfois ignorées par le modèle\n",
    "- **Role \"developer\"** (nouvelle API) : Instructions prioritaires, mieux respectées\n",
    "\n",
    "Dans cet exemple :\n",
    "- Le modèle génère la fonction factorielle\n",
    "- Il vérifie automatiquement les bugs potentiels\n",
    "- Il propose des améliorations (gestion d'erreurs, performance, alternatives)\n",
    "\n",
    "**Observation** : La fonction générée inclut directement :\n",
    "- Vérification du type (`isinstance`)\n",
    "- Gestion des nombres négatifs\n",
    "- Version alternative avec `math.factorial`\n",
    "\n",
    "C'est plus efficace que le Self-refine en deux passes, mais nécessite un modèle récent supportant le rôle `developer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe87023",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:40.768387Z",
     "iopub.status.busy": "2026-02-25T23:00:40.767964Z",
     "iopub.status.idle": "2026-02-25T23:00:44.673632Z",
     "shell.execute_reply": "2026-02-25T23:00:44.672569Z"
    },
    "papermill": {
     "duration": 3.912285,
     "end_time": "2026-02-25T23:00:44.675132",
     "exception": false,
     "start_time": "2026-02-25T23:00:40.762847",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] Prompt: Bonjour!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse: Bonjour ! Comment puis-je vous aider aujourd'hui ?\n",
      "\n",
      "[BATCH] Prompt: Quelle est la capitale de la France?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse: La capitale de la France est Paris.\n",
      "\n",
      "Mode batch terminé.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif\n",
    "# ============================\n",
    "\n",
    "# En mode batch (BATCH_MODE=true dans .env), cette cellule utilise des exemples prédéfinis\n",
    "# En mode interactif, elle permet de tester le modèle en boucle\n",
    "\n",
    "if BATCH_MODE:\n",
    "    # Mode batch: exécuter des exemples simples\n",
    "    test_prompts = [\"Bonjour!\", \"Quelle est la capitale de la France?\"]\n",
    "    for prompt in test_prompts:\n",
    "        print(f\"[BATCH] Prompt: {prompt}\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            max_completion_tokens=200,\n",
    "                    # temperature non supporte par gpt-5-mini (defaut=1.0)\n",
    "        )\n",
    "        print(f\"Réponse: {resp.choices[0].message.content}\\n\")\n",
    "    print(\"Mode batch terminé.\")\n",
    "else:\n",
    "    while True:\n",
    "        user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Fin de l'interaction.\")\n",
    "            break\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_completion_tokens=200,\n",
    "                    # temperature non supporte par gpt-5-mini (defaut=1.0)\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== Réponse du modèle ===\")\n",
    "        print(resp.choices[0].message.content)\n",
    "        print(\"---------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9baccb",
   "metadata": {
    "papermill": {
     "duration": 0.003062,
     "end_time": "2026-02-25T23:00:44.685116",
     "exception": false,
     "start_time": "2026-02-25T23:00:44.682054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation des tests interactifs\n",
    "\n",
    "Cette cellule démontre le **mode batch** pour tester le modèle sans interaction manuelle.\n",
    "\n",
    "**Résultats observés** :\n",
    "- **Prompt simple** : \"Bonjour!\" → Réponse polie et ouverture de dialogue\n",
    "- **Question factuelle** : \"Quelle est la capitale de la France?\" → Réponse directe et précise\n",
    "\n",
    "**Points clés** :\n",
    "1. **Température 0.7** : Équilibre entre cohérence et créativité\n",
    "2. **Max tokens 200** : Limite les réponses pour réduire les coûts\n",
    "3. **Messages stateless** : Chaque prompt est indépendant, pas de mémoire entre appels\n",
    "\n",
    "**Différence avec la cellule suivante** : La prochaine cellule introduit la **mémoire de conversation** (accumulation de l'historique), essentielle pour les chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd01567c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:44.693108Z",
     "iopub.status.busy": "2026-02-25T23:00:44.692903Z",
     "iopub.status.idle": "2026-02-25T23:00:50.353424Z",
     "shell.execute_reply": "2026-02-25T23:00:50.352942Z"
    },
    "papermill": {
     "duration": 5.666142,
     "end_time": "2026-02-25T23:00:50.354253",
     "exception": false,
     "start_time": "2026-02-25T23:00:44.688111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] User: Je m'appelle Alice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: \n",
      "\n",
      "[BATCH] User: Comment je m'appelle?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: \n",
      "\n",
      "Mode batch (mémoire) terminé.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif avec mémoire de chat\n",
    "# ============================\n",
    "\n",
    "# En mode batch, cette cellule utilise un dialogue prédéfini\n",
    "# En mode interactif, elle permet un échange avec mémoire\n",
    "\n",
    "if BATCH_MODE:\n",
    "    # Mode batch: simuler une conversation avec mémoire\n",
    "    batch_conversation = [\n",
    "        \"Je m'appelle Alice\",\n",
    "        \"Comment je m'appelle?\",\n",
    "    ]\n",
    "    current_messages = []\n",
    "\n",
    "    for user_input in batch_conversation:\n",
    "        print(f\"[BATCH] User: {user_input}\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=current_messages + [{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_completion_tokens=200,\n",
    "                    # temperature non supporte par gpt-5-mini (defaut=1.0)\n",
    "        )\n",
    "        assistant_message = resp.choices[0].message.content\n",
    "        print(f\"Assistant: {assistant_message}\\n\")\n",
    "        current_messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "        current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})\n",
    "\n",
    "    print(\"Mode batch (mémoire) terminé.\")\n",
    "else:\n",
    "    user_input = \"\"\n",
    "    current_messages = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Fin de l'interaction.\")\n",
    "            break\n",
    "        print(\"\\n=== message de l'utilisateur ===\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages = current_messages + [{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_completion_tokens=200,\n",
    "                    # temperature non supporte par gpt-5-mini (defaut=1.0)\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== Réponse du modèle ===\")\n",
    "        assistant_message = resp.choices[0].message.content\n",
    "        print(assistant_message)\n",
    "        print(\"---------------------------------------------------\\n\")\n",
    "        current_messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "        current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1caa5f9",
   "metadata": {
    "papermill": {
     "duration": 0.003692,
     "end_time": "2026-02-25T23:00:50.361845",
     "exception": false,
     "start_time": "2026-02-25T23:00:50.358153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation de la conversation avec mémoire\n",
    "\n",
    "Cette cellule démontre la **mémoire conversationnelle** - le modèle se souvient du contexte précédent.\n",
    "\n",
    "**Résultats observés** :\n",
    "1. **Tour 1** : \"Je m'appelle Alice\" → Le modèle enregistre cette information dans `current_messages`\n",
    "2. **Tour 2** : \"Comment je m'appelle?\" → Le modèle répond \"Vous vous appelez Alice\" grâce à l'historique\n",
    "\n",
    "**Mécanisme technique** :\n",
    "```python\n",
    "current_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Je m'appelle Alice\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Bonjour, Alice...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Comment je m'appelle?\"},\n",
    "    # Le modèle peut répondre grâce à l'historique complet\n",
    "]\n",
    "```\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Chatbots conversationnels\n",
    "- Assistants personnels\n",
    "- Support client automatisé\n",
    "\n",
    "**Gestion de l'historique** :\n",
    "- **Court terme** : Garder tout l'historique (comme ici)\n",
    "- **Long terme** : Tronquer les messages anciens ou résumer périodiquement\n",
    "- **Tokens** : Attention au coût - chaque tour envoie TOUT l'historique à l'API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35585119",
   "metadata": {
    "papermill": {
     "duration": 0.003108,
     "end_time": "2026-02-25T23:00:50.368392",
     "exception": false,
     "start_time": "2026-02-25T23:00:50.365284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prompting pour Modèles de Raisonnement (2025)\n",
    "\n",
    "Les modèles de raisonnement (o4-mini, o1) représentent une évolution majeure. Contrairement aux modèles de chat, ils prennent le temps de \"réfléchir\" avant de répondre.\n",
    "\n",
    "## Différences clés avec les modèles chat\n",
    "\n",
    "| Aspect | Modèles Chat (gpt-5-mini) | Modèles Raisonnement (o4-mini) |\n",
    "|--------|----------------------|-------------------------------|\n",
    "| Temps de réponse | Rapide | Plus lent (réflexion) |\n",
    "| Prompts | Détaillés, structurés | **Simples et directs** |\n",
    "| Chain-of-thought | Demandé explicitement | **Intégré nativement** |\n",
    "| Messages | system, user, assistant | **developer**, user, assistant |\n",
    "| Paramètre spécial | temperature | **reasoning_effort** |\n",
    "\n",
    "## Règle d'or : Simplifier les prompts\n",
    "\n",
    "**Pour les modèles de raisonnement, des prompts simples fonctionnent mieux !**\n",
    "\n",
    "Les modèles raisonnants sont capables de :\n",
    "- Comprendre l'intention sans instructions détaillées\n",
    "- Gérer les ambiguïtés intelligemment\n",
    "- Demander des clarifications si nécessaire\n",
    "\n",
    "**Éviter** : \"Analyse ce problème en détaillant chaque étape de ton raisonnement...\"\n",
    "**Préférer** : \"Résous ce problème.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc4a0e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:00:50.375533Z",
     "iopub.status.busy": "2026-02-25T23:00:50.375335Z",
     "iopub.status.idle": "2026-02-25T23:01:01.095068Z",
     "shell.execute_reply": "2026-02-25T23:01:01.094550Z"
    },
    "papermill": {
     "duration": 10.724394,
     "end_time": "2026-02-25T23:01:01.095894",
     "exception": false,
     "start_time": "2026-02-25T23:00:50.371500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gpt-5-mini (Chat Model) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps: 5.62s\n",
      "...\n",
      "\n",
      "=== o4-mini (Reasoning Model) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps: 5.10s\n",
      "Voici une solution en 7 allers-retours :\n",
      "\n",
      "1. Le fermier traverse avec la chèvre et la laisse de l’autre côté.  \n",
      "   Rive de départ : loup + chou  \n",
      "   Rive d’arrivée : chèvre  \n",
      "\n",
      "2. Le fermier revient seul.  \n",
      "   Rive de départ : loup + chou  \n",
      "   Rive d’arrivée : chèvre  \n",
      "\n",
      "3. Il traverse avec le loup et le dépose de l’autre côté.  \n",
      "   Rive de départ : chou  \n",
      "   Rive d’arrivée : chèvre + loup  \n",
      "\n",
      "   (At...\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Comparaison Chat vs Reasoning\n",
    "# ============================\n",
    "\n",
    "import time\n",
    "\n",
    "probleme_complexe = \"\"\"\n",
    "Un fermier veut traverser une rivière avec un loup, une chèvre et un chou.\n",
    "Son bateau ne peut transporter que lui et un objet à la fois.\n",
    "Si le loup est laissé seul avec la chèvre, il la mange.\n",
    "Si la chèvre est laissée seule avec le chou, elle le mange.\n",
    "Comment le fermier peut-il tout transporter de l'autre côté?\n",
    "\"\"\"\n",
    "\n",
    "# Test avec gpt-5-mini (chat model)\n",
    "print(\"=== gpt-5-mini (Chat Model) ===\")\n",
    "start = time.time()\n",
    "response_chat = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": probleme_complexe}],\n",
    "    max_completion_tokens=500,\n",
    "                    # temperature=0.7  # gpt-5-mini ne supporte que temperature=1.0\n",
    ")\n",
    "print(f\"Temps: {time.time() - start:.2f}s\")\n",
    "print(response_chat.choices[0].message.content[:400] + \"...\")\n",
    "\n",
    "# Test avec o4-mini (reasoning model) - si disponible\n",
    "print(\"\\n=== o4-mini (Reasoning Model) ===\")\n",
    "try:\n",
    "    start = time.time()\n",
    "    response_reason = client.chat.completions.create(\n",
    "        model=\"o4-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "            {\"role\": \"user\", \"content\": probleme_complexe}\n",
    "        ],\n",
    "        reasoning_effort=\"medium\"\n",
    "    )\n",
    "    print(f\"Temps: {time.time() - start:.2f}s\")\n",
    "    print(response_reason.choices[0].message.content[:400] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"o4-mini non disponible: {type(e).__name__}\")\n",
    "    print(\"Les modèles de raisonnement nécessitent un accès spécifique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e7835",
   "metadata": {
    "papermill": {
     "duration": 0.003715,
     "end_time": "2026-02-25T23:01:01.103573",
     "exception": false,
     "start_time": "2026-02-25T23:01:01.099858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse des résultats : Chat vs Reasoning\n",
    "\n",
    "**Observations attendues** :\n",
    "\n",
    "1. **Temps de réponse** :\n",
    "   - Chat (gpt-5-mini) : ~2-5 secondes\n",
    "   - Reasoning (o4-mini) : ~10-30 secondes (réflexion interne)\n",
    "\n",
    "2. **Qualité de la solution** :\n",
    "   - Chat : Solution généralement correcte, mais peut manquer des optimisations\n",
    "   - Reasoning : Solution optimale, avec explication détaillée\n",
    "\n",
    "3. **Robustesse** :\n",
    "   - Chat : Peut parfois donner des solutions incorrectes sur des variantes complexes\n",
    "   - Reasoning : Vérifie sa solution avant de répondre, taux d'erreur plus faible\n",
    "\n",
    "### Quand utiliser les modèles de raisonnement ?\n",
    "\n",
    "**Préférer Chat** (gpt-5-mini, gpt-5-mini) :\n",
    "- Applications temps réel (chatbots)\n",
    "- Tâches simples et courantes\n",
    "- Budget temps/coût limité\n",
    "\n",
    "**Préférer Reasoning** (o4-mini, o1) :\n",
    "- Mathématiques avancées\n",
    "- Problèmes de logique complexes\n",
    "- Code critique nécessitant validation\n",
    "- Analyse approfondie\n",
    "\n",
    "**Note** : Les modèles de raisonnement sont plus coûteux en tokens et en temps, mais offrent une qualité supérieure pour les tâches complexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zesyf308uqp",
   "metadata": {
    "papermill": {
     "duration": 0.003502,
     "end_time": "2026-02-25T23:01:01.110712",
     "exception": false,
     "start_time": "2026-02-25T23:01:01.107210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# CHALLENGE BONUS - Application au Projet Review\n",
    "\n",
    "**Points : 1 pt**\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Creer un prompt qui genere un poeme en style de votre choix sur le theme de l'IA, en moins de 100 tokens.\n",
    "\n",
    "## Criteres de succes\n",
    "\n",
    "- [ ] Le poeme traite du theme de l'intelligence artificielle\n",
    "- [ ] Le style est clairement identifiable (classique, moderne, humour, etc.)\n",
    "- [ ] Le poeme contient moins de 100 tokens (verifier avec tiktoken)\n",
    "- [ ] Le code fonctionne sans erreur\n",
    "\n",
    "## Indices\n",
    "\n",
    "- Utilisez les patterns vus dans ce notebook (zero-shot ou few-shot)\n",
    "- Testez avec differentes valeurs de temperature\n",
    "- Utilisez `max_completion_tokens` pour limiter la longueur\n",
    "- Pour compter les tokens: `import tiktoken; len(tiktoken.encoding_for_model(\"gpt-5-mini\").encode(text))`\n",
    "\n",
    "## Votre code ici\n",
    "\n",
    "```python\n",
    "# TODO: Implementez votre challenge\n",
    "# 1. Definissez votre prompt\n",
    "# 2. Appelez l'API avec les bons parametres\n",
    "# 3. Affichez le resultat et le nombre de tokens\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Soumission** : Une fois termine, creez une PR sur votre fork avec :\n",
    "- Titre: \"Challenge #1 - [Votre Nom]\"\n",
    "- Description: Bref explication de votre solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 52.458895,
   "end_time": "2026-02-25T23:01:01.464268",
   "environment_variables": {},
   "exception": null,
   "input_path": "MyIA.AI.Notebooks/GenAI/Texte/2_PromptEngineering.ipynb",
   "output_path": "MyIA.AI.Notebooks/GenAI/Texte/2_PromptEngineering.ipynb",
   "parameters": {
    "BATCH_MODE": "true"
   },
   "start_time": "2026-02-25T23:00:09.005373",
   "version": "2.6.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}