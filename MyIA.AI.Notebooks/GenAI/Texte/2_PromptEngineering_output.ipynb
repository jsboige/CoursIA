{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798c1be7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T07:55:45.266106Z",
     "iopub.status.busy": "2026-02-18T07:55:45.265914Z",
     "iopub.status.idle": "2026-02-18T07:55:45.270121Z",
     "shell.execute_reply": "2026-02-18T07:55:45.269387Z"
    },
    "papermill": {
     "duration": 0.012623,
     "end_time": "2026-02-18T07:55:45.272079",
     "exception": false,
     "start_time": "2026-02-18T07:55:45.259456",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_MODE = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f26e6",
   "metadata": {
    "papermill": {
     "duration": 0.002917,
     "end_time": "2026-02-18T07:55:45.280237",
     "exception": false,
     "start_time": "2026-02-18T07:55:45.277320",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prompt Engineering : Advanced Prompting avec OpenAI\n",
    "\n",
    "Dans ce notebook, nous allons tester différentes techniques avancées de **prompt engineering**:\n",
    "- **Zero-shot prompting**\n",
    "- **Few-shot prompting**\n",
    "- **Chain-of-thought** (CoT)\n",
    "- **Self-refine** (ou auto-amélioration)\n",
    "\n",
    "Nous utiliserons la **nouvelle API** de la bibliothèque `openai` (>=1.0.0) via la classe `OpenAI` et ses méthodes de chat (`client.chat.completions.create`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4b75c",
   "metadata": {
    "papermill": {
     "duration": 0.002604,
     "end_time": "2026-02-18T07:55:45.285749",
     "exception": false,
     "start_time": "2026-02-18T07:55:45.283145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Prompt Engineering : Techniques Avancées\n",
    "\n",
    "**Navigation** : [<< Precedent](1_OpenAI_Intro.ipynb) | [Index](../../README.md) | [Suivant >>](3_Structured_Outputs.ipynb)\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Maîtriser les techniques de zero-shot et few-shot prompting\n",
    "2. Comprendre et appliquer le chain-of-thought (CoT)\n",
    "3. Implémenter le self-refine pour l'amélioration itérative\n",
    "4. Distinguer modèles chat et modèles de raisonnement\n",
    "\n",
    "### Prerequis\n",
    "- Notebook 1 (Introduction a l'IA generative)\n",
    "- Python 3.10+\n",
    "- Cle API OpenAI configuree\n",
    "\n",
    "### Duree estimee : 60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "# Prompt Engineering : Advanced Prompting avec OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e31e04",
   "metadata": {
    "papermill": {
     "duration": 0.002908,
     "end_time": "2026-02-18T07:55:45.291582",
     "exception": false,
     "start_time": "2026-02-18T07:55:45.288674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Installation des dépendances\n",
    "\n",
    "Avant de commencer, nous devons installer les bibliothèques Python nécessaires.\n",
    "\n",
    "**Packages requis** :\n",
    "- **openai** : Bibliothèque officielle pour interagir avec l'API OpenAI (>=1.0.0)\n",
    "- **tiktoken** : Encodeur de tokens pour compter et gérer les tokens GPT\n",
    "- **python-dotenv** : Gestion sécurisée des clés API via fichiers .env\n",
    "\n",
    "> **Note de sécurité** : Ne jamais inclure vos clés API directement dans le code. Toujours utiliser un fichier  exclu du contrôle de version (.gitignore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331404b4",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:55:45.297818Z",
     "iopub.status.busy": "2026-02-18T07:55:45.297626Z",
     "iopub.status.idle": "2026-02-18T07:55:47.006046Z",
     "shell.execute_reply": "2026-02-18T07:55:47.005184Z"
    },
    "papermill": {
     "duration": 1.712429,
     "end_time": "2026-02-18T07:55:47.006927",
     "exception": false,
     "start_time": "2026-02-18T07:55:45.294498",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.109.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python313\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in c:\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\python313\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: colorama in c:\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 1 : Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85933556",
   "metadata": {
    "papermill": {
     "duration": 0.002884,
     "end_time": "2026-02-18T07:55:47.013179",
     "exception": false,
     "start_time": "2026-02-18T07:55:47.010295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pourquoi le Prompt Engineering ?\n",
    "\n",
    "Le **prompt engineering** est l'art de formuler des instructions efficaces pour obtenir les meilleures reponses des modeles de langage. C'est une competence essentielle car :\n",
    "\n",
    "1. **Impact direct sur la qualite** : Un bon prompt peut transformer une reponse mediocre en resultat excellent\n",
    "2. **Economie de tokens** : Des prompts bien concus reduisent les iterations et donc les couts\n",
    "3. **Reproductibilite** : Des techniques structurees permettent des resultats coherents\n",
    "\n",
    "### Progression de ce notebook\n",
    "\n",
    "| Technique | Complexite | Cas d'usage |\n",
    "|-----------|------------|-------------|\n",
    "| Zero-shot | Simple | Questions generales, taches courantes |\n",
    "| Few-shot | Moyenne | Format specifique, style personnalise |\n",
    "| Chain-of-thought | Moyenne | Raisonnement, mathematiques, logique |\n",
    "| Self-refine | Avancee | Code, textes critiques, haute qualite |\n",
    "\n",
    "> **Documentation officielle** : [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b9f43da",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:55:47.021042Z",
     "iopub.status.busy": "2026-02-18T07:55:47.020818Z",
     "iopub.status.idle": "2026-02-18T07:55:47.031336Z",
     "shell.execute_reply": "2026-02-18T07:55:47.030755Z"
    },
    "papermill": {
     "duration": 0.016458,
     "end_time": "2026-02-18T07:55:47.032688",
     "exception": false,
     "start_time": "2026-02-18T07:55:47.016230",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: BATCH\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 2 : Configuration\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger la configuration depuis le fichier .env (dans le répertoire parent GenAI/)\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# On suppose que ton .env contient :\n",
    "# OPENAI_API_KEY=sk-xxxxxx\n",
    "# (ou autre variable si tu utilises Azure)\n",
    "#\n",
    "# Récupère la clé d'API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Clé API introuvable. Vérifie ton fichier .env.\")\n",
    "\n",
    "# Mode batch pour exécution non-interactive (Papermill, tests automatisés)\n",
    "# Détection automatique si exécution via Papermill ou si stdin non disponible\n",
    "def is_interactive():\n",
    "    \"\"\"Détecte si l'exécution est interactive (terminal) ou batch (Papermill)\"\"\"\n",
    "    try:\n",
    "        # Check if running in Papermill\n",
    "        import __main__\n",
    "        if hasattr(__main__, '__file__') and 'papermill' in str(getattr(__main__, '__file__', '')).lower():\n",
    "            return False\n",
    "        # Check if stdin is available\n",
    "        if not sys.stdin.isatty():\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "BATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\" or not is_interactive()\n",
    "print(f\"Mode: {'BATCH' if BATCH_MODE else 'INTERACTIF'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a87bf8",
   "metadata": {
    "papermill": {
     "duration": 0.003962,
     "end_time": "2026-02-18T07:55:47.040396",
     "exception": false,
     "start_time": "2026-02-18T07:55:47.036434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration de l'environnement\n",
    "\n",
    "Avant de commencer les expérimentations, nous devons configurer l'accès à l'API OpenAI et gérer les modes d'exécution.\n",
    "\n",
    "### Mode batch vs mode interactif\n",
    "\n",
    "Le notebook supporte deux modes d'exécution :\n",
    "\n",
    "- **Mode interactif** : Pour l'apprentissage, avec saisie utilisateur et expérimentation libre\n",
    "- **Mode batch** : Pour l'exécution automatisée (Papermill, tests, CI/CD), sans interaction\n",
    "\n",
    "La détection est automatique, mais vous pouvez forcer le mode batch via la variable d'environnement `BATCH_MODE=true` dans le fichier `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abdf011",
   "metadata": {
    "papermill": {
     "duration": 0.003568,
     "end_time": "2026-02-18T07:55:47.047055",
     "exception": false,
     "start_time": "2026-02-18T07:55:47.043487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialisation du client OpenAI\n",
    "\n",
    "Le client OpenAI moderne (>=1.0.0) utilise une API orientée objet :\n",
    "\n",
    "```python\n",
    "client = OpenAI(api_key=\"...\")\n",
    "response = client.chat.completions.create(...)\n",
    "```\n",
    "\n",
    "**Paramètres importants** :\n",
    "- `model` : Le modèle à utiliser (gpt-4o-mini, o4-mini, etc.)\n",
    "- `max_tokens` : Longueur maximale de la réponse (évite les réponses trop longues et coûteuses)\n",
    "- `temperature` : Contrôle la créativité (0.0 = déterministe, 2.0 = très créatif)\n",
    "\n",
    "Par défaut, nous utilisons `gpt-4o-mini` configuré dans le fichier `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830e31c",
   "metadata": {
    "papermill": {
     "duration": 0.003187,
     "end_time": "2026-02-18T07:55:47.054488",
     "exception": false,
     "start_time": "2026-02-18T07:55:47.051301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 1 : Zero-shot Prompting\n",
    "\n",
    "Le **zero-shot prompting** est la technique la plus directe : on pose une question sans fournir d'exemples préalables.\n",
    "\n",
    "### Quand utiliser Zero-shot ?\n",
    "\n",
    "- Questions générales ou conversationnelles\n",
    "- Tâches courantes bien comprises par le modèle (résumés, traductions simples)\n",
    "- Prototypage rapide\n",
    "- Budget tokens limité\n",
    "\n",
    "Testons avec une demande simple : générer des idées de recettes végétariennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddcaa015",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:55:47.061885Z",
     "iopub.status.busy": "2026-02-18T07:55:47.061391Z",
     "iopub.status.idle": "2026-02-18T07:55:48.566634Z",
     "shell.execute_reply": "2026-02-18T07:55:48.565518Z"
    },
    "papermill": {
     "duration": 1.510971,
     "end_time": "2026-02-18T07:55:48.568379",
     "exception": false,
     "start_time": "2026-02-18T07:55:47.057408",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client OpenAI initialisé avec succès !\n",
      "Modèle par défaut: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Client OpenAI\n",
    "# ============================\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Charger le modèle depuis .env ou utiliser gpt-4o-mini par défaut\n",
    "DEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "MODEL_NAME = DEFAULT_MODEL\n",
    "\n",
    "# Instanciation du client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "print(\"Client OpenAI initialisé avec succès !\")\n",
    "print(f\"Modèle par défaut: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4437657",
   "metadata": {
    "papermill": {
     "duration": 0.005386,
     "end_time": "2026-02-18T07:55:48.579612",
     "exception": false,
     "start_time": "2026-02-18T07:55:48.574226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Rappel des différences entre Zero-shot, Few-shot, Chain-of-thought et Self-refine\n",
    "\n",
    "1. **Zero-shot Prompting**  \n",
    "   - Aucune instruction ou exemple préalable (à part la demande de l'utilisateur).  \n",
    "   - Simple et direct, mais parfois moins précis ou cohérent.\n",
    "\n",
    "2. **Few-shot Prompting**  \n",
    "   - Fournir quelques exemples “input → output” pour guider la réponse.  \n",
    "   - Permet de **spécifier le format**, le style, ou le contenu souhaité.  \n",
    "   - Améliore significativement la qualité des réponses sur des tâches complexes.\n",
    "\n",
    "3. **Chain-of-thought (CoT)**  \n",
    "   - On **incite** le modèle à détailler son raisonnement étape par étape.  \n",
    "   - Souvent utile pour des questions de logique, mathématiques, programmation ou raisonnement complexe.  \n",
    "   - Peut **augmenter** la cohérence et la justesse de la réponse (mais attention à ne pas divulguer ces “étapes” si elles sont confidentielles).\n",
    "\n",
    "4. **Self-refine**  \n",
    "   - Demander au modèle de s’auto-critiquer puis de proposer une réponse améliorée.  \n",
    "   - Mise en œuvre en plusieurs appels (réponse initiale, re-demande d’analyse, ré-énoncé final).  \n",
    "   - Intéressant pour du code, des textes longs, ou des situations nécessitant un contrôle qualité.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9619dc4",
   "metadata": {
    "papermill": {
     "duration": 0.0058,
     "end_time": "2026-02-18T07:55:48.591089",
     "exception": false,
     "start_time": "2026-02-18T07:55:48.585289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 2 : Few-shot Prompting\n",
    "\n",
    "Le **few-shot prompting** consiste à fournir 2-3 exemples de la tâche souhaitée avant la vraie question.\n",
    "\n",
    "### Mécanisme d'apprentissage en contexte\n",
    "\n",
    "Le modèle :\n",
    "1. Analyse les exemples fournis\n",
    "2. Détecte le **pattern** (format, style, structure)\n",
    "3. Applique ce pattern à la nouvelle question\n",
    "\n",
    "C'est ce qu'on appelle **l'apprentissage en contexte** (in-context learning) : le modèle s'adapte sans modifier ses poids.\n",
    "\n",
    "### Premier exemple : Rédaction d'emails professionnels\n",
    "\n",
    "Nous allons guider le modèle à produire un email avec un format et un ton spécifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a60165",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:55:48.604130Z",
     "iopub.status.busy": "2026-02-18T07:55:48.603577Z",
     "iopub.status.idle": "2026-02-18T07:55:57.471447Z",
     "shell.execute_reply": "2026-02-18T07:55:57.470652Z"
    },
    "papermill": {
     "duration": 8.876095,
     "end_time": "2026-02-18T07:55:57.472866",
     "exception": false,
     "start_time": "2026-02-18T07:55:48.596771",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zero-shot Prompt ===\n",
      "Prompt: Donne-moi 3 idées de recettes végétariennes à base de tomates.\n",
      "\n",
      "Réponse du modèle :\n",
      "\n",
      "Bien sûr ! Voici trois idées de recettes végétariennes à base de tomates :\n",
      "\n",
      "### 1. **Tartines de tomates et mozzarella**\n",
      "**Ingrédients :**\n",
      "- Pain de campagne ou pain complet\n",
      "- Tomates mûres\n",
      "- Boules de mozzarella\n",
      "- Basilic frais\n",
      "- Huile d'olive\n",
      "- Sel et poivre\n",
      "\n",
      "**Instructions :**\n",
      "1. Préchauffez votre four à 200°C.\n",
      "2. Tranchez le pain et disposez les tranches sur une plaque de cuisson. Arrosez d'un filet d'huile d'olive.\n",
      "3. Enfournez pendant environ 5 à 7 minutes jusqu'à ce que le pain soit légèrement doré.\n",
      "4. Pendant ce temps, coupez les tomates et la mozzarella en tranches.\n",
      "5. Disposez les tranches de tomates et de mozzarella sur les tartines de pain, assaisonnez avec du sel et du poivre.\n",
      "6. Ajoutez quelques feuilles de basilic frais sur le dessus et un filet d'huile d'olive avant de servir.\n",
      "\n",
      "### 2. **Ratatouille**\n",
      "**Ingrédients :**\n",
      "- 2 grandes tomates\n",
      "- 1 courgette\n",
      "- 1 aubergine\n",
      "- 1 poivron (rouge ou jaune)\n",
      "- 1 oignon\n",
      "- 2 gousses d'ail\n",
      "- Herbes de Provence\n",
      "- Huile d'olive\n",
      "- Sel et poivre\n",
      "\n",
      "**Instructions :**\n",
      "1. Dans une grande poêle, faites chauffer de l'huile d'olive et ajoutez l'oignon émincé et l'ail haché. Faites revenir jusqu'à ce qu'ils soient translucides.\n",
      "2. Ajoutez l'aubergine coupée en dés, le poivron en dés et la courgette en rondelles. Faites cuire pendant environ 10 minutes.\n",
      "3. Incorporez les tomates coupées en dés et ass\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Zero-shot\n",
    "# ============================\n",
    "\n",
    "prompt_1 = \"Donne-moi 3 idées de recettes végétariennes à base de tomates.\"\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt_1}\n",
    "    ],\n",
    "    # Contrôle du style\n",
    "    max_completion_tokens=400,\n",
    "    temperature=0.7  # plus la température est haute, plus c'est créatif\n",
    ")\n",
    "\n",
    "print(\"=== Zero-shot Prompt ===\")\n",
    "print(f\"Prompt: {prompt_1}\\n\")\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_1.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7616a0d",
   "metadata": {
    "papermill": {
     "duration": 0.003065,
     "end_time": "2026-02-18T07:55:57.480223",
     "exception": false,
     "start_time": "2026-02-18T07:55:57.477158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Résultat du Zero-shot prompting\n",
    "\n",
    "Le zero-shot prompting est la technique la plus simple : **aucun exemple préalable**, juste une instruction directe.\n",
    "\n",
    "**Avantages** :\n",
    "- Rapide à mettre en œuvre\n",
    "- Fonctionne bien pour des tâches courantes (résumés, traductions, questions générales)\n",
    "- Économique en tokens\n",
    "\n",
    "**Limites** :\n",
    "- Moins précis sur des tâches complexes ou spécialisées\n",
    "- Le format de sortie peut être imprévisible\n",
    "- Nécessite des prompts très clairs et bien formulés\n",
    "\n",
    "Dans cet exemple, le modèle génère 3 recettes végétariennes à base de tomates sans aucun exemple préalable. La qualité dépend fortement de la clarté du prompt et de la capacité du modèle à comprendre le domaine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767fb79",
   "metadata": {
    "papermill": {
     "duration": 0.004246,
     "end_time": "2026-02-18T07:55:57.487527",
     "exception": false,
     "start_time": "2026-02-18T07:55:57.483281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 3 : Chain-of-thought (CoT)\n",
    "\n",
    "Le **Chain-of-thought** demande explicitement au modèle de détailler son raisonnement étape par étape.\n",
    "\n",
    "### Pourquoi le CoT fonctionne ?\n",
    "\n",
    "Les modèles de langage sont entraînés sur des textes où les raisonnements sont explicités. En demandant les étapes intermédiaires, on active ce pattern et on améliore la précision.\n",
    "\n",
    "### Applications du CoT\n",
    "\n",
    "| Domaine | Exemple |\n",
    "|---------|---------|\n",
    "| Mathématiques | Résolution d'équations, problèmes de mots |\n",
    "| Logique | Syllogismes, déductions |\n",
    "| Programmation | Debugging, conception d'algorithmes |\n",
    "| Analyse | Cas juridiques, diagnostics médicaux |\n",
    "\n",
    "### Exemple : Problème arithmétique simple\n",
    "\n",
    "Testons avec un calcul impliquant plusieurs étapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb36ea",
   "metadata": {
    "papermill": {
     "duration": 0.003425,
     "end_time": "2026-02-18T07:55:57.495021",
     "exception": false,
     "start_time": "2026-02-18T07:55:57.491596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 4 : Self-refine (Auto-amélioration)\n",
    "\n",
    "Le **Self-refine** exploite la capacité du modèle à critiquer et améliorer ses propres productions.\n",
    "\n",
    "### Processus en deux étapes\n",
    "\n",
    "1. **Génération initiale** : Produire une première version (potentiellement bugguée ou imparfaite)\n",
    "2. **Critique et correction** : Analyser la première version, identifier les problèmes, proposer une amélioration\n",
    "\n",
    "### Avantages du Self-refine\n",
    "\n",
    "- **Qualité supérieure** : Deux passes donnent généralement de meilleurs résultats\n",
    "- **Détection de bugs** : Le modèle peut repérer ses propres erreurs\n",
    "- **Amélioration itérative** : Peut être répété plusieurs fois si nécessaire\n",
    "\n",
    "### Compromis\n",
    "\n",
    "- **Coût** : Double les tokens consommés (deux appels API)\n",
    "- **Temps** : Latence multipliée\n",
    "- **À utiliser pour** : Code critique, documents importants, tâches complexes\n",
    "\n",
    "### Exemple : Code Python avec bug volontaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52374bc",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:55:57.504798Z",
     "iopub.status.busy": "2026-02-18T07:55:57.504095Z",
     "iopub.status.idle": "2026-02-18T07:56:00.639958Z",
     "shell.execute_reply": "2026-02-18T07:56:00.638948Z"
    },
    "papermill": {
     "duration": 3.142859,
     "end_time": "2026-02-18T07:56:00.641393",
     "exception": false,
     "start_time": "2026-02-18T07:55:57.498534",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exemple Few-shot (e-mail professionnel) ===\n",
      "Sujet: Changement de planning et invitation à une réunion de suivi\n",
      "\n",
      "Bonjour [Nom du Collaborateur],\n",
      "\n",
      "Je souhaite vous informer d'un changement dans notre planning concernant le projet [Nom du Projet]. En raison de [raison du changement], nous avons dû ajuster certaines échéances.\n",
      "\n",
      "Pour discuter de ces modifications et nous assurer que nous restons alignés, je vous invite à une réunion de suivi qui se tiendra le [date] à [heure] dans [lieu ou lien de visioconférence].\n",
      "\n",
      "Merci de me confirmer votre disponibilité. \n",
      "\n",
      "Cordialement,\n",
      "\n",
      "[Votre Nom]  \n",
      "[Votre Poste]  \n",
      "[Votre Entreprise]  \n",
      "[Votre Numéro de Téléphone]  \n",
      "[Votre Adresse E-mail]  \n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Few-shot supplémentaire\n",
    "# ============================\n",
    "\n",
    "few_shot_prompt_2 = \"\"\"\n",
    "Tu es un assistant spécialisé en rédaction d'e-mails professionnels.\n",
    "Voici quelques exemples de style :\n",
    "\n",
    "Exemple 1:\n",
    "Q: Rédige un e-mail pour informer un client d'un retard de livraison\n",
    "A: \n",
    "Sujet: Information concernant le retard de votre livraison\n",
    "\n",
    "Bonjour [Nom du Client],\n",
    "\n",
    "Nous tenions à vous informer que votre commande #1234 a pris du retard...\n",
    "[...suite du mail...]\n",
    "\n",
    "Exemple 2:\n",
    "Q: Envoie un e-mail de remerciement pour un entretien d'embauche\n",
    "A:\n",
    "Sujet: Remerciements suite à notre entretien\n",
    "\n",
    "Bonjour [Nom du Contact],\n",
    "\n",
    "Je tiens à vous remercier pour le temps que vous m'avez accordé...\n",
    "[...suite du mail...]\n",
    "\n",
    "Maintenant, voici ma demande:\n",
    "\n",
    "Q: Écris un e-mail pour informer un collaborateur d'un changement de planning et l'inviter à une réunion de suivi.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response_few_shot_2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,  # ex. \"gpt-4o-mini\"\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt_2}\n",
    "    ],\n",
    "    max_completion_tokens=300,\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "print(\"=== Exemple Few-shot (e-mail professionnel) ===\")\n",
    "print(response_few_shot_2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb1a44",
   "metadata": {
    "papermill": {
     "duration": 0.005006,
     "end_time": "2026-02-18T07:56:00.652969",
     "exception": false,
     "start_time": "2026-02-18T07:56:00.647963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Résultat du Few-shot : Format cohérent\n",
    "\n",
    "Le modèle a reproduit fidèlement la structure des exemples :\n",
    "\n",
    "1. **Sujet** : Clair et professionnel\n",
    "2. **Salutation** : Formule de politesse appropriée\n",
    "3. **Corps** : Structure logique (contexte → action → conclusion)\n",
    "4. **Signature** : Complète avec coordonnées\n",
    "\n",
    "**Observation clé** : Sans les exemples, le modèle aurait pu générer un email plus informel ou moins structuré. Le few-shot garantit la cohérence du format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6294e",
   "metadata": {
    "papermill": {
     "duration": 0.006292,
     "end_time": "2026-02-18T07:56:00.663324",
     "exception": false,
     "start_time": "2026-02-18T07:56:00.657032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse du Few-shot prompting\n",
    "\n",
    "Le few-shot prompting apporte une **amélioration significative** par rapport au zero-shot :\n",
    "\n",
    "**Mécanisme** :\n",
    "1. On fournit 2-3 exemples de la tâche souhaitée (paires question/réponse)\n",
    "2. Le modèle apprend le **pattern** et le **format** attendu\n",
    "3. Il applique ensuite ce pattern à la nouvelle question\n",
    "\n",
    "**Avantages observables** :\n",
    "- **Format cohérent** : Le modèle reproduit la structure des exemples (sujet, salutation, corps, signature)\n",
    "- **Ton approprié** : Le style professionnel est maintenu\n",
    "- **Contenu pertinent** : La réponse suit les conventions des exemples fournis\n",
    "\n",
    "**Quand utiliser Few-shot ?**\n",
    "- Tâches avec un format spécifique (emails, rapports, analyses structurées)\n",
    "- Cas où le zero-shot donne des résultats trop variables\n",
    "- Besoin de cohérence stylistique\n",
    "\n",
    "**Compromis** : Chaque exemple consomme des tokens supplémentaires, donc à utiliser avec modération pour des contextes très longs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4164e",
   "metadata": {
    "papermill": {
     "duration": 0.006859,
     "end_time": "2026-02-18T07:56:00.676766",
     "exception": false,
     "start_time": "2026-02-18T07:56:00.669907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 5 : Interactions conversationnelles\n",
    "\n",
    "Au-delà des techniques de prompting, la structure des conversations avec le modèle est cruciale pour des applications interactives.\n",
    "\n",
    "### Prompt simple sans mémoire\n",
    "\n",
    "La première cellule interactive montre un échange **stateless** (sans état) : chaque prompt est indépendant, le modèle n'a aucune mémoire des interactions précédentes.\n",
    "\n",
    "**Cas d'usage** :\n",
    "- Tests rapides d'un modèle\n",
    "- Questions indépendantes\n",
    "- Prototypage de prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff357bdc",
   "metadata": {
    "papermill": {
     "duration": 0.004486,
     "end_time": "2026-02-18T07:56:00.685831",
     "exception": false,
     "start_time": "2026-02-18T07:56:00.681345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Conversation avec mémoire\n",
    "\n",
    "La cellule suivante introduit un mécanisme de **mémoire de conversation** essentiel pour les chatbots et assistants.\n",
    "\n",
    "**Principe** :\n",
    "1. Accumuler les messages dans une liste `current_messages`\n",
    "2. À chaque tour, envoyer **tout l'historique** + le nouveau message\n",
    "3. Ajouter la réponse du modèle à l'historique\n",
    "\n",
    "**Structure des messages** :\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"Je m'appelle Alice\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Enchanté Alice !\"},\n",
    "    {\"role\": \"user\", \"content\": \"Comment je m'appelle ?\"},\n",
    "    # Le modèle peut répondre \"Vous vous appelez Alice\" grâce à l'historique\n",
    "]\n",
    "```\n",
    "\n",
    "**Attention** : L'historique consomme des tokens à chaque appel. Pour de longues conversations, il faut :\n",
    "- Tronquer les messages anciens\n",
    "- Résumer l'historique périodiquement\n",
    "- Utiliser des techniques de compression (embeddings, résumés automatiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2c7b0",
   "metadata": {
    "papermill": {
     "duration": 0.004815,
     "end_time": "2026-02-18T07:56:00.697825",
     "exception": false,
     "start_time": "2026-02-18T07:56:00.693010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Partie 6 : Modèles de raisonnement (2025)\n",
    "\n",
    "Les **modèles de raisonnement** (reasoning models) comme `o4-mini` et `gpt-5-thinking` représentent une évolution majeure des LLMs.\n",
    "\n",
    "### Différence fondamentale\n",
    "\n",
    "Les modèles chat génèrent token par token en temps réel. Les modèles de raisonnement :\n",
    "1. **Réfléchissent** en interne avant de répondre (pensées non visibles)\n",
    "2. **Explorent** plusieurs pistes de réflexion\n",
    "3. **Valident** leur raisonnement avant de produire la réponse finale\n",
    "\n",
    "### Paramètre clé : reasoning_effort\n",
    "\n",
    "Remplace le paramètre `temperature` pour les modèles de raisonnement :\n",
    "\n",
    "| Valeur | Durée | Cas d'usage |\n",
    "|--------|-------|-------------|\n",
    "| `low` | Rapide | Questions simples, prototypage |\n",
    "| `medium` | Modéré | Problèmes standards |\n",
    "| `high` | Lent | Problèmes complexes, mathématiques avancées |\n",
    "\n",
    "### Role \"developer\" obligatoire\n",
    "\n",
    "Les modèles de raisonnement n'utilisent plus le role `system`. Le role `developer` configure le comportement global, et peut activer/désactiver le formatage de la pensée interne.\n",
    "\n",
    "### Comparaison pratique\n",
    "\n",
    "Testons le même problème avec un modèle chat (gpt-4o-mini) et un modèle de raisonnement (o4-mini)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d1544b",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:56:00.708872Z",
     "iopub.status.busy": "2026-02-18T07:56:00.708375Z",
     "iopub.status.idle": "2026-02-18T07:56:01.157400Z",
     "shell.execute_reply": "2026-02-18T07:56:01.156238Z"
    },
    "papermill": {
     "duration": 0.456094,
     "end_time": "2026-02-18T07:56:01.158749",
     "exception": false,
     "start_time": "2026-02-18T07:56:00.702655",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chain-of-thought Prompt ===\n",
      "Réponse du modèle (avec raisonnement) :\n",
      "\n",
      "Alice a 3 pommes à la fin.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Chain-of-thought\n",
    "# ============================\n",
    "\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "Donne directement la réponse sans étape intermédiaire.\n",
    "\"\"\"\n",
    "\n",
    "# Explique ton raisonnement étape par étape, puis donne la réponse finale.\n",
    "response_3 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": cot_prompt}\n",
    "    ],\n",
    "    max_completion_tokens=200,\n",
    "    temperature=0.2  # on réduit la température pour moins de fantaisie\n",
    ")\n",
    "\n",
    "print(\"=== Chain-of-thought Prompt ===\")\n",
    "print(\"Réponse du modèle (avec raisonnement) :\\n\")\n",
    "print(response_3.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5eb015",
   "metadata": {
    "papermill": {
     "duration": 0.003174,
     "end_time": "2026-02-18T07:56:01.167570",
     "exception": false,
     "start_time": "2026-02-18T07:56:01.164396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse du résultat CoT\n",
    "\n",
    "**Observation** : Le prompt demandait une réponse directe SANS étapes intermédiaires, et le modèle a bien obéi en donnant simplement \"3 pommes\".\n",
    "\n",
    "**Expérimentation recommandée** : Modifiez le prompt pour demander explicitement le raisonnement :\n",
    "\n",
    "```python\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "Explique ton raisonnement étape par étape avant de donner la réponse finale.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Vous devriez alors obtenir :\n",
    "- Étape 1 : 5 - 2 = 3\n",
    "- Étape 2 : 3 - 1 = 2\n",
    "- Étape 3 : 2 + 1 = 3\n",
    "- **Réponse : 3 pommes**\n",
    "\n",
    "### Bonnes pratiques CoT\n",
    "\n",
    "1. **Temperature basse** (0.0-0.3) pour les calculs et la logique\n",
    "2. **Prompt explicite** : \"Explique ton raisonnement étape par étape\"\n",
    "3. **Vérification** : Le raisonnement explicite permet de détecter les erreurs\n",
    "4. **Masquage optionnel** : Pour les applications de production, on peut demander au modèle de ne pas révéler son raisonnement à l'utilisateur final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a6f88",
   "metadata": {
    "papermill": {
     "duration": 0.005014,
     "end_time": "2026-02-18T07:56:01.176649",
     "exception": false,
     "start_time": "2026-02-18T07:56:01.171635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du Chain-of-thought\n",
    "\n",
    "Le Chain-of-thought (CoT) est particulièrement efficace pour les **problèmes de raisonnement** :\n",
    "\n",
    "**Analyse du résultat** :\n",
    "Le modèle devrait avoir détaillé :\n",
    "1. État initial : Alice a 5 pommes\n",
    "2. Étape 1 : Elle en jette 2 → 5 - 2 = 3 pommes\n",
    "3. Étape 2 : Elle en donne 1 à Bob → 3 - 1 = 2 pommes\n",
    "4. Étape 3 : Bob lui rend 1 pomme → 2 + 1 = 3 pommes\n",
    "5. **Réponse finale : 3 pommes**\n",
    "\n",
    "**Pourquoi c'est important ?**\n",
    "- **Transparence** : On peut vérifier le raisonnement étape par étape\n",
    "- **Détection d'erreurs** : Si le résultat est faux, on peut identifier où le modèle s'est trompé\n",
    "- **Confiance** : Le raisonnement explicite augmente la crédibilité\n",
    "- **Debugging** : Facilite la correction du prompt si nécessaire\n",
    "\n",
    "**Applications** :\n",
    "- Calculs mathématiques\n",
    "- Raisonnement logique\n",
    "- Résolution de problèmes complexes\n",
    "- Analyse de cas juridiques ou médicaux\n",
    "\n",
    "**Note** : Temperature=0.2 garantit un raisonnement cohérent et reproductible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30edbb7c",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:56:01.184226Z",
     "iopub.status.busy": "2026-02-18T07:56:01.184032Z",
     "iopub.status.idle": "2026-02-18T07:56:05.673841Z",
     "shell.execute_reply": "2026-02-18T07:56:05.672933Z"
    },
    "papermill": {
     "duration": 4.495232,
     "end_time": "2026-02-18T07:56:05.674923",
     "exception": false,
     "start_time": "2026-02-18T07:56:01.179691",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (1) : Code buggy ===\n",
      "\n",
      "Voici une fonction Python qui calcule la somme d'une liste, avec un bug intentionnel :\n",
      "\n",
      "```python\n",
      "def somme_liste(ma_liste):\n",
      "    total = 0\n",
      "    for nombre in ma_liste:\n",
      "        total += nombre\n",
      "    return total\n",
      "\n",
      "# Bug intentionnel : On oublie de convertir les éléments en nombres, ce qui provoquera une erreur si la liste contient des chaînes de caractères\n",
      "liste_exemple = [1, 2, '3', 4]\n",
      "resultat = somme_liste(liste_exemple)\n",
      "print(resultat)\n",
      "```\n",
      "\n",
      "Dans ce code, le bug est que l'élément '3' (une chaîne de caractères) dans `liste_exemple` va provoquer une erreur lors de l'addition, car on essaie d'additionner une chaîne avec des entiers.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8a : Self-refine Step 1\n",
    "# ============================\n",
    "\n",
    "prompt_sr1 = \"\"\"\n",
    "Ecris une courte fonction Python pour calculer la somme d'une liste. \n",
    "Ajoute un bug volontaire dans le code. \n",
    "\"\"\"\n",
    "\n",
    "response_sr1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr1}],\n",
    "    max_completion_tokens=300\n",
    ")\n",
    "\n",
    "buggy_code = response_sr1.choices[0].message.content\n",
    "\n",
    "print(\"=== Self-refine (1) : Code buggy ===\\n\")\n",
    "print(buggy_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75b428",
   "metadata": {
    "papermill": {
     "duration": 0.003659,
     "end_time": "2026-02-18T07:56:05.682528",
     "exception": false,
     "start_time": "2026-02-18T07:56:05.678869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 8b. Self-critique et amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d54be4e",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:56:05.691690Z",
     "iopub.status.busy": "2026-02-18T07:56:05.691447Z",
     "iopub.status.idle": "2026-02-18T07:56:11.715834Z",
     "shell.execute_reply": "2026-02-18T07:56:11.714381Z"
    },
    "papermill": {
     "duration": 6.031207,
     "end_time": "2026-02-18T07:56:11.717705",
     "exception": false,
     "start_time": "2026-02-18T07:56:05.686498",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (2) : Correction ===\n",
      "\n",
      "Bien sûr ! Analysons le code que vous avez fourni.\n",
      "\n",
      "### Analyse du code\n",
      "\n",
      "La fonction `somme_liste` est censée calculer la somme des éléments d'une liste. Cependant, si la liste contient des chaînes de caractères (comme dans l'exemple avec `'3'`), cela provoquera une erreur de type lors de l'opération d'addition. En Python, vous ne pouvez pas additionner un entier et une chaîne de caractères, ce qui entraînera une exception `TypeError`.\n",
      "\n",
      "### Bug identifié\n",
      "\n",
      "Le bug se trouve dans la tentative d'addition d'éléments de types différents (entiers et chaînes de caractères). \n",
      "\n",
      "### Correctif\n",
      "\n",
      "Pour corriger ce bug, nous devons nous assurer que tous les éléments de la liste sont des nombres avant de les additionner. Nous pouvons le faire en convertissant chaque élément en entier (ou en flottant) si cela est possible. Si un élément ne peut pas être converti, nous pouvons choisir de l'ignorer ou de lever une exception.\n",
      "\n",
      "### Version améliorée du code\n",
      "\n",
      "Voici une version améliorée de la fonction qui gère les erreurs de conversion et ignore les éléments non numériques :\n",
      "\n",
      "```python\n",
      "def somme_liste(ma_liste):\n",
      "    total = 0\n",
      "    for element in ma_liste:\n",
      "        try:\n",
      "            # Convertir l'élément en nombre (entier ou flottant)\n",
      "            nombre = float(element)  # Utilisation de float pour gérer les nombres décimaux\n",
      "            total += nombre\n",
      "        except (ValueError, TypeError):\n",
      "            # Ignorer les éléments qui ne peuvent pas être convertis en nombre\n",
      "            print(f\"Élément ignoré : {element} (non convertible en nombre)\")\n",
      "    return total\n",
      "\n",
      "# Liste d'exemple avec un élément non numérique\n",
      "liste_exemple = [1, 2, '3', 4, 'abc', None]\n",
      "resultat = somme_liste(liste_exemple)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8b : Self-refine Step 2\n",
    "# ============================\n",
    "\n",
    "prompt_sr2 = f\"\"\"\n",
    "Voici un code Python qui contient un bug:\n",
    "\n",
    "{buggy_code}\n",
    "\n",
    "Peux-tu l'analyser, détecter le bug, proposer un correctif et une version améliorée du code ? \n",
    "Explique la correction.\n",
    "\"\"\"\n",
    "\n",
    "response_sr2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr2}],\n",
    "    max_completion_tokens=400,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine (2) : Correction ===\\n\")\n",
    "print(response_sr2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa422e",
   "metadata": {
    "papermill": {
     "duration": 0.007568,
     "end_time": "2026-02-18T07:56:11.733059",
     "exception": false,
     "start_time": "2026-02-18T07:56:11.725491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Résultat du Self-refine : Amélioration itérative\n",
    "\n",
    "Le modèle a :\n",
    "1. **Identifié le bug** : L'ajout de `+ 1` dans le `return` fausse le résultat\n",
    "2. **Proposé une correction** : Retirer le `+ 1`\n",
    "3. **Suggéré une amélioration** : Utiliser la fonction native `sum()` pour plus de simplicité\n",
    "\n",
    "**Enseignement** : Le modèle possède une capacité de **méta-cognition** - il peut raisonner sur ses propres productions et les améliorer.\n",
    "\n",
    "### Variante avancée : Self-refine avec role \"developer\"\n",
    "\n",
    "Au lieu d'un processus en deux appels API, on peut utiliser le rôle `developer` pour configurer le comportement d'auto-amélioration directement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c8a9c",
   "metadata": {
    "papermill": {
     "duration": 0.007115,
     "end_time": "2026-02-18T07:56:11.747190",
     "exception": false,
     "start_time": "2026-02-18T07:56:11.740075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Bilan du Self-refine\n",
    "\n",
    "Le processus Self-refine en deux étapes démontre la capacité du modèle à s'**auto-améliorer** :\n",
    "\n",
    "**Première étape** : Génération intentionnellement bugguée\n",
    "- Le modèle crée du code avec un bug volontaire (par exemple, division par zéro, mauvaise initialisation, etc.)\n",
    "\n",
    "**Deuxième étape** : Critique et correction\n",
    "- Le modèle analyse son propre code\n",
    "- Identifie le bug\n",
    "- Propose une version corrigée\n",
    "- Explique la nature du problème\n",
    "\n",
    "**Enseignements** :\n",
    "1. **Méta-cognition** : Le LLM peut raisonner sur ses propres productions\n",
    "2. **Amélioration itérative** : Chaque passe peut affiner la qualité\n",
    "3. **Détection de bugs** : Utile pour du code review automatisé\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Génération de code robuste (plusieurs passes de correction)\n",
    "- Rédaction de documents (brouillon → révision → version finale)\n",
    "- Traduction (première traduction → révision → amélioration)\n",
    "\n",
    "**Limite** : Chaque itération consomme des tokens et du temps. À utiliser pour des tâches critiques nécessitant haute qualité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b94379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T07:56:11.770983Z",
     "iopub.status.busy": "2026-02-18T07:56:11.770479Z",
     "iopub.status.idle": "2026-02-18T07:56:19.047473Z",
     "shell.execute_reply": "2026-02-18T07:56:19.045630Z"
    },
    "papermill": {
     "duration": 7.290099,
     "end_time": "2026-02-18T07:56:19.052182",
     "exception": false,
     "start_time": "2026-02-18T07:56:11.762083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine avec 'developer role' ===\n",
      "Voici une fonction Python qui calcule la factorielle d'un nombre entier. Je vais également m'assurer qu'il n'y a pas de bugs et que le code est optimisé.\n",
      "\n",
      "```python\n",
      "def factorielle(n):\n",
      "    \"\"\"Calcule la factorielle d'un nombre entier n.\"\"\"\n",
      "    if not isinstance(n, int):\n",
      "        raise ValueError(\"L'argument doit être un entier.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"La factorielle n'est pas définie pour les nombres négatifs.\")\n",
      "    \n",
      "    resultat = 1\n",
      "    for i in range(2, n + 1):\n",
      "        resultat *= i\n",
      "    return resultat\n",
      "```\n",
      "\n",
      "### Améliorations et vérifications :\n",
      "\n",
      "1. **Validation de l'entrée** : La fonction vérifie maintenant si l'argument est un entier et s'il est positif. Cela évite des erreurs lors de l'exécution.\n",
      "2. **Utilisation d'une boucle** : La méthode itérative est utilisée pour éviter les problèmes de récursion qui peuvent survenir avec des nombres très élevés.\n",
      "3. **Documentation** : J'ai ajouté une docstring pour expliquer ce que fait la fonction.\n",
      "\n",
      "### Test de la fonction\n",
      "\n",
      "Pour s'assurer que la fonction fonctionne correctement, voici quelques tests :\n",
      "\n",
      "```python\n",
      "print(factorielle(5))  # Devrait afficher 120\n",
      "print(factorielle(0))  # Devrait afficher 1\n",
      "print(factorielle(1))  # Devrait afficher\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Self-refine avec developer role\n",
    "# ============================\n",
    "\n",
    "messages_sr = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": (\n",
    "            \"You are a self-improving coding assistant. Whenever you provide code, \"\n",
    "            \"you will automatically search for potential bugs or improvements \"\n",
    "            \"and refine your output.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Écris une fonction Python qui calcule la factorielle d'un nombre entier. \"\n",
    "            \"Ensuite, relis-toi et corrige d'éventuels bugs.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "response_self_refine = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages_sr,\n",
    "    max_completion_tokens=300,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine avec 'developer role' ===\")\n",
    "print(response_self_refine.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad7e6c4",
   "metadata": {
    "papermill": {
     "duration": 0.01473,
     "end_time": "2026-02-18T07:56:19.080488",
     "exception": false,
     "start_time": "2026-02-18T07:56:19.065758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du Self-refine avec role \"developer\"\n",
    "\n",
    "Le rôle `developer` (introduit avec les modèles de raisonnement) permet de définir le comportement global du modèle, contrairement au rôle `system` qui était utilisé auparavant.\n",
    "\n",
    "**Différence clé** :\n",
    "- **Role \"system\"** (ancienne API) : Instructions générales, parfois ignorées par le modèle\n",
    "- **Role \"developer\"** (nouvelle API) : Instructions prioritaires, mieux respectées\n",
    "\n",
    "Dans cet exemple :\n",
    "- Le modèle génère la fonction factorielle\n",
    "- Il vérifie automatiquement les bugs potentiels\n",
    "- Il propose des améliorations (gestion d'erreurs, performance, alternatives)\n",
    "\n",
    "**Observation** : La fonction générée inclut directement :\n",
    "- Vérification du type (`isinstance`)\n",
    "- Gestion des nombres négatifs\n",
    "- Version alternative avec `math.factorial`\n",
    "\n",
    "C'est plus efficace que le Self-refine en deux passes, mais nécessite un modèle récent supportant le rôle `developer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe87023",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:56:19.095865Z",
     "iopub.status.busy": "2026-02-18T07:56:19.095570Z",
     "iopub.status.idle": "2026-02-18T07:56:20.387585Z",
     "shell.execute_reply": "2026-02-18T07:56:20.386777Z"
    },
    "papermill": {
     "duration": 1.300487,
     "end_time": "2026-02-18T07:56:20.389806",
     "exception": false,
     "start_time": "2026-02-18T07:56:19.089319",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] Prompt: Bonjour!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse: Bonjour ! Comment puis-je vous aider aujourd'hui ?\n",
      "\n",
      "[BATCH] Prompt: Quelle est la capitale de la France?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse: La capitale de la France est Paris.\n",
      "\n",
      "Mode batch terminé.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif\n",
    "# ============================\n",
    "\n",
    "# En mode batch (BATCH_MODE=true dans .env), cette cellule utilise des exemples prédéfinis\n",
    "# En mode interactif, elle permet de tester le modèle en boucle\n",
    "\n",
    "if BATCH_MODE:\n",
    "    # Mode batch: exécuter des exemples simples\n",
    "    test_prompts = [\"Bonjour!\", \"Quelle est la capitale de la France?\"]\n",
    "    for prompt in test_prompts:\n",
    "        print(f\"[BATCH] Prompt: {prompt}\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            max_completion_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        print(f\"Réponse: {resp.choices[0].message.content}\\n\")\n",
    "    print(\"Mode batch terminé.\")\n",
    "else:\n",
    "    while True:\n",
    "        user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Fin de l'interaction.\")\n",
    "            break\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_completion_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== Réponse du modèle ===\")\n",
    "        print(resp.choices[0].message.content)\n",
    "        print(\"---------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9baccb",
   "metadata": {
    "papermill": {
     "duration": 0.005388,
     "end_time": "2026-02-18T07:56:20.402639",
     "exception": false,
     "start_time": "2026-02-18T07:56:20.397251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation des tests interactifs\n",
    "\n",
    "Cette cellule démontre le **mode batch** pour tester le modèle sans interaction manuelle.\n",
    "\n",
    "**Résultats observés** :\n",
    "- **Prompt simple** : \"Bonjour!\" → Réponse polie et ouverture de dialogue\n",
    "- **Question factuelle** : \"Quelle est la capitale de la France?\" → Réponse directe et précise\n",
    "\n",
    "**Points clés** :\n",
    "1. **Température 0.7** : Équilibre entre cohérence et créativité\n",
    "2. **Max tokens 200** : Limite les réponses pour réduire les coûts\n",
    "3. **Messages stateless** : Chaque prompt est indépendant, pas de mémoire entre appels\n",
    "\n",
    "**Différence avec la cellule suivante** : La prochaine cellule introduit la **mémoire de conversation** (accumulation de l'historique), essentielle pour les chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd01567c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T07:56:20.411642Z",
     "iopub.status.busy": "2026-02-18T07:56:20.411173Z",
     "iopub.status.idle": "2026-02-18T07:56:22.126311Z",
     "shell.execute_reply": "2026-02-18T07:56:22.125709Z"
    },
    "papermill": {
     "duration": 1.720917,
     "end_time": "2026-02-18T07:56:22.127091",
     "exception": false,
     "start_time": "2026-02-18T07:56:20.406174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] User: Je m'appelle Alice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Bonjour Alice ! Comment puis-je vous aider aujourd'hui ?\n",
      "\n",
      "[BATCH] User: Comment je m'appelle?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Vous vous appelez Alice. Comment puis-je vous aider aujourd'hui ?\n",
      "\n",
      "Mode batch (mémoire) terminé.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif avec mémoire de chat\n",
    "# ============================\n",
    "\n",
    "# En mode batch, cette cellule utilise un dialogue prédéfini\n",
    "# En mode interactif, elle permet un échange avec mémoire\n",
    "\n",
    "if BATCH_MODE:\n",
    "    # Mode batch: simuler une conversation avec mémoire\n",
    "    batch_conversation = [\n",
    "        \"Je m'appelle Alice\",\n",
    "        \"Comment je m'appelle?\",\n",
    "    ]\n",
    "    current_messages = []\n",
    "\n",
    "    for user_input in batch_conversation:\n",
    "        print(f\"[BATCH] User: {user_input}\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=current_messages + [{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_completion_tokens=200,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        assistant_message = resp.choices[0].message.content\n",
    "        print(f\"Assistant: {assistant_message}\\n\")\n",
    "        current_messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "        current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})\n",
    "\n",
    "    print(\"Mode batch (mémoire) terminé.\")\n",
    "else:\n",
    "    user_input = \"\"\n",
    "    current_messages = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Fin de l'interaction.\")\n",
    "            break\n",
    "        print(\"\\n=== message de l'utilisateur ===\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages = current_messages + [{\"role\":\"user\",\"content\":user_input}],\n",
    "            max_completion_tokens=200,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== Réponse du modèle ===\")\n",
    "        assistant_message = resp.choices[0].message.content\n",
    "        print(assistant_message)\n",
    "        print(\"---------------------------------------------------\\n\")\n",
    "        current_messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "        current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1caa5f9",
   "metadata": {
    "papermill": {
     "duration": 0.003352,
     "end_time": "2026-02-18T07:56:22.133985",
     "exception": false,
     "start_time": "2026-02-18T07:56:22.130633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation de la conversation avec mémoire\n",
    "\n",
    "Cette cellule démontre la **mémoire conversationnelle** - le modèle se souvient du contexte précédent.\n",
    "\n",
    "**Résultats observés** :\n",
    "1. **Tour 1** : \"Je m'appelle Alice\" → Le modèle enregistre cette information dans `current_messages`\n",
    "2. **Tour 2** : \"Comment je m'appelle?\" → Le modèle répond \"Vous vous appelez Alice\" grâce à l'historique\n",
    "\n",
    "**Mécanisme technique** :\n",
    "```python\n",
    "current_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Je m'appelle Alice\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Bonjour, Alice...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Comment je m'appelle?\"},\n",
    "    # Le modèle peut répondre grâce à l'historique complet\n",
    "]\n",
    "```\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Chatbots conversationnels\n",
    "- Assistants personnels\n",
    "- Support client automatisé\n",
    "\n",
    "**Gestion de l'historique** :\n",
    "- **Court terme** : Garder tout l'historique (comme ici)\n",
    "- **Long terme** : Tronquer les messages anciens ou résumer périodiquement\n",
    "- **Tokens** : Attention au coût - chaque tour envoie TOUT l'historique à l'API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35585119",
   "metadata": {
    "papermill": {
     "duration": 0.003268,
     "end_time": "2026-02-18T07:56:22.140495",
     "exception": false,
     "start_time": "2026-02-18T07:56:22.137227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prompting pour Modèles de Raisonnement (2025)\n",
    "\n",
    "Les modèles de raisonnement (o4-mini, gpt-5-thinking) représentent une évolution majeure. Contrairement aux modèles de chat, ils prennent le temps de \"réfléchir\" avant de répondre.\n",
    "\n",
    "## Différences clés avec les modèles chat\n",
    "\n",
    "| Aspect | Modèles Chat (gpt-4o) | Modèles Raisonnement (o4-mini) |\n",
    "|--------|----------------------|-------------------------------|\n",
    "| Temps de réponse | Rapide | Plus lent (réflexion) |\n",
    "| Prompts | Détaillés, structurés | **Simples et directs** |\n",
    "| Chain-of-thought | Demandé explicitement | **Intégré nativement** |\n",
    "| Messages | system, user, assistant | **developer**, user, assistant |\n",
    "| Paramètre spécial | temperature | **reasoning_effort** |\n",
    "\n",
    "## Règle d'or : Simplifier les prompts\n",
    "\n",
    "**Pour les modèles de raisonnement, des prompts simples fonctionnent mieux !**\n",
    "\n",
    "Les modèles raisonnants sont capables de :\n",
    "- Comprendre l'intention sans instructions détaillées\n",
    "- Gérer les ambiguïtés intelligemment\n",
    "- Demander des clarifications si nécessaire\n",
    "\n",
    "**Éviter** : \"Analyse ce problème en détaillant chaque étape de ton raisonnement...\"\n",
    "**Préférer** : \"Résous ce problème.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbc4a0e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T07:56:22.147912Z",
     "iopub.status.busy": "2026-02-18T07:56:22.147591Z",
     "iopub.status.idle": "2026-02-18T07:56:32.062854Z",
     "shell.execute_reply": "2026-02-18T07:56:32.062029Z"
    },
    "papermill": {
     "duration": 9.920403,
     "end_time": "2026-02-18T07:56:32.063986",
     "exception": false,
     "start_time": "2026-02-18T07:56:22.143583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gpt-4o-mini (Chat Model) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps: 4.67s\n",
      "Le fermier peut transporter le loup, la chèvre et le chou de l'autre côté de la rivière en suivant ces étapes :\n",
      "\n",
      "1. Le fermier prend la chèvre et la transporte de l'autre côté de la rivière.\n",
      "2. Il laisse la chèvre de l'autre côté et retourne seul.\n",
      "3. Il prend le loup et le transporte de l'autre côté.\n",
      "4. Il laisse le loup de l'autre côté, mais il prend la chèvre avec lui pour la ramener de l'autre ...\n",
      "\n",
      "=== o4-mini (Reasoning Model) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps: 5.24s\n",
      "Voici une façon de procéder en 7 déplacements :\n",
      "\n",
      "1. Le fermier prend la chèvre et la traverse sur l’autre rive.  \n",
      "   Rive de départ : loup + chou  \n",
      "   Rive d’arrivée : chèvre\n",
      "\n",
      "2. Il revient seul à la rive de départ.  \n",
      "   Rive de départ : loup + chou  \n",
      "   Rive d’arrivée : chèvre\n",
      "\n",
      "3. Il prend le loup et le traverse sur l’autre rive.  \n",
      "   Rive de départ : chou  \n",
      "   Rive d’arrivée : chèvre + loup\n",
      "\n",
      "4. ...\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Comparaison Chat vs Reasoning\n",
    "# ============================\n",
    "\n",
    "import time\n",
    "\n",
    "probleme_complexe = \"\"\"\n",
    "Un fermier veut traverser une rivière avec un loup, une chèvre et un chou.\n",
    "Son bateau ne peut transporter que lui et un objet à la fois.\n",
    "Si le loup est laissé seul avec la chèvre, il la mange.\n",
    "Si la chèvre est laissée seule avec le chou, elle le mange.\n",
    "Comment le fermier peut-il tout transporter de l'autre côté?\n",
    "\"\"\"\n",
    "\n",
    "# Test avec gpt-4o-mini (chat model)\n",
    "print(\"=== gpt-4o-mini (Chat Model) ===\")\n",
    "start = time.time()\n",
    "response_chat = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": probleme_complexe}],\n",
    "    max_completion_tokens=500,\n",
    "    temperature=0.2\n",
    ")\n",
    "print(f\"Temps: {time.time() - start:.2f}s\")\n",
    "print(response_chat.choices[0].message.content[:400] + \"...\")\n",
    "\n",
    "# Test avec o4-mini (reasoning model) - si disponible\n",
    "print(\"\\n=== o4-mini (Reasoning Model) ===\")\n",
    "try:\n",
    "    start = time.time()\n",
    "    response_reason = client.chat.completions.create(\n",
    "        model=\"o4-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "            {\"role\": \"user\", \"content\": probleme_complexe}\n",
    "        ],\n",
    "        reasoning_effort=\"medium\"\n",
    "    )\n",
    "    print(f\"Temps: {time.time() - start:.2f}s\")\n",
    "    print(response_reason.choices[0].message.content[:400] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"o4-mini non disponible: {type(e).__name__}\")\n",
    "    print(\"Les modèles de raisonnement nécessitent un accès spécifique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e7835",
   "metadata": {
    "papermill": {
     "duration": 0.003929,
     "end_time": "2026-02-18T07:56:32.072749",
     "exception": false,
     "start_time": "2026-02-18T07:56:32.068820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse des résultats : Chat vs Reasoning\n",
    "\n",
    "**Observations attendues** :\n",
    "\n",
    "1. **Temps de réponse** :\n",
    "   - Chat (gpt-4o-mini) : ~2-5 secondes\n",
    "   - Reasoning (o4-mini) : ~10-30 secondes (réflexion interne)\n",
    "\n",
    "2. **Qualité de la solution** :\n",
    "   - Chat : Solution généralement correcte, mais peut manquer des optimisations\n",
    "   - Reasoning : Solution optimale, avec explication détaillée\n",
    "\n",
    "3. **Robustesse** :\n",
    "   - Chat : Peut parfois donner des solutions incorrectes sur des variantes complexes\n",
    "   - Reasoning : Vérifie sa solution avant de répondre, taux d'erreur plus faible\n",
    "\n",
    "### Quand utiliser les modèles de raisonnement ?\n",
    "\n",
    "**Préférer Chat** (gpt-4o, gpt-4o-mini) :\n",
    "- Applications temps réel (chatbots)\n",
    "- Tâches simples et courantes\n",
    "- Budget temps/coût limité\n",
    "\n",
    "**Préférer Reasoning** (o4-mini, gpt-5-thinking) :\n",
    "- Mathématiques avancées\n",
    "- Problèmes de logique complexes\n",
    "- Code critique nécessitant validation\n",
    "- Analyse approfondie\n",
    "\n",
    "**Note** : Les modèles de raisonnement sont plus coûteux en tokens et en temps, mais offrent une qualité supérieure pour les tâches complexes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 48.845858,
   "end_time": "2026-02-18T07:56:32.440939",
   "environment_variables": {},
   "exception": null,
   "input_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Texte\\2_PromptEngineering.ipynb",
   "output_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Texte\\2_PromptEngineering_output.ipynb",
   "parameters": {
    "BATCH_MODE": "true"
   },
   "start_time": "2026-02-18T07:55:43.595081",
   "version": "2.6.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}