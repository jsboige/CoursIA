{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction à l'IA générative avec l'API OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan global du Notebook\n",
    "\n",
    "1. **Introduction Générale**  \n",
    "   - Contexte et définitions (IA Générative, LLMs, prompts, etc.)\n",
    "   - Présentation succincte des enjeux éthiques et de responsabilité\n",
    "\n",
    "2. **Premier Exemple de Code : Vérification de l'Environnement**  \n",
    "   - Installation et importation des bibliothèques (OpenAI ou Azure OpenAI)\n",
    "   - Test d’un prompt de base\n",
    "\n",
    "3. **Notions de Base sur les Prompts**  \n",
    "   - Tokens, embeddings, bases de la génération\n",
    "   - Exercices pratiques : tokenisation et génération\n",
    "\n",
    "4. **Exemple : Fabrications (Hallucinations) et Fiabilité**  \n",
    "   - Démonstration via un prompt volontairement ambigu\n",
    "   - Discussion sur la vérification des faits\n",
    "\n",
    "5. **Conclusion**  \n",
    "   - Récapitulatif des points abordés\n",
    "   - Proposition d’activité (rédaction d’une synthèse ou d’une extension)\n",
    "\n",
    "---\n",
    "\n",
    "# Introduction à l'IA Générative\n",
    "\n",
    "Dans ce notebook, nous explorerons les fondamentaux de l’Intelligence Artificielle Générative :\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre ce qu’est l’IA Générative (texte, images, audio…)\n",
    "- Découvrir le fonctionnement des modèles de langage de grande taille (LLMs)\n",
    "- Mettre en pratique des expérimentations simples avec les prompts\n",
    "- Aborder les questions de fiabilité (hallucinations) et d’éthique\n",
    "\n",
    "## Qu’est-ce que l’IA Générative ?\n",
    "\n",
    "L’IA générative est une branche de l’apprentissage automatique qui **génère** de nouveaux contenus (texte, image, audio, code…) à partir de modèles probabilistes entraînés sur de vastes ensembles de données. Les modèles les plus avancés, appelés **Large Language Models** (LLMs), utilisent des architectures de type **Transformer** pour prédire et générer des séquences.\n",
    "\n",
    "Exemples concrets :\n",
    "- **ChatGPT** (OpenAI) : génération et compréhension de texte\n",
    "- **Stable Diffusion** : génération d’images\n",
    "- **Audiocraft** et **Whisper** : génération et transcription audio\n",
    "- **Copilot** (GitHub) : génération de code et assistance à la programmation\n",
    "\n",
    "## Enjeux et Limites\n",
    "\n",
    "- **Hallucinations / Fabrications** : Le modèle peut générer des réponses inventées ou inexactes, même si elles semblent plausibles.\n",
    "- **Biais** : Les réponses peuvent refléter des biais présents dans les données d’entraînement.\n",
    "- **Coût Énergétique** : L’entraînement de grands modèles consomme beaucoup de ressources.\n",
    "- **Régulation et Éthique** : Confidentialité, respect des lois et usage responsable de la technologie.\n",
    "\n",
    "Nous allons maintenant configurer l'environnement et réaliser un premier test de prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": "# ==========================================\n# Cellule 2 : Installation et Configuration\n# ==========================================\n\n# Installation des packages nécessaires (à lancer uniquement si non déjà installés)\n%pip install openai tiktoken python-dotenv\n\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\n# Charger la configuration depuis le fichier .env (dans le répertoire parent GenAI/)\nload_dotenv('../.env')\n\n# Initialiser le client OpenAI (détecte automatiquement OPENAI_API_KEY)\nclient = OpenAI()\n\nprint(\"Client OpenAI initialisé avec succès (syntaxe moderne) !\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu'est-ce qu'un prompt ?\n",
    "\n",
    "Dans le contexte des LLMs (Large Language Models), un **prompt** est la consigne initiale que l’on fournit au modèle. Il peut s'agir d'une question, d'une instruction, d'un texte partiel, voire d'un exemple de conversation. Le prompt influe directement sur la qualité de la réponse générée.\n",
    "\n",
    "### Chat Completions vs. Completions\n",
    "\n",
    "OpenAI propose principalement deux approches pour générer du texte :\n",
    "\n",
    "1. **Completions API** (versions historiques)  \n",
    "   - On envoie un simple prompt (ex.: `text-davinci-003`) et on récupère un texte.  \n",
    "   - Peu pratique pour les dialogues complexes, car il faut manuellement gérer l’historique de la conversation.\n",
    "\n",
    "2. **Chat Completions API** (recommandée)  \n",
    "   - On structure le prompt en plusieurs messages avec des rôles (`system`, `user`, `assistant`, etc.).  \n",
    "   - Permet des conversations plus riches (mémoire de conversation, enchaînements de tours) et un meilleur contrôle du style.  \n",
    "\n",
    "Dans ce notebook, nous utilisons principalement la **Chat Completions API** via `client.chat.completions.create()` (syntaxe moderne). \n",
    "[En savoir plus dans la documentation officielle](https://platform.openai.com/docs/api-reference/chat).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premier prompt\n",
    "\n",
    "La list des paramètres est accessible dans le [documentation officielle](https://platform.openai.com/docs/api-reference/chat/create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Premier Prompt\n",
    "# ============================\n",
    "\n",
    "# Exemple simple : Génération d'une courte phrase à partir d'un prompt de base\n",
    "# Utilisation de l'API Chat avec le modèle spécifié (gpt-4o-mini ou gpt-4)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Allons enfants de la Patrie, \"\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",  # Remplacer par 'gpt-4' si tu y as accès\n",
    "    max_tokens=50,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Interprétation du résultat\n\nLe modèle a complété le prompt « Allons enfants de la Patrie, » en continuant naturellement les paroles de **La Marseillaise**, l'hymne national français. Cela démontre plusieurs capacités importantes des LLMs :\n\n1. **Mémoire culturelle** : Le modèle a été entraîné sur de vastes corpus de textes et reconnaît des références culturelles célèbres.\n\n2. **Prédiction contextuelle** : À partir d'un début de phrase, le modèle génère la suite la plus probable statistiquement.\n\n3. **Limite de tokens** : Avec `max_tokens=50`, la génération s'arrête après environ 50 tokens, ce qui correspond à quelques phrases.\n\n**Paramètres clés utilisés** :\n- `temperature=0` : Génération déterministe, le modèle choisit toujours le token le plus probable (idéal pour des résultats cohérents et reproductibles)\n- `max_tokens=50` : Limite la longueur de la réponse\n- `model=\"gpt-4o-mini\"` : Version économique du modèle GPT-4",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Analyse de l'impact du message 'system'\n\nCette cellule illustre l'importance du **rôle 'system'** dans les conversations avec les LLMs :\n\n**Rôle des messages** :\n- **system** : Définit le comportement global et le style du modèle (instructions méta)\n- **user** : Les questions ou demandes de l'utilisateur\n- **assistant** : Les réponses précédentes du modèle (dans un contexte conversationnel)\n\nDans cet exemple, le message system « Tu es un assistant poétique qui répond toujours en haiku » transforme complètement la nature de la réponse. Au lieu d'une réponse explicative classique, le modèle produit un poème court de 3 vers (5-7-5 syllabes).\n\n**Applications pratiques** :\n- Chatbots avec personnalité spécifique (formel, humoristique, technique...)\n- Assistants spécialisés (code, marketing, éducation...)\n- Contrôle du ton et du format de sortie\n\n**Note** : `temperature=0.7` permet une certaine créativité tout en restant cohérent.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 4 (NOUVELLE) : Prompt avec un message 'system'\n",
    "# ============================\n",
    "\n",
    "# Ici, on utilise un message \"system\" pour donner une consigne globale sur le style du modèle.\n",
    "# Le message \"user\" reste notre question. Le modèle \"gpt-4o-mini\" sert d'exemple.\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Tu es un assistant poétique qui répond toujours en haiku. \"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Que penses-tu de la tour Eiffel ?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response_system = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"gpt-4o-mini\",  # ou gpt-4 si disponible\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"=== Réponse avec un message 'system' ===\")\n",
    "print(response_system.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Analyse comparative de la tokenisation\n\nLes résultats ci-dessus révèlent des **différences importantes** entre les tokenizers :\n\n**Observations clés** :\n1. **Nombre de tokens différent** : La même phrase peut être découpée en un nombre différent de tokens selon le modèle\n2. **Granularité variable** : Certains tokenizers créent des tokens plus fins (ex: espaces séparés), d'autres plus larges\n3. **Traitement des espaces** : Notez comment les espaces sont parfois inclus dans les tokens, parfois séparés\n\n**Implications pratiques** :\n- **Facturation** : Chaque appel API est facturé selon le nombre de tokens (input + output)\n- **Limites de contexte** : Chaque modèle a une fenêtre maximale (ex: 8k, 32k, 128k tokens)\n- **Optimisation** : Pour un même texte, le coût peut varier selon le modèle choisi\n\n**Exemple concret** :\nSi `text-davinci-003` utilise 5 tokens et `gpt-4o` en utilise 4 pour la même phrase, cela représente une économie de 20% sur le volume de tokens à traiter.\n\n**Recommandation** : Toujours vérifier la tokenisation avant d'envoyer de longs documents pour estimer le coût réel.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notions de Base : Tokenisation\n",
    "\n",
    "Lorsque nous envoyons un prompt à un LLM, le texte est converti en une séquence de **tokens**.  \n",
    "- Un **token** est généralement un morceau de mot, un caractère spécial ou un sous-mot.  \n",
    "- Chaque modèle possède sa propre manière de découper le texte, influençant le nombre total de tokens.  \n",
    "\n",
    "**Pourquoi c’est important ?**  \n",
    "- La facturation (ou la limitation) se base souvent sur le nombre total de tokens (entrée + sortie).  \n",
    "- Une requête trop longue peut dépasser la *context window* du modèle (limite de tokens cumulés).  \n",
    "\n",
    "> **Bonnes pratiques**  \n",
    "> - Surveiller la longueur du prompt pour limiter le coût et éviter les dépassements.  \n",
    "> - Utiliser des fonctions d’analyse (ex. `tiktoken`) pour **estimer** le nombre de tokens d’un texte avant l’envoi.  \n",
    "> - Tester divers modèles (`text-davinci-003`, `gpt-4`, `gpt-4o-mini`, etc.) car la tokenisation et le coût peuvent varier.\n",
    "\n",
    "Dans la bibliothèque `tiktoken` d'OpenAI, on peut directement encoder et décoder les tokens pour comprendre la segmentation.  \n",
    "Ci-dessous, un exemple détaillé de la façon dont la phrase « Oh say can you see » est découpée différemment selon le modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Analyse de la Tokenisation\n",
    "# ============================\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# --- Partie 1 : Utilisation de l'encodeur pour \"text-davinci-003\" ---\n",
    "encoder_td = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "texte = \"Oh say can you see\"\n",
    "tokens_td = encoder_td.encode(texte)\n",
    "print(\"Liste des tokens pour text-davinci-003 (indexes) :\\n\", tokens_td)\n",
    "\n",
    "# Décodage token par token\n",
    "decoded_td = [encoder_td.decode([t]) for t in tokens_td]\n",
    "print(\"\\nDécodage token par token (text-davinci-003) :\")\n",
    "for i, token in enumerate(decoded_td):\n",
    "    print(f\"Token {i}: '{token}'\")\n",
    "\n",
    "# --- Partie 2 : Utilisation de l'encodeur pour \"gpt-4o\" (ou gpt-4o-mini) ---\n",
    "encoder_gpt4 = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "tokens_gpt4 = encoder_gpt4.encode(texte)\n",
    "print(\"\\nListe des tokens pour gpt-4o (indexes) :\\n\", tokens_gpt4)\n",
    "\n",
    "decoded_gpt4 = [encoder_gpt4.decode([t]) for t in tokens_gpt4]\n",
    "print(\"\\nDécodage token par token (gpt-4o) :\")\n",
    "for i, token in enumerate(decoded_gpt4):\n",
    "    print(f\"Token {i}: '{token}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Analyse du phénomène d'hallucination\n\nLe résultat ci-dessus est très instructif sur les **limites des LLMs** :\n\n**Observation** : Le modèle a probablement généré une réponse plausible mais **totalement inventée**, car il n'y a eu aucune guerre sur Mars en 2076 (cet événement n'existe pas).\n\n**Pourquoi le modèle invente-t-il ?**\n1. **Architecture probabiliste** : Le modèle prédit les tokens les plus probables selon son entraînement, sans vérifier la véracité\n2. **Pression à répondre** : Par défaut, le LLM tente de fournir une réponse même en l'absence d'information\n3. **Cohérence narrative** : Le modèle génère des détails cohérents entre eux (noms de traités, dates, acteurs) qui renforcent l'illusion de véracité\n\n**Comment réduire les hallucinations ?**\n- **Prompt engineering** : « Si tu ne sais pas, dis \"Je ne sais pas\" »\n- **RAG (Retrieval Augmented Generation)** : Fournir des sources documentaires vérifiées\n- **Validation externe** : Vérifier les faits importants avec des sources fiables\n- **Temperature basse** : Réduire la créativité pour des tâches factuelles\n\n**Leçon importante** : Ne jamais faire confiance aveuglément aux réponses d'un LLM, surtout sur des faits historiques, médicaux, juridiques ou scientifiques. Toujours **vérifier les sources**.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice : Impact de la Température\n",
    "\n",
    "Testez différentes valeurs de température pour constater l'impact sur la créativité des réponses.\n",
    "\n",
    "1. Créez un nouveau prompt qui demande une courte histoire (par exemple : *“Raconte une histoire de 3 lignes sur un chat aventurier.”*).\n",
    "2. Réglez la température à 0.0, exécutez la cellule et notez la réponse.\n",
    "3. Réglez ensuite la température à 1.0 (ou même 1.2 si le modèle l’accepte), et comparez la différence de style ou de structure.\n",
    "\n",
    "> **Remarques :**\n",
    "> - Une température basse (~0.0) rend le modèle plus “strict”, proche d’une réponse déterministe.  \n",
    "> - Une température haute (~1.0 ou plus) favorise la créativité mais peut entraîner des réponses moins cohérentes ou plus fantaisistes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de Fabrication (ou \"Hallucination\")\n",
    "\n",
    "Les modèles de langage peuvent parfois générer des informations inventées ou inexactes.  \n",
    "Pour illustrer ce phénomène, nous allons utiliser un prompt ambigu ou factuellement erroné et observer la réponse du modèle.\n",
    "\n",
    "**Exemples de prompt :**\n",
    "- Décrire « la guerre de 2076 sur Mars »\n",
    "- Fournir des détails sur une loi imaginaire\n",
    "\n",
    "L'objectif est d'analyser comment le modèle invente des détails ou admet son ignorance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Test de Fabrication (Hallucination)\n",
    "# ============================\n",
    "\n",
    "prompt_fabrication = \"\"\"\n",
    "Décris-moi la célèbre guerre de 2076 sur la planète Mars :\n",
    "- Qui étaient les grandes puissances en conflit ?\n",
    "- Quels traités de paix ont été signés ?\n",
    "\"\"\"\n",
    "\n",
    "response_fabrication = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_fabrication}],\n",
    "    model=\"gpt-4o-mini\",  # Remplacer par le modèle souhaité\n",
    "    max_tokens=150,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_fabrication.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion de cette Introduction\n",
    "\n",
    "Nous avons couvert les points suivants :\n",
    "- Les bases de l'IA générative et des LLMs\n",
    "- La configuration et l'appel rapide à un modèle via l'API OpenAI\n",
    "- Les notions de tokenisation et leur impact sur la génération\n",
    "- Un aperçu des « hallucinations » (fabrications) pouvant survenir dans les réponses du modèle\n",
    "\n",
    "## Pistes pour Aller Plus Loin\n",
    "\n",
    "- **Varier la température :** Expérimente avec différentes valeurs (0.0, 0.7, etc.) pour influencer la créativité des réponses.\n",
    "- **Explorer l'API Chat :** Utilise l'API de chat pour gérer des dialogues contextuels complexes plutôt que l'API `Completion`.\n",
    "- **Mise en place de la RAG :** Intègre une approche de Retrieval Augmented Generation pour limiter les hallucinations en fournissant des sources documentaires externes.\n",
    "- **Autres applications :** Essaie la traduction, la génération de code (similaire à Copilot) ou la création de contenu marketing.\n",
    "\n",
    "**Prochaines étapes dans le cours :**\n",
    "1. Approfondir le **prompt engineering** (structure des prompts, chaînes d'invocations, etc.).\n",
    "2. Explorer d’autres types de modèles génératifs (images, audio).\n",
    "3. Analyser en détail les **enjeux éthiques** (biais, usage responsable, confidentialité).\n",
    "\n",
    "\n",
    "### Liens utiles et bonnes pratiques\n",
    "\n",
    "1. [Documentation OpenAI](https://platform.openai.com/docs/) :  \n",
    "   - Consulter la section *chat completions* pour tous les paramètres disponibles (température, top_p, frequency_penalty, etc.).\n",
    "2. [Choisir un modèle adapté](/docs/models) :  \n",
    "   - **gpt-4o** pour des réponses plus “intelligentes” et détaillées,  \n",
    "   - **gpt-4o-mini** pour des réponses plus rapides et moins onéreuses.\n",
    "3. [Exemples officiels](/docs/examples) :  \n",
    "   - Vous y trouverez des prompts pour différents usages : résumé, traduction, JSON structuré, etc.\n",
    "4. [Prompt Engineering Guide](/docs/guides/prompt-engineering) :  \n",
    "   - Conseils avancés pour concevoir des prompts clairs et informatifs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Merci d'avoir suivi cette introduction au monde de l'IA générative !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}