{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction à l'IA générative avec l'API OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan global du Notebook\n",
    "\n",
    "1. **Introduction Générale**  \n",
    "   - Contexte et définitions (IA Générative, LLMs, prompts, etc.)\n",
    "   - Présentation succincte des enjeux éthiques et de responsabilité\n",
    "\n",
    "2. **Premier Exemple de Code : Vérification de l'Environnement**  \n",
    "   - Installation et importation des bibliothèques (OpenAI ou Azure OpenAI)\n",
    "   - Test d’un prompt de base\n",
    "\n",
    "3. **Notions de Base sur les Prompts**  \n",
    "   - Tokens, embeddings, bases de la génération\n",
    "   - Exercices pratiques : tokenisation et génération\n",
    "\n",
    "4. **Exemple : Fabrications (Hallucinations) et Fiabilité**  \n",
    "   - Démonstration via un prompt volontairement ambigu\n",
    "   - Discussion sur la vérification des faits\n",
    "\n",
    "5. **Conclusion**  \n",
    "   - Récapitulatif des points abordés\n",
    "   - Proposition d’activité (rédaction d’une synthèse ou d’une extension)\n",
    "\n",
    "---\n",
    "\n",
    "# Introduction à l'IA Générative\n",
    "\n",
    "Dans ce notebook, nous explorerons les fondamentaux de l’Intelligence Artificielle Générative :\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre ce qu’est l’IA Générative (texte, images, audio…)\n",
    "- Découvrir le fonctionnement des modèles de langage de grande taille (LLMs)\n",
    "- Mettre en pratique des expérimentations simples avec les prompts\n",
    "- Aborder les questions de fiabilité (hallucinations) et d’éthique\n",
    "\n",
    "## Qu’est-ce que l’IA Générative ?\n",
    "\n",
    "L’IA générative est une branche de l’apprentissage automatique qui **génère** de nouveaux contenus (texte, image, audio, code…) à partir de modèles probabilistes entraînés sur de vastes ensembles de données. Les modèles les plus avancés, appelés **Large Language Models** (LLMs), utilisent des architectures de type **Transformer** pour prédire et générer des séquences.\n",
    "\n",
    "Exemples concrets :\n",
    "- **ChatGPT** (OpenAI) : génération et compréhension de texte\n",
    "- **Stable Diffusion** : génération d’images\n",
    "- **Audiocraft** et **Whisper** : génération et transcription audio\n",
    "- **Copilot** (GitHub) : génération de code et assistance à la programmation\n",
    "\n",
    "## Enjeux et Limites\n",
    "\n",
    "- **Hallucinations / Fabrications** : Le modèle peut générer des réponses inventées ou inexactes, même si elles semblent plausibles.\n",
    "- **Biais** : Les réponses peuvent refléter des biais présents dans les données d’entraînement.\n",
    "- **Coût Énergétique** : L’entraînement de grands modèles consomme beaucoup de ressources.\n",
    "- **Régulation et Éthique** : Confidentialité, respect des lois et usage responsable de la technologie.\n",
    "\n",
    "Nous allons maintenant configurer l'environnement et réaliser un premier test de prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.109.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (0.11.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.11.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2025.9.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Client OpenAI initialisé avec succès (syntaxe moderne) !\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cellule 2 : Installation et Configuration\n",
    "# ==========================================\n",
    "\n",
    "# Installation des packages nécessaires (à lancer uniquement si non déjà installés)\n",
    "%pip install openai tiktoken python-dotenv\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger la configuration depuis le fichier .env (dans le répertoire parent GenAI/)\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Initialiser le client OpenAI (détecte automatiquement OPENAI_API_KEY)\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"Client OpenAI initialisé avec succès (syntaxe moderne) !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu'est-ce qu'un prompt ?\n",
    "\n",
    "Dans le contexte des LLMs (Large Language Models), un **prompt** est la consigne initiale que l’on fournit au modèle. Il peut s'agir d'une question, d'une instruction, d'un texte partiel, voire d'un exemple de conversation. Le prompt influe directement sur la qualité de la réponse générée.\n",
    "\n",
    "### Chat Completions vs. Completions\n",
    "\n",
    "OpenAI propose principalement deux approches pour générer du texte :\n",
    "\n",
    "1. **Completions API** (versions historiques)  \n",
    "   - On envoie un simple prompt (ex.: `text-davinci-003`) et on récupère un texte.  \n",
    "   - Peu pratique pour les dialogues complexes, car il faut manuellement gérer l’historique de la conversation.\n",
    "\n",
    "2. **Chat Completions API** (recommandée)  \n",
    "   - On structure le prompt en plusieurs messages avec des rôles (`system`, `user`, `assistant`, etc.).  \n",
    "   - Permet des conversations plus riches (mémoire de conversation, enchaînements de tours) et un meilleur contrôle du style.  \n",
    "\n",
    "Dans ce notebook, nous utilisons principalement la **Chat Completions API** via `client.chat.completions.create()` (syntaxe moderne). \n",
    "[En savoir plus dans la documentation officielle](https://platform.openai.com/docs/api-reference/chat).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premier prompt\n",
    "\n",
    "La list des paramètres est accessible dans le [documentation officielle](https://platform.openai.com/docs/api-reference/chat/create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": "# ============================\n# Cellule 3 : Premier Prompt\n# ============================\n\n# Exemple simple : Génération d'une courte phrase à partir d'un prompt de base\n# Utilisation de l'API Chat avec le modèle gpt-5-mini\n\nresponse = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Allons enfants de la Patrie, \"\n        }\n    ],\n    model=\"gpt-5-mini\"\n)\n\nprint(\"Réponse du modèle :\")\nprint(response.choices[0].message.content)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Interprétation du résultat\n\nLe modèle a complété le prompt « Allons enfants de la Patrie, » en continuant naturellement les paroles de **La Marseillaise**, l'hymne national français. Cela démontre plusieurs capacités importantes des LLMs :\n\n1. **Mémoire culturelle** : Le modèle a été entraîné sur de vastes corpus de textes et reconnaît des références culturelles célèbres.\n\n2. **Prédiction contextuelle** : À partir d'un début de phrase, le modèle génère la suite la plus probable statistiquement.\n\n3. **Génération naturelle** : Le modèle continue le texte de manière cohérente sans paramètres de limitation explicites.\n\n**Paramètres utilisés** :\n- `model=\"gpt-5-mini\"` : Version économique et rapide du modèle GPT-5\n- Pas de limitation de tokens : Le modèle génère jusqu'à ce qu'il considère la réponse complète"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de l'impact du message 'system'\n",
    "\n",
    "Cette cellule illustre l'importance du **rôle 'system'** dans les conversations avec les LLMs :\n",
    "\n",
    "**Rôle des messages** :\n",
    "- **system** : Définit le comportement global et le style du modèle (instructions méta)\n",
    "- **user** : Les questions ou demandes de l'utilisateur\n",
    "- **assistant** : Les réponses précédentes du modèle (dans un contexte conversationnel)\n",
    "\n",
    "Dans cet exemple, le message system « Tu es un assistant poétique qui répond toujours en haiku » transforme complètement la nature de la réponse. Au lieu d'une réponse explicative classique, le modèle produit un poème court de 3 vers (5-7-5 syllabes).\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Chatbots avec personnalité spécifique (formel, humoristique, technique...)\n",
    "- Assistants spécialisés (code, marketing, éducation...)\n",
    "- Contrôle du ton et du format de sortie\n",
    "\n",
    "**Note** : `temperature=0.7` permet une certaine créativité tout en restant cohérent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================\n# Cellule 4 (NOUVELLE) : Prompt avec un message 'system'\n# ============================\n\n# Ici, on utilise un message \"system\" pour donner une consigne globale sur le style du modèle.\n# Le message \"user\" reste notre question.\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"Tu es un assistant poétique qui répond toujours en haiku. \"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Que penses-tu de la tour Eiffel ?\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Fleur de fer dressée\\nParis tisse son ciel d'acier\\nÉtreint les nuages doux\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Que penses-tu de ton poème ?\"\n    }\n]\n\nresponse_system = client.chat.completions.create(\n    messages=messages,\n    model=\"gpt-5-mini\"\n)\n\nprint(\"=== Réponse avec un message 'system' ===\")\nprint(response_system.choices[0].message.content)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse comparative de la tokenisation\n",
    "\n",
    "Les résultats ci-dessus révèlent des **différences importantes** entre les tokenizers :\n",
    "\n",
    "**Observations clés** :\n",
    "1. **Nombre de tokens différent** : La même phrase peut être découpée en un nombre différent de tokens selon le modèle\n",
    "2. **Granularité variable** : Certains tokenizers créent des tokens plus fins (ex: espaces séparés), d'autres plus larges\n",
    "3. **Traitement des espaces** : Notez comment les espaces sont parfois inclus dans les tokens, parfois séparés\n",
    "\n",
    "**Implications pratiques** :\n",
    "- **Facturation** : Chaque appel API est facturé selon le nombre de tokens (input + output)\n",
    "- **Limites de contexte** : Chaque modèle a une fenêtre maximale (ex: 8k, 32k, 128k tokens)\n",
    "- **Optimisation** : Pour un même texte, le coût peut varier selon le modèle choisi\n",
    "\n",
    "**Exemple concret** :\n",
    "Si `text-davinci-003` utilise 5 tokens et `gpt-4o` en utilise 4 pour la même phrase, cela représente une économie de 20% sur le volume de tokens à traiter.\n",
    "\n",
    "**Recommandation** : Toujours vérifier la tokenisation avant d'envoyer de longs documents pour estimer le coût réel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notions de Base : Tokenisation\n",
    "\n",
    "Lorsque nous envoyons un prompt à un LLM, le texte est converti en une séquence de **tokens**.  \n",
    "- Un **token** est généralement un morceau de mot, un caractère spécial ou un sous-mot.  \n",
    "- Chaque modèle possède sa propre manière de découper le texte, influençant le nombre total de tokens.  \n",
    "\n",
    "**Pourquoi c’est important ?**  \n",
    "- La facturation (ou la limitation) se base souvent sur le nombre total de tokens (entrée + sortie).  \n",
    "- Une requête trop longue peut dépasser la *context window* du modèle (limite de tokens cumulés).  \n",
    "\n",
    "> **Bonnes pratiques**  \n",
    "> - Surveiller la longueur du prompt pour limiter le coût et éviter les dépassements.  \n",
    "> - Utiliser des fonctions d’analyse (ex. `tiktoken`) pour **estimer** le nombre de tokens d’un texte avant l’envoi.  \n",
    "> - Tester divers modèles (`text-davinci-003`, `gpt-4`, `gpt-4o-mini`, etc.) car la tokenisation et le coût peuvent varier.\n",
    "\n",
    "Dans la bibliothèque `tiktoken` d'OpenAI, on peut directement encoder et décoder les tokens pour comprendre la segmentation.  \n",
    "Ci-dessous, un exemple détaillé de la façon dont la phrase « Oh say can you see » est découpée différemment selon le modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens pour text-davinci-003 (indexes) :\n",
      " [5812, 910, 460, 345, 766]\n",
      "\n",
      "Décodage token par token (text-davinci-003) :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n",
      "\n",
      "Liste des tokens pour gpt-4o (indexes) :\n",
      " [18009, 2891, 665, 481, 1921]\n",
      "\n",
      "Décodage token par token (gpt-4o) :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n",
      "\n",
      "Liste des tokens pour gpt-5 (indexes) :\n",
      " [18009, 2891, 665, 481, 1921]\n",
      "\n",
      "Décodage token par token (gpt-5) :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Analyse de la Tokenisation\n",
    "# ============================\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# --- Partie 1 : Utilisation de l'encodeur pour \"text-davinci-003\" ---\n",
    "encoder_td = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "texte = \"Oh say can you see\"\n",
    "tokens_td = encoder_td.encode(texte)\n",
    "print(\"Liste des tokens pour text-davinci-003 (indexes) :\\n\", tokens_td)\n",
    "\n",
    "# Décodage token par token\n",
    "decoded_td = [encoder_td.decode([t]) for t in tokens_td]\n",
    "print(\"\\nDécodage token par token (text-davinci-003) :\")\n",
    "for i, token in enumerate(decoded_td):\n",
    "    print(f\"Token {i}: '{token}'\")\n",
    "\n",
    "# --- Partie 2 : Utilisation de l'encodeur pour \"gpt-4o\" (ou gpt-4o-mini) ---\n",
    "encoder_gpt4 = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "tokens_gpt4 = encoder_gpt4.encode(texte)\n",
    "print(\"\\nListe des tokens pour gpt-4o (indexes) :\\n\", tokens_gpt4)\n",
    "\n",
    "decoded_gpt4 = [encoder_gpt4.decode([t]) for t in tokens_gpt4]\n",
    "print(\"\\nDécodage token par token (gpt-4o) :\")\n",
    "for i, token in enumerate(decoded_gpt4):\n",
    "    print(f\"Token {i}: '{token}'\")\n",
    "    \n",
    "# --- Partie 3 : Utilisation de l'encodeur pour \"gpt-5\" (ou gpt-5-mini) ---\n",
    "encoder_gpt5 = tiktoken.encoding_for_model(\"gpt-5-mini\")\n",
    "tokens_gpt5 = encoder_gpt5.encode(texte)\n",
    "print(\"\\nListe des tokens pour gpt-5 (indexes) :\\n\", tokens_gpt5)\n",
    "\n",
    "decoded_gpt5 = [encoder_gpt5.decode([t]) for t in tokens_gpt5]\n",
    "print(\"\\nDécodage token par token (gpt-5) :\")\n",
    "for i, token in enumerate(decoded_gpt5):\n",
    "    print(f\"Token {i}: '{token}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du phénomène d'hallucination\n",
    "\n",
    "Le résultat ci-dessus est très instructif sur les **limites des LLMs** :\n",
    "\n",
    "**Observation** : Le modèle a probablement généré une réponse plausible mais **totalement inventée**, car il n'y a eu aucune guerre sur Mars en 2076 (cet événement n'existe pas).\n",
    "\n",
    "**Pourquoi le modèle invente-t-il ?**\n",
    "1. **Architecture probabiliste** : Le modèle prédit les tokens les plus probables selon son entraînement, sans vérifier la véracité\n",
    "2. **Pression à répondre** : Par défaut, le LLM tente de fournir une réponse même en l'absence d'information\n",
    "3. **Cohérence narrative** : Le modèle génère des détails cohérents entre eux (noms de traités, dates, acteurs) qui renforcent l'illusion de véracité\n",
    "\n",
    "**Comment réduire les hallucinations ?**\n",
    "- **Prompt engineering** : « Si tu ne sais pas, dis \"Je ne sais pas\" »\n",
    "- **RAG (Retrieval Augmented Generation)** : Fournir des sources documentaires vérifiées\n",
    "- **Validation externe** : Vérifier les faits importants avec des sources fiables\n",
    "- **Temperature basse** : Réduire la créativité pour des tâches factuelles\n",
    "\n",
    "**Leçon importante** : Ne jamais faire confiance aveuglément aux réponses d'un LLM, surtout sur des faits historiques, médicaux, juridiques ou scientifiques. Toujours **vérifier les sources**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice : Impact de la Température\n",
    "\n",
    "Testez différentes valeurs de température pour constater l'impact sur la créativité des réponses.\n",
    "\n",
    "1. Créez un nouveau prompt qui demande une courte histoire (par exemple : *“Raconte une histoire de 3 lignes sur un chat aventurier.”*).\n",
    "2. Réglez la température à 0.0, exécutez la cellule et notez la réponse.\n",
    "3. Réglez ensuite la température à 1.0 (ou même 1.2 si le modèle l’accepte), et comparez la différence de style ou de structure.\n",
    "\n",
    "> **Remarques :**\n",
    "> - Une température basse (~0.0) rend le modèle plus “strict”, proche d’une réponse déterministe.  \n",
    "> - Une température haute (~1.0 ou plus) favorise la créativité mais peut entraîner des réponses moins cohérentes ou plus fantaisistes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de Fabrication (ou \"Hallucination\")\n",
    "\n",
    "Les modèles de langage peuvent parfois générer des informations inventées ou inexactes.  \n",
    "Pour illustrer ce phénomène, nous allons utiliser un prompt ambigu ou factuellement erroné et observer la réponse du modèle.\n",
    "\n",
    "**Exemples de prompt :**\n",
    "- Décrire « la guerre de 2076 sur Mars »\n",
    "- Fournir des détails sur une loi imaginaire\n",
    "\n",
    "L'objectif est d'analyser comment le modèle invente des détails ou admet son ignorance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": "# ============================\n# Cellule 7 : Test de Fabrication (Hallucination)\n# ============================\n\nprompt_fabrication = \"\"\"\nTu es journaliste à la fin du 21è siècle, décris-moi la célèbre guerre de 2076 sur la planète Mars :\n- Qui étaient les grandes puissances en conflit ?\n- Quels traités de paix ont été signés ?\n\"\"\"\n\nresponse_fabrication = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": prompt_fabrication}],\n    model=\"gpt-5-mini\"\n)\n\nprint(\"Réponse du modèle :\\n\")\nprint(response_fabrication.choices[0].message.content)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responses API : La Nouvelle Génération (2025)\n",
    "\n",
    "OpenAI a introduit la **Responses API** comme nouvelle approche recommandée pour interagir avec les modèles. Elle offre plusieurs avantages par rapport à Chat Completions :\n",
    "\n",
    "**Avantages de la Responses API :**\n",
    "- **Persistance d'état** : Avec `store: true`, les réponses sont sauvegardées et peuvent être chaînées\n",
    "- **Meilleure utilisation du cache** : 40-80% d'économies sur les tokens répétés\n",
    "- **Boucle agentique** : Support natif pour les appels d'outils multiples\n",
    "- **Chaînage simplifié** : `previous_response_id` pour maintenir le contexte\n",
    "\n",
    "**Chat Completions vs Responses API :**\n",
    "\n",
    "| Aspect | Chat Completions | Responses API |\n",
    "|--------|------------------|---------------|\n",
    "| Syntaxe | `client.chat.completions.create()` | `client.responses.create()` |\n",
    "| État | Manuel (passer tous les messages) | Automatique avec `store: true` |\n",
    "| Cache | Basique | Optimisé (40-80% économies) |\n",
    "| Outils | Support | Support + boucle agentique |\n",
    "\n",
    "**Note** : Chat Completions reste supporté et fonctionnel. La Responses API est recommandée pour les nouveaux projets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================\n# Cellule : Exemple Responses API\n# ============================\n\n# Exemple avec la Responses API (nouvelle approche recommandée)\n# Note: Nécessite openai >= 1.50.0\n\ntry:\n    # Premier appel avec store=True pour activer la persistance\n    response1 = client.responses.create(\n        model=\"gpt-5-mini\",\n        store=True,\n        input=\"Je m'appelle Alice et j'aime la programmation Python.\"\n    )\n    \n    print(\"=== Responses API - Premier appel ===\")\n    print(f\"Response ID: {response1.id}\")\n    if response1.output:\n        print(f\"Contenu: {response1.output[0].content[:200]}...\")\n    \n    # Deuxième appel avec chaînage (le contexte est préservé)\n    response2 = client.responses.create(\n        model=\"gpt-5-mini\",\n        store=True,\n        previous_response_id=response1.id,\n        input=\"Quel est mon prénom et qu'est-ce que j'aime?\"\n    )\n    \n    print(\"\\n=== Responses API - Appel chaîné ===\")\n    print(f\"Response ID: {response2.id}\")\n    if response2.output:\n        print(f\"Contenu: {response2.output[0].content}\")\n    \n    print(\"\\n✅ La Responses API fonctionne correctement!\")\n    \nexcept AttributeError:\n    print(\"⚠️ La Responses API n'est pas disponible dans cette version d'openai.\")\n    print(\"   Installez la dernière version: pip install --upgrade openai\")\nexcept Exception as e:\n    print(f\"⚠️ Erreur: {type(e).__name__}: {e}\")\n    print(\"   La Responses API peut ne pas être disponible pour tous les comptes.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Conclusion de cette Introduction\n\nNous avons couvert les points suivants :\n- Les bases de l'IA générative et des LLMs\n- La configuration et l'appel rapide à un modèle via l'API OpenAI\n- Les notions de tokenisation et leur impact sur la génération\n- Un aperçu des « hallucinations » (fabrications) pouvant survenir dans les réponses du modèle\n\n## Pistes pour Aller Plus Loin\n\n- **Varier la température :** Expérimente avec différentes valeurs (0.0, 0.7, etc.) pour influencer la créativité des réponses.\n- **Explorer l'API Chat :** Utilise l'API de chat pour gérer des dialogues contextuels complexes plutôt que l'API `Completion`.\n- **Mise en place de la RAG :** Intègre une approche de Retrieval Augmented Generation pour limiter les hallucinations en fournissant des sources documentaires externes.\n- **Autres applications :** Essaie la traduction, la génération de code (similaire à Copilot) ou la création de contenu marketing.\n\n**Prochaines étapes dans le cours :**\n1. Approfondir le **prompt engineering** (structure des prompts, chaînes d'invocations, etc.).\n2. Explorer d'autres types de modèles génératifs (images, audio).\n3. Analyser en détail les **enjeux éthiques** (biais, usage responsable, confidentialité).\n\n\n### Liens utiles et bonnes pratiques\n\n1. [Documentation OpenAI](https://platform.openai.com/docs/) :  \n   - Consulter la section *chat completions* pour tous les paramètres disponibles (température, top_p, frequency_penalty, etc.).\n2. [Choisir un modèle adapté](/docs/models) :  \n   - **gpt-5-mini** pour des réponses rapides et économiques,  \n   - **gpt-5** pour des tâches plus complexes nécessitant plus de réflexion.\n3. [Exemples officiels](/docs/examples) :  \n   - Vous y trouverez des prompts pour différents usages : résumé, traduction, JSON structuré, etc.\n4. [Prompt Engineering Guide](/docs/guides/prompt-engineering) :  \n   - Conseils avancés pour concevoir des prompts clairs et informatifs.\n\n\n---\n\nMerci d'avoir suivi cette introduction au monde de l'IA générative !"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}