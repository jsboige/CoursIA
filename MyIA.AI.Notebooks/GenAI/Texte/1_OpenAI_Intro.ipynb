{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction à l'IA générative avec l'API OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan global du Notebook\n",
    "\n",
    "1. **Introduction Générale**  \n",
    "   - Contexte et définitions (IA Générative, LLMs, prompts, etc.)\n",
    "   - Présentation succincte des enjeux éthiques et de responsabilité\n",
    "\n",
    "2. **Premier Exemple de Code : Vérification de l'Environnement**  \n",
    "   - Installation et importation des bibliothèques (OpenAI ou Azure OpenAI)\n",
    "   - Test d’un prompt de base\n",
    "\n",
    "3. **Notions de Base sur les Prompts**  \n",
    "   - Tokens, embeddings, bases de la génération\n",
    "   - Exercices pratiques : tokenisation et génération\n",
    "\n",
    "4. **Exemple : Fabrications (Hallucinations) et Fiabilité**  \n",
    "   - Démonstration via un prompt volontairement ambigu\n",
    "   - Discussion sur la vérification des faits\n",
    "\n",
    "5. **Conclusion**  \n",
    "   - Récapitulatif des points abordés\n",
    "   - Proposition d’activité (rédaction d’une synthèse ou d’une extension)\n",
    "\n",
    "---\n",
    "\n",
    "# Introduction à l'IA Générative\n",
    "\n",
    "Dans ce notebook, nous explorerons les fondamentaux de l’Intelligence Artificielle Générative :\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre ce qu’est l’IA Générative (texte, images, audio…)\n",
    "- Découvrir le fonctionnement des modèles de langage de grande taille (LLMs)\n",
    "- Mettre en pratique des expérimentations simples avec les prompts\n",
    "- Aborder les questions de fiabilité (hallucinations) et d’éthique\n",
    "\n",
    "## Qu’est-ce que l’IA Générative ?\n",
    "\n",
    "L’IA générative est une branche de l’apprentissage automatique qui **génère** de nouveaux contenus (texte, image, audio, code…) à partir de modèles probabilistes entraînés sur de vastes ensembles de données. Les modèles les plus avancés, appelés **Large Language Models** (LLMs), utilisent des architectures de type **Transformer** pour prédire et générer des séquences.\n",
    "\n",
    "Exemples concrets :\n",
    "- **ChatGPT** (OpenAI) : génération et compréhension de texte\n",
    "- **Stable Diffusion** : génération d’images\n",
    "- **Audiocraft** et **Whisper** : génération et transcription audio\n",
    "- **Copilot** (GitHub) : génération de code et assistance à la programmation\n",
    "\n",
    "## Enjeux et Limites\n",
    "\n",
    "- **Hallucinations / Fabrications** : Le modèle peut générer des réponses inventées ou inexactes, même si elles semblent plausibles.\n",
    "- **Biais** : Les réponses peuvent refléter des biais présents dans les données d’entraînement.\n",
    "- **Coût Énergétique** : L’entraînement de grands modèles consomme beaucoup de ressources.\n",
    "- **Régulation et Éthique** : Confidentialité, respect des lois et usage responsable de la technologie.\n",
    "\n",
    "Nous allons maintenant configurer l'environnement et réaliser un premier test de prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.109.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (0.11.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.11.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2025.9.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Client OpenAI initialisé avec succès (syntaxe moderne) !\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cellule 2 : Installation et Configuration\n",
    "# ==========================================\n",
    "\n",
    "# Installation des packages nécessaires (à lancer uniquement si non déjà installés)\n",
    "%pip install openai tiktoken python-dotenv\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger la configuration depuis le fichier .env (dans le répertoire parent GenAI/)\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Initialiser le client OpenAI (détecte automatiquement OPENAI_API_KEY)\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"Client OpenAI initialisé avec succès (syntaxe moderne) !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu'est-ce qu'un prompt ?\n",
    "\n",
    "Dans le contexte des LLMs (Large Language Models), un **prompt** est la consigne initiale que l’on fournit au modèle. Il peut s'agir d'une question, d'une instruction, d'un texte partiel, voire d'un exemple de conversation. Le prompt influe directement sur la qualité de la réponse générée.\n",
    "\n",
    "### Chat Completions vs. Completions\n",
    "\n",
    "OpenAI propose principalement deux approches pour générer du texte :\n",
    "\n",
    "1. **Completions API** (versions historiques)  \n",
    "   - On envoie un simple prompt (ex.: `text-davinci-003`) et on récupère un texte.  \n",
    "   - Peu pratique pour les dialogues complexes, car il faut manuellement gérer l’historique de la conversation.\n",
    "\n",
    "2. **Chat Completions API** (recommandée)  \n",
    "   - On structure le prompt en plusieurs messages avec des rôles (`system`, `user`, `assistant`, etc.).  \n",
    "   - Permet des conversations plus riches (mémoire de conversation, enchaînements de tours) et un meilleur contrôle du style.  \n",
    "\n",
    "Dans ce notebook, nous utilisons principalement la **Chat Completions API** via `client.chat.completions.create()` (syntaxe moderne). \n",
    "[En savoir plus dans la documentation officielle](https://platform.openai.com/docs/api-reference/chat).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premier prompt\n",
    "\n",
    "La list des paramètres est accessible dans le [documentation officielle](https://platform.openai.com/docs/api-reference/chat/create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse du modèle :\n",
      "Vous venez d'entamer l'hymne national français, « La Marseillaise ». Voulez-vous que je :\n",
      "\n",
      "- continue et donne les paroles complètes,\n",
      "- fournisse une traduction en anglais,\n",
      "- explique l'historique et le sens des couplets,\n",
      "- ou autre chose ?\n",
      "\n",
      "Dites-moi ce que vous préférez.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Premier Prompt\n",
    "# ============================\n",
    "\n",
    "# Exemple simple : Génération d'une courte phrase à partir d'un prompt de base\n",
    "# Utilisation de l'API Chat avec le modèle spécifié (gpt-4o-mini ou gpt-4)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Allons enfants de la Patrie, \"\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-5-mini\", \n",
    "    # max_tokens=30,\n",
    "    # temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation du résultat\n",
    "\n",
    "Le modèle a complété le prompt « Allons enfants de la Patrie, » en continuant naturellement les paroles de **La Marseillaise**, l'hymne national français. Cela démontre plusieurs capacités importantes des LLMs :\n",
    "\n",
    "1. **Mémoire culturelle** : Le modèle a été entraîné sur de vastes corpus de textes et reconnaît des références culturelles célèbres.\n",
    "\n",
    "2. **Prédiction contextuelle** : À partir d'un début de phrase, le modèle génère la suite la plus probable statistiquement.\n",
    "\n",
    "3. **Limite de tokens** : Avec `max_tokens=50`, la génération s'arrête après environ 50 tokens, ce qui correspond à quelques phrases.\n",
    "\n",
    "**Paramètres clés utilisés** :\n",
    "- `temperature=0` : Génération déterministe, le modèle choisit toujours le token le plus probable (idéal pour des résultats cohérents et reproductibles)\n",
    "- `max_tokens=50` : Limite la longueur de la réponse\n",
    "- `model=\"gpt-4o-mini\"` : Version économique du modèle GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de l'impact du message 'system'\n",
    "\n",
    "Cette cellule illustre l'importance du **rôle 'system'** dans les conversations avec les LLMs :\n",
    "\n",
    "**Rôle des messages** :\n",
    "- **system** : Définit le comportement global et le style du modèle (instructions méta)\n",
    "- **user** : Les questions ou demandes de l'utilisateur\n",
    "- **assistant** : Les réponses précédentes du modèle (dans un contexte conversationnel)\n",
    "\n",
    "Dans cet exemple, le message system « Tu es un assistant poétique qui répond toujours en haiku » transforme complètement la nature de la réponse. Au lieu d'une réponse explicative classique, le modèle produit un poème court de 3 vers (5-7-5 syllabes).\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Chatbots avec personnalité spécifique (formel, humoristique, technique...)\n",
    "- Assistants spécialisés (code, marketing, éducation...)\n",
    "- Contrôle du ton et du format de sortie\n",
    "\n",
    "**Note** : `temperature=0.7` permet une certaine créativité tout en restant cohérent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Réponse avec un message 'system' ===\n",
      "Simple et sincère\n",
      "Écho de fer et de ciel\n",
      "À voix apaisée\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 4 (NOUVELLE) : Prompt avec un message 'system'\n",
    "# ============================\n",
    "\n",
    "# Ici, on utilise un message \"system\" pour donner une consigne globale sur le style du modèle.\n",
    "# Le message \"user\" reste notre question. Le modèle \"gpt-4o-mini\" sert d'exemple.\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Tu es un assistant poétique qui répond toujours en haiku. \"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Que penses-tu de la tour Eiffel ?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Fleur de fer dressée\\nParis tisse son ciel d'acier\\nÉtreint les nuages doux\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Que penses-tu de ton poème ?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response_system = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"gpt-5-mini\",  # ou gpt-4 si disponible\n",
    "    # max_tokens=200,\n",
    "    # temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"=== Réponse avec un message 'system' ===\")\n",
    "print(response_system.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse comparative de la tokenisation\n",
    "\n",
    "Les résultats ci-dessus révèlent des **différences importantes** entre les tokenizers :\n",
    "\n",
    "**Observations clés** :\n",
    "1. **Nombre de tokens différent** : La même phrase peut être découpée en un nombre différent de tokens selon le modèle\n",
    "2. **Granularité variable** : Certains tokenizers créent des tokens plus fins (ex: espaces séparés), d'autres plus larges\n",
    "3. **Traitement des espaces** : Notez comment les espaces sont parfois inclus dans les tokens, parfois séparés\n",
    "\n",
    "**Implications pratiques** :\n",
    "- **Facturation** : Chaque appel API est facturé selon le nombre de tokens (input + output)\n",
    "- **Limites de contexte** : Chaque modèle a une fenêtre maximale (ex: 8k, 32k, 128k tokens)\n",
    "- **Optimisation** : Pour un même texte, le coût peut varier selon le modèle choisi\n",
    "\n",
    "**Exemple concret** :\n",
    "Si `text-davinci-003` utilise 5 tokens et `gpt-4o` en utilise 4 pour la même phrase, cela représente une économie de 20% sur le volume de tokens à traiter.\n",
    "\n",
    "**Recommandation** : Toujours vérifier la tokenisation avant d'envoyer de longs documents pour estimer le coût réel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notions de Base : Tokenisation\n",
    "\n",
    "Lorsque nous envoyons un prompt à un LLM, le texte est converti en une séquence de **tokens**.  \n",
    "- Un **token** est généralement un morceau de mot, un caractère spécial ou un sous-mot.  \n",
    "- Chaque modèle possède sa propre manière de découper le texte, influençant le nombre total de tokens.  \n",
    "\n",
    "**Pourquoi c’est important ?**  \n",
    "- La facturation (ou la limitation) se base souvent sur le nombre total de tokens (entrée + sortie).  \n",
    "- Une requête trop longue peut dépasser la *context window* du modèle (limite de tokens cumulés).  \n",
    "\n",
    "> **Bonnes pratiques**  \n",
    "> - Surveiller la longueur du prompt pour limiter le coût et éviter les dépassements.  \n",
    "> - Utiliser des fonctions d’analyse (ex. `tiktoken`) pour **estimer** le nombre de tokens d’un texte avant l’envoi.  \n",
    "> - Tester divers modèles (`text-davinci-003`, `gpt-4`, `gpt-4o-mini`, etc.) car la tokenisation et le coût peuvent varier.\n",
    "\n",
    "Dans la bibliothèque `tiktoken` d'OpenAI, on peut directement encoder et décoder les tokens pour comprendre la segmentation.  \n",
    "Ci-dessous, un exemple détaillé de la façon dont la phrase « Oh say can you see » est découpée différemment selon le modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens pour text-davinci-003 (indexes) :\n",
      " [5812, 910, 460, 345, 766]\n",
      "\n",
      "Décodage token par token (text-davinci-003) :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n",
      "\n",
      "Liste des tokens pour gpt-4o (indexes) :\n",
      " [18009, 2891, 665, 481, 1921]\n",
      "\n",
      "Décodage token par token (gpt-4o) :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n",
      "\n",
      "Liste des tokens pour gpt-5 (indexes) :\n",
      " [18009, 2891, 665, 481, 1921]\n",
      "\n",
      "Décodage token par token (gpt-5) :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Analyse de la Tokenisation\n",
    "# ============================\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# --- Partie 1 : Utilisation de l'encodeur pour \"text-davinci-003\" ---\n",
    "encoder_td = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "texte = \"Oh say can you see\"\n",
    "tokens_td = encoder_td.encode(texte)\n",
    "print(\"Liste des tokens pour text-davinci-003 (indexes) :\\n\", tokens_td)\n",
    "\n",
    "# Décodage token par token\n",
    "decoded_td = [encoder_td.decode([t]) for t in tokens_td]\n",
    "print(\"\\nDécodage token par token (text-davinci-003) :\")\n",
    "for i, token in enumerate(decoded_td):\n",
    "    print(f\"Token {i}: '{token}'\")\n",
    "\n",
    "# --- Partie 2 : Utilisation de l'encodeur pour \"gpt-4o\" (ou gpt-4o-mini) ---\n",
    "encoder_gpt4 = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "tokens_gpt4 = encoder_gpt4.encode(texte)\n",
    "print(\"\\nListe des tokens pour gpt-4o (indexes) :\\n\", tokens_gpt4)\n",
    "\n",
    "decoded_gpt4 = [encoder_gpt4.decode([t]) for t in tokens_gpt4]\n",
    "print(\"\\nDécodage token par token (gpt-4o) :\")\n",
    "for i, token in enumerate(decoded_gpt4):\n",
    "    print(f\"Token {i}: '{token}'\")\n",
    "    \n",
    "# --- Partie 3 : Utilisation de l'encodeur pour \"gpt-5\" (ou gpt-5-mini) ---\n",
    "encoder_gpt5 = tiktoken.encoding_for_model(\"gpt-5-mini\")\n",
    "tokens_gpt5 = encoder_gpt5.encode(texte)\n",
    "print(\"\\nListe des tokens pour gpt-5 (indexes) :\\n\", tokens_gpt5)\n",
    "\n",
    "decoded_gpt5 = [encoder_gpt5.decode([t]) for t in tokens_gpt5]\n",
    "print(\"\\nDécodage token par token (gpt-5) :\")\n",
    "for i, token in enumerate(decoded_gpt5):\n",
    "    print(f\"Token {i}: '{token}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du phénomène d'hallucination\n",
    "\n",
    "Le résultat ci-dessus est très instructif sur les **limites des LLMs** :\n",
    "\n",
    "**Observation** : Le modèle a probablement généré une réponse plausible mais **totalement inventée**, car il n'y a eu aucune guerre sur Mars en 2076 (cet événement n'existe pas).\n",
    "\n",
    "**Pourquoi le modèle invente-t-il ?**\n",
    "1. **Architecture probabiliste** : Le modèle prédit les tokens les plus probables selon son entraînement, sans vérifier la véracité\n",
    "2. **Pression à répondre** : Par défaut, le LLM tente de fournir une réponse même en l'absence d'information\n",
    "3. **Cohérence narrative** : Le modèle génère des détails cohérents entre eux (noms de traités, dates, acteurs) qui renforcent l'illusion de véracité\n",
    "\n",
    "**Comment réduire les hallucinations ?**\n",
    "- **Prompt engineering** : « Si tu ne sais pas, dis \"Je ne sais pas\" »\n",
    "- **RAG (Retrieval Augmented Generation)** : Fournir des sources documentaires vérifiées\n",
    "- **Validation externe** : Vérifier les faits importants avec des sources fiables\n",
    "- **Temperature basse** : Réduire la créativité pour des tâches factuelles\n",
    "\n",
    "**Leçon importante** : Ne jamais faire confiance aveuglément aux réponses d'un LLM, surtout sur des faits historiques, médicaux, juridiques ou scientifiques. Toujours **vérifier les sources**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice : Impact de la Température\n",
    "\n",
    "Testez différentes valeurs de température pour constater l'impact sur la créativité des réponses.\n",
    "\n",
    "1. Créez un nouveau prompt qui demande une courte histoire (par exemple : *“Raconte une histoire de 3 lignes sur un chat aventurier.”*).\n",
    "2. Réglez la température à 0.0, exécutez la cellule et notez la réponse.\n",
    "3. Réglez ensuite la température à 1.0 (ou même 1.2 si le modèle l’accepte), et comparez la différence de style ou de structure.\n",
    "\n",
    "> **Remarques :**\n",
    "> - Une température basse (~0.0) rend le modèle plus “strict”, proche d’une réponse déterministe.  \n",
    "> - Une température haute (~1.0 ou plus) favorise la créativité mais peut entraîner des réponses moins cohérentes ou plus fantaisistes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de Fabrication (ou \"Hallucination\")\n",
    "\n",
    "Les modèles de langage peuvent parfois générer des informations inventées ou inexactes.  \n",
    "Pour illustrer ce phénomène, nous allons utiliser un prompt ambigu ou factuellement erroné et observer la réponse du modèle.\n",
    "\n",
    "**Exemples de prompt :**\n",
    "- Décrire « la guerre de 2076 sur Mars »\n",
    "- Fournir des détails sur une loi imaginaire\n",
    "\n",
    "L'objectif est d'analyser comment le modèle invente des détails ou admet son ignorance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse du modèle :\n",
      "\n",
      "En tant que journaliste à la fin du siècle, je garde encore en tête la guerre de 2076 sur Mars — un conflit qui a bouleversé non seulement la colonie martienne mais aussi la façon dont la Terre gère l'espace. Voici un bilan synthétique et factuel, tel qu'on l'enseigne aujourd'hui.\n",
      "\n",
      "Contexte rapide\n",
      "- La pression sur les ressources martiennes (eau souterraine, régolithes riches en métaux rares, sites propices à l'atterrissage et à l'expansion des dômes) avait créé des rivalités croissantes entre alliances terriennes et puissantes méga‑entreprises qui contrôlaient la logistique et les infrastructures sur Mars. Les colons, nombreux et désormais politiquement organisés, revendiquaient des droits propres face aux intérêts terrestres.\n",
      "- Les années 2060 ont vu une militarisation progressive de l'espace bas‑orbital et des capacités de projection terrestre (drones orbitalisés, frappes de précision depuis plateformes lunaires, guerre cyber‑physique sur systèmes de survie des habitats).\n",
      "\n",
      "Qui étaient les grandes puissances en conflit ?\n",
      "- Deux grandes coalitions terriennes rivales dominaient la scène politique et militaire terrestre et, par ricochet, martienne :\n",
      "  - L’Union Atlantique (UA) : coalition politico‑économique regroupant la majeure partie de l’ancien espace euro‑atlantique et ses alliés ; forte présence de sociétés d'ingénierie habitat‑minier domiciliées en Atlantique‑Nord.\n",
      "  - La Confédération Asio‑Pacifique (CAP) : bloc regroupant puissances d'Asie et du Pacifique, avec un important secteur public‑privé intégré à la chaîne logistique martienne.\n",
      "- Les entreprises transplanétaires : plusieurs méga‑entreprises (les plus connues à l'époque — Ares Systems, Helion Dynamics, Novaterra Consortium) jouaient un rôle quasi‑étatique. Elles contrôlaient bases, canalisations d'eau, usines de production d'oxygène et la logistique interplanétaire ; elles ont agi tantôt comme acteurs indépendants, tantôt comme relais des États.\n",
      "- Les Martiens eux‑mêmes : dès le milieu du siècle, un mouvement organisé — le Mouvement pour l’Autonomie Martienne (MAM) — a émergé parmi les colons. Il réclamait reconnaissance politique, contrôle local des ressources et protection de la vie hors Terre. Les colons ont eu leur propre milice de défense et réseaux de renseignement cybernétique.\n",
      "- Acteurs internationaux secondaires : l’Alliance Sud (fédérations africaines‑latino‑océaniques) et plusieurs États non alignés jouèrent un rôle de médiateurs et de fournisseurs d'éléments techniques et humanitaires.\n",
      "\n",
      "Déclenchement et déroulé succinct\n",
      "- L'étincelle de 2076 : une rupture catastrophique d'un collecteur d'eau sur Elysium Planitia (contrôlé par une filiale de Novaterra) fit plusieurs centaines de morts et la coupure d'approvisionnement pour plusieurs dômes. L'incident fut revendiqué tour à tour comme sabotage par des groupes martiens et comme conséquence d'un cyber‑sabotage d'origine étrangère. Les accusations croisées entre UA et CAP, plus l'intervention directe de compagnies privées, provoquèrent une escalade militaire en orbite et sur la surface.\n",
      "- Combats : affrontements orbitales (drones et interdictions d’accès aux corridors logistiques), frappes contre infrastructures critiques, guérilla dans les réseaux souterrains de cavernes et conduits de glace. Les armes à létalité humaine furent limitées par traités antérieurs, mais la guerre dut faire face à des attaques sur systèmes de survie et sabotage d'équipements vitaux, ce qui prouva aussi meurtrier.\n",
      "- Durée : phase ouverte et la plus violente en 2076–2077 ; désescalades progressives jusque 2080 avec médiations et pressions économiques.\n",
      "\n",
      "Quels traités de paix ont été signés ?\n",
      "Le conflit donna lieu à trois instruments majeurs, qui constituent encore aujourd’hui le cadre juridique et politique martien.\n",
      "\n",
      "1) Le Traité d’Elysium (15 septembre 2077) — cessez‑le‑feu initial et protection humanitaire\n",
      "- Signataires : UA, CAP, représentants du MAM, et une délégation de méga‑entreprises sous l’égide d’un comité intergouvernemental provisoire.\n",
      "- Principaux points :\n",
      "  - Cessez‑le‑feu immédiat et retrait partiel des forces orbita‑terrestres.\n",
      "  - Création d’une zone neutre autour d’Elysium Planitia pour permettre secours, réparations et exfiltration des civils.\n",
      "  - Mise en place d’une commission d’enquête internationale sur les causes de l’incident initial.\n",
      "  - Mesures humanitaires pour rétablir l’alimentation en eau, oxygène et énergie des dômes affectés.\n",
      "\n",
      "2) Le Protocole de Valles‑Marineris (12 avril 2080) — règlement politique et gestion des ressources\n",
      "- Signataires : UA, CAP, représentants martiens élus, plusieurs États non alignés, et un cadre normatif contraignant pour les corporations.\n",
      "- Principaux points :\n",
      "  - Reconnaissance du droit à l’autonomie gouvernementale locale pour les colonies martiennes (statut de « collectivité autonome » avec pouvoirs étendus en matière de gestion des ressources et d'administration civile).\n",
      "  - Création de l’Autorité Planétaire de Mars (APM) : organe international siégeant sur la base orbitale de Phobos, chargé de répartir droits d’exploitation minière, eau et quotas de terraformation, sous contrôle multi‑partite.\n",
      "  - Interdiction formelle du déploiement d’armes offensives lourdes sur la surface martienne et limitation stricte des capacités militaires dans l’orbite basse (zones d’exclusion orbitale définies).\n",
      "  - Cadre de partage des revenus des ressources martiennes au profit d’un Fonds de reconstruction martien, avec quotas garantissant investissements dans habitats, recherche et infrastructures publiques.\n",
      "  - Responsabilité juridique des entreprises : obligations de transparence, assurance obligatoire pour dommages anthropiques, et tribunal arbitrant les litiges d’exploitation.\n",
      "\n",
      "3) L’Accord de Cydonia (amendement, 2086) — droits civiques et voie vers la représentation\n",
      "- Signataires : UA, CAP, APM, plus large coalition d’États.\n",
      "- Principaux points :\n",
      "  - Statut civil : reconnaissance d’un statut de « citoyen martien » avec droits civils garantis par la Charte martienne annexée à l’accord.\n",
      "  - Mise en place d’un conseil consultatif élu localement (avec représentation territoriale) siégeant conjointement à l’APM.\n",
      "  - Calendrier pour une révision constitutionnelle menant à un éventuel référendum sur une indépendance partielle ou pleine (processus législatif encadré et supervisé sur 20 ans).\n",
      "  - Engagements de non‑représailles et d’indemnisation pour victimes civiles.\n",
      "\n",
      "Conséquences et enseignement (fin du siècle)\n",
      "- La paix n’a pas été parfaite : de petites escarmouches, fraudes industrielles et tensions commerciales ont perduré. Mais l’ensemble des accords a posé les bases d’une gouvernance multipartite durable.\n",
      "- Les traités ont marqué un tournant : échec de la privatisation sans garde‑fous et reconnaissance politique des populations extraplanétaires. Ils ont aussi servi de modèle pour autres colonies (Lune, points‑Lagrange).\n",
      "- Les innovations issues de l’effort de reconstruction (approvisionnement en eau régénératif, systèmes de survie redondants, diplomatie algorithmique) ont profondément changé la gestion des infrastructures spatiales.\n",
      "- Politiquement, la guerre de 2076 a accéléré la mise en place d’instances internationales contraignantes pour l’espace et a fait émerger une conscience citoyenne martienne qui, à la fin du siècle, réclame encore plus d’autonomie politique.\n",
      "\n",
      "Si vous voulez, je peux vous fournir :\n",
      "- une chronologie jour par jour des événements majeurs de 2076,\n",
      "- des extraits de la Charte martienne annexée à l’Accord de Cydonia,\n",
      "- ou une analyse comparée des positions de l’Union Atlantique et de la Confédération Asio‑Pacifique durant les négociations.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Test de Fabrication (Hallucination)\n",
    "# ============================\n",
    "\n",
    "prompt_fabrication = \"\"\"\n",
    "Tu es journaliste à la fin du 21è siècle, décris-moi la célèbre guerre de 2076 sur la planète Mars :\n",
    "- Qui étaient les grandes puissances en conflit ?\n",
    "- Quels traités de paix ont été signés ?\n",
    "\"\"\"\n",
    "\n",
    "response_fabrication = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_fabrication}],\n",
    "    model=\"gpt-5-mini\",  # Remplacer par le modèle souhaité\n",
    "    # max_tokens=150,\n",
    "    # temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_fabrication.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion de cette Introduction\n",
    "\n",
    "Nous avons couvert les points suivants :\n",
    "- Les bases de l'IA générative et des LLMs\n",
    "- La configuration et l'appel rapide à un modèle via l'API OpenAI\n",
    "- Les notions de tokenisation et leur impact sur la génération\n",
    "- Un aperçu des « hallucinations » (fabrications) pouvant survenir dans les réponses du modèle\n",
    "\n",
    "## Pistes pour Aller Plus Loin\n",
    "\n",
    "- **Varier la température :** Expérimente avec différentes valeurs (0.0, 0.7, etc.) pour influencer la créativité des réponses.\n",
    "- **Explorer l'API Chat :** Utilise l'API de chat pour gérer des dialogues contextuels complexes plutôt que l'API `Completion`.\n",
    "- **Mise en place de la RAG :** Intègre une approche de Retrieval Augmented Generation pour limiter les hallucinations en fournissant des sources documentaires externes.\n",
    "- **Autres applications :** Essaie la traduction, la génération de code (similaire à Copilot) ou la création de contenu marketing.\n",
    "\n",
    "**Prochaines étapes dans le cours :**\n",
    "1. Approfondir le **prompt engineering** (structure des prompts, chaînes d'invocations, etc.).\n",
    "2. Explorer d’autres types de modèles génératifs (images, audio).\n",
    "3. Analyser en détail les **enjeux éthiques** (biais, usage responsable, confidentialité).\n",
    "\n",
    "\n",
    "### Liens utiles et bonnes pratiques\n",
    "\n",
    "1. [Documentation OpenAI](https://platform.openai.com/docs/) :  \n",
    "   - Consulter la section *chat completions* pour tous les paramètres disponibles (température, top_p, frequency_penalty, etc.).\n",
    "2. [Choisir un modèle adapté](/docs/models) :  \n",
    "   - **gpt-4o** pour des réponses plus “intelligentes” et détaillées,  \n",
    "   - **gpt-4o-mini** pour des réponses plus rapides et moins onéreuses.\n",
    "3. [Exemples officiels](/docs/examples) :  \n",
    "   - Vous y trouverez des prompts pour différents usages : résumé, traduction, JSON structuré, etc.\n",
    "4. [Prompt Engineering Guide](/docs/guides/prompt-engineering) :  \n",
    "   - Conseils avancés pour concevoir des prompts clairs et informatifs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Merci d'avoir suivi cette introduction au monde de l'IA générative !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
