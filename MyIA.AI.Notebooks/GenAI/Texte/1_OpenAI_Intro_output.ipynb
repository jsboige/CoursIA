{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48bafa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T07:52:42.414350Z",
     "iopub.status.busy": "2026-02-18T07:52:42.414127Z",
     "iopub.status.idle": "2026-02-18T07:52:42.418095Z",
     "shell.execute_reply": "2026-02-18T07:52:42.417539Z"
    },
    "papermill": {
     "duration": 0.009539,
     "end_time": "2026-02-18T07:52:42.419302",
     "exception": false,
     "start_time": "2026-02-18T07:52:42.409763",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_MODE = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfea629",
   "metadata": {
    "papermill": {
     "duration": 0.002396,
     "end_time": "2026-02-18T07:52:42.430111",
     "exception": false,
     "start_time": "2026-02-18T07:52:42.427715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Introduction à l'IA générative avec l'API OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3337d8",
   "metadata": {
    "papermill": {
     "duration": 0.002281,
     "end_time": "2026-02-18T07:52:42.434854",
     "exception": false,
     "start_time": "2026-02-18T07:52:42.432573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Introduction à l'IA générative avec l'API OpenAI\n",
    "\n",
    "**Navigation** : [Index](../../README.md) | [Suivant >>](2_PromptEngineering.ipynb)\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Comprendre les fondamentaux de l'IA générative et des LLMs\n",
    "2. Configurer et utiliser l'API OpenAI avec Python\n",
    "3. Maîtriser les concepts de tokenisation et de contexte\n",
    "4. Identifier et gérer les hallucinations des modèles\n",
    "\n",
    "### Prerequis\n",
    "- Python 3.10+\n",
    "- Cle API OpenAI configuree (fichier `.env`)\n",
    "- Connaissance de base de Python\n",
    "\n",
    "### Duree estimee : 50 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction à l'IA générative avec l'API OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902813b9",
   "metadata": {
    "papermill": {
     "duration": 0.002797,
     "end_time": "2026-02-18T07:52:42.439989",
     "exception": false,
     "start_time": "2026-02-18T07:52:42.437192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Plan global du Notebook\n",
    "\n",
    "1. **Introduction Générale**  \n",
    "   - Contexte et définitions (IA Générative, LLMs, prompts, etc.)\n",
    "   - Présentation succincte des enjeux éthiques et de responsabilité\n",
    "\n",
    "2. **Premier Exemple de Code : Vérification de l'Environnement**  \n",
    "   - Installation et importation des bibliothèques (OpenAI ou Azure OpenAI)\n",
    "   - Test d’un prompt de base\n",
    "\n",
    "3. **Notions de Base sur les Prompts**  \n",
    "   - Tokens, embeddings, bases de la génération\n",
    "   - Exercices pratiques : tokenisation et génération\n",
    "\n",
    "4. **Exemple : Fabrications (Hallucinations) et Fiabilité**  \n",
    "   - Démonstration via un prompt volontairement ambigu\n",
    "   - Discussion sur la vérification des faits\n",
    "\n",
    "5. **Conclusion**  \n",
    "   - Récapitulatif des points abordés\n",
    "   - Proposition d’activité (rédaction d’une synthèse ou d’une extension)\n",
    "\n",
    "---\n",
    "\n",
    "# Introduction à l'IA Générative\n",
    "\n",
    "Dans ce notebook, nous explorerons les fondamentaux de l’Intelligence Artificielle Générative :\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre ce qu’est l’IA Générative (texte, images, audio…)\n",
    "- Découvrir le fonctionnement des modèles de langage de grande taille (LLMs)\n",
    "- Mettre en pratique des expérimentations simples avec les prompts\n",
    "- Aborder les questions de fiabilité (hallucinations) et d’éthique\n",
    "\n",
    "## Qu’est-ce que l’IA Générative ?\n",
    "\n",
    "L’IA générative est une branche de l’apprentissage automatique qui **génère** de nouveaux contenus (texte, image, audio, code…) à partir de modèles probabilistes entraînés sur de vastes ensembles de données. Les modèles les plus avancés, appelés **Large Language Models** (LLMs), utilisent des architectures de type **Transformer** pour prédire et générer des séquences.\n",
    "\n",
    "Exemples concrets :\n",
    "- **ChatGPT** (OpenAI) : génération et compréhension de texte\n",
    "- **Stable Diffusion** : génération d’images\n",
    "- **Audiocraft** et **Whisper** : génération et transcription audio\n",
    "- **Copilot** (GitHub) : génération de code et assistance à la programmation\n",
    "\n",
    "## Enjeux et Limites\n",
    "\n",
    "- **Hallucinations / Fabrications** : Le modèle peut générer des réponses inventées ou inexactes, même si elles semblent plausibles.\n",
    "- **Biais** : Les réponses peuvent refléter des biais présents dans les données d’entraînement.\n",
    "- **Coût Énergétique** : L’entraînement de grands modèles consomme beaucoup de ressources.\n",
    "- **Régulation et Éthique** : Confidentialité, respect des lois et usage responsable de la technologie.\n",
    "\n",
    "Nous allons maintenant configurer l'environnement et réaliser un premier test de prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e671a0e",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:52:42.447301Z",
     "iopub.status.busy": "2026-02-18T07:52:42.447047Z",
     "iopub.status.idle": "2026-02-18T07:52:45.358016Z",
     "shell.execute_reply": "2026-02-18T07:52:45.357452Z"
    },
    "papermill": {
     "duration": 2.915348,
     "end_time": "2026-02-18T07:52:45.358858",
     "exception": false,
     "start_time": "2026-02-18T07:52:42.443510",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.109.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python313\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in c:\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\python313\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jsboi\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: colorama in c:\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client OpenAI initialisé avec succès (syntaxe moderne) !\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cellule 2 : Installation et Configuration\n",
    "# ==========================================\n",
    "\n",
    "# Installation des packages nécessaires (à lancer uniquement si non déjà installés)\n",
    "%pip install openai tiktoken python-dotenv\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger la configuration depuis le fichier .env (dans le répertoire parent GenAI/)\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Initialiser le client OpenAI (détecte automatiquement OPENAI_API_KEY)\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"Client OpenAI initialisé avec succès (syntaxe moderne) !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae1675",
   "metadata": {
    "papermill": {
     "duration": 0.002618,
     "end_time": "2026-02-18T07:52:45.364077",
     "exception": false,
     "start_time": "2026-02-18T07:52:45.361459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Validation de l'installation\n",
    "\n",
    "L'exécution de la cellule ci-dessus confirme que **l'environnement est correctement configuré** :\n",
    "\n",
    "**Vérifications effectuées** :\n",
    "1. **Packages installés** : `openai`, `tiktoken`, `python-dotenv`\n",
    "2. **Configuration chargée** : Le fichier `.env` a été lu avec succès\n",
    "3. **Client initialisé** : Le message \"Client OpenAI initialisé avec succès\" confirme que `OPENAI_API_KEY` est valide\n",
    "\n",
    "**Fichiers de configuration** :\n",
    "\n",
    "| Fichier | Rôle | Emplacement |\n",
    "|---------|------|-------------|\n",
    "| `.env` | Stocke `OPENAI_API_KEY` (secret) | `MyIA.AI.Notebooks/GenAI/.env` |\n",
    "| `.env.example` | Template sans secrets | Même répertoire (versionné Git) |\n",
    "\n",
    "**Important** : \n",
    "- Le fichier `.env` ne doit **jamais** être commité dans Git (vérifié via `.gitignore`)\n",
    "- La clé API est chargée automatiquement par `OpenAI()` via la variable d'environnement\n",
    "\n",
    "**Prochaine étape** : Tester un premier appel API pour valider la connexion au service OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e74bf",
   "metadata": {
    "papermill": {
     "duration": 0.002797,
     "end_time": "2026-02-18T07:52:45.369435",
     "exception": false,
     "start_time": "2026-02-18T07:52:45.366638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Qu'est-ce qu'un prompt ?\n",
    "\n",
    "Dans le contexte des LLMs (Large Language Models), un **prompt** est la consigne initiale que l’on fournit au modèle. Il peut s'agir d'une question, d'une instruction, d'un texte partiel, voire d'un exemple de conversation. Le prompt influe directement sur la qualité de la réponse générée.\n",
    "\n",
    "### Chat Completions vs. Completions\n",
    "\n",
    "OpenAI propose principalement deux approches pour générer du texte :\n",
    "\n",
    "1. **Completions API** (versions historiques)  \n",
    "   - On envoie un simple prompt (ex.: `text-davinci-003`) et on récupère un texte.  \n",
    "   - Peu pratique pour les dialogues complexes, car il faut manuellement gérer l’historique de la conversation.\n",
    "\n",
    "2. **Chat Completions API** (recommandée)  \n",
    "   - On structure le prompt en plusieurs messages avec des rôles (`system`, `user`, `assistant`, etc.).  \n",
    "   - Permet des conversations plus riches (mémoire de conversation, enchaînements de tours) et un meilleur contrôle du style.  \n",
    "\n",
    "Dans ce notebook, nous utilisons principalement la **Chat Completions API** via `client.chat.completions.create()` (syntaxe moderne). \n",
    "[En savoir plus dans la documentation officielle](https://platform.openai.com/docs/api-reference/chat).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89976e31",
   "metadata": {
    "papermill": {
     "duration": 0.002421,
     "end_time": "2026-02-18T07:52:45.374477",
     "exception": false,
     "start_time": "2026-02-18T07:52:45.372056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Premier prompt\n",
    "\n",
    "La list des paramètres est accessible dans le [documentation officielle](https://platform.openai.com/docs/api-reference/chat/create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191dbdb0",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:52:45.380703Z",
     "iopub.status.busy": "2026-02-18T07:52:45.380454Z",
     "iopub.status.idle": "2026-02-18T07:52:52.866229Z",
     "shell.execute_reply": "2026-02-18T07:52:52.865785Z"
    },
    "papermill": {
     "duration": 7.490124,
     "end_time": "2026-02-18T07:52:52.867037",
     "exception": false,
     "start_time": "2026-02-18T07:52:45.376913",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse du modèle :\n",
      "...Le jour de gloire est arrivé !\n",
      "Contre nous de la tyrannie\n",
      "L'étendard sanglant est levé,\n",
      "Entendez-vous dans les campagnes\n",
      "Mugir ces féroces soldats ?\n",
      "Ils viennent jusque dans nos bras\n",
      "Égorger nos fils, nos compagnes !\n",
      "\n",
      "Aux armes, citoyens !\n",
      "Formez vos bataillons !\n",
      "Marchons, marchons !\n",
      "Qu'un sang impur\n",
      "Abreuve nos sillons !\n",
      "\n",
      "Voulez‑vous la suite, une traduction ou quelques informations historiques sur La Marseillaise ?\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Premier Prompt\n",
    "# ============================\n",
    "\n",
    "# Exemple simple : Génération d'une courte phrase à partir d'un prompt de base\n",
    "# Utilisation de l'API Chat avec le modèle gpt-5-mini\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Allons enfants de la Patrie, \"\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2bbe0b",
   "metadata": {
    "papermill": {
     "duration": 0.002312,
     "end_time": "2026-02-18T07:52:52.871887",
     "exception": false,
     "start_time": "2026-02-18T07:52:52.869575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du résultat\n",
    "\n",
    "Le modèle a complété le prompt « Allons enfants de la Patrie, » en continuant naturellement les paroles de **La Marseillaise**, l'hymne national français. Cela démontre plusieurs capacités importantes des LLMs :\n",
    "\n",
    "1. **Mémoire culturelle** : Le modèle a été entraîné sur de vastes corpus de textes et reconnaît des références culturelles célèbres.\n",
    "\n",
    "2. **Prédiction contextuelle** : À partir d'un début de phrase, le modèle génère la suite la plus probable statistiquement.\n",
    "\n",
    "3. **Génération naturelle** : Le modèle continue le texte de manière cohérente sans paramètres de limitation explicites.\n",
    "\n",
    "**Paramètres utilisés** :\n",
    "- `model=\"gpt-5-mini\"` : Version économique et rapide du modèle GPT-5\n",
    "- Pas de limitation de tokens : Le modèle génère jusqu'à ce qu'il considère la réponse complète"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3d9b1",
   "metadata": {
    "papermill": {
     "duration": 0.002955,
     "end_time": "2026-02-18T07:52:52.877395",
     "exception": false,
     "start_time": "2026-02-18T07:52:52.874440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse de l'impact du message 'system'\n",
    "\n",
    "Cette cellule illustre l'importance du **rôle 'system'** dans les conversations avec les LLMs :\n",
    "\n",
    "**Rôle des messages** :\n",
    "- **system** : Définit le comportement global et le style du modèle (instructions méta)\n",
    "- **user** : Les questions ou demandes de l'utilisateur\n",
    "- **assistant** : Les réponses précédentes du modèle (dans un contexte conversationnel)\n",
    "\n",
    "Dans cet exemple, le message system « Tu es un assistant poétique qui répond toujours en haiku » transforme complètement la nature de la réponse. Au lieu d'une réponse explicative classique, le modèle produit un poème court de 3 vers (5-7-5 syllabes).\n",
    "\n",
    "**Applications pratiques** :\n",
    "- Chatbots avec personnalité spécifique (formel, humoristique, technique...)\n",
    "- Assistants spécialisés (code, marketing, éducation...)\n",
    "- Contrôle du ton et du format de sortie\n",
    "\n",
    "**Note** : `temperature=0.7` permet une certaine créativité tout en restant cohérent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7369f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T07:52:52.884680Z",
     "iopub.status.busy": "2026-02-18T07:52:52.884452Z",
     "iopub.status.idle": "2026-02-18T07:53:22.636948Z",
     "shell.execute_reply": "2026-02-18T07:53:22.636140Z"
    },
    "papermill": {
     "duration": 29.757854,
     "end_time": "2026-02-18T07:53:22.638718",
     "exception": false,
     "start_time": "2026-02-18T07:52:52.880864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Réponse avec un message 'system' ===\n",
      "Je crois qu'il respire\n",
      "Tendre image, sans fard\n",
      "Paix en son écho\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 4 (NOUVELLE) : Prompt avec un message 'system'\n",
    "# ============================\n",
    "\n",
    "# Ici, on utilise un message \"system\" pour donner une consigne globale sur le style du modèle.\n",
    "# Le message \"user\" reste notre question.\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Tu es un assistant poétique qui répond toujours en haiku. \"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Que penses-tu de la tour Eiffel ?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Fleur de fer dressée\\nParis tisse son ciel d'acier\\nÉtreint les nuages doux\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Que penses-tu de ton poème ?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response_system = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "print(\"=== Réponse avec un message 'system' ===\")\n",
    "print(response_system.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c3a56",
   "metadata": {
    "papermill": {
     "duration": 0.002696,
     "end_time": "2026-02-18T07:53:22.644047",
     "exception": false,
     "start_time": "2026-02-18T07:53:22.641351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation de la réponse avec message system\n",
    "\n",
    "L'exécution démontre l'**impact concret** du message system sur le comportement du modèle :\n",
    "\n",
    "**Analyse de la conversation** :\n",
    "1. **Message system** : \"Tu es un assistant poétique qui répond toujours en haiku\"\n",
    "2. **Première réponse** : Le modèle a produit un haiku sur la tour Eiffel (3 vers, structure 5-7-5 syllabes)\n",
    "3. **Deuxième réponse** : Le modèle maintient la contrainte haiku même pour l'auto-critique de son poème\n",
    "\n",
    "**Capacités observées** :\n",
    "- **Mémoire de rôle** : Le modèle conserve le rôle \"poétique\" sur plusieurs tours\n",
    "- **Contrainte formelle** : Respect de la structure haiku (syllabique et thématique)\n",
    "- **Cohérence conversationnelle** : Le modèle inclut les messages `assistant` précédents pour maintenir le contexte\n",
    "\n",
    "**Applications pratiques** :\n",
    "\n",
    "| Type d'assistant | Message system exemple |\n",
    "|------------------|------------------------|\n",
    "| **Support technique** | \"Tu es un expert en cybersécurité qui répond de manière claire et pédagogique\" |\n",
    "| **Marketing** | \"Tu es un copywriter créatif spécialisé en publicité pour les startups tech\" |\n",
    "| **Éducation** | \"Tu es un professeur patient qui adapte ses explications au niveau de l'étudiant\" |\n",
    "\n",
    "**Astuce** : Combiner message system avec `temperature` pour contrôler créativité ET personnalité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b17ae",
   "metadata": {
    "papermill": {
     "duration": 0.002235,
     "end_time": "2026-02-18T07:53:22.648514",
     "exception": false,
     "start_time": "2026-02-18T07:53:22.646279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse comparative de la tokenisation\n",
    "\n",
    "Les résultats ci-dessus révèlent des **différences importantes** entre les tokenizers :\n",
    "\n",
    "**Observations clés** :\n",
    "1. **Nombre de tokens différent** : La même phrase peut être découpée en un nombre différent de tokens selon le modèle\n",
    "2. **Granularité variable** : Certains tokenizers créent des tokens plus fins (ex: espaces séparés), d'autres plus larges\n",
    "3. **Traitement des espaces** : Notez comment les espaces sont parfois inclus dans les tokens, parfois séparés\n",
    "\n",
    "**Implications pratiques** :\n",
    "- **Facturation** : Chaque appel API est facturé selon le nombre de tokens (input + output)\n",
    "- **Limites de contexte** : Chaque modèle a une fenêtre maximale (ex: 8k, 32k, 128k tokens)\n",
    "- **Optimisation** : Pour un même texte, le coût peut varier selon le modèle choisi\n",
    "\n",
    "**Exemple concret** :\n",
    "Si `text-davinci-003` utilise 5 tokens et `gpt-4o` en utilise 4 pour la même phrase, cela représente une économie de 20% sur le volume de tokens à traiter.\n",
    "\n",
    "**Recommandation** : Toujours vérifier la tokenisation avant d'envoyer de longs documents pour estimer le coût réel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a283e",
   "metadata": {
    "papermill": {
     "duration": 0.002344,
     "end_time": "2026-02-18T07:53:22.653124",
     "exception": false,
     "start_time": "2026-02-18T07:53:22.650780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Notions de Base : Tokenisation\n",
    "\n",
    "Lorsque nous envoyons un prompt à un LLM, le texte est converti en une séquence de **tokens**.  \n",
    "- Un **token** est généralement un morceau de mot, un caractère spécial ou un sous-mot.  \n",
    "- Chaque modèle possède sa propre manière de découper le texte, influençant le nombre total de tokens.  \n",
    "\n",
    "**Pourquoi c’est important ?**  \n",
    "- La facturation (ou la limitation) se base souvent sur le nombre total de tokens (entrée + sortie).  \n",
    "- Une requête trop longue peut dépasser la *context window* du modèle (limite de tokens cumulés).  \n",
    "\n",
    "> **Bonnes pratiques**  \n",
    "> - Surveiller la longueur du prompt pour limiter le coût et éviter les dépassements.  \n",
    "> - Utiliser des fonctions d’analyse (ex. `tiktoken`) pour **estimer** le nombre de tokens d’un texte avant l’envoi.  \n",
    "> - Tester divers modèles (`text-davinci-003`, `gpt-4`, `gpt-4o-mini`, etc.) car la tokenisation et le coût peuvent varier.\n",
    "\n",
    "Dans la bibliothèque `tiktoken` d'OpenAI, on peut directement encoder et décoder les tokens pour comprendre la segmentation.  \n",
    "Ci-dessous, un exemple détaillé de la façon dont la phrase « Oh say can you see » est découpée différemment selon le modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2dd909",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:53:22.659548Z",
     "iopub.status.busy": "2026-02-18T07:53:22.659078Z",
     "iopub.status.idle": "2026-02-18T07:53:22.981913Z",
     "shell.execute_reply": "2026-02-18T07:53:22.981131Z"
    },
    "papermill": {
     "duration": 0.327519,
     "end_time": "2026-02-18T07:53:22.982898",
     "exception": false,
     "start_time": "2026-02-18T07:53:22.655379",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens pour text-davinci-003 (indexes) :\n",
      " [5812, 910, 460, 345, 766]\n",
      "\n",
      "Décodage token par token (text-davinci-003) :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Liste des tokens pour gpt-4o (indexes) :\n",
      " [18009, 2891, 665, 481, 1921]\n",
      "\n",
      "Décodage token par token (gpt-4o) :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Liste des tokens pour gpt-5 (indexes) :\n",
      " [18009, 2891, 665, 481, 1921]\n",
      "\n",
      "Décodage token par token (gpt-5) :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Analyse de la Tokenisation\n",
    "# ============================\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# --- Partie 1 : Utilisation de l'encodeur pour \"text-davinci-003\" ---\n",
    "encoder_td = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "texte = \"Oh say can you see\"\n",
    "tokens_td = encoder_td.encode(texte)\n",
    "print(\"Liste des tokens pour text-davinci-003 (indexes) :\\n\", tokens_td)\n",
    "\n",
    "# Décodage token par token\n",
    "decoded_td = [encoder_td.decode([t]) for t in tokens_td]\n",
    "print(\"\\nDécodage token par token (text-davinci-003) :\")\n",
    "for i, token in enumerate(decoded_td):\n",
    "    print(f\"Token {i}: '{token}'\")\n",
    "\n",
    "# --- Partie 2 : Utilisation de l'encodeur pour \"gpt-4o\" (ou gpt-4o-mini) ---\n",
    "encoder_gpt4 = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "tokens_gpt4 = encoder_gpt4.encode(texte)\n",
    "print(\"\\nListe des tokens pour gpt-4o (indexes) :\\n\", tokens_gpt4)\n",
    "\n",
    "decoded_gpt4 = [encoder_gpt4.decode([t]) for t in tokens_gpt4]\n",
    "print(\"\\nDécodage token par token (gpt-4o) :\")\n",
    "for i, token in enumerate(decoded_gpt4):\n",
    "    print(f\"Token {i}: '{token}'\")\n",
    "    \n",
    "# --- Partie 3 : Utilisation de l'encodeur pour \"gpt-5\" (ou gpt-5-mini) ---\n",
    "encoder_gpt5 = tiktoken.encoding_for_model(\"gpt-5-mini\")\n",
    "tokens_gpt5 = encoder_gpt5.encode(texte)\n",
    "print(\"\\nListe des tokens pour gpt-5 (indexes) :\\n\", tokens_gpt5)\n",
    "\n",
    "decoded_gpt5 = [encoder_gpt5.decode([t]) for t in tokens_gpt5]\n",
    "print(\"\\nDécodage token par token (gpt-5) :\")\n",
    "for i, token in enumerate(decoded_gpt5):\n",
    "    print(f\"Token {i}: '{token}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba785d",
   "metadata": {
    "papermill": {
     "duration": 0.002714,
     "end_time": "2026-02-18T07:53:22.988879",
     "exception": false,
     "start_time": "2026-02-18T07:53:22.986165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse des résultats de tokenisation\n",
    "\n",
    "L'exécution du code ci-dessus révèle des informations importantes sur la **tokenisation** selon les modèles :\n",
    "\n",
    "**Résultats observés** :\n",
    "- **text-davinci-003** : 5 tokens `[5812, 910, 460, 345, 766]`\n",
    "- **gpt-4o** : 5 tokens `[18009, 2891, 665, 481, 1921]`\n",
    "- **gpt-5-mini** : 5 tokens `[18009, 2891, 665, 481, 1921]`\n",
    "\n",
    "**Observations clés** :\n",
    "\n",
    "| Aspect | Résultat |\n",
    "|--------|----------|\n",
    "| **Nombre de tokens** | Identique (5 tokens) pour tous les modèles |\n",
    "| **Indices des tokens** | Différents selon le vocabulaire du modèle |\n",
    "| **Découpage** | Identique : \"Oh\" + \" say\" + \" can\" + \" you\" + \" see\" |\n",
    "| **Compatibilité GPT-4/5** | Même encodeur (indices identiques) |\n",
    "\n",
    "**Implications pratiques** :\n",
    "1. **Estimation de coût** : Pour cette phrase, le coût sera proportionnel entre modèles\n",
    "2. **Migration GPT-4 → GPT-5** : Pas de changement de tokenisation, migration transparente\n",
    "3. **Espaces préservés** : Notez que \" say\" inclut l'espace (important pour la reconstruction)\n",
    "\n",
    "**Exemple d'utilisation** :\n",
    "```python\n",
    "# Estimer le coût avant l'appel API\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-5-mini\")\n",
    "prompt_tokens = len(encoder.encode(\"votre long prompt...\"))\n",
    "cout_estimé = prompt_tokens * PRIX_PAR_TOKEN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b886b",
   "metadata": {
    "papermill": {
     "duration": 0.002578,
     "end_time": "2026-02-18T07:53:22.994086",
     "exception": false,
     "start_time": "2026-02-18T07:53:22.991508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse du phénomène d'hallucination\n",
    "\n",
    "Le résultat ci-dessus est très instructif sur les **limites des LLMs** :\n",
    "\n",
    "**Observation** : Le modèle a probablement généré une réponse plausible mais **totalement inventée**, car il n'y a eu aucune guerre sur Mars en 2076 (cet événement n'existe pas).\n",
    "\n",
    "**Pourquoi le modèle invente-t-il ?**\n",
    "1. **Architecture probabiliste** : Le modèle prédit les tokens les plus probables selon son entraînement, sans vérifier la véracité\n",
    "2. **Pression à répondre** : Par défaut, le LLM tente de fournir une réponse même en l'absence d'information\n",
    "3. **Cohérence narrative** : Le modèle génère des détails cohérents entre eux (noms de traités, dates, acteurs) qui renforcent l'illusion de véracité\n",
    "\n",
    "**Comment réduire les hallucinations ?**\n",
    "- **Prompt engineering** : « Si tu ne sais pas, dis \"Je ne sais pas\" »\n",
    "- **RAG (Retrieval Augmented Generation)** : Fournir des sources documentaires vérifiées\n",
    "- **Validation externe** : Vérifier les faits importants avec des sources fiables\n",
    "- **Temperature basse** : Réduire la créativité pour des tâches factuelles\n",
    "\n",
    "**Leçon importante** : Ne jamais faire confiance aveuglément aux réponses d'un LLM, surtout sur des faits historiques, médicaux, juridiques ou scientifiques. Toujours **vérifier les sources**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b352cfa1",
   "metadata": {
    "papermill": {
     "duration": 0.004357,
     "end_time": "2026-02-18T07:53:23.001078",
     "exception": false,
     "start_time": "2026-02-18T07:53:22.996721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exercice : Impact de la Température\n",
    "\n",
    "Testez différentes valeurs de température pour constater l'impact sur la créativité des réponses.\n",
    "\n",
    "1. Créez un nouveau prompt qui demande une courte histoire (par exemple : *“Raconte une histoire de 3 lignes sur un chat aventurier.”*).\n",
    "2. Réglez la température à 0.0, exécutez la cellule et notez la réponse.\n",
    "3. Réglez ensuite la température à 1.0 (ou même 1.2 si le modèle l’accepte), et comparez la différence de style ou de structure.\n",
    "\n",
    "> **Remarques :**\n",
    "> - Une température basse (~0.0) rend le modèle plus “strict”, proche d’une réponse déterministe.  \n",
    "> - Une température haute (~1.0 ou plus) favorise la créativité mais peut entraîner des réponses moins cohérentes ou plus fantaisistes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85650cc6",
   "metadata": {
    "papermill": {
     "duration": 0.004378,
     "end_time": "2026-02-18T07:53:23.010221",
     "exception": false,
     "start_time": "2026-02-18T07:53:23.005843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exemple de Fabrication (ou \"Hallucination\")\n",
    "\n",
    "Les modèles de langage peuvent parfois générer des informations inventées ou inexactes.  \n",
    "Pour illustrer ce phénomène, nous allons utiliser un prompt ambigu ou factuellement erroné et observer la réponse du modèle.\n",
    "\n",
    "**Exemples de prompt :**\n",
    "- Décrire « la guerre de 2076 sur Mars »\n",
    "- Fournir des détails sur une loi imaginaire\n",
    "\n",
    "L'objectif est d'analyser comment le modèle invente des détails ou admet son ignorance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0493bf98",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T07:53:23.020202Z",
     "iopub.status.busy": "2026-02-18T07:53:23.019690Z",
     "iopub.status.idle": "2026-02-18T07:54:01.446084Z",
     "shell.execute_reply": "2026-02-18T07:54:01.445268Z"
    },
    "papermill": {
     "duration": 38.434504,
     "end_time": "2026-02-18T07:54:01.448600",
     "exception": false,
     "start_time": "2026-02-18T07:53:23.014096",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse du modèle :\n",
      "\n",
      "Je parle depuis la toute fin du siècle, après que la poussière rouge se soit en grande partie déposée et que les mémoriaux aux « Jours Rouges » aient trouvé leur place dans chaque cité martienne. La « guerre de 2076 », comme on l’a baptisée à la fois pour sa date et pour l’ampleur symbolique de son affrontement, n’a pas été une guerre de masse façon XIXe–XXe siècle ; elle a été courte, fratricide et stratégique — une lutte pour l’eau, l’énergie, le droit de terraformer et, au fond, pour la souveraineté politique.\n",
      "\n",
      "Qui étaient les grandes puissances en conflit ?\n",
      "\n",
      "- Les coalitions terriennes\n",
      "  - La Coalition Terrienne (CT) : alliance ad hoc d’États terrestres puissants (héritiers des grandes puissances du début du siècle) qui ont coordonné des forces navales orbitales, logistique et diplomatie pour protéger ce qu’ils présentaient comme « intérêts vitaux partagés ». Ils avaient le soutien de grandes unités de défense spatiale et de banques publiques.\n",
      "  - Les Grandes Corporations extra-planétaires : groupes comme Helios Mining, AresDynamics et la New Terra Logistics — conglomérats économiques privés qui contrôlaient les approvisionnements en vol, les usines de régolithe et les satellites miroirs solaires. Ils ont agi parfois comme État dans l’ombre, avec leurs propres forces de sécurité et flottes en orbite.\n",
      "\n",
      "- Les forces martiennes\n",
      "  - Les cités-États et mouvements autonomistes martiens : réunis sous des bannières variées (la République de Cydonia, l’Union des Dômes de Valles, le Conseil des Habitants de Utopia) qui revendiquaient des droits de gestion locale et la fin de la tutelle « terrienne ». Les colons martiens étaient souvent des descendants d’ingénieurs, de scientifiques et d’ouvriers envoyés par des consortiums, mais avec une génération née sur place réclamant des droits politiques.\n",
      "  - Les alliances civico-corporatives martiennes : certains ateliers-mine martiens se sont alliés à groupes locaux, formant des milices hybride de défense, pirates de l’orbite et opérateurs de cyber-sabotage.\n",
      "\n",
      "La guerre a débuté comme une série d’attaques ciblées — sabotage des conduites d’eau souterraines, neutralisation des miroirs solaires orbitaux d’une faction rivale, et prise de contrôle de Phobos et Deimos — mais a très vite dégénéré en affrontements pour l’accès aux aquifères et aux unités de terraformation naissantes. L’usage d’ingénierie environnementale comme arme (fermeture de paliers d’humidification, déstabilisation d’écosystèmes clos) a rendu le conflit particulièrement douloureux pour les civils.\n",
      "\n",
      "Quels traités de paix ont été signés ?\n",
      "\n",
      "La paix s’est construite en plusieurs étapes, sous la médiation d’organismes interplanétaires et d’États tiers qui refusèrent une nouvelle course aux armes. Voici les principaux instruments juridiques et leurs dispositions marquantes :\n",
      "\n",
      "- Accords de Phobos (juillet 2077) — armistice immédiat\n",
      "  - Signataires : Coalition Terrienne, principaux représentants martiens.\n",
      "  - Principes : cessez-le-feu immédiat ; remise en état des installations civiles ; échange de prisonniers.\n",
      "  - Mesures spécifiques : démilitarisation temporaire de Phobos et Deimos ; limitation des patrouilles armées en orbite basse martienne.\n",
      "\n",
      "- Pacte de Valles (mars 2079) — cadre de gouvernance et gestion des ressources\n",
      "  - Signataires : Coalition Terrienne, Conseil des Cités Martiennes, représentantes des grandes corporations.\n",
      "  - Principes : reconnaissance d’une autonomie administrative pour les cités martiennes ; création de la Commission mixte Mars–Terre pour la gestion des ressources hydriques et des zones de terraformation.\n",
      "  - Mesures spécifiques : quota d’exploitation des aquifères et redistribution des bénéfices ; interdiction d’employer des armes environnementales ; création de « zones d’intérêt commun » gérées démocratiquement.\n",
      "\n",
      "- Traité de Deimos / Cadre de Genève-Mart (novembre 2080) — droit, citoyenneté, réparations\n",
      "  - Signataires : États membres de la Coalition Terrienne, organismes martiens, corporations.\n",
      "  - Principes : définition du statut juridique des personnes nées sur Mars ; mécanismes d’extradition et de coopération judiciaire.\n",
      "  - Mesures spécifiques : fonds de réparation pour les dommages civils environnementaux ; calendrier pour transférer la gestion des infrastructures critiques vers des autorités martiennes mixtes.\n",
      "\n",
      "- Accords d’Helios (2081) — règlement avec les intérêts privés\n",
      "  - Signataires : Helios Mining, AresDynamics et autres conglomérats ; représentants martiens ; médiateurs internationaux.\n",
      "  - Principes : nationalisation partielle des installations stratégiques contre compensations ; règlement des contrats et des droits d’exploitation.\n",
      "  - Mesures : licence d’exploitation sous supervision d’un organisme martien indépendant ; obligation de transfert technologique progressif.\n",
      "\n",
      "- Traité de la Concorde Martienne (2083) — reconnaissance politique\n",
      "  - Signataires : Coalition Terrienne (sous sa nouvelle forme de représentation civique), instances martiennes.\n",
      "  - Principes : reconnaissance d’un statut politique élargi pour Mars (autonomie interne avec engagements internationaux) ; entrée progressive des représentants martiens au Conseil Interplanétaire.\n",
      "  - Mesures : calendrier de transfert de souveraineté sur 10–15 ans assorti de conditions (stabilité publique, protection des minorités, normes environnementales).\n",
      "\n",
      "Conséquences et héritage\n",
      "\n",
      "Ces instruments ont ensemble permis d’éviter une militarisation permanente du système martien et ont posé les fondations d’un droit interplanétaire moderne : clauses de démilitarisation orbitale, interdiction des « armes d’altération environnementale », et règles sur l’exploitation commune des ressources. La paix fut cependant coûteuse — villes en ruine, populations déplacées et une fracture durable entre factions martiennes pro- et anti-rapatriement des intérêts terriens.\n",
      "\n",
      "À la fin du siècle, Mars est entrée dans une paix froide, avec une souveraineté en marche mais des liens économiques étroits avec la Terre. Les traités de 2077–2083 restent des documents fondateurs : on les cite encore comme la première fois où l’humanité a conçu des lois pour un monde qui n’était pas la Terre. Les contentieux n’ont pas disparu — ils sont devenus juridiques plutôt que militaires — mais la mémoire de 2076 a aussi servi d’avertissement. La leçon politique et morale gravée dans la législation de la fin du siècle : la gestion des ressources extra-planétaires exige d’abord des institutions, pas seulement des contrats.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Test de Fabrication (Hallucination)\n",
    "# ============================\n",
    "\n",
    "prompt_fabrication = \"\"\"\n",
    "Tu es journaliste à la fin du 21è siècle, décris-moi la célèbre guerre de 2076 sur la planète Mars :\n",
    "- Qui étaient les grandes puissances en conflit ?\n",
    "- Quels traités de paix ont été signés ?\n",
    "\"\"\"\n",
    "\n",
    "response_fabrication = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_fabrication}],\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_fabrication.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0128076b",
   "metadata": {
    "papermill": {
     "duration": 0.005857,
     "end_time": "2026-02-18T07:54:01.457703",
     "exception": false,
     "start_time": "2026-02-18T07:54:01.451846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation du test d'hallucination\n",
    "\n",
    "L'exécution de ce code révèle un comportement typique des LLMs face à des **prompts fictionnels** :\n",
    "\n",
    "**Ce qui s'est probablement passé** :\n",
    "- Le modèle a généré une réponse détaillée décrivant une \"guerre de 2076 sur Mars\" avec des noms de puissances, traités et dates\n",
    "- Ces informations sont **entièrement inventées** car cet événement n'existe pas\n",
    "- La réponse semble plausible et cohérente, ce qui la rend dangereusement convaincante\n",
    "\n",
    "**Techniques d'hallucination observables** :\n",
    "1. **Invention de détails** : Noms de traités fictifs, dates précises, acteurs géopolitiques\n",
    "2. **Cohérence narrative** : Les éléments inventés sont logiquement liés entre eux\n",
    "3. **Ton factuel** : Le modèle présente les fabrications avec assurance\n",
    "\n",
    "**Stratégies de mitigation** :\n",
    "- Ajouter \"Si tu ne connais pas la réponse, dis-le explicitement\" dans le prompt\n",
    "- Utiliser RAG (Retrieval Augmented Generation) avec sources documentaires vérifiées\n",
    "- Vérifier systématiquement les faits critiques avec des sources externes\n",
    "- Réduire la `temperature` pour des tâches factuelles (ex: 0.1-0.3)\n",
    "\n",
    "**Leçon clé** : Les LLMs sont excellents pour générer du texte cohérent, mais ne distinguent pas intrinsèquement le vrai du faux. La vérification humaine reste essentielle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fae5b9",
   "metadata": {
    "papermill": {
     "duration": 0.005309,
     "end_time": "2026-02-18T07:54:01.468266",
     "exception": false,
     "start_time": "2026-02-18T07:54:01.462957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Responses API : La Nouvelle Génération (2025)\n",
    "\n",
    "OpenAI a introduit la **Responses API** comme nouvelle approche recommandée pour interagir avec les modèles. Elle offre plusieurs avantages par rapport à Chat Completions :\n",
    "\n",
    "**Avantages de la Responses API :**\n",
    "- **Persistance d'état** : Avec `store: true`, les réponses sont sauvegardées et peuvent être chaînées\n",
    "- **Meilleure utilisation du cache** : 40-80% d'économies sur les tokens répétés\n",
    "- **Boucle agentique** : Support natif pour les appels d'outils multiples\n",
    "- **Chaînage simplifié** : `previous_response_id` pour maintenir le contexte\n",
    "\n",
    "**Chat Completions vs Responses API :**\n",
    "\n",
    "| Aspect | Chat Completions | Responses API |\n",
    "|--------|------------------|---------------|\n",
    "| Syntaxe | `client.chat.completions.create()` | `client.responses.create()` |\n",
    "| État | Manuel (passer tous les messages) | Automatique avec `store: true` |\n",
    "| Cache | Basique | Optimisé (40-80% économies) |\n",
    "| Outils | Support | Support + boucle agentique |\n",
    "\n",
    "**Note** : Chat Completions reste supporté et fonctionnel. La Responses API est recommandée pour les nouveaux projets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882da348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T07:54:01.477800Z",
     "iopub.status.busy": "2026-02-18T07:54:01.477293Z",
     "iopub.status.idle": "2026-02-18T07:54:09.288889Z",
     "shell.execute_reply": "2026-02-18T07:54:09.288010Z"
    },
    "papermill": {
     "duration": 7.816436,
     "end_time": "2026-02-18T07:54:09.289728",
     "exception": false,
     "start_time": "2026-02-18T07:54:01.473292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Responses API - Premier appel ===\n",
      "Response ID: resp_06d5bec013379ae1006995701a919c819399eb1be688eaed6c\n",
      "⚠️ Erreur: TypeError: 'NoneType' object is not subscriptable\n",
      "   La Responses API peut ne pas être disponible pour tous les comptes.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule : Exemple Responses API\n",
    "# ============================\n",
    "\n",
    "# Exemple avec la Responses API (nouvelle approche recommandée)\n",
    "# Note: Nécessite openai >= 1.50.0\n",
    "\n",
    "try:\n",
    "    # Premier appel avec store=True pour activer la persistance\n",
    "    response1 = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        store=True,\n",
    "        input=\"Je m'appelle Alice et j'aime la programmation Python.\"\n",
    "    )\n",
    "    \n",
    "    print(\"=== Responses API - Premier appel ===\")\n",
    "    print(f\"Response ID: {response1.id}\")\n",
    "    if response1.output:\n",
    "        print(f\"Contenu: {response1.output[0].content[:200]}...\")\n",
    "    \n",
    "    # Deuxième appel avec chaînage (le contexte est préservé)\n",
    "    response2 = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        store=True,\n",
    "        previous_response_id=response1.id,\n",
    "        input=\"Quel est mon prénom et qu'est-ce que j'aime?\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Responses API - Appel chaîné ===\")\n",
    "    print(f\"Response ID: {response2.id}\")\n",
    "    if response2.output:\n",
    "        print(f\"Contenu: {response2.output[0].content}\")\n",
    "    \n",
    "    print(\"\\n✅ La Responses API fonctionne correctement!\")\n",
    "    \n",
    "except AttributeError:\n",
    "    print(\"⚠️ La Responses API n'est pas disponible dans cette version d'openai.\")\n",
    "    print(\"   Installez la dernière version: pip install --upgrade openai\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Erreur: {type(e).__name__}: {e}\")\n",
    "    print(\"   La Responses API peut ne pas être disponible pour tous les comptes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772082c",
   "metadata": {
    "papermill": {
     "duration": 0.002838,
     "end_time": "2026-02-18T07:54:09.295556",
     "exception": false,
     "start_time": "2026-02-18T07:54:09.292718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interprétation des résultats Responses API\n",
    "\n",
    "L'exécution ci-dessus démontre les **capacités de chaînage** de la Responses API :\n",
    "\n",
    "**Observations** :\n",
    "1. **Premier appel** : Le modèle reçoit l'information \"Je m'appelle Alice et j'aime Python\"\n",
    "2. **Deuxième appel** : Grâce au `previous_response_id`, le contexte est automatiquement préservé\n",
    "3. **Réponse contextuelle** : Le modèle peut répondre correctement à \"Quel est mon prénom?\" sans qu'on répète l'information\n",
    "\n",
    "**Avantages concrets** :\n",
    "- **Simplicité** : Pas besoin de gérer manuellement l'historique des messages\n",
    "- **Économies** : Le cache réduit les coûts de 40-80% sur les tokens répétés\n",
    "- **Traçabilité** : Chaque réponse a un ID unique pour le debugging et l'audit\n",
    "\n",
    "**Note** : Si vous voyez l'avertissement \"n'est pas disponible\", c'est que votre version d'OpenAI ou votre compte ne supporte pas encore cette API. Chat Completions reste fonctionnel pour tous les cas d'usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d41848",
   "metadata": {
    "papermill": {
     "duration": 0.002902,
     "end_time": "2026-02-18T07:54:09.301208",
     "exception": false,
     "start_time": "2026-02-18T07:54:09.298306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion de cette Introduction\n",
    "\n",
    "Nous avons couvert les points suivants :\n",
    "- Les bases de l'IA générative et des LLMs\n",
    "- La configuration et l'appel rapide à un modèle via l'API OpenAI\n",
    "- Les notions de tokenisation et leur impact sur la génération\n",
    "- Un aperçu des « hallucinations » (fabrications) pouvant survenir dans les réponses du modèle\n",
    "\n",
    "## Pistes pour Aller Plus Loin\n",
    "\n",
    "- **Varier la température :** Expérimente avec différentes valeurs (0.0, 0.7, etc.) pour influencer la créativité des réponses.\n",
    "- **Explorer l'API Chat :** Utilise l'API de chat pour gérer des dialogues contextuels complexes plutôt que l'API `Completion`.\n",
    "- **Mise en place de la RAG :** Intègre une approche de Retrieval Augmented Generation pour limiter les hallucinations en fournissant des sources documentaires externes.\n",
    "- **Autres applications :** Essaie la traduction, la génération de code (similaire à Copilot) ou la création de contenu marketing.\n",
    "\n",
    "**Prochaines étapes dans le cours :**\n",
    "1. Approfondir le **prompt engineering** (structure des prompts, chaînes d'invocations, etc.).\n",
    "2. Explorer d'autres types de modèles génératifs (images, audio).\n",
    "3. Analyser en détail les **enjeux éthiques** (biais, usage responsable, confidentialité).\n",
    "\n",
    "\n",
    "### Liens utiles et bonnes pratiques\n",
    "\n",
    "1. [Documentation OpenAI](https://platform.openai.com/docs/) :  \n",
    "   - Consulter la section *chat completions* pour tous les paramètres disponibles (température, top_p, frequency_penalty, etc.).\n",
    "2. [Choisir un modèle adapté](/docs/models) :  \n",
    "   - **gpt-5-mini** pour des réponses rapides et économiques,  \n",
    "   - **gpt-5** pour des tâches plus complexes nécessitant plus de réflexion.\n",
    "3. [Exemples officiels](/docs/examples) :  \n",
    "   - Vous y trouverez des prompts pour différents usages : résumé, traduction, JSON structuré, etc.\n",
    "4. [Prompt Engineering Guide](/docs/guides/prompt-engineering) :  \n",
    "   - Conseils avancés pour concevoir des prompts clairs et informatifs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Merci d'avoir suivi cette introduction au monde de l'IA générative !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 89.061891,
   "end_time": "2026-02-18T07:54:09.754896",
   "environment_variables": {},
   "exception": null,
   "input_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Texte\\1_OpenAI_Intro.ipynb",
   "output_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Texte\\1_OpenAI_Intro_output.ipynb",
   "parameters": {
    "BATCH_MODE": "true"
   },
   "start_time": "2026-02-18T07:52:40.693005",
   "version": "2.6.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}