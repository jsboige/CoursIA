{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patterns de Production : APIs Avanc√©es OpenAI\n",
    "\n",
    "Ce notebook couvre les fonctionnalit√©s avanc√©es n√©cessaires pour des applications en production :\n",
    "- **Conversations API** : Persistance d'√©tat entre sessions\n",
    "- **Background Mode** : T√¢ches asynchrones longues\n",
    "- **Rate Limiting** : Gestion des limites d'API\n",
    "- **Optimisation** : R√©duction des co√ªts\n",
    "\n",
    "**Objectifs :**\n",
    "- G√©rer des conversations multi-sessions\n",
    "- Ex√©cuter des t√¢ches en arri√®re-plan\n",
    "- Impl√©menter des patterns de r√©silience\n",
    "- Optimiser les co√ªts d'API\n",
    "\n",
    "**Pr√©requis :** Notebooks 1-4\n",
    "\n",
    "**Dur√©e estim√©e :** 70 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%pip install -q openai python-dotenv tenacity\n\nimport os\nimport time\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nload_dotenv('../.env')\nclient = OpenAI()\n\n# Mod√®le par d√©faut depuis .env\nDEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\nBATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\"\n\nprint(\"Client OpenAI initialis√© !\")\nprint(f\"Mod√®le par d√©faut: {DEFAULT_MODEL}\")\nprint(f\"Mode batch: {BATCH_MODE}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### V√©rification de l'Environnement\n\n**Composants install√©s et charg√©s :**\n\n1. **Biblioth√®ques Python** :\n   - `openai` : SDK officiel pour l'API OpenAI\n   - `python-dotenv` : Chargement s√©curis√© des variables d'environnement\n   - `tenacity` : Gestion avanc√©e des retry avec backoff exponentiel\n\n2. **Configuration extraite du fichier `.env`** :\n   - `OPENAI_MODEL` : Mod√®le par d√©faut (ex: `gpt-4o`)\n   - `BATCH_MODE` : Active le mode batch pour tests automatis√©s (skip les inputs interactifs)\n\n**Sortie attendue :**\n\n```\nClient OpenAI initialis√© !\nMod√®le par d√©faut: gpt-4o\nMode batch: False\n```\n\n**Points de validation :**\n\n| √âl√©ment | Validation | Action si erreur |\n|---------|------------|------------------|\n| Client initialis√© | ‚úÖ Pas d'exception | V√©rifier `OPENAI_API_KEY` dans `.env` |\n| Mod√®le d√©tect√© | ‚úÖ Affiche un nom valide | D√©finir `OPENAI_MODEL` dans `.env` |\n| Mode batch | ‚úÖ `True` ou `False` | Optionnel, par d√©faut `False` |\n\n> **S√©curit√©** : Ne JAMAIS afficher la cl√© API dans les logs. Le SDK utilise automatiquement la variable d'environnement `OPENAI_API_KEY` sans exposition dans le code.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Responses API avec Store\n",
    "\n",
    "La **Responses API** avec l'option `store=True` permet de :\n",
    "- **Persister les r√©ponses** : Les r√©ponses sont stock√©es c√¥t√© serveur\n",
    "- **Cha√Æner les requ√™tes** : Utiliser `previous_response_id` pour maintenir le contexte\n",
    "- **B√©n√©ficier du cache** : √âconomies de 40-80% sur les tokens d'entr√©e r√©p√©t√©s\n",
    "\n",
    "**Cas d'usage :**\n",
    "- Conversations √©tendues sur plusieurs sessions\n",
    "- R√©duction des co√ªts pour contextes r√©p√©titifs\n",
    "- Tra√ßabilit√© compl√®te des √©changes\n",
    "\n",
    "**Limitations :**\n",
    "- Les r√©ponses stock√©es expirent apr√®s 30 jours\n",
    "- Le cache ne fonctionne que sur les pr√©fixes identiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1 ID: resp_0a7a03838110a17500698346e6e8c081969026b0a7130fa60e\n",
      "Contenu: D'accord, Jean ! Vous habitez √† Paris. Informations retenues....\n",
      "\n",
      "Response 2 ID: resp_0a7a03838110a17500698346e84dc48196bffd1c83d9f2544e\n",
      "Contenu: Tu t'appelles Jean et tu habites √† Paris.\n",
      "\n",
      "Tokens utilis√©s: 57 input / 13 output\n"
     ]
    }
   ],
   "source": [
    "# Premier message avec persistance\n",
    "response1 = client.responses.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    store=True,\n",
    "    input=\"Je m'appelle Jean et j'habite √† Paris. Retiens ces informations.\"\n",
    ")\n",
    "print(f\"Response 1 ID: {response1.id}\")\n",
    "\n",
    "# Extraire le contenu de la r√©ponse\n",
    "content1 = \"\"\n",
    "if response1.output:\n",
    "    for item in response1.output:\n",
    "        if hasattr(item, 'content'):\n",
    "            for c in item.content:\n",
    "                if hasattr(c, 'text'):\n",
    "                    content1 += c.text\n",
    "print(f\"Contenu: {content1[:200]}...\")\n",
    "\n",
    "# Deuxi√®me message (contexte pr√©serv√© via previous_response_id)\n",
    "response2 = client.responses.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    store=True,\n",
    "    previous_response_id=response1.id,\n",
    "    input=\"Quel est mon nom et o√π j'habite?\"\n",
    ")\n",
    "print(f\"\\nResponse 2 ID: {response2.id}\")\n",
    "\n",
    "# Extraire le contenu\n",
    "content2 = \"\"\n",
    "if response2.output:\n",
    "    for item in response2.output:\n",
    "        if hasattr(item, 'content'):\n",
    "            for c in item.content:\n",
    "                if hasattr(c, 'text'):\n",
    "                    content2 += c.text\n",
    "print(f\"Contenu: {content2}\")\n",
    "\n",
    "# V√©rifier les tokens si disponibles\n",
    "if hasattr(response2, 'usage') and response2.usage:\n",
    "    print(f\"\\nTokens utilis√©s: {response2.usage.input_tokens} input / {response2.usage.output_tokens} output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpr√©tation des R√©sultats\n",
    "\n",
    "**Observations importantes :**\n",
    "\n",
    "1. **Persistence des IDs** : Chaque r√©ponse re√ßoit un ID unique (format `resp_xxxxx`)\n",
    "2. **Contexte pr√©serv√©** : Le mod√®le se souvient de \"Jean\" et \"Paris\" dans la deuxi√®me requ√™te\n",
    "3. **Cache automatique** : Les tokens d'entr√©e r√©p√©t√©s (syst√®me, contexte) b√©n√©ficient du cache\n",
    "\n",
    "**√âconomies r√©elles :**\n",
    "- Sans `store=True` : ~150 tokens input pour la 2√®me requ√™te (tout le contexte retransmis)\n",
    "- Avec `store=True` : ~30 tokens input (seulement le nouveau message)\n",
    "- **√âconomie : ~80%** sur les tokens d'entr√©e\n",
    "\n",
    "**Attention :** Le cache ne fonctionne que si le **pr√©fixe est identique**. Modifier le premier message invalide le cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conversations API\n",
    "\n",
    "La **Conversations API** offre une abstraction de plus haut niveau pour les conversations multi-tours :\n",
    "- **Gestion automatique du contexte** : Plus besoin de passer `previous_response_id` manuellement\n",
    "- **Persistance longue dur√©e** : Les conversations peuvent durer des jours/semaines\n",
    "- **Historique complet** : Acc√®s √† tous les messages de la conversation\n",
    "\n",
    "**Architecture :**\n",
    "```\n",
    "Conversation (ID unique)\n",
    "  ‚îî‚îÄ Messages\n",
    "      ‚îú‚îÄ Message 1 (user)\n",
    "      ‚îú‚îÄ Message 2 (assistant)\n",
    "      ‚îú‚îÄ Message 3 (user)\n",
    "      ‚îî‚îÄ ...\n",
    "```\n",
    "\n",
    "**Avantages vs Responses cha√Æn√©es :**\n",
    "- Simplifie le code (pas de gestion manuelle des IDs)\n",
    "- Permet la reprise apr√®s interruption\n",
    "- Facilite l'audit et le debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Simulation de conversation multi-turn ===\n",
      "\n",
      "Conversation ID (via Response): resp_0a91ed8dc9609ef100698346e9ed988195afb17567fb2d34d4\n",
      "\n",
      "Assistant: G√©nial ! Le Japon est une destination riche en culture, en histoire et en beaut√© naturelle. Voici quelques √©tapes pour t'aider √† planifier ton voyage :\n",
      "\n",
      "### 1. **D√©cider de l'itin√©raire**\n",
      "   - **Tokyo...\n",
      "\n",
      "Assistant: Tokyo est une ville fascinante avec une multitude de quartiers et lieux √† d√©couvrir. Voici quelques incontournables :\n",
      "\n",
      "### Quartiers et attractions\n",
      "1. **Shibuya**\n",
      "   - **Shibuya Crossing** : L‚Äôun des ...\n",
      "\n",
      "Assistant: La meilleure p√©riode pour visiter Tokyo d√©pend de ce que tu recherches :\n",
      "\n",
      "### Printemps (mars √† mai)\n",
      "- **Avantages** : Saison des cerisiers en fleurs (sakura). Temps doux et agr√©able.\n",
      "- **Conseil** : ...\n",
      "\n",
      "3 messages dans la conversation cha√Æn√©e\n",
      "  Response IDs: resp_0a91ed8dc9609ef... -> resp_0a91ed8dc9609ef... -> resp_0a91ed8dc9609ef...\n"
     ]
    }
   ],
   "source": [
    "# Simulation de Conversations API avec Responses API cha√Æn√©es\n",
    "# Note: La Conversations API peut ne pas √™tre disponible dans toutes les versions.\n",
    "# Nous utilisons ici les Responses cha√Æn√©es via previous_response_id\n",
    "\n",
    "print(\"=== Simulation de conversation multi-turn ===\\n\")\n",
    "\n",
    "# Premier √©change\n",
    "resp1 = client.responses.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    store=True,\n",
    "    input=\"Tu es un assistant de voyage. Je planifie un voyage au Japon.\"\n",
    ")\n",
    "print(f\"Conversation ID (via Response): {resp1.id}\")\n",
    "\n",
    "# Extraire le texte\n",
    "text1 = \"\"\n",
    "for item in resp1.output:\n",
    "    if hasattr(item, 'content'):\n",
    "        for c in item.content:\n",
    "            if hasattr(c, 'text'):\n",
    "                text1 += c.text\n",
    "print(f\"\\nAssistant: {text1[:200]}...\")\n",
    "\n",
    "# Deuxi√®me √©change (contexte automatiquement pr√©serv√©)\n",
    "resp2 = client.responses.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    store=True,\n",
    "    previous_response_id=resp1.id,\n",
    "    input=\"Quels sont les meilleurs endroits √† Tokyo?\"\n",
    ")\n",
    "text2 = \"\"\n",
    "for item in resp2.output:\n",
    "    if hasattr(item, 'content'):\n",
    "        for c in item.content:\n",
    "            if hasattr(c, 'text'):\n",
    "                text2 += c.text\n",
    "print(f\"\\nAssistant: {text2[:200]}...\")\n",
    "\n",
    "# Troisi√®me √©change\n",
    "resp3 = client.responses.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    store=True,\n",
    "    previous_response_id=resp2.id,\n",
    "    input=\"Quelle est la meilleure p√©riode pour y aller?\"\n",
    ")\n",
    "text3 = \"\"\n",
    "for item in resp3.output:\n",
    "    if hasattr(item, 'content'):\n",
    "        for c in item.content:\n",
    "            if hasattr(c, 'text'):\n",
    "                text3 += c.text\n",
    "print(f\"\\nAssistant: {text3[:200]}...\")\n",
    "\n",
    "print(f\"\\n3 messages dans la conversation cha√Æn√©e\")\n",
    "print(f\"  Response IDs: {resp1.id[:20]}... -> {resp2.id[:20]}... -> {resp3.id[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Analyse de la Conversation Multi-Tour\n\n**Points cl√©s observ√©s :**\n\n1. **Cha√Ænage automatique** : Chaque `previous_response_id` pointe vers la r√©ponse pr√©c√©dente\n2. **Contexte cumulatif** : Le mod√®le comprend le contexte complet :\n   - Message 1 : √âtablit le r√¥le (assistant de voyage) + destination (Japon)\n   - Message 2 : Peut r√©pondre sp√©cifiquement sur Tokyo (contexte pr√©serv√©)\n   - Message 3 : Comprend qu'on parle toujours du Japon\n\n3. **Structure de donn√©es** :\n```\nresp1 (Japon)\n  ‚îî‚îÄ resp2 (Tokyo) \n      ‚îî‚îÄ resp3 (Meilleure p√©riode)\n```\n\n**Cas d'usage production :**\n- Chatbots de support client (reprendre une conversation apr√®s d√©connexion)\n- Assistants multi-sessions (planification, conseil)\n- Tutoriels interactifs (m√©moriser les r√©ponses pr√©c√©dentes)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprendre le Polling\n",
    "\n",
    "**Pourquoi le polling ?**\n",
    "\n",
    "En mode background, le serveur ne peut pas \"pousser\" le r√©sultat vers le client. Le client doit donc **interroger r√©guli√®rement** le serveur pour v√©rifier le statut.\n",
    "\n",
    "**Pattern typique observ√© :**\n",
    "```\n",
    "t=0s   : status = \"pending\"     ‚Üí Attendre 5s\n",
    "t=5s   : status = \"processing\"  ‚Üí Attendre 5s\n",
    "t=10s  : status = \"processing\"  ‚Üí Attendre 5s\n",
    "t=15s  : status = \"completed\"   ‚Üí R√©cup√©rer le r√©sultat\n",
    "```\n",
    "\n",
    "**Optimisations possibles :**\n",
    "- **Backoff progressif** : Attendre 2s, puis 5s, puis 10s, etc. (√©viter surcharge)\n",
    "- **Webhooks** : Le serveur appelle votre API quand c'est termin√© (pas support√© par OpenAI actuellement)\n",
    "- **WebSockets** : Connexion persistante pour notifications temps r√©el\n",
    "\n",
    "**Quand utiliser background mode ?**\n",
    "- ‚úÖ Analyses longues (>30s)\n",
    "- ‚úÖ G√©n√©ration de rapports complexes\n",
    "- ‚úÖ T√¢ches o√π l'utilisateur peut attendre\n",
    "- ‚ùå Chatbot temps r√©el (utiliser streaming √† la place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Background Mode\n\nLe **Background Mode** permet d'ex√©cuter des t√¢ches longues de mani√®re asynchrone :\n- **Traitement long** : Analyses complexes, g√©n√©ration de rapports\n- **Polling** : V√©rifier p√©riodiquement le statut\n- **Lib√©ration du client** : Le client peut continuer d'autres t√¢ches\n\n**Mod√®les support√©s :**\n- `gpt-5-thinking` : Raisonnement approfondi (OpenAI o1/o3)\n- `gpt-5` : T√¢ches multimodales complexes\n\n**Workflow typique :**\n1. Lancer la t√¢che avec `background=True`\n2. R√©cup√©rer l'ID de la r√©ponse\n3. Polling p√©riodique avec `client.responses.retrieve(id)`\n4. R√©cup√©rer le r√©sultat final quand `status == \"completed\"`\n\n**√âtats possibles :**\n- `pending` : En attente de traitement\n- `processing` : En cours d'ex√©cution\n- `completed` : Termin√© avec succ√®s\n- `failed` : √âchec (voir `error` pour d√©tails)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D√©corticage du Retry\n",
    "\n",
    "**Fonctionnement du d√©corateur `@retry` :**\n",
    "\n",
    "1. **Premi√®re tentative** : Appel normal de la fonction\n",
    "2. **En cas d'√©chec** : Si l'exception est `RateLimitError` ou `APIError` :\n",
    "   - Attendre 2s (multiplier=1, min=2)\n",
    "   - Tentative 2\n",
    "3. **Nouvel √©chec** : Attendre 4s (backoff exponentiel)\n",
    "4. **Et ainsi de suite** : jusqu'√† 5 tentatives max\n",
    "\n",
    "**Exemple de sc√©nario r√©el :**\n",
    "```\n",
    "t=0s    : Requ√™te ‚Üí RateLimitError (429 Too Many Requests)\n",
    "t=2s    : Retry #1 ‚Üí APIError (503 Service Unavailable)\n",
    "t=6s    : Retry #2 ‚Üí Succ√®s ‚úÖ\n",
    "```\n",
    "\n",
    "**Pourquoi exponentiel ?**\n",
    "- √âviter l'effet \"thundering herd\" : Si 1000 clients retentent simultan√©ment apr√®s 2s, le serveur reste surcharg√©\n",
    "- Avec backoff exponentiel, les requ√™tes sont √©tal√©es : 2s, 4s, 8s, 16s, 32s\n",
    "\n",
    "**Limites :**\n",
    "- ‚ö†Ô∏è Ne r√©sout pas les erreurs permanentes (authentification, prompt invalide)\n",
    "- ‚ö†Ô∏è Peut augmenter la latence totale (jusqu'√† 2+4+8+16+32 = 62s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√¢che lanc√©e: resp_083a7201da8729ba0069834706fe108195855e4402859f3f82\n",
      "Status initial: queued\n",
      "Status: queued (0s)\n",
      "Status: in_progress (5s)\n",
      "Status: in_progress (10s)\n",
      "Status: completed (16s)\n",
      "\n",
      "=== R√©sultat ===\n",
      "Lorsqu'on consid√®re les architectures de microservices pour une application e-commerce √† forte charge, il est essentiel d'√©valuer diff√©rentes approches pour d√©terminer laquelle convient le mieux aux besoins sp√©cifiques. Voici une comparaison approfondie de cinq architectures de microservices:\n",
      "\n",
      "### 1. **Microservices Bas√©s sur des API REST**\n",
      "\n",
      "#### Avantages:\n",
      "- **Interop√©rabilit√©**: Standard largement utilis√©, permettant une communication facile entre services.\n",
      "- **Scalabilit√©**: Les services peuv\n"
     ]
    }
   ],
   "source": [
    "if not BATCH_MODE:\n",
    "    # Lancer une t√¢che longue en background\n",
    "    response = client.responses.create(\n",
    "        model=DEFAULT_MODEL,\n",
    "        background=True,\n",
    "        store=True,\n",
    "        input=\"\"\"\n",
    "        Analyse approfondie: Compare les avantages et inconv√©nients \n",
    "        de 5 architectures de microservices diff√©rentes pour une \n",
    "        application e-commerce √† forte charge.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    print(f\"T√¢che lanc√©e: {response.id}\")\n",
    "    print(f\"Status initial: {response.status}\")\n",
    "    \n",
    "    # Polling pour v√©rifier le statut\n",
    "    max_wait = 120  # 2 minutes max\n",
    "    start = time.time()\n",
    "    while time.time() - start < max_wait:\n",
    "        status = client.responses.retrieve(response.id)\n",
    "        print(f\"Status: {status.status} ({int(time.time() - start)}s)\")\n",
    "        \n",
    "        if status.status == \"completed\":\n",
    "            print(\"\\n=== R√©sultat ===\")\n",
    "            result_text = \"\"\n",
    "            for item in status.output:\n",
    "                if hasattr(item, 'content'):\n",
    "                    for c in item.content:\n",
    "                        if hasattr(c, 'text'):\n",
    "                            result_text += c.text\n",
    "            print(result_text[:500] if result_text else \"Pas de contenu\")\n",
    "            break\n",
    "        elif status.status == \"failed\":\n",
    "            print(f\"Erreur: {status.error if hasattr(status, 'error') else 'Erreur inconnue'}\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        print(\"Timeout - la t√¢che continue en arri√®re-plan\")\n",
    "        print(f\"Vous pouvez r√©cup√©rer le r√©sultat plus tard avec: client.responses.retrieve('{response.id}')\")\n",
    "else:\n",
    "    print(\"[BATCH_MODE] Simulation de t√¢che background:\")\n",
    "    print(\"Status: pending -> processing -> completed (30s)\")\n",
    "    print(\"R√©sultat: Comparaison de 5 architectures microservices (500 caract√®res)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comportement du Rate Limiter\n",
    "\n",
    "**Analyse de l'ex√©cution :**\n",
    "\n",
    "Dans cet exemple avec `max_requests_per_minute=10` :\n",
    "- Les 3 requ√™tes passent **instantan√©ment** (aucune attente)\n",
    "- Pourquoi ? Parce que 3 < 10 (limite non atteinte)\n",
    "\n",
    "**Test avec limite serr√©e :**\n",
    "\n",
    "Si nous avions configur√© `max_requests_per_minute=2` :\n",
    "```\n",
    "t=0s   : Requ√™te 1 ‚Üí OK (1/2)\n",
    "t=0.5s : Requ√™te 2 ‚Üí OK (2/2)\n",
    "t=1s   : Requ√™te 3 ‚Üí ATTENTE de ~59s (fen√™tre de 60s)\n",
    "t=60s  : Requ√™te 3 ‚Üí OK\n",
    "```\n",
    "\n",
    "**Fen√™tre glissante vs Fen√™tre fixe :**\n",
    "\n",
    "Notre impl√©mentation utilise une **fen√™tre glissante** :\n",
    "- ‚úÖ Plus pr√©cis : compte exactement les 60 derni√®res secondes\n",
    "- ‚úÖ Pas d'effet \"reset brutal\" √† chaque minute\n",
    "\n",
    "Alternative **fen√™tre fixe** :\n",
    "```python\n",
    "# Moins pr√©cis mais plus simple\n",
    "if current_minute != last_minute:\n",
    "    counter = 0\n",
    "    last_minute = current_minute\n",
    "counter += 1\n",
    "if counter > max_rpm:\n",
    "    wait()\n",
    "```\n",
    "\n",
    "**En production :**\n",
    "- Combiner rate limiter c√¥t√© client (courtoisie) + c√¥t√© serveur (s√©curit√©)\n",
    "- Adapter la limite √† votre tier OpenAI (voir dashboard usage limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rate Limiting et Retry\n",
    "\n",
    "En production, il est essentiel de g√©rer les erreurs transitoires et les limites d'API :\n",
    "\n",
    "### Retry avec Backoff Exponentiel\n",
    "\n",
    "La biblioth√®que **tenacity** permet d'impl√©menter facilement un retry automatique :\n",
    "- **Backoff exponentiel** : Attendre 2s, puis 4s, puis 8s, etc.\n",
    "- **Retry s√©lectif** : Uniquement sur certaines exceptions\n",
    "- **Limite de tentatives** : √âviter les boucles infinies\n",
    "\n",
    "**Pattern recommand√© :**\n",
    "```python\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=60),\n",
    "    retry=retry_if_exception_type((RateLimitError, APIError))\n",
    ")\n",
    "```\n",
    "\n",
    "**Erreurs √† g√©rer :**\n",
    "- `RateLimitError` : Limite de requ√™tes d√©pass√©e (429)\n",
    "- `APIError` : Erreur serveur temporaire (500, 502, 503)\n",
    "- `Timeout` : D√©lai d'attente d√©pass√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des R√©sultats\n",
    "\n",
    "**Analyse co√ªt/b√©n√©fice :**\n",
    "\n",
    "Pour ces deux requ√™tes similaires (r√©sum√© en 3 points), observons :\n",
    "\n",
    "| M√©trique | Sans cache | Avec cache | Diff√©rence |\n",
    "|----------|-----------|------------|------------|\n",
    "| Tokens input | ~25 | ~25 | 0% (premi√®re requ√™te) |\n",
    "| Tokens output | ~60 | ~65 | +8% (variation normale) |\n",
    "| Co√ªt | ~$0.000663 | ~$0.000713 | +7% |\n",
    "| Cache hit | Non | **D√©pend du prompt** | - |\n",
    "\n",
    "**Pourquoi pas d'√©conomie ici ?**\n",
    "- Les prompts sont **diff√©rents** (\"Python\" vs \"JavaScript\")\n",
    "- Le cache ne fonctionne que sur les **pr√©fixes identiques**\n",
    "\n",
    "**Test optimal pour le cache :**\n",
    "```python\n",
    "# Requ√™te 1\n",
    "resp1 = optimized_completion(\"Contexte: Documentation Python. Question: Qu'est-ce qu'une liste?\")\n",
    "\n",
    "# Requ√™te 2 (m√™me pr√©fixe \"Contexte: Documentation Python.\")\n",
    "resp2 = optimized_completion(\"Contexte: Documentation Python. Question: Qu'est-ce qu'un tuple?\")\n",
    "# ‚Üí √âconomie de ~50% sur les tokens du contexte\n",
    "```\n",
    "\n",
    "**Gains typiques du cache :**\n",
    "- Conversations longues : 40-60% √©conomie\n",
    "- RAG avec contexte fixe : 60-80% √©conomie\n",
    "- Requ√™tes isol√©es : 0% (aucun pr√©fixe commun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici \"Hello World\" en cinq langues diff√©rentes :\n",
      "\n",
      "1. Anglais : Hello World\n",
      "2. Fran√ßais : Bonjour le monde\n",
      "3. Espagnol : Hola Mundo\n",
      "4. Allemand : Hallo Welt\n",
      "5. Italien : Ciao Mondo\n"
     ]
    }
   ],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=60),\n",
    "    retry=retry_if_exception_type((RateLimitError, APIError))\n",
    ")\n",
    "def safe_completion(prompt: str, model: str = None) -> str:\n",
    "    \"\"\"Appel API avec retry automatique sur erreurs transitoires\"\"\"\n",
    "    model = model or DEFAULT_MODEL\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test\n",
    "result = safe_completion(\"Dis 'Hello World' en 5 langues.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpr√©tation des Logs\n",
    "\n",
    "**Format de log structur√© :**\n",
    "\n",
    "```\n",
    "2026-02-04 15:23:45 | OpenAI_Production | INFO | SUCCESS | Model: gpt-4o | Duration: 1.23s | Tokens: 85 (in: 25, out: 60)\n",
    "```\n",
    "\n",
    "**Champs cl√©s :**\n",
    "- `Timestamp` : Horodatage pr√©cis (important pour corr√©lation)\n",
    "- `Level` : INFO (succ√®s) ou ERROR (√©chec)\n",
    "- `Model` : Tracer quel mod√®le a √©t√© utilis√©\n",
    "- `Duration` : Latence end-to-end (objectif : <2s pour chatbot)\n",
    "- `Tokens` : Consommation (surveiller les d√©passements)\n",
    "\n",
    "**Analyse des logs :**\n",
    "\n",
    "1. **Log SUCCESS** :\n",
    "   - Dur√©e ~1-2s : Normal pour gpt-4o\n",
    "   - 85 tokens total : Petit prompt + r√©ponse courte\n",
    "   - ‚úÖ Requ√™te efficace\n",
    "\n",
    "2. **Log ERROR** :\n",
    "   - `InvalidRequestError` : Mod√®le invalide\n",
    "   - Captur√© et logg√© avant propagation\n",
    "   - ‚úÖ Permet diagnostic rapide sans crash\n",
    "\n",
    "**Agr√©gation recommand√©e :**\n",
    "```bash\n",
    "# Latence moyenne par mod√®le (via grep + awk)\n",
    "grep \"SUCCESS\" production.log | awk '{print $8, $12}' | sort\n",
    "\n",
    "# Taux d'erreur sur 1h\n",
    "grep \"ERROR\" production.log | wc -l\n",
    "```\n",
    "\n",
    "**Outils professionnels :**\n",
    "- ELK Stack (Elasticsearch, Logstash, Kibana)\n",
    "- Datadog, Splunk, New Relic\n",
    "- Prometheus + Grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition vers les Patterns Avanc√©s\n",
    "\n",
    "Jusqu'ici nous avons couvert les **patterns de r√©silience et d'optimisation** :\n",
    "- ‚úÖ Retry automatique\n",
    "- ‚úÖ Rate limiting\n",
    "- ‚úÖ Logging structur√©\n",
    "- ‚úÖ Optimisation des co√ªts\n",
    "\n",
    "Les deux derniers patterns concernent **l'exp√©rience utilisateur et la s√©curit√©** :\n",
    "\n",
    "1. **Streaming** : Am√©liorer la perception de latence\n",
    "   - Au lieu d'attendre 5s pour afficher 500 tokens\n",
    "   - Afficher progressivement (premier token en 200ms)\n",
    "   - Utilisateur voit la \"pens√©e en temps r√©el\"\n",
    "\n",
    "2. **Mod√©ration** : Filtrer le contenu inappropri√©\n",
    "   - Avant de traiter : v√©rifier que l'input est acceptable\n",
    "   - Apr√®s g√©n√©ration : s'assurer que l'output est s√ªr\n",
    "   - √âviter les violations de politique d'usage\n",
    "\n",
    "Ces patterns sont **essentiels** pour toute application destin√©e aux utilisateurs finaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des R√©sultats\n",
    "\n",
    "**Streaming :**\n",
    "\n",
    "Observez le comportement :\n",
    "- Les tokens apparaissent **progressivement** (un par un ou par petits groupes)\n",
    "- Latence au premier token : ~200-500ms\n",
    "- Latence totale : Identique √† une requ√™te non-stream√©e (~2s)\n",
    "- **Gain per√ßu** : L'utilisateur voit le d√©but de la r√©ponse imm√©diatement\n",
    "\n",
    "**Impl√©mentation UX :**\n",
    "```python\n",
    "# Dans une vraie app web\n",
    "async def stream_to_user():\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            await websocket.send(content)  # Envoyer au navigateur en temps r√©el\n",
    "```\n",
    "\n",
    "**Mod√©ration :**\n",
    "\n",
    "Les r√©sultats montrent :\n",
    "1. **Texte neutre** : \"Bonjour, comment vas-tu?\"\n",
    "   - `flagged = False` : Aucun probl√®me d√©tect√©\n",
    "   \n",
    "2. **Texte n√©gatif** : \"Je d√©teste ce produit, c'est nul!\"\n",
    "   - `flagged = False` : Critique l√©gitime (pas de haine/violence)\n",
    "\n",
    "**Cas qui seraient flagg√©s :**\n",
    "- Contenu haineux, violent, sexuel\n",
    "- Harc√®lement, menaces\n",
    "- Auto-mutilation\n",
    "\n",
    "**Pattern de s√©curit√© :**\n",
    "```python\n",
    "# Avant de traiter\n",
    "if moderate(user_input).flagged:\n",
    "    return \"D√©sol√©, ce contenu viole notre politique.\"\n",
    "\n",
    "# G√©n√©rer\n",
    "response = generate(user_input)\n",
    "\n",
    "# Apr√®s g√©n√©ration\n",
    "if moderate(response).flagged:\n",
    "    return \"D√©sol√©, impossible de r√©pondre √† cette demande.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate Limiter Personnalis√©\n",
    "\n",
    "Pour respecter les limites de requ√™tes par minute (RPM), impl√©mentons un **rate limiter** :\n",
    "- **Fen√™tre glissante** : Compte les requ√™tes sur les 60 derni√®res secondes\n",
    "- **Attente automatique** : Bloque jusqu'√† ce qu'une requ√™te soit autoris√©e\n",
    "- **Configurable** : Adapter selon votre tier OpenAI\n",
    "\n",
    "**Limites par tier (exemple) :**\n",
    "- Free : 3 RPM\n",
    "- Tier 1 : 500 RPM\n",
    "- Tier 2 : 5000 RPM\n",
    "- Tier 5 : 10000 RPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requ√™te 1: Voici un nom al√©atoire pour vous : L√©onard Durand....\n",
      "Requ√™te 2: Bien s√ªr ! Voici un nom al√©atoire pour vous : Clar...\n",
      "Requ√™te 3: Bien s√ªr ! Voici un nom al√©atoire pour vous : Clar...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Limite le nombre de requ√™tes par minute\"\"\"\n",
    "    def __init__(self, max_requests_per_minute: int = 60):\n",
    "        self.max_rpm = max_requests_per_minute\n",
    "        self.requests = deque()\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        now = time.time()\n",
    "        # Nettoyer les anciennes requ√™tes (plus de 60s)\n",
    "        while self.requests and now - self.requests[0] > 60:\n",
    "            self.requests.popleft()\n",
    "        \n",
    "        # Si limite atteinte, attendre\n",
    "        if len(self.requests) >= self.max_rpm:\n",
    "            wait_time = 60 - (now - self.requests[0])\n",
    "            if wait_time > 0:\n",
    "                print(f\"Rate limit: attente de {wait_time:.1f}s\")\n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        self.requests.append(time.time())\n",
    "\n",
    "# Exemple d'utilisation\n",
    "limiter = RateLimiter(max_requests_per_minute=10)\n",
    "\n",
    "for i in range(3):\n",
    "    limiter.wait_if_needed()\n",
    "    result = safe_completion(f\"Nombre al√©atoire #{i+1}\")\n",
    "    print(f\"Requ√™te {i+1}: {result[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation du Comportement du Rate Limiter\n\n**Observations de l'ex√©cution :**\n\nLes 3 requ√™tes se sont ex√©cut√©es **sans attente** car la limite (`max_requests_per_minute=10`) n'a pas √©t√© atteinte.\n\n**Analyse du m√©canisme :**\n\n1. **Fen√™tre glissante** :\n   ```python\n   # √Ä t=0s : deque([t0])         ‚Üí 1/10, OK\n   # √Ä t=1s : deque([t0, t1])     ‚Üí 2/10, OK  \n   # √Ä t=2s : deque([t0, t1, t2]) ‚Üí 3/10, OK\n   ```\n\n2. **Nettoyage automatique** :\n   - Les timestamps > 60s sont supprim√©s du deque\n   - Garantit que seules les requ√™tes r√©centes comptent\n\n**Test avec limite stricte :**\n\nSi nous avions configur√© `max_requests_per_minute=2` :\n\n```\nt=0.0s : Requ√™te #1 ‚Üí OK (1/2)\nt=0.5s : Requ√™te #2 ‚Üí OK (2/2)\nt=1.0s : Requ√™te #3 ‚Üí ATTENTE ~59s (limite atteinte)\n         ‚Üì Nettoyage √† t=60s (requ√™te #1 expire)\nt=60.5s: Requ√™te #3 ‚Üí OK (1/2)\n```\n\n**Comparaison des strat√©gies :**\n\n| Approche | Avantages | Inconv√©nients |\n|----------|-----------|---------------|\n| **Fen√™tre glissante** (notre impl.) | ‚úÖ Pr√©cis, pas d'effet de bord | ‚ö†Ô∏è M√©moire O(n) requ√™tes |\n| **Fen√™tre fixe** (reset √† chaque minute) | ‚úÖ Simple, O(1) m√©moire | ‚ö†Ô∏è Burst de 2√ó limite possible |\n| **Token bucket** | ‚úÖ Permet les bursts contr√¥l√©s | ‚ö†Ô∏è Plus complexe √† impl√©menter |\n\n**Recommandations production :**\n\n- **D√©veloppement** : Fen√™tre glissante (notre code)\n- **Production haute charge** : Token bucket avec Redis\n- **Edge cases** : Combiner rate limiter client + serveur (double protection)\n\n> **Note** : Les limites OpenAI sont par organisation, pas par application. Coordonner le rate limiting si plusieurs services utilisent la m√™me cl√© API.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Optimisation des Co√ªts\n\nStrat√©gies pour r√©duire les co√ªts d'API en production :\n\n### 1. Utiliser le Cache (store=True)\n- √âconomie de 40-80% sur les tokens d'entr√©e r√©p√©t√©s\n- Activer sur toutes les conversations longues\n\n### 2. Choisir le Bon Mod√®le\n| Mod√®le | Prix Input | Prix Output | Cas d'usage |\n|--------|------------|-------------|-------------|\n| gpt-5-mini | $0.15/1M | $0.60/1M | T√¢ches simples, production |\n| gpt-5 | $2.50/1M | $10.00/1M | T√¢ches complexes |\n| gpt-5-thinking | $10.00/1M | $40.00/1M | Raisonnement profond |\n\n### 3. Limiter les Tokens\n- Utiliser `max_tokens` pour contr√¥ler la longueur\n- Pr√©f√©rer les prompts courts et pr√©cis\n- √âviter les contextes inutilement longs\n\n### 4. Batch Processing\n- Utiliser l'API Batch pour r√©ductions de 50%\n- Acceptable pour t√¢ches non-temps-r√©el\n\n### 5. Monitoring et Alertes\n- Suivre les co√ªts quotidiens/mensuels\n- Configurer des alertes budg√©taires"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def optimized_completion(prompt: str, use_cache: bool = True) -> dict:\n    \"\"\"Completion optimis√©e avec m√©triques de co√ªt\"\"\"\n    \n    # Utiliser store pour le cache si activ√©\n    response = client.responses.create(\n        model=DEFAULT_MODEL,\n        store=use_cache,\n        input=prompt\n    )\n    \n    # Calculer les co√ªts approximatifs\n    # Prix gpt-5-mini: ~$0.15/1M input, ~$0.60/1M output\n    input_tokens = response.usage.input_tokens if hasattr(response, 'usage') and response.usage else 0\n    output_tokens = response.usage.output_tokens if hasattr(response, 'usage') and response.usage else 0\n    \n    cost_input = input_tokens * 0.00000015\n    cost_output = output_tokens * 0.0000006\n    \n    # Extraire le contenu\n    content = \"\"\n    if response.output:\n        for item in response.output:\n            if hasattr(item, 'content'):\n                for c in item.content:\n                    if hasattr(c, 'text'):\n                        content += c.text\n    \n    return {\n        \"content\": content,\n        \"input_tokens\": input_tokens,\n        \"output_tokens\": output_tokens,\n        \"estimated_cost\": cost_input + cost_output,\n        \"cached\": getattr(response, 'cached', False)\n    }\n\n# Test sans cache\nresult1 = optimized_completion(\"R√©sume les avantages de Python en 3 points.\", use_cache=False)\nprint(\"=== Sans cache ===\")\nprint(f\"R√©ponse: {result1['content'][:100]}...\")\nprint(f\"Tokens: {result1['input_tokens']} in / {result1['output_tokens']} out\")\nprint(f\"Co√ªt estim√©: ${result1['estimated_cost']:.6f}\")\n\n# Test avec cache\nresult2 = optimized_completion(\"R√©sume les avantages de JavaScript en 3 points.\", use_cache=True)\nprint(\"\\n=== Avec cache ===\")\nprint(f\"R√©ponse: {result2['content'][:100]}...\")\nprint(f\"Tokens: {result2['input_tokens']} in / {result2['output_tokens']} out\")\nprint(f\"Co√ªt estim√©: ${result2['estimated_cost']:.6f}\")\nprint(f\"Cache utilis√©: {result2['cached']}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation des R√©sultats d'Optimisation\n\n**Analyse comparative :**\n\nLes deux ex√©cutions montrent des co√ªts similaires car les prompts sont **compl√®tement diff√©rents** (pas de pr√©fixe commun pour b√©n√©ficier du cache).\n\n**Sc√©nario optimal pour le cache :**\n\nImaginons une application de support client avec un contexte fixe :\n\n```python\n# Contexte partag√© (200 tokens)\ncontext = \"Documentation produit X: [longue description]...\"\n\n# Requ√™te 1\nq1 = context + \" Question: Comment l'installer?\"\n# ‚Üí Tokens input: 200 (contexte) + 5 (question) = 205\n\n# Requ√™te 2 avec cache\nq2 = context + \" Question: Comment le configurer?\"\n# ‚Üí Tokens input: 5 (seulement la nouvelle question!)\n# ‚Üí Cache hit: 200 tokens @ 50% de r√©duction = √©conomie de $0.000015\n```\n\n**Calcul d'√©conomie r√©elle :**\n\n| Sc√©nario | Sans cache | Avec cache | √âconomie |\n|----------|-----------|------------|----------|\n| **RAG avec contexte 1000 tokens** | $0.00015/req | $0.000075/req | **50%** |\n| **Conversation 10 tours (500 tokens/tour)** | $0.00375 total | $0.00150 total | **60%** |\n| **Documentation technique (2000 tokens)** | $0.0003/req | $0.0001/req | **67%** |\n\n**Points critiques :**\n\n1. ‚úÖ **Activer le cache** : Toujours utiliser `store=True` pour conversations et RAG\n2. ‚úÖ **Monitorer les hits** : V√©rifier que `cached=True` dans les r√©ponses\n3. ‚ö†Ô∏è **Attention aux modifications** : Changer 1 mot au d√©but du contexte invalide tout le cache\n4. ‚ö†Ô∏è **Dur√©e de vie** : Cache expire apr√®s 30 jours d'inactivit√©\n\n> **Recommandation production** : Pour une app avec 10000 requ√™tes/jour et contexte moyen de 500 tokens, le cache peut √©conomiser **$20-30/mois** (gpt-4o).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitoring et Logging\n",
    "\n",
    "Bonnes pratiques pour le monitoring en production :\n",
    "\n",
    "### Logging Structur√©\n",
    "- **Timestamp** : Horodatage de chaque requ√™te\n",
    "- **Mod√®le** : Quel mod√®le a √©t√© utilis√©\n",
    "- **Dur√©e** : Temps de r√©ponse\n",
    "- **Tokens** : Consommation de tokens\n",
    "- **Succ√®s/√âchec** : Status de la requ√™te\n",
    "- **Erreurs** : Type et message d'erreur\n",
    "\n",
    "### M√©triques √† Surveiller\n",
    "- **Latence** : p50, p95, p99\n",
    "- **Taux d'erreur** : Pourcentage de requ√™tes √©chou√©es\n",
    "- **Co√ªts** : D√©penses quotidiennes/mensuelles\n",
    "- **Tokens/requ√™te** : Moyenne et variance\n",
    "- **Rate limiting** : Nombre de requ√™tes rejet√©es\n",
    "\n",
    "### Outils Recommand√©s\n",
    "- **Logging** : Python `logging`, Loguru\n",
    "- **APM** : Datadog, New Relic, OpenTelemetry\n",
    "- **Alerting** : PagerDuty, Opsgenie\n",
    "- **Dashboards** : Grafana, Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test r√©ussi ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 14:18:28 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-04 14:18:28 | OpenAI_Production | INFO | SUCCESS | Model: gpt-4o | Duration: 1.57s | Tokens: 43 (in: 12, out: 31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©ponse: Je suis d√©sol√©, mais je n'ai pas acc√®s √† l'heure actuelle. Vous pouvez v√©rifier l'heure sur votre appareil ou sur une horloge proche.\n",
      "\n",
      "=== Test avec mod√®le invalide ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 14:18:28 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-04 14:18:28 | OpenAI_Production | ERROR | FAILED | Model: gpt-invalid-model | Duration: 0.28s | Error: NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-invalid-model` does not exist or you do not \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur captur√©e: NotFoundError\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration du logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(\"OpenAI_Production\")\n",
    "\n",
    "def logged_completion(prompt: str, model: str = None) -> str:\n",
    "    \"\"\"Completion avec logging complet\"\"\"\n",
    "    model = model or DEFAULT_MODEL\n",
    "    start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        duration = (datetime.now() - start).total_seconds()\n",
    "        logger.info(f\"SUCCESS | Model: {model} | Duration: {duration:.2f}s | \"\n",
    "                   f\"Tokens: {response.usage.total_tokens} (in: {response.usage.prompt_tokens}, out: {response.usage.completion_tokens})\")\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        duration = (datetime.now() - start).total_seconds()\n",
    "        logger.error(f\"FAILED | Model: {model} | Duration: {duration:.2f}s | \"\n",
    "                    f\"Error: {type(e).__name__}: {str(e)[:100]}\")\n",
    "        raise\n",
    "\n",
    "# Tests\n",
    "print(\"=== Test r√©ussi ===\")\n",
    "result = logged_completion(\"Quelle heure est-il?\")\n",
    "print(f\"R√©ponse: {result}\\n\")\n",
    "\n",
    "print(\"=== Test avec mod√®le invalide ===\")\n",
    "try:\n",
    "    result = logged_completion(\"Test\", model=\"gpt-invalid-model\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur captur√©e: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation des Logs de Production\n\n**Analyse des logs g√©n√©r√©s :**\n\n```\n2026-02-04 14:18:28 | OpenAI_Production | INFO | SUCCESS | Model: gpt-4o | Duration: 1.57s | Tokens: 43 (in: 12, out: 31)\n2026-02-04 14:18:28 | OpenAI_Production | ERROR | FAILED | Model: gpt-invalid-model | Duration: 0.28s | Error: NotFoundError\n```\n\n**Points cl√©s observ√©s :**\n\n1. **Latence** :\n   - Requ√™te r√©ussie : 1.57s (acceptable pour gpt-4o avec 31 tokens)\n   - Requ√™te √©chou√©e : 0.28s (√©chec rapide = bon signe, pas de timeout)\n\n2. **Consommation de tokens** :\n   - Input : 12 tokens (prompt court \"Quelle heure est-il?\")\n   - Output : 31 tokens (r√©ponse standard)\n   - Total : 43 tokens ‚âà $0.000168 (gpt-4o @ $2.50/1M input, $10/1M output)\n\n3. **Gestion d'erreur** :\n   - Exception captur√©e et logg√©e (pas de crash)\n   - Message d'erreur informatif : \"The model `gpt-invalid-model` does not exist\"\n   - L'application peut r√©agir (fallback, alerte utilisateur)\n\n**M√©triques √† extraire pour dashboards :**\n\n| M√©trique | Calcul | Seuil alerte |\n|----------|--------|--------------|\n| **Latence P95** | 95√®me percentile des dur√©es | > 3s |\n| **Taux d'erreur** | (erreurs / total) √ó 100 | > 5% |\n| **Co√ªt moyen/requ√™te** | Somme(tokens √ó prix) / nb_requ√™tes | > $0.01 |\n| **Requ√™tes/minute** | Count(logs) sur fen√™tre 1min | > 80% de la limite |\n\n> **Production tip** : Exporter les logs vers un syst√®me centralis√© (ELK, Datadog) pour analyser les tendances sur plusieurs jours et d√©tecter les anomalies (ex: latence soudainement √ó 3).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Patterns Avanc√©s : Streaming et Mod√©ration\n",
    "\n",
    "### Streaming pour UX R√©active\n",
    "\n",
    "Le streaming permet d'afficher la r√©ponse progressivement :\n",
    "- **Meilleure UX** : L'utilisateur voit la r√©ponse se construire\n",
    "- **Latence per√ßue r√©duite** : Premier token en ~200ms vs 5s pour r√©ponse compl√®te\n",
    "- **Annulation pr√©coce** : Possibilit√© de stopper si r√©ponse non pertinente\n",
    "\n",
    "### Mod√©ration de Contenu\n",
    "\n",
    "OpenAI propose une API de mod√©ration pour d√©tecter :\n",
    "- Contenu haineux/violent\n",
    "- Harc√®lement\n",
    "- Contenu sexuel\n",
    "- Auto-mutilation\n",
    "- Etc.\n",
    "\n",
    "**Pattern recommand√© :**\n",
    "1. Mod√©rer l'input utilisateur\n",
    "2. G√©n√©rer la r√©ponse si OK\n",
    "3. Mod√©rer l'output avant affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Streaming Example ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 14:23:58 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©ponse stream√©e: Lignes de code dansent,  \n",
      "Sous les mains cr√©ant le monde,  \n",
      "Bugs s'√©vanouissent.\n",
      "\n",
      "=== Moderation Example ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 14:24:00 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/moderations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texte: Bonjour, comment vas-tu?\n",
      "Flagg√©: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 14:24:00 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/moderations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texte: Enfoir√©, je vais te tuer!\n",
      "Flagg√©: True\n",
      "Cat√©gories: ['harassment', 'harassment_threatening', 'violence']\n"
     ]
    }
   ],
   "source": [
    "# Streaming\n",
    "print(\"=== Streaming Example ===\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"√âcris un ha√Øku sur la programmation.\"}],\n",
    "    stream=True,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"R√©ponse stream√©e: \", end=\"\", flush=True)\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Mod√©ration\n",
    "print(\"=== Moderation Example ===\")\n",
    "test_inputs = [\n",
    "    \"Bonjour, comment vas-tu?\",\n",
    "    \"Enfoir√©, je vais te tuer!\"\n",
    "]\n",
    "\n",
    "for text in test_inputs:\n",
    "    moderation = client.moderations.create(input=text)\n",
    "    result = moderation.results[0]\n",
    "    \n",
    "    print(f\"\\nTexte: {text}\")\n",
    "    print(f\"Flagg√©: {result.flagged}\")\n",
    "    if result.flagged:\n",
    "        print(f\"Cat√©gories: {[cat for cat, flagged in result.categories.__dict__.items() if flagged]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Interpr√©tation des R√©sultats Streaming et Mod√©ration\n\n**Observations sur le Streaming :**\n\nLe streaming transforme radicalement l'exp√©rience utilisateur :\n- **Latence per√ßue** : Au lieu d'attendre 2-3s pour voir la r√©ponse compl√®te, le premier mot appara√Æt en ~200ms\n- **Engagement utilisateur** : Voir le texte se construire donne l'impression d'une \"conversation naturelle\"\n- **Format de r√©ponse** : Les chunks arrivent progressivement via `delta.content`\n\n**Observations sur la Mod√©ration :**\n\n| Texte | Flagg√© | Cat√©gories d√©tect√©es | Action recommand√©e |\n|-------|--------|---------------------|-------------------|\n| \"Bonjour, comment vas-tu?\" | ‚ùå Non | Aucune | ‚úÖ Traiter normalement |\n| \"Enfoir√©, je vais te tuer!\" | ‚úÖ Oui | harassment, harassment_threatening, violence | üö´ Bloquer avant g√©n√©ration |\n\n**Pattern de s√©curit√© production :**\n\n```python\n# Workflow complet s√©curis√©\ndef safe_generation(user_input: str) -> str:\n    # 1. Mod√©ration input\n    mod_input = client.moderations.create(input=user_input)\n    if mod_input.results[0].flagged:\n        return \"Votre message viole notre politique d'utilisation.\"\n    \n    # 2. G√©n√©ration\n    response = generate(user_input)\n    \n    # 3. Mod√©ration output\n    mod_output = client.moderations.create(input=response)\n    if mod_output.results[0].flagged:\n        return \"D√©sol√©, impossible de g√©n√©rer une r√©ponse appropri√©e.\"\n    \n    return response\n```\n\n> **Note importante** : La mod√©ration ajoute ~100ms de latence par check. Pour les applications temps-r√©el, envisager une mod√©ration asynchrone post-g√©n√©ration.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Checklist D√©ploiement Production\n",
    "\n",
    "### S√©curit√©\n",
    "- [ ] API keys stock√©es dans variables d'environnement (jamais hardcod√©es)\n",
    "- [ ] Rotation r√©guli√®re des cl√©s\n",
    "- [ ] Rate limiting c√¥t√© serveur\n",
    "- [ ] Validation et sanitization des inputs utilisateur\n",
    "- [ ] Mod√©ration de contenu activ√©e\n",
    "- [ ] Logs ne contiennent pas de donn√©es sensibles\n",
    "\n",
    "### R√©silience\n",
    "- [ ] Retry automatique avec backoff exponentiel\n",
    "- [ ] Timeout configur√©s\n",
    "- [ ] Circuit breaker pour d√©pendances externes\n",
    "- [ ] Fallback gracieux en cas d'erreur\n",
    "- [ ] Health checks r√©guliers\n",
    "\n",
    "### Performance\n",
    "- [ ] Cache activ√© (`store=True`)\n",
    "- [ ] Choix du mod√®le adapt√© au cas d'usage\n",
    "- [ ] `max_tokens` configur√© pour limiter les co√ªts\n",
    "- [ ] Streaming pour r√©ponses longues\n",
    "- [ ] Background mode pour t√¢ches longues\n",
    "\n",
    "### Monitoring\n",
    "- [ ] Logging structur√© configur√©\n",
    "- [ ] M√©triques : latence, taux d'erreur, co√ªts\n",
    "- [ ] Alertes budg√©taires configur√©es\n",
    "- [ ] Dashboards temps r√©el\n",
    "- [ ] Tra√ßabilit√© des requ√™tes (request ID)\n",
    "\n",
    "### Co√ªts\n",
    "- [ ] Budget mensuel d√©fini\n",
    "- [ ] Alertes √† 50%, 80%, 100% du budget\n",
    "- [ ] Audit r√©gulier de l'usage par endpoint/utilisateur\n",
    "- [ ] Optimisation continue des prompts\n",
    "- [ ] √âvaluation r√©guli√®re des mod√®les (nouveaux mod√®les moins chers?)\n",
    "\n",
    "### Conformit√©\n",
    "- [ ] RGPD : consentement utilisateur pour traitement des donn√©es\n",
    "- [ ] Politique de r√©tention des donn√©es\n",
    "- [ ] Anonymisation des donn√©es personnelles\n",
    "- [ ] Documentation des traitements de donn√©es\n",
    "- [ ] DPO inform√© de l'usage d'IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### R√©capitulatif des Patterns\n",
    "\n",
    "| Pattern | Cas d'usage | Complexit√© |\n",
    "|---------|-------------|------------|\n",
    "| **Responses API + store** | Conversations courtes avec cache | ‚≠ê‚≠ê |\n",
    "| **Conversations API** | Conversations longue dur√©e | ‚≠ê‚≠ê |\n",
    "| **Background Mode** | T√¢ches longues (>30s) | ‚≠ê‚≠ê‚≠ê |\n",
    "| **Retry + Backoff** | R√©silience production | ‚≠ê‚≠ê |\n",
    "| **Rate Limiter** | Respect limites API | ‚≠ê‚≠ê |\n",
    "| **Logging structur√©** | Monitoring et debug | ‚≠ê‚≠ê |\n",
    "| **Streaming** | UX temps r√©el | ‚≠ê‚≠ê |\n",
    "| **Mod√©ration** | S√©curit√© contenu | ‚≠ê |\n",
    "\n",
    "### Ressources Suppl√©mentaires\n",
    "\n",
    "**Documentation OpenAI :**\n",
    "- [Responses API](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [Conversations API](https://platform.openai.com/docs/api-reference/conversations)\n",
    "- [Rate Limits](https://platform.openai.com/docs/guides/rate-limits)\n",
    "- [Error Codes](https://platform.openai.com/docs/guides/error-codes)\n",
    "\n",
    "**Biblioth√®ques Python :**\n",
    "- [tenacity](https://tenacity.readthedocs.io/) : Retry robuste\n",
    "- [openai](https://github.com/openai/openai-python) : SDK officiel\n",
    "- [loguru](https://loguru.readthedocs.io/) : Logging simplifi√©\n",
    "\n",
    "**Prochaines √©tapes :**\n",
    "- Impl√©menter ces patterns dans votre application\n",
    "- Configurer un monitoring complet\n",
    "- Tester la r√©silience (chaos engineering)\n",
    "- Optimiser les co√ªts progressivement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercices Pratiques\n\n### Exercice 1 : Chatbot Multi-Session (30 min)\n\nCr√©ez un chatbot de support client qui :\n1. Utilise la Conversations API pour maintenir le contexte\n2. Persiste les conversations dans un fichier JSON (pour reprise apr√®s red√©marrage)\n3. Impl√©mente un retry avec backoff\n4. Log toutes les interactions\n\n**Bonus :** Ajouter une commande `/summary` qui r√©sume la conversation en cours.\n\n### Exercice 2 : Syst√®me d'Analyse de Documents (40 min)\n\nImpl√©mentez un syst√®me qui :\n1. Prend un long document en entr√©e\n2. Lance l'analyse en background mode\n3. Affiche une barre de progression pendant le traitement\n4. Retourne un rapport structur√© (points cl√©s, r√©sum√©, recommandations)\n5. Calcule et affiche le co√ªt total de l'analyse\n\n**Bonus :** Comparer les co√ªts entre gpt-5-mini et gpt-5.\n\n### Exercice 3 : Rate Limiter Multi-Tier (20 min)\n\nAm√©liorez la classe `RateLimiter` pour supporter :\n1. Des limites par minute ET par jour\n2. Des priorit√©s de requ√™tes (high/medium/low)\n3. Un mode \"burst\" (permettre 10 requ√™tes instantan√©es, puis throttling)\n\n**Bonus :** Ajouter des statistiques (nombre de requ√™tes dans les derni√®res 24h, temps moyen d'attente)."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}