{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patterns de Production : APIs Avancées OpenAI\n",
    "\n",
    "Ce notebook couvre les fonctionnalités avancées nécessaires pour des applications en production :\n",
    "- **Conversations API** : Persistance d'état entre sessions\n",
    "- **Background Mode** : Tâches asynchrones longues\n",
    "- **Rate Limiting** : Gestion des limites d'API\n",
    "- **Optimisation** : Réduction des coûts\n",
    "\n",
    "**Objectifs :**\n",
    "- Gérer des conversations multi-sessions\n",
    "- Exécuter des tâches en arrière-plan\n",
    "- Implémenter des patterns de résilience\n",
    "- Optimiser les coûts d'API\n",
    "\n",
    "**Prérequis :** Notebooks 1-4\n",
    "\n",
    "**Durée estimée :** 70 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%pip install -q openai python-dotenv tenacity\n\nimport os\nimport time\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nload_dotenv('../.env')\nclient = OpenAI()\n\n# Modèle par défaut depuis .env\nDEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\nBATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\"\n\nprint(\"Client OpenAI initialisé !\")\nprint(f\"Modèle par défaut: {DEFAULT_MODEL}\")\nprint(f\"Mode batch: {BATCH_MODE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Responses API avec Store\n",
    "\n",
    "La **Responses API** avec l'option `store=True` permet de :\n",
    "- **Persister les réponses** : Les réponses sont stockées côté serveur\n",
    "- **Chaîner les requêtes** : Utiliser `previous_response_id` pour maintenir le contexte\n",
    "- **Bénéficier du cache** : Économies de 40-80% sur les tokens d'entrée répétés\n",
    "\n",
    "**Cas d'usage :**\n",
    "- Conversations étendues sur plusieurs sessions\n",
    "- Réduction des coûts pour contextes répétitifs\n",
    "- Traçabilité complète des échanges\n",
    "\n",
    "**Limitations :**\n",
    "- Les réponses stockées expirent après 30 jours\n",
    "- Le cache ne fonctionne que sur les préfixes identiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Premier message avec persistance\nresponse1 = client.responses.create(\n    model=DEFAULT_MODEL,\n    store=True,\n    input=\"Je m'appelle Jean et j'habite à Paris. Retiens ces informations.\"\n)\nprint(f\"Response 1 ID: {response1.id}\")\n\n# Extraire le contenu de la réponse\ncontent1 = \"\"\nif response1.output:\n    for item in response1.output:\n        if hasattr(item, 'content'):\n            for c in item.content:\n                if hasattr(c, 'text'):\n                    content1 += c.text\nprint(f\"Contenu: {content1[:200]}...\")\n\n# Deuxième message (contexte préservé via previous_response_id)\nresponse2 = client.responses.create(\n    model=DEFAULT_MODEL,\n    store=True,\n    previous_response_id=response1.id,\n    input=\"Quel est mon nom et où j'habite?\"\n)\nprint(f\"\\nResponse 2 ID: {response2.id}\")\n\n# Extraire le contenu\ncontent2 = \"\"\nif response2.output:\n    for item in response2.output:\n        if hasattr(item, 'content'):\n            for c in item.content:\n                if hasattr(c, 'text'):\n                    content2 += c.text\nprint(f\"Contenu: {content2}\")\n\n# Vérifier les tokens si disponibles\nif hasattr(response2, 'usage') and response2.usage:\n    print(f\"\\nTokens utilisés: {response2.usage.input_tokens} input / {response2.usage.output_tokens} output\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conversations API\n",
    "\n",
    "La **Conversations API** offre une abstraction de plus haut niveau pour les conversations multi-tours :\n",
    "- **Gestion automatique du contexte** : Plus besoin de passer `previous_response_id` manuellement\n",
    "- **Persistance longue durée** : Les conversations peuvent durer des jours/semaines\n",
    "- **Historique complet** : Accès à tous les messages de la conversation\n",
    "\n",
    "**Architecture :**\n",
    "```\n",
    "Conversation (ID unique)\n",
    "  └─ Messages\n",
    "      ├─ Message 1 (user)\n",
    "      ├─ Message 2 (assistant)\n",
    "      ├─ Message 3 (user)\n",
    "      └─ ...\n",
    "```\n",
    "\n",
    "**Avantages vs Responses chaînées :**\n",
    "- Simplifie le code (pas de gestion manuelle des IDs)\n",
    "- Permet la reprise après interruption\n",
    "- Facilite l'audit et le debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulation de Conversations API avec Responses API chaînées\n# Note: La Conversations API peut ne pas être disponible dans toutes les versions.\n# Nous utilisons ici les Responses chaînées via previous_response_id\n\nprint(\"=== Simulation de conversation multi-turn ===\\n\")\n\n# Premier échange\nresp1 = client.responses.create(\n    model=DEFAULT_MODEL,\n    store=True,\n    input=\"Tu es un assistant de voyage. Je planifie un voyage au Japon.\"\n)\nprint(f\"Conversation ID (via Response): {resp1.id}\")\n\n# Extraire le texte\ntext1 = \"\"\nfor item in resp1.output:\n    if hasattr(item, 'content'):\n        for c in item.content:\n            if hasattr(c, 'text'):\n                text1 += c.text\nprint(f\"\\nAssistant: {text1[:200]}...\")\n\n# Deuxième échange (contexte automatiquement préservé)\nresp2 = client.responses.create(\n    model=DEFAULT_MODEL,\n    store=True,\n    previous_response_id=resp1.id,\n    input=\"Quels sont les meilleurs endroits à Tokyo?\"\n)\ntext2 = \"\"\nfor item in resp2.output:\n    if hasattr(item, 'content'):\n        for c in item.content:\n            if hasattr(c, 'text'):\n                text2 += c.text\nprint(f\"\\nAssistant: {text2[:200]}...\")\n\n# Troisième échange\nresp3 = client.responses.create(\n    model=DEFAULT_MODEL,\n    store=True,\n    previous_response_id=resp2.id,\n    input=\"Quelle est la meilleure période pour y aller?\"\n)\ntext3 = \"\"\nfor item in resp3.output:\n    if hasattr(item, 'content'):\n        for c in item.content:\n            if hasattr(c, 'text'):\n                text3 += c.text\nprint(f\"\\nAssistant: {text3[:200]}...\")\n\nprint(f\"\\n3 messages dans la conversation chaînée\")\nprint(f\"  Response IDs: {resp1.id[:20]}... -> {resp2.id[:20]}... -> {resp3.id[:20]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Background Mode\n",
    "\n",
    "Le **Background Mode** permet d'exécuter des tâches longues de manière asynchrone :\n",
    "- **Traitement long** : Analyses complexes, génération de rapports\n",
    "- **Polling** : Vérifier périodiquement le statut\n",
    "- **Libération du client** : Le client peut continuer d'autres tâches\n",
    "\n",
    "**Modèles supportés :**\n",
    "- `gpt-5-thinking` : Raisonnement approfondi (OpenAI o1/o3)\n",
    "- `gpt-4o` : Tâches multimodales complexes\n",
    "\n",
    "**Workflow typique :**\n",
    "1. Lancer la tâche avec `background=True`\n",
    "2. Récupérer l'ID de la réponse\n",
    "3. Polling périodique avec `client.responses.retrieve(id)`\n",
    "4. Récupérer le résultat final quand `status == \"completed\"`\n",
    "\n",
    "**États possibles :**\n",
    "- `pending` : En attente de traitement\n",
    "- `processing` : En cours d'exécution\n",
    "- `completed` : Terminé avec succès\n",
    "- `failed` : Échec (voir `error` pour détails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not BATCH_MODE:\n    # Lancer une tâche longue en background\n    response = client.responses.create(\n        model=DEFAULT_MODEL,\n        background=True,\n        store=True,\n        input=\"\"\"\n        Analyse approfondie: Compare les avantages et inconvénients \n        de 5 architectures de microservices différentes pour une \n        application e-commerce à forte charge.\n        \"\"\"\n    )\n    \n    print(f\"Tâche lancée: {response.id}\")\n    print(f\"Status initial: {response.status}\")\n    \n    # Polling pour vérifier le statut\n    max_wait = 120  # 2 minutes max\n    start = time.time()\n    while time.time() - start < max_wait:\n        status = client.responses.retrieve(response.id)\n        print(f\"Status: {status.status} ({int(time.time() - start)}s)\")\n        \n        if status.status == \"completed\":\n            print(\"\\n=== Résultat ===\")\n            result_text = \"\"\n            for item in status.output:\n                if hasattr(item, 'content'):\n                    for c in item.content:\n                        if hasattr(c, 'text'):\n                            result_text += c.text\n            print(result_text[:500] if result_text else \"Pas de contenu\")\n            break\n        elif status.status == \"failed\":\n            print(f\"Erreur: {status.error if hasattr(status, 'error') else 'Erreur inconnue'}\")\n            break\n        \n        time.sleep(5)\n    else:\n        print(\"Timeout - la tâche continue en arrière-plan\")\n        print(f\"Vous pouvez récupérer le résultat plus tard avec: client.responses.retrieve('{response.id}')\")\nelse:\n    print(\"[BATCH_MODE] Simulation de tâche background:\")\n    print(\"Status: pending -> processing -> completed (30s)\")\n    print(\"Résultat: Comparaison de 5 architectures microservices (500 caractères)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rate Limiting et Retry\n",
    "\n",
    "En production, il est essentiel de gérer les erreurs transitoires et les limites d'API :\n",
    "\n",
    "### Retry avec Backoff Exponentiel\n",
    "\n",
    "La bibliothèque **tenacity** permet d'implémenter facilement un retry automatique :\n",
    "- **Backoff exponentiel** : Attendre 2s, puis 4s, puis 8s, etc.\n",
    "- **Retry sélectif** : Uniquement sur certaines exceptions\n",
    "- **Limite de tentatives** : Éviter les boucles infinies\n",
    "\n",
    "**Pattern recommandé :**\n",
    "```python\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=60),\n",
    "    retry=retry_if_exception_type((RateLimitError, APIError))\n",
    ")\n",
    "```\n",
    "\n",
    "**Erreurs à gérer :**\n",
    "- `RateLimitError` : Limite de requêtes dépassée (429)\n",
    "- `APIError` : Erreur serveur temporaire (500, 502, 503)\n",
    "- `Timeout` : Délai d'attente dépassé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom openai import RateLimitError, APIError\n\n@retry(\n    stop=stop_after_attempt(5),\n    wait=wait_exponential(multiplier=1, min=2, max=60),\n    retry=retry_if_exception_type((RateLimitError, APIError))\n)\ndef safe_completion(prompt: str, model: str = None) -> str:\n    \"\"\"Appel API avec retry automatique sur erreurs transitoires\"\"\"\n    model = model or DEFAULT_MODEL\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=200\n    )\n    return response.choices[0].message.content\n\n# Test\nresult = safe_completion(\"Dis 'Hello World' en 5 langues.\")\nprint(result)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate Limiter Personnalisé\n",
    "\n",
    "Pour respecter les limites de requêtes par minute (RPM), implémentons un **rate limiter** :\n",
    "- **Fenêtre glissante** : Compte les requêtes sur les 60 dernières secondes\n",
    "- **Attente automatique** : Bloque jusqu'à ce qu'une requête soit autorisée\n",
    "- **Configurable** : Adapter selon votre tier OpenAI\n",
    "\n",
    "**Limites par tier (exemple) :**\n",
    "- Free : 3 RPM\n",
    "- Tier 1 : 500 RPM\n",
    "- Tier 2 : 5000 RPM\n",
    "- Tier 5 : 10000 RPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requête 1: Voici un nombre aléatoire : **27**. Si tu as besoi...\n",
      "Requête 2: Bien sûr! Voilà un nombre aléatoire: 47. Si souhai...\n",
      "Requête 3: Voici un nom aléatoire pour vous : **Léa Fontaine*...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Limite le nombre de requêtes par minute\"\"\"\n",
    "    def __init__(self, max_requests_per_minute: int = 60):\n",
    "        self.max_rpm = max_requests_per_minute\n",
    "        self.requests = deque()\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        now = time.time()\n",
    "        # Nettoyer les anciennes requêtes (plus de 60s)\n",
    "        while self.requests and now - self.requests[0] > 60:\n",
    "            self.requests.popleft()\n",
    "        \n",
    "        # Si limite atteinte, attendre\n",
    "        if len(self.requests) >= self.max_rpm:\n",
    "            wait_time = 60 - (now - self.requests[0])\n",
    "            if wait_time > 0:\n",
    "                print(f\"Rate limit: attente de {wait_time:.1f}s\")\n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        self.requests.append(time.time())\n",
    "\n",
    "# Exemple d'utilisation\n",
    "limiter = RateLimiter(max_requests_per_minute=10)\n",
    "\n",
    "for i in range(3):\n",
    "    limiter.wait_if_needed()\n",
    "    result = safe_completion(f\"Nombre aléatoire #{i+1}\")\n",
    "    print(f\"Requête {i+1}: {result[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimisation des Coûts\n",
    "\n",
    "Stratégies pour réduire les coûts d'API en production :\n",
    "\n",
    "### 1. Utiliser le Cache (store=True)\n",
    "- Économie de 40-80% sur les tokens d'entrée répétés\n",
    "- Activer sur toutes les conversations longues\n",
    "\n",
    "### 2. Choisir le Bon Modèle\n",
    "| Modèle | Prix Input | Prix Output | Cas d'usage |\n",
    "|--------|------------|-------------|-------------|\n",
    "| gpt-4o-mini | $0.15/1M | $0.60/1M | Tâches simples, production |\n",
    "| gpt-4o | $2.50/1M | $10.00/1M | Tâches complexes |\n",
    "| gpt-5-thinking | $10.00/1M | $40.00/1M | Raisonnement profond |\n",
    "\n",
    "### 3. Limiter les Tokens\n",
    "- Utiliser `max_tokens` pour contrôler la longueur\n",
    "- Préférer les prompts courts et précis\n",
    "- Éviter les contextes inutilement longs\n",
    "\n",
    "### 4. Batch Processing\n",
    "- Utiliser l'API Batch pour réductions de 50%\n",
    "- Acceptable pour tâches non-temps-réel\n",
    "\n",
    "### 5. Monitoring et Alertes\n",
    "- Suivre les coûts quotidiens/mensuels\n",
    "- Configurer des alertes budgétaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def optimized_completion(prompt: str, use_cache: bool = True) -> dict:\n    \"\"\"Completion optimisée avec métriques de coût\"\"\"\n    \n    # Utiliser store pour le cache si activé\n    response = client.responses.create(\n        model=DEFAULT_MODEL,\n        store=use_cache,\n        input=prompt\n    )\n    \n    # Calculer les coûts approximatifs\n    # Prix gpt-4o: ~$2.50/1M input, ~$10.00/1M output\n    input_tokens = response.usage.input_tokens if hasattr(response, 'usage') and response.usage else 0\n    output_tokens = response.usage.output_tokens if hasattr(response, 'usage') and response.usage else 0\n    \n    cost_input = input_tokens * 0.0000025\n    cost_output = output_tokens * 0.00001\n    \n    # Extraire le contenu\n    content = \"\"\n    if response.output:\n        for item in response.output:\n            if hasattr(item, 'content'):\n                for c in item.content:\n                    if hasattr(c, 'text'):\n                        content += c.text\n    \n    return {\n        \"content\": content,\n        \"input_tokens\": input_tokens,\n        \"output_tokens\": output_tokens,\n        \"estimated_cost\": cost_input + cost_output,\n        \"cached\": getattr(response, 'cached', False)\n    }\n\n# Test sans cache\nresult1 = optimized_completion(\"Résume les avantages de Python en 3 points.\", use_cache=False)\nprint(\"=== Sans cache ===\")\nprint(f\"Réponse: {result1['content'][:100]}...\")\nprint(f\"Tokens: {result1['input_tokens']} in / {result1['output_tokens']} out\")\nprint(f\"Coût estimé: ${result1['estimated_cost']:.6f}\")\n\n# Test avec cache\nresult2 = optimized_completion(\"Résume les avantages de JavaScript en 3 points.\", use_cache=True)\nprint(\"\\n=== Avec cache ===\")\nprint(f\"Réponse: {result2['content'][:100]}...\")\nprint(f\"Tokens: {result2['input_tokens']} in / {result2['output_tokens']} out\")\nprint(f\"Coût estimé: ${result2['estimated_cost']:.6f}\")\nprint(f\"Cache utilisé: {result2['cached']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitoring et Logging\n",
    "\n",
    "Bonnes pratiques pour le monitoring en production :\n",
    "\n",
    "### Logging Structuré\n",
    "- **Timestamp** : Horodatage de chaque requête\n",
    "- **Modèle** : Quel modèle a été utilisé\n",
    "- **Durée** : Temps de réponse\n",
    "- **Tokens** : Consommation de tokens\n",
    "- **Succès/Échec** : Status de la requête\n",
    "- **Erreurs** : Type et message d'erreur\n",
    "\n",
    "### Métriques à Surveiller\n",
    "- **Latence** : p50, p95, p99\n",
    "- **Taux d'erreur** : Pourcentage de requêtes échouées\n",
    "- **Coûts** : Dépenses quotidiennes/mensuelles\n",
    "- **Tokens/requête** : Moyenne et variance\n",
    "- **Rate limiting** : Nombre de requêtes rejetées\n",
    "\n",
    "### Outils Recommandés\n",
    "- **Logging** : Python `logging`, Loguru\n",
    "- **APM** : Datadog, New Relic, OpenTelemetry\n",
    "- **Alerting** : PagerDuty, Opsgenie\n",
    "- **Dashboards** : Grafana, Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import logging\nfrom datetime import datetime\n\n# Configuration du logger\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\nlogger = logging.getLogger(\"OpenAI_Production\")\n\ndef logged_completion(prompt: str, model: str = None) -> str:\n    \"\"\"Completion avec logging complet\"\"\"\n    model = model or DEFAULT_MODEL\n    start = datetime.now()\n    \n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=200\n        )\n        \n        duration = (datetime.now() - start).total_seconds()\n        logger.info(f\"SUCCESS | Model: {model} | Duration: {duration:.2f}s | \"\n                   f\"Tokens: {response.usage.total_tokens} (in: {response.usage.prompt_tokens}, out: {response.usage.completion_tokens})\")\n        \n        return response.choices[0].message.content\n        \n    except Exception as e:\n        duration = (datetime.now() - start).total_seconds()\n        logger.error(f\"FAILED | Model: {model} | Duration: {duration:.2f}s | \"\n                    f\"Error: {type(e).__name__}: {str(e)[:100]}\")\n        raise\n\n# Tests\nprint(\"=== Test réussi ===\")\nresult = logged_completion(\"Quelle heure est-il?\")\nprint(f\"Réponse: {result}\\n\")\n\nprint(\"=== Test avec modèle invalide ===\")\ntry:\n    result = logged_completion(\"Test\", model=\"gpt-invalid-model\")\nexcept Exception as e:\n    print(f\"Erreur capturée: {type(e).__name__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Patterns Avancés : Streaming et Modération\n",
    "\n",
    "### Streaming pour UX Réactive\n",
    "\n",
    "Le streaming permet d'afficher la réponse progressivement :\n",
    "- **Meilleure UX** : L'utilisateur voit la réponse se construire\n",
    "- **Latence perçue réduite** : Premier token en ~200ms vs 5s pour réponse complète\n",
    "- **Annulation précoce** : Possibilité de stopper si réponse non pertinente\n",
    "\n",
    "### Modération de Contenu\n",
    "\n",
    "OpenAI propose une API de modération pour détecter :\n",
    "- Contenu haineux/violent\n",
    "- Harcèlement\n",
    "- Contenu sexuel\n",
    "- Auto-mutilation\n",
    "- Etc.\n",
    "\n",
    "**Pattern recommandé :**\n",
    "1. Modérer l'input utilisateur\n",
    "2. Générer la réponse si OK\n",
    "3. Modérer l'output avant affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Streaming\nprint(\"=== Streaming Example ===\")\nstream = client.chat.completions.create(\n    model=DEFAULT_MODEL,\n    messages=[{\"role\": \"user\", \"content\": \"Écris un haïku sur la programmation.\"}],\n    stream=True,\n    max_tokens=100\n)\n\nprint(\"Réponse streamée: \", end=\"\", flush=True)\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\nprint(\"\\n\")\n\n# Modération\nprint(\"=== Moderation Example ===\")\ntest_inputs = [\n    \"Bonjour, comment vas-tu?\",\n    \"Je déteste ce produit, c'est nul!\"\n]\n\nfor text in test_inputs:\n    moderation = client.moderations.create(input=text)\n    result = moderation.results[0]\n    \n    print(f\"\\nTexte: {text}\")\n    print(f\"Flaggé: {result.flagged}\")\n    if result.flagged:\n        print(f\"Catégories: {[cat for cat, flagged in result.categories.__dict__.items() if flagged]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Checklist Déploiement Production\n",
    "\n",
    "### Sécurité\n",
    "- [ ] API keys stockées dans variables d'environnement (jamais hardcodées)\n",
    "- [ ] Rotation régulière des clés\n",
    "- [ ] Rate limiting côté serveur\n",
    "- [ ] Validation et sanitization des inputs utilisateur\n",
    "- [ ] Modération de contenu activée\n",
    "- [ ] Logs ne contiennent pas de données sensibles\n",
    "\n",
    "### Résilience\n",
    "- [ ] Retry automatique avec backoff exponentiel\n",
    "- [ ] Timeout configurés\n",
    "- [ ] Circuit breaker pour dépendances externes\n",
    "- [ ] Fallback gracieux en cas d'erreur\n",
    "- [ ] Health checks réguliers\n",
    "\n",
    "### Performance\n",
    "- [ ] Cache activé (`store=True`)\n",
    "- [ ] Choix du modèle adapté au cas d'usage\n",
    "- [ ] `max_tokens` configuré pour limiter les coûts\n",
    "- [ ] Streaming pour réponses longues\n",
    "- [ ] Background mode pour tâches longues\n",
    "\n",
    "### Monitoring\n",
    "- [ ] Logging structuré configuré\n",
    "- [ ] Métriques : latence, taux d'erreur, coûts\n",
    "- [ ] Alertes budgétaires configurées\n",
    "- [ ] Dashboards temps réel\n",
    "- [ ] Traçabilité des requêtes (request ID)\n",
    "\n",
    "### Coûts\n",
    "- [ ] Budget mensuel défini\n",
    "- [ ] Alertes à 50%, 80%, 100% du budget\n",
    "- [ ] Audit régulier de l'usage par endpoint/utilisateur\n",
    "- [ ] Optimisation continue des prompts\n",
    "- [ ] Évaluation régulière des modèles (nouveaux modèles moins chers?)\n",
    "\n",
    "### Conformité\n",
    "- [ ] RGPD : consentement utilisateur pour traitement des données\n",
    "- [ ] Politique de rétention des données\n",
    "- [ ] Anonymisation des données personnelles\n",
    "- [ ] Documentation des traitements de données\n",
    "- [ ] DPO informé de l'usage d'IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Récapitulatif des Patterns\n",
    "\n",
    "| Pattern | Cas d'usage | Complexité |\n",
    "|---------|-------------|------------|\n",
    "| **Responses API + store** | Conversations courtes avec cache | ⭐⭐ |\n",
    "| **Conversations API** | Conversations longue durée | ⭐⭐ |\n",
    "| **Background Mode** | Tâches longues (>30s) | ⭐⭐⭐ |\n",
    "| **Retry + Backoff** | Résilience production | ⭐⭐ |\n",
    "| **Rate Limiter** | Respect limites API | ⭐⭐ |\n",
    "| **Logging structuré** | Monitoring et debug | ⭐⭐ |\n",
    "| **Streaming** | UX temps réel | ⭐⭐ |\n",
    "| **Modération** | Sécurité contenu | ⭐ |\n",
    "\n",
    "### Ressources Supplémentaires\n",
    "\n",
    "**Documentation OpenAI :**\n",
    "- [Responses API](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [Conversations API](https://platform.openai.com/docs/api-reference/conversations)\n",
    "- [Rate Limits](https://platform.openai.com/docs/guides/rate-limits)\n",
    "- [Error Codes](https://platform.openai.com/docs/guides/error-codes)\n",
    "\n",
    "**Bibliothèques Python :**\n",
    "- [tenacity](https://tenacity.readthedocs.io/) : Retry robuste\n",
    "- [openai](https://github.com/openai/openai-python) : SDK officiel\n",
    "- [loguru](https://loguru.readthedocs.io/) : Logging simplifié\n",
    "\n",
    "**Prochaines étapes :**\n",
    "- Implémenter ces patterns dans votre application\n",
    "- Configurer un monitoring complet\n",
    "- Tester la résilience (chaos engineering)\n",
    "- Optimiser les coûts progressivement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Chatbot Multi-Session (30 min)\n",
    "\n",
    "Créez un chatbot de support client qui :\n",
    "1. Utilise la Conversations API pour maintenir le contexte\n",
    "2. Persiste les conversations dans un fichier JSON (pour reprise après redémarrage)\n",
    "3. Implémente un retry avec backoff\n",
    "4. Log toutes les interactions\n",
    "\n",
    "**Bonus :** Ajouter une commande `/summary` qui résume la conversation en cours.\n",
    "\n",
    "### Exercice 2 : Système d'Analyse de Documents (40 min)\n",
    "\n",
    "Implémentez un système qui :\n",
    "1. Prend un long document en entrée\n",
    "2. Lance l'analyse en background mode\n",
    "3. Affiche une barre de progression pendant le traitement\n",
    "4. Retourne un rapport structuré (points clés, résumé, recommandations)\n",
    "5. Calcule et affiche le coût total de l'analyse\n",
    "\n",
    "**Bonus :** Comparer les coûts entre gpt-4o-mini et gpt-4o.\n",
    "\n",
    "### Exercice 3 : Rate Limiter Multi-Tier (20 min)\n",
    "\n",
    "Améliorez la classe `RateLimiter` pour supporter :\n",
    "1. Des limites par minute ET par jour\n",
    "2. Des priorités de requêtes (high/medium/low)\n",
    "3. Un mode \"burst\" (permettre 10 requêtes instantanées, puis throttling)\n",
    "\n",
    "**Bonus :** Ajouter des statistiques (nombre de requêtes dans les dernières 24h, temps moyen d'attente)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}