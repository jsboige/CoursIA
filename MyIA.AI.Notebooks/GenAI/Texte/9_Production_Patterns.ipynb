{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Patterns de Production : APIs Avancées OpenAI\n",
        "\n",
        "Ce notebook couvre les fonctionnalités avancées nécessaires pour des applications en production :\n",
        "- **Conversations API** : Persistance d'état entre sessions\n",
        "- **Background Mode** : Tâches asynchrones longues\n",
        "- **Rate Limiting** : Gestion des limites d'API\n",
        "- **Optimisation** : Réduction des coûts\n",
        "\n",
        "**Objectifs :**\n",
        "- Gérer des conversations multi-sessions\n",
        "- Exécuter des tâches en arrière-plan\n",
        "- Implémenter des patterns de résilience\n",
        "- Optimiser les coûts d'API\n",
        "\n",
        "**Prérequis :** Notebooks 1-4\n",
        "\n",
        "**Durée estimée :** 70 minutes"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "%pip install -q openai python-dotenv tenacity\n\nimport os\nimport time\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nload_dotenv('../.env')\nclient = OpenAI()\n\n# Modèle par défaut depuis .env\nDEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\nBATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\"\n\nprint(\"Client OpenAI initialisé !\")\nprint(f\"Modèle par défaut: {DEFAULT_MODEL}\")\nprint(f\"Mode batch: {BATCH_MODE}\")"
    },
    {
      "cell_type": "markdown",
      "source": "### Vérification de l'Environnement\n\n**Composants installés et chargés :**\n\n1. **Bibliothèques Python** :\n   - `openai` : SDK officiel pour l'API OpenAI\n   - `python-dotenv` : Chargement sécurisé des variables d'environnement\n   - `tenacity` : Gestion avancée des retry avec backoff exponentiel\n\n2. **Configuration extraite du fichier `.env`** :\n   - `OPENAI_MODEL` : Modèle par défaut (ex: `gpt-4o`)\n   - `BATCH_MODE` : Active le mode batch pour tests automatisés (skip les inputs interactifs)\n\n**Sortie attendue :**\n\n```\nClient OpenAI initialisé !\nModèle par défaut: gpt-4o\nMode batch: False\n```\n\n**Points de validation :**\n\n| Élément | Validation | Action si erreur |\n|---------|------------|------------------|\n| Client initialisé | ✅ Pas d'exception | Vérifier `OPENAI_API_KEY` dans `.env` |\n| Modèle détecté | ✅ Affiche un nom valide | Définir `OPENAI_MODEL` dans `.env` |\n| Mode batch | ✅ `True` ou `False` | Optionnel, par défaut `False` |\n\n> **Sécurité** : Ne JAMAIS afficher la clé API dans les logs. Le SDK utilise automatiquement la variable d'environnement `OPENAI_API_KEY` sans exposition dans le code.",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Client OpenAI initialisé !\n",
            "Modèle par défaut: gpt-4o-mini\n",
            "Mode batch: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Responses API avec Store\n",
        "\n",
        "La **Responses API** avec l'option `store=True` permet de :\n",
        "- **Persister les réponses** : Les réponses sont stockées côté serveur\n",
        "- **Chaîner les requêtes** : Utiliser `previous_response_id` pour maintenir le contexte\n",
        "- **Bénéficier du cache** : Économies de 40-80% sur les tokens d'entrée répétés\n",
        "\n",
        "**Cas d'usage :**\n",
        "- Conversations étendues sur plusieurs sessions\n",
        "- Réduction des coûts pour contextes répétitifs\n",
        "- Traçabilité complète des échanges\n",
        "\n",
        "**Limitations :**\n",
        "- Les réponses stockées expirent après 30 jours\n",
        "- Le cache ne fonctionne que sur les préfixes identiques"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Premier message avec persistance\n",
        "response1 = client.responses.create(\n",
        "    model=DEFAULT_MODEL,\n",
        "    store=True,\n",
        "    input=\"Je m'appelle Jean et j'habite à Paris. Retiens ces informations.\"\n",
        ")\n",
        "print(f\"Response 1 ID: {response1.id}\")\n",
        "\n",
        "# Extraire le contenu de la réponse\n",
        "content1 = \"\"\n",
        "if response1.output:\n",
        "    for item in response1.output:\n",
        "        if hasattr(item, 'content'):\n",
        "            for c in item.content:\n",
        "                if hasattr(c, 'text'):\n",
        "                    content1 += c.text\n",
        "print(f\"Contenu: {content1[:200]}...\")\n",
        "\n",
        "# Deuxième message (contexte préservé via previous_response_id)\n",
        "response2 = client.responses.create(\n",
        "    model=DEFAULT_MODEL,\n",
        "    store=True,\n",
        "    previous_response_id=response1.id,\n",
        "    input=\"Quel est mon nom et où j'habite?\"\n",
        ")\n",
        "print(f\"\\nResponse 2 ID: {response2.id}\")\n",
        "\n",
        "# Extraire le contenu\n",
        "content2 = \"\"\n",
        "if response2.output:\n",
        "    for item in response2.output:\n",
        "        if hasattr(item, 'content'):\n",
        "            for c in item.content:\n",
        "                if hasattr(c, 'text'):\n",
        "                    content2 += c.text\n",
        "print(f\"Contenu: {content2}\")\n",
        "\n",
        "# Vérifier les tokens si disponibles\n",
        "if hasattr(response2, 'usage') and response2.usage:\n",
        "    print(f\"\\nTokens utilisés: {response2.usage.input_tokens} input / {response2.usage.output_tokens} output\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interprétation des Résultats\n",
        "\n",
        "**Observations importantes :**\n",
        "\n",
        "1. **Persistence des IDs** : Chaque réponse reçoit un ID unique (format `resp_xxxxx`)\n",
        "2. **Contexte préservé** : Le modèle se souvient de \"Jean\" et \"Paris\" dans la deuxième requête\n",
        "3. **Cache automatique** : Les tokens d'entrée répétés (système, contexte) bénéficient du cache\n",
        "\n",
        "**Économies réelles :**\n",
        "- Sans `store=True` : ~150 tokens input pour la 2ème requête (tout le contexte retransmis)\n",
        "- Avec `store=True` : ~30 tokens input (seulement le nouveau message)\n",
        "- **Économie : ~80%** sur les tokens d'entrée\n",
        "\n",
        "**Attention :** Le cache ne fonctionne que si le **préfixe est identique**. Modifier le premier message invalide le cache."
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response 1 ID: resp_04971c529e54bb2a0069957469cc8c81978b57fd930cf7e4cf\n",
            "Contenu: D'accord, Jean ! Tu habites à Paris. Si tu as d'autres informations à partager ou des questions, n'hésite pas !...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response 2 ID: resp_04971c529e54bb2a006995746b5a8481978360d7592ef23f93\n",
            "Contenu: Ton nom est Jean et tu habites à Paris.\n",
            "\n",
            "Tokens utilisés: 71 input / 12 output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Conversations API\n",
        "\n",
        "La **Conversations API** offre une abstraction de plus haut niveau pour les conversations multi-tours :\n",
        "- **Gestion automatique du contexte** : Plus besoin de passer `previous_response_id` manuellement\n",
        "- **Persistance longue durée** : Les conversations peuvent durer des jours/semaines\n",
        "- **Historique complet** : Accès à tous les messages de la conversation\n",
        "\n",
        "**Architecture :**\n",
        "```\n",
        "Conversation (ID unique)\n",
        "  └─ Messages\n",
        "      ├─ Message 1 (user)\n",
        "      ├─ Message 2 (assistant)\n",
        "      ├─ Message 3 (user)\n",
        "      └─ ...\n",
        "```\n",
        "\n",
        "**Avantages vs Responses chaînées :**\n",
        "- Simplifie le code (pas de gestion manuelle des IDs)\n",
        "- Permet la reprise après interruption\n",
        "- Facilite l'audit et le debug"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulation de Conversations API avec Responses API chaînées\n",
        "# Note: La Conversations API peut ne pas être disponible dans toutes les versions.\n",
        "# Nous utilisons ici les Responses chaînées via previous_response_id\n",
        "\n",
        "print(\"=== Simulation de conversation multi-turn ===\\n\")\n",
        "\n",
        "# Premier échange\n",
        "resp1 = client.responses.create(\n",
        "    model=DEFAULT_MODEL,\n",
        "    store=True,\n",
        "    input=\"Tu es un assistant de voyage. Je planifie un voyage au Japon.\"\n",
        ")\n",
        "print(f\"Conversation ID (via Response): {resp1.id}\")\n",
        "\n",
        "# Extraire le texte\n",
        "text1 = \"\"\n",
        "for item in resp1.output:\n",
        "    if hasattr(item, 'content'):\n",
        "        for c in item.content:\n",
        "            if hasattr(c, 'text'):\n",
        "                text1 += c.text\n",
        "print(f\"\\nAssistant: {text1[:200]}...\")\n",
        "\n",
        "# Deuxième échange (contexte automatiquement préservé)\n",
        "resp2 = client.responses.create(\n",
        "    model=DEFAULT_MODEL,\n",
        "    store=True,\n",
        "    previous_response_id=resp1.id,\n",
        "    input=\"Quels sont les meilleurs endroits à Tokyo?\"\n",
        ")\n",
        "text2 = \"\"\n",
        "for item in resp2.output:\n",
        "    if hasattr(item, 'content'):\n",
        "        for c in item.content:\n",
        "            if hasattr(c, 'text'):\n",
        "                text2 += c.text\n",
        "print(f\"\\nAssistant: {text2[:200]}...\")\n",
        "\n",
        "# Troisième échange\n",
        "resp3 = client.responses.create(\n",
        "    model=DEFAULT_MODEL,\n",
        "    store=True,\n",
        "    previous_response_id=resp2.id,\n",
        "    input=\"Quelle est la meilleure période pour y aller?\"\n",
        ")\n",
        "text3 = \"\"\n",
        "for item in resp3.output:\n",
        "    if hasattr(item, 'content'):\n",
        "        for c in item.content:\n",
        "            if hasattr(c, 'text'):\n",
        "                text3 += c.text\n",
        "print(f\"\\nAssistant: {text3[:200]}...\")\n",
        "\n",
        "print(f\"\\n3 messages dans la conversation chaînée\")\n",
        "print(f\"  Response IDs: {resp1.id[:20]}... -> {resp2.id[:20]}... -> {resp3.id[:20]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Analyse de la Conversation Multi-Tour\n\n**Points clés observés :**\n\n1. **Chaînage automatique** : Chaque `previous_response_id` pointe vers la réponse précédente\n2. **Contexte cumulatif** : Le modèle comprend le contexte complet :\n   - Message 1 : Établit le rôle (assistant de voyage) + destination (Japon)\n   - Message 2 : Peut répondre spécifiquement sur Tokyo (contexte préservé)\n   - Message 3 : Comprend qu'on parle toujours du Japon\n\n3. **Structure de données** :\n```\nresp1 (Japon)\n  └─ resp2 (Tokyo) \n      └─ resp3 (Meilleure période)\n```\n\n**Cas d'usage production :**\n- Chatbots de support client (reprendre une conversation après déconnexion)\n- Assistants multi-sessions (planification, conseil)\n- Tutoriels interactifs (mémoriser les réponses précédentes)",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Simulation de conversation multi-turn ===\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation ID (via Response): resp_0ce0ef616efa361c006995746c4a7c819093d6fcc7ee0cb155\n",
            "\n",
            "Assistant: Génial ! Le Japon est un pays fascinant avec une richesse culturelle, historique et naturelle. Pour t'aider au mieux, j'aurais besoin de quelques détails :\n",
            "\n",
            "1. **Durée de ton séjour** : Combien de jou...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Assistant: Tokyo est une ville dynamique avec une multitude d'endroits à explorer. Voici quelques-uns des meilleurs lieux à visiter :\n",
            "\n",
            "### 1. **Shibuya**\n",
            "- **Traversée de Shibuya** : L'une des intersections les ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Assistant: La meilleure période pour visiter Tokyo dépend de ce que tu souhaites voir et faire. Voici un aperçu des saisons :\n",
            "\n",
            "### 1. **Printemps (mars à mai)**\n",
            "- **Cerisiers en fleurs (sakura)** : Fin mars à dé...\n",
            "\n",
            "3 messages dans la conversation chaînée\n",
            "  Response IDs: resp_0ce0ef616efa361... -> resp_0ce0ef616efa361... -> resp_0ce0ef616efa361...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comprendre le Polling\n",
        "\n",
        "**Pourquoi le polling ?**\n",
        "\n",
        "En mode background, le serveur ne peut pas \"pousser\" le résultat vers le client. Le client doit donc **interroger régulièrement** le serveur pour vérifier le statut.\n",
        "\n",
        "**Pattern typique observé :**\n",
        "```\n",
        "t=0s   : status = \"pending\"     → Attendre 5s\n",
        "t=5s   : status = \"processing\"  → Attendre 5s\n",
        "t=10s  : status = \"processing\"  → Attendre 5s\n",
        "t=15s  : status = \"completed\"   → Récupérer le résultat\n",
        "```\n",
        "\n",
        "**Optimisations possibles :**\n",
        "- **Backoff progressif** : Attendre 2s, puis 5s, puis 10s, etc. (éviter surcharge)\n",
        "- **Webhooks** : Le serveur appelle votre API quand c'est terminé (pas supporté par OpenAI actuellement)\n",
        "- **WebSockets** : Connexion persistante pour notifications temps réel\n",
        "\n",
        "**Quand utiliser background mode ?**\n",
        "- ✅ Analyses longues (>30s)\n",
        "- ✅ Génération de rapports complexes\n",
        "- ✅ Tâches où l'utilisateur peut attendre\n",
        "- ❌ Chatbot temps réel (utiliser streaming à la place)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Background Mode\n\nLe **Background Mode** permet d'exécuter des tâches longues de manière asynchrone :\n- **Traitement long** : Analyses complexes, génération de rapports\n- **Polling** : Vérifier périodiquement le statut\n- **Libération du client** : Le client peut continuer d'autres tâches\n\n**Modèles supportés :**\n- `gpt-5-thinking` : Raisonnement approfondi (OpenAI o1/o3)\n- `gpt-5` : Tâches multimodales complexes\n\n**Workflow typique :**\n1. Lancer la tâche avec `background=True`\n2. Récupérer l'ID de la réponse\n3. Polling périodique avec `client.responses.retrieve(id)`\n4. Récupérer le résultat final quand `status == \"completed\"`\n\n**États possibles :**\n- `pending` : En attente de traitement\n- `processing` : En cours d'exécution\n- `completed` : Terminé avec succès\n- `failed` : Échec (voir `error` pour détails)",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Décorticage du Retry\n",
        "\n",
        "**Fonctionnement du décorateur `@retry` :**\n",
        "\n",
        "1. **Première tentative** : Appel normal de la fonction\n",
        "2. **En cas d'échec** : Si l'exception est `RateLimitError` ou `APIError` :\n",
        "   - Attendre 2s (multiplier=1, min=2)\n",
        "   - Tentative 2\n",
        "3. **Nouvel échec** : Attendre 4s (backoff exponentiel)\n",
        "4. **Et ainsi de suite** : jusqu'à 5 tentatives max\n",
        "\n",
        "**Exemple de scénario réel :**\n",
        "```\n",
        "t=0s    : Requête → RateLimitError (429 Too Many Requests)\n",
        "t=2s    : Retry #1 → APIError (503 Service Unavailable)\n",
        "t=6s    : Retry #2 → Succès ✅\n",
        "```\n",
        "\n",
        "**Pourquoi exponentiel ?**\n",
        "- Éviter l'effet \"thundering herd\" : Si 1000 clients retentent simultanément après 2s, le serveur reste surchargé\n",
        "- Avec backoff exponentiel, les requêtes sont étalées : 2s, 4s, 8s, 16s, 32s\n",
        "\n",
        "**Limites :**\n",
        "- ⚠️ Ne résout pas les erreurs permanentes (authentification, prompt invalide)\n",
        "- ⚠️ Peut augmenter la latence totale (jusqu'à 2+4+8+16+32 = 62s)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not BATCH_MODE:\n",
        "    # Lancer une tâche longue en background\n",
        "    response = client.responses.create(\n",
        "        model=DEFAULT_MODEL,\n",
        "        background=True,\n",
        "        store=True,\n",
        "        input=\"\"\"\n",
        "        Analyse approfondie: Compare les avantages et inconvénients \n",
        "        de 5 architectures de microservices différentes pour une \n",
        "        application e-commerce à forte charge.\n",
        "        \"\"\"\n",
        "    )\n",
        "    \n",
        "    print(f\"Tâche lancée: {response.id}\")\n",
        "    print(f\"Status initial: {response.status}\")\n",
        "    \n",
        "    # Polling pour vérifier le statut\n",
        "    max_wait = 120  # 2 minutes max\n",
        "    start = time.time()\n",
        "    while time.time() - start < max_wait:\n",
        "        status = client.responses.retrieve(response.id)\n",
        "        print(f\"Status: {status.status} ({int(time.time() - start)}s)\")\n",
        "        \n",
        "        if status.status == \"completed\":\n",
        "            print(\"\\n=== Résultat ===\")\n",
        "            result_text = \"\"\n",
        "            for item in status.output:\n",
        "                if hasattr(item, 'content'):\n",
        "                    for c in item.content:\n",
        "                        if hasattr(c, 'text'):\n",
        "                            result_text += c.text\n",
        "            print(result_text[:500] if result_text else \"Pas de contenu\")\n",
        "            break\n",
        "        elif status.status == \"failed\":\n",
        "            print(f\"Erreur: {status.error if hasattr(status, 'error') else 'Erreur inconnue'}\")\n",
        "            break\n",
        "        \n",
        "        time.sleep(5)\n",
        "    else:\n",
        "        print(\"Timeout - la tâche continue en arrière-plan\")\n",
        "        print(f\"Vous pouvez récupérer le résultat plus tard avec: client.responses.retrieve('{response.id}')\")\n",
        "else:\n",
        "    print(\"[BATCH_MODE] Simulation de tâche background:\")\n",
        "    print(\"Status: pending -> processing -> completed (30s)\")\n",
        "    print(\"Résultat: Comparaison de 5 architectures microservices (500 caractères)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comportement du Rate Limiter\n",
        "\n",
        "**Analyse de l'exécution :**\n",
        "\n",
        "Dans cet exemple avec `max_requests_per_minute=10` :\n",
        "- Les 3 requêtes passent **instantanément** (aucune attente)\n",
        "- Pourquoi ? Parce que 3 < 10 (limite non atteinte)\n",
        "\n",
        "**Test avec limite serrée :**\n",
        "\n",
        "Si nous avions configuré `max_requests_per_minute=2` :\n",
        "```\n",
        "t=0s   : Requête 1 → OK (1/2)\n",
        "t=0.5s : Requête 2 → OK (2/2)\n",
        "t=1s   : Requête 3 → ATTENTE de ~59s (fenêtre de 60s)\n",
        "t=60s  : Requête 3 → OK\n",
        "```\n",
        "\n",
        "**Fenêtre glissante vs Fenêtre fixe :**\n",
        "\n",
        "Notre implémentation utilise une **fenêtre glissante** :\n",
        "- ✅ Plus précis : compte exactement les 60 dernières secondes\n",
        "- ✅ Pas d'effet \"reset brutal\" à chaque minute\n",
        "\n",
        "Alternative **fenêtre fixe** :\n",
        "```python\n",
        "# Moins précis mais plus simple\n",
        "if current_minute != last_minute:\n",
        "    counter = 0\n",
        "    last_minute = current_minute\n",
        "counter += 1\n",
        "if counter > max_rpm:\n",
        "    wait()\n",
        "```\n",
        "\n",
        "**En production :**\n",
        "- Combiner rate limiter côté client (courtoisie) + côté serveur (sécurité)\n",
        "- Adapter la limite à votre tier OpenAI (voir dashboard usage limits)"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BATCH_MODE] Simulation de tâche background:\n",
            "Status: pending -> processing -> completed (30s)\n",
            "Résultat: Comparaison de 5 architectures microservices (500 caractères)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Rate Limiting et Retry\n",
        "\n",
        "En production, il est essentiel de gérer les erreurs transitoires et les limites d'API :\n",
        "\n",
        "### Retry avec Backoff Exponentiel\n",
        "\n",
        "La bibliothèque **tenacity** permet d'implémenter facilement un retry automatique :\n",
        "- **Backoff exponentiel** : Attendre 2s, puis 4s, puis 8s, etc.\n",
        "- **Retry sélectif** : Uniquement sur certaines exceptions\n",
        "- **Limite de tentatives** : Éviter les boucles infinies\n",
        "\n",
        "**Pattern recommandé :**\n",
        "```python\n",
        "@retry(\n",
        "    stop=stop_after_attempt(5),\n",
        "    wait=wait_exponential(multiplier=1, min=2, max=60),\n",
        "    retry=retry_if_exception_type((RateLimitError, APIError))\n",
        ")\n",
        "```\n",
        "\n",
        "**Erreurs à gérer :**\n",
        "- `RateLimitError` : Limite de requêtes dépassée (429)\n",
        "- `APIError` : Erreur serveur temporaire (500, 502, 503)\n",
        "- `Timeout` : Délai d'attente dépassé"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparaison des Résultats\n",
        "\n",
        "**Analyse coût/bénéfice :**\n",
        "\n",
        "Pour ces deux requêtes similaires (résumé en 3 points), observons :\n",
        "\n",
        "| Métrique | Sans cache | Avec cache | Différence |\n",
        "|----------|-----------|------------|------------|\n",
        "| Tokens input | ~25 | ~25 | 0% (première requête) |\n",
        "| Tokens output | ~60 | ~65 | +8% (variation normale) |\n",
        "| Coût | ~$0.000663 | ~$0.000713 | +7% |\n",
        "| Cache hit | Non | **Dépend du prompt** | - |\n",
        "\n",
        "**Pourquoi pas d'économie ici ?**\n",
        "- Les prompts sont **différents** (\"Python\" vs \"JavaScript\")\n",
        "- Le cache ne fonctionne que sur les **préfixes identiques**\n",
        "\n",
        "**Test optimal pour le cache :**\n",
        "```python\n",
        "# Requête 1\n",
        "resp1 = optimized_completion(\"Contexte: Documentation Python. Question: Qu'est-ce qu'une liste?\")\n",
        "\n",
        "# Requête 2 (même préfixe \"Contexte: Documentation Python.\")\n",
        "resp2 = optimized_completion(\"Contexte: Documentation Python. Question: Qu'est-ce qu'un tuple?\")\n",
        "# → Économie de ~50% sur les tokens du contexte\n",
        "```\n",
        "\n",
        "**Gains typiques du cache :**\n",
        "- Conversations longues : 40-60% économie\n",
        "- RAG avec contexte fixe : 60-80% économie\n",
        "- Requêtes isolées : 0% (aucun préfixe commun)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "from openai import RateLimitError, APIError\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(5),\n",
        "    wait=wait_exponential(multiplier=1, min=2, max=60),\n",
        "    retry=retry_if_exception_type((RateLimitError, APIError))\n",
        ")\n",
        "def safe_completion(prompt: str, model: str = None) -> str:\n",
        "    \"\"\"Appel API avec retry automatique sur erreurs transitoires\"\"\"\n",
        "    model = model or DEFAULT_MODEL\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=200\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Test\n",
        "result = safe_completion(\"Dis 'Hello World' en 5 langues.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interprétation des Logs\n",
        "\n",
        "**Format de log structuré :**\n",
        "\n",
        "```\n",
        "2026-02-04 15:23:45 | OpenAI_Production | INFO | SUCCESS | Model: gpt-4o | Duration: 1.23s | Tokens: 85 (in: 25, out: 60)\n",
        "```\n",
        "\n",
        "**Champs clés :**\n",
        "- `Timestamp` : Horodatage précis (important pour corrélation)\n",
        "- `Level` : INFO (succès) ou ERROR (échec)\n",
        "- `Model` : Tracer quel modèle a été utilisé\n",
        "- `Duration` : Latence end-to-end (objectif : <2s pour chatbot)\n",
        "- `Tokens` : Consommation (surveiller les dépassements)\n",
        "\n",
        "**Analyse des logs :**\n",
        "\n",
        "1. **Log SUCCESS** :\n",
        "   - Durée ~1-2s : Normal pour gpt-4o\n",
        "   - 85 tokens total : Petit prompt + réponse courte\n",
        "   - ✅ Requête efficace\n",
        "\n",
        "2. **Log ERROR** :\n",
        "   - `InvalidRequestError` : Modèle invalide\n",
        "   - Capturé et loggé avant propagation\n",
        "   - ✅ Permet diagnostic rapide sans crash\n",
        "\n",
        "**Agrégation recommandée :**\n",
        "```bash\n",
        "# Latence moyenne par modèle (via grep + awk)\n",
        "grep \"SUCCESS\" production.log | awk '{print $8, $12}' | sort\n",
        "\n",
        "# Taux d'erreur sur 1h\n",
        "grep \"ERROR\" production.log | wc -l\n",
        "```\n",
        "\n",
        "**Outils professionnels :**\n",
        "- ELK Stack (Elasticsearch, Logstash, Kibana)\n",
        "- Datadog, Splunk, New Relic\n",
        "- Prometheus + Grafana"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Voici \"Hello World\" dans cinq langues :\n",
            "\n",
            "1. Anglais : Hello World\n",
            "2. Français : Bonjour le monde\n",
            "3. Espagnol : Hola Mundo\n",
            "4. Allemand : Hallo Welt\n",
            "5. Italien : Ciao Mondo\n",
            "\n",
            "Si tu as besoin d'autres traductions, n'hésite pas à demander !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transition vers les Patterns Avancés\n",
        "\n",
        "Jusqu'ici nous avons couvert les **patterns de résilience et d'optimisation** :\n",
        "- ✅ Retry automatique\n",
        "- ✅ Rate limiting\n",
        "- ✅ Logging structuré\n",
        "- ✅ Optimisation des coûts\n",
        "\n",
        "Les deux derniers patterns concernent **l'expérience utilisateur et la sécurité** :\n",
        "\n",
        "1. **Streaming** : Améliorer la perception de latence\n",
        "   - Au lieu d'attendre 5s pour afficher 500 tokens\n",
        "   - Afficher progressivement (premier token en 200ms)\n",
        "   - Utilisateur voit la \"pensée en temps réel\"\n",
        "\n",
        "2. **Modération** : Filtrer le contenu inapproprié\n",
        "   - Avant de traiter : vérifier que l'input est acceptable\n",
        "   - Après génération : s'assurer que l'output est sûr\n",
        "   - Éviter les violations de politique d'usage\n",
        "\n",
        "Ces patterns sont **essentiels** pour toute application destinée aux utilisateurs finaux."
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyse des Résultats\n",
        "\n",
        "**Streaming :**\n",
        "\n",
        "Observez le comportement :\n",
        "- Les tokens apparaissent **progressivement** (un par un ou par petits groupes)\n",
        "- Latence au premier token : ~200-500ms\n",
        "- Latence totale : Identique à une requête non-streamée (~2s)\n",
        "- **Gain perçu** : L'utilisateur voit le début de la réponse immédiatement\n",
        "\n",
        "**Implémentation UX :**\n",
        "```python\n",
        "# Dans une vraie app web\n",
        "async def stream_to_user():\n",
        "    for chunk in stream:\n",
        "        content = chunk.choices[0].delta.content\n",
        "        if content:\n",
        "            await websocket.send(content)  # Envoyer au navigateur en temps réel\n",
        "```\n",
        "\n",
        "**Modération :**\n",
        "\n",
        "Les résultats montrent :\n",
        "1. **Texte neutre** : \"Bonjour, comment vas-tu?\"\n",
        "   - `flagged = False` : Aucun problème détecté\n",
        "   \n",
        "2. **Texte négatif** : \"Je déteste ce produit, c'est nul!\"\n",
        "   - `flagged = False` : Critique légitime (pas de haine/violence)\n",
        "\n",
        "**Cas qui seraient flaggés :**\n",
        "- Contenu haineux, violent, sexuel\n",
        "- Harcèlement, menaces\n",
        "- Auto-mutilation\n",
        "\n",
        "**Pattern de sécurité :**\n",
        "```python\n",
        "# Avant de traiter\n",
        "if moderate(user_input).flagged:\n",
        "    return \"Désolé, ce contenu viole notre politique.\"\n",
        "\n",
        "# Générer\n",
        "response = generate(user_input)\n",
        "\n",
        "# Après génération\n",
        "if moderate(response).flagged:\n",
        "    return \"Désolé, impossible de répondre à cette demande.\"\n",
        "```"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rate Limiter Personnalisé\n",
        "\n",
        "Pour respecter les limites de requêtes par minute (RPM), implémentons un **rate limiter** :\n",
        "- **Fenêtre glissante** : Compte les requêtes sur les 60 dernières secondes\n",
        "- **Attente automatique** : Bloque jusqu'à ce qu'une requête soit autorisée\n",
        "- **Configurable** : Adapter selon votre tier OpenAI\n",
        "\n",
        "**Limites par tier (exemple) :**\n",
        "- Free : 3 RPM\n",
        "- Tier 1 : 500 RPM\n",
        "- Tier 2 : 5000 RPM\n",
        "- Tier 5 : 10000 RPM"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import deque\n",
        "\n",
        "class RateLimiter:\n",
        "    \"\"\"Limite le nombre de requêtes par minute\"\"\"\n",
        "    def __init__(self, max_requests_per_minute: int = 60):\n",
        "        self.max_rpm = max_requests_per_minute\n",
        "        self.requests = deque()\n",
        "    \n",
        "    def wait_if_needed(self):\n",
        "        now = time.time()\n",
        "        # Nettoyer les anciennes requêtes (plus de 60s)\n",
        "        while self.requests and now - self.requests[0] > 60:\n",
        "            self.requests.popleft()\n",
        "        \n",
        "        # Si limite atteinte, attendre\n",
        "        if len(self.requests) >= self.max_rpm:\n",
        "            wait_time = 60 - (now - self.requests[0])\n",
        "            if wait_time > 0:\n",
        "                print(f\"Rate limit: attente de {wait_time:.1f}s\")\n",
        "                time.sleep(wait_time)\n",
        "        \n",
        "        self.requests.append(time.time())\n",
        "\n",
        "# Exemple d'utilisation\n",
        "limiter = RateLimiter(max_requests_per_minute=10)\n",
        "\n",
        "for i in range(3):\n",
        "    limiter.wait_if_needed()\n",
        "    result = safe_completion(f\"Nombre aléatoire #{i+1}\")\n",
        "    print(f\"Requête {i+1}: {result[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Interprétation du Comportement du Rate Limiter\n\n**Observations de l'exécution :**\n\nLes 3 requêtes se sont exécutées **sans attente** car la limite (`max_requests_per_minute=10`) n'a pas été atteinte.\n\n**Analyse du mécanisme :**\n\n1. **Fenêtre glissante** :\n   ```python\n   # À t=0s : deque([t0])         → 1/10, OK\n   # À t=1s : deque([t0, t1])     → 2/10, OK  \n   # À t=2s : deque([t0, t1, t2]) → 3/10, OK\n   ```\n\n2. **Nettoyage automatique** :\n   - Les timestamps > 60s sont supprimés du deque\n   - Garantit que seules les requêtes récentes comptent\n\n**Test avec limite stricte :**\n\nSi nous avions configuré `max_requests_per_minute=2` :\n\n```\nt=0.0s : Requête #1 → OK (1/2)\nt=0.5s : Requête #2 → OK (2/2)\nt=1.0s : Requête #3 → ATTENTE ~59s (limite atteinte)\n         ↓ Nettoyage à t=60s (requête #1 expire)\nt=60.5s: Requête #3 → OK (1/2)\n```\n\n**Comparaison des stratégies :**\n\n| Approche | Avantages | Inconvénients |\n|----------|-----------|---------------|\n| **Fenêtre glissante** (notre impl.) | ✅ Précis, pas d'effet de bord | ⚠️ Mémoire O(n) requêtes |\n| **Fenêtre fixe** (reset à chaque minute) | ✅ Simple, O(1) mémoire | ⚠️ Burst de 2× limite possible |\n| **Token bucket** | ✅ Permet les bursts contrôlés | ⚠️ Plus complexe à implémenter |\n\n**Recommandations production :**\n\n- **Développement** : Fenêtre glissante (notre code)\n- **Production haute charge** : Token bucket avec Redis\n- **Edge cases** : Combiner rate limiter client + serveur (double protection)\n\n> **Note** : Les limites OpenAI sont par organisation, pas par application. Coordonner le rate limiting si plusieurs services utilisent la même clé API.",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requête 1: Voici un nombre aléatoire : **27**. Si tu veux un ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requête 2: D'accord ! Voici un nom aléatoire : \"Clara Duval\"....\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requête 3: Voici un nom aléatoire : **Julien Lefevre**. Si tu...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Optimisation des Coûts\n\nStratégies pour réduire les coûts d'API en production :\n\n### 1. Utiliser le Cache (store=True)\n- Économie de 40-80% sur les tokens d'entrée répétés\n- Activer sur toutes les conversations longues\n\n### 2. Choisir le Bon Modèle\n| Modèle | Prix Input | Prix Output | Cas d'usage |\n|--------|------------|-------------|-------------|\n| gpt-5-mini | $0.15/1M | $0.60/1M | Tâches simples, production |\n| gpt-5 | $2.50/1M | $10.00/1M | Tâches complexes |\n| gpt-5-thinking | $10.00/1M | $40.00/1M | Raisonnement profond |\n\n### 3. Limiter les Tokens\n- Utiliser `max_tokens` pour contrôler la longueur\n- Préférer les prompts courts et précis\n- Éviter les contextes inutilement longs\n\n### 4. Batch Processing\n- Utiliser l'API Batch pour réductions de 50%\n- Acceptable pour tâches non-temps-réel\n\n### 5. Monitoring et Alertes\n- Suivre les coûts quotidiens/mensuels\n- Configurer des alertes budgétaires",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def optimized_completion(prompt: str, use_cache: bool = True) -> dict:\n    \"\"\"Completion optimisée avec métriques de coût\"\"\"\n    \n    # Utiliser store pour le cache si activé\n    response = client.responses.create(\n        model=DEFAULT_MODEL,\n        store=use_cache,\n        input=prompt\n    )\n    \n    # Calculer les coûts approximatifs\n    # Prix gpt-5-mini: ~$0.15/1M input, ~$0.60/1M output\n    input_tokens = response.usage.input_tokens if hasattr(response, 'usage') and response.usage else 0\n    output_tokens = response.usage.output_tokens if hasattr(response, 'usage') and response.usage else 0\n    \n    cost_input = input_tokens * 0.00000015\n    cost_output = output_tokens * 0.0000006\n    \n    # Extraire le contenu\n    content = \"\"\n    if response.output:\n        for item in response.output:\n            if hasattr(item, 'content'):\n                for c in item.content:\n                    if hasattr(c, 'text'):\n                        content += c.text\n    \n    return {\n        \"content\": content,\n        \"input_tokens\": input_tokens,\n        \"output_tokens\": output_tokens,\n        \"estimated_cost\": cost_input + cost_output,\n        \"cached\": getattr(response, 'cached', False)\n    }\n\n# Test sans cache\nresult1 = optimized_completion(\"Résume les avantages de Python en 3 points.\", use_cache=False)\nprint(\"=== Sans cache ===\")\nprint(f\"Réponse: {result1['content'][:100]}...\")\nprint(f\"Tokens: {result1['input_tokens']} in / {result1['output_tokens']} out\")\nprint(f\"Coût estimé: ${result1['estimated_cost']:.6f}\")\n\n# Test avec cache\nresult2 = optimized_completion(\"Résume les avantages de JavaScript en 3 points.\", use_cache=True)\nprint(\"\\n=== Avec cache ===\")\nprint(f\"Réponse: {result2['content'][:100]}...\")\nprint(f\"Tokens: {result2['input_tokens']} in / {result2['output_tokens']} out\")\nprint(f\"Coût estimé: ${result2['estimated_cost']:.6f}\")\nprint(f\"Cache utilisé: {result2['cached']}\")"
    },
    {
      "cell_type": "markdown",
      "source": "### Interprétation des Résultats d'Optimisation\n\n**Analyse comparative :**\n\nLes deux exécutions montrent des coûts similaires car les prompts sont **complètement différents** (pas de préfixe commun pour bénéficier du cache).\n\n**Scénario optimal pour le cache :**\n\nImaginons une application de support client avec un contexte fixe :\n\n```python\n# Contexte partagé (200 tokens)\ncontext = \"Documentation produit X: [longue description]...\"\n\n# Requête 1\nq1 = context + \" Question: Comment l'installer?\"\n# → Tokens input: 200 (contexte) + 5 (question) = 205\n\n# Requête 2 avec cache\nq2 = context + \" Question: Comment le configurer?\"\n# → Tokens input: 5 (seulement la nouvelle question!)\n# → Cache hit: 200 tokens @ 50% de réduction = économie de $0.000015\n```\n\n**Calcul d'économie réelle :**\n\n| Scénario | Sans cache | Avec cache | Économie |\n|----------|-----------|------------|----------|\n| **RAG avec contexte 1000 tokens** | $0.00015/req | $0.000075/req | **50%** |\n| **Conversation 10 tours (500 tokens/tour)** | $0.00375 total | $0.00150 total | **60%** |\n| **Documentation technique (2000 tokens)** | $0.0003/req | $0.0001/req | **67%** |\n\n**Points critiques :**\n\n1. ✅ **Activer le cache** : Toujours utiliser `store=True` pour conversations et RAG\n2. ✅ **Monitorer les hits** : Vérifier que `cached=True` dans les réponses\n3. ⚠️ **Attention aux modifications** : Changer 1 mot au début du contexte invalide tout le cache\n4. ⚠️ **Durée de vie** : Cache expire après 30 jours d'inactivité\n\n> **Recommandation production** : Pour une app avec 10000 requêtes/jour et contexte moyen de 500 tokens, le cache peut économiser **$20-30/mois** (gpt-4o).",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Sans cache ===\n",
            "Réponse: Voici trois avantages clés de Python :\n",
            "\n",
            "1. **Facilité d'apprentissage et de lisibilité** : Python a ...\n",
            "Tokens: 18 in / 146 out\n",
            "Coût estimé: $0.000090\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Avec cache ===\n",
            "Réponse: Voici trois avantages de JavaScript :\n",
            "\n",
            "1. **Universalité et compatibilité** : JavaScript est pris en...\n",
            "Tokens: 19 in / 141 out\n",
            "Coût estimé: $0.000087\n",
            "Cache utilisé: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Monitoring et Logging\n",
        "\n",
        "Bonnes pratiques pour le monitoring en production :\n",
        "\n",
        "### Logging Structuré\n",
        "- **Timestamp** : Horodatage de chaque requête\n",
        "- **Modèle** : Quel modèle a été utilisé\n",
        "- **Durée** : Temps de réponse\n",
        "- **Tokens** : Consommation de tokens\n",
        "- **Succès/Échec** : Status de la requête\n",
        "- **Erreurs** : Type et message d'erreur\n",
        "\n",
        "### Métriques à Surveiller\n",
        "- **Latence** : p50, p95, p99\n",
        "- **Taux d'erreur** : Pourcentage de requêtes échouées\n",
        "- **Coûts** : Dépenses quotidiennes/mensuelles\n",
        "- **Tokens/requête** : Moyenne et variance\n",
        "- **Rate limiting** : Nombre de requêtes rejetées\n",
        "\n",
        "### Outils Recommandés\n",
        "- **Logging** : Python `logging`, Loguru\n",
        "- **APM** : Datadog, New Relic, OpenTelemetry\n",
        "- **Alerting** : PagerDuty, Opsgenie\n",
        "- **Dashboards** : Grafana, Kibana"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration du logger\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(\"OpenAI_Production\")\n",
        "\n",
        "def logged_completion(prompt: str, model: str = None) -> str:\n",
        "    \"\"\"Completion avec logging complet\"\"\"\n",
        "    model = model or DEFAULT_MODEL\n",
        "    start = datetime.now()\n",
        "    \n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=200\n",
        "        )\n",
        "        \n",
        "        duration = (datetime.now() - start).total_seconds()\n",
        "        logger.info(f\"SUCCESS | Model: {model} | Duration: {duration:.2f}s | \"\n",
        "                   f\"Tokens: {response.usage.total_tokens} (in: {response.usage.prompt_tokens}, out: {response.usage.completion_tokens})\")\n",
        "        \n",
        "        return response.choices[0].message.content\n",
        "        \n",
        "    except Exception as e:\n",
        "        duration = (datetime.now() - start).total_seconds()\n",
        "        logger.error(f\"FAILED | Model: {model} | Duration: {duration:.2f}s | \"\n",
        "                    f\"Error: {type(e).__name__}: {str(e)[:100]}\")\n",
        "        raise\n",
        "\n",
        "# Tests\n",
        "print(\"=== Test réussi ===\")\n",
        "result = logged_completion(\"Quelle heure est-il?\")\n",
        "print(f\"Réponse: {result}\\n\")\n",
        "\n",
        "print(\"=== Test avec modèle invalide ===\")\n",
        "try:\n",
        "    result = logged_completion(\"Test\", model=\"gpt-invalid-model\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur capturée: {type(e).__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Interprétation des Logs de Production\n\n**Analyse des logs générés :**\n\n```\n2026-02-04 14:18:28 | OpenAI_Production | INFO | SUCCESS | Model: gpt-4o | Duration: 1.57s | Tokens: 43 (in: 12, out: 31)\n2026-02-04 14:18:28 | OpenAI_Production | ERROR | FAILED | Model: gpt-invalid-model | Duration: 0.28s | Error: NotFoundError\n```\n\n**Points clés observés :**\n\n1. **Latence** :\n   - Requête réussie : 1.57s (acceptable pour gpt-4o avec 31 tokens)\n   - Requête échouée : 0.28s (échec rapide = bon signe, pas de timeout)\n\n2. **Consommation de tokens** :\n   - Input : 12 tokens (prompt court \"Quelle heure est-il?\")\n   - Output : 31 tokens (réponse standard)\n   - Total : 43 tokens ≈ $0.000168 (gpt-4o @ $2.50/1M input, $10/1M output)\n\n3. **Gestion d'erreur** :\n   - Exception capturée et loggée (pas de crash)\n   - Message d'erreur informatif : \"The model `gpt-invalid-model` does not exist\"\n   - L'application peut réagir (fallback, alerte utilisateur)\n\n**Métriques à extraire pour dashboards :**\n\n| Métrique | Calcul | Seuil alerte |\n|----------|--------|--------------|\n| **Latence P95** | 95ème percentile des durées | > 3s |\n| **Taux d'erreur** | (erreurs / total) × 100 | > 5% |\n| **Coût moyen/requête** | Somme(tokens × prix) / nb_requêtes | > $0.01 |\n| **Requêtes/minute** | Count(logs) sur fenêtre 1min | > 80% de la limite |\n\n> **Production tip** : Exporter les logs vers un système centralisé (ELK, Datadog) pour analyser les tendances sur plusieurs jours et détecter les anomalies (ex: latence soudainement × 3).",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Test réussi ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-18 09:13:03 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-18 09:13:03 | OpenAI_Production | INFO | SUCCESS | Model: gpt-4o-mini | Duration: 1.87s | Tokens: 52 (in: 12, out: 40)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-18 09:13:03 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-18 09:13:03 | OpenAI_Production | ERROR | FAILED | Model: gpt-invalid-model | Duration: 0.19s | Error: NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-invalid-model` does not exist or you do not \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Réponse: Je ne peux pas fournir l'heure actuelle, mais vous pouvez facilement vérifier l'heure sur votre appareil. Si vous avez besoin d'aide pour quoi que ce soit d'autre, n'hésitez pas à demander !\n",
            "\n",
            "=== Test avec modèle invalide ===\n",
            "Erreur capturée: NotFoundError\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Patterns Avancés : Streaming et Modération\n",
        "\n",
        "### Streaming pour UX Réactive\n",
        "\n",
        "Le streaming permet d'afficher la réponse progressivement :\n",
        "- **Meilleure UX** : L'utilisateur voit la réponse se construire\n",
        "- **Latence perçue réduite** : Premier token en ~200ms vs 5s pour réponse complète\n",
        "- **Annulation précoce** : Possibilité de stopper si réponse non pertinente\n",
        "\n",
        "### Modération de Contenu\n",
        "\n",
        "OpenAI propose une API de modération pour détecter :\n",
        "- Contenu haineux/violent\n",
        "- Harcèlement\n",
        "- Contenu sexuel\n",
        "- Auto-mutilation\n",
        "- Etc.\n",
        "\n",
        "**Pattern recommandé :**\n",
        "1. Modérer l'input utilisateur\n",
        "2. Générer la réponse si OK\n",
        "3. Modérer l'output avant affichage"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming\n",
        "print(\"=== Streaming Example ===\")\n",
        "stream = client.chat.completions.create(\n",
        "    model=DEFAULT_MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Écris un haïku sur la programmation.\"}],\n",
        "    stream=True,\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "print(\"Réponse streamée: \", end=\"\", flush=True)\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Modération\n",
        "print(\"=== Moderation Example ===\")\n",
        "test_inputs = [\n",
        "    \"Bonjour, comment vas-tu?\",\n",
        "    \"Enfoiré, je vais te tuer!\"\n",
        "]\n",
        "\n",
        "for text in test_inputs:\n",
        "    moderation = client.moderations.create(input=text)\n",
        "    result = moderation.results[0]\n",
        "    \n",
        "    print(f\"\\nTexte: {text}\")\n",
        "    print(f\"Flaggé: {result.flagged}\")\n",
        "    if result.flagged:\n",
        "        print(f\"Catégories: {[cat for cat, flagged in result.categories.__dict__.items() if flagged]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Interprétation des Résultats Streaming et Modération\n\n**Observations sur le Streaming :**\n\nLe streaming transforme radicalement l'expérience utilisateur :\n- **Latence perçue** : Au lieu d'attendre 2-3s pour voir la réponse complète, le premier mot apparaît en ~200ms\n- **Engagement utilisateur** : Voir le texte se construire donne l'impression d'une \"conversation naturelle\"\n- **Format de réponse** : Les chunks arrivent progressivement via `delta.content`\n\n**Observations sur la Modération :**\n\n| Texte | Flaggé | Catégories détectées | Action recommandée |\n|-------|--------|---------------------|-------------------|\n| \"Bonjour, comment vas-tu?\" | ❌ Non | Aucune | ✅ Traiter normalement |\n| \"Enfoiré, je vais te tuer!\" | ✅ Oui | harassment, harassment_threatening, violence | 🚫 Bloquer avant génération |\n\n**Pattern de sécurité production :**\n\n```python\n# Workflow complet sécurisé\ndef safe_generation(user_input: str) -> str:\n    # 1. Modération input\n    mod_input = client.moderations.create(input=user_input)\n    if mod_input.results[0].flagged:\n        return \"Votre message viole notre politique d'utilisation.\"\n    \n    # 2. Génération\n    response = generate(user_input)\n    \n    # 3. Modération output\n    mod_output = client.moderations.create(input=response)\n    if mod_output.results[0].flagged:\n        return \"Désolé, impossible de générer une réponse appropriée.\"\n    \n    return response\n```\n\n> **Note importante** : La modération ajoute ~100ms de latence par check. Pour les applications temps-réel, envisager une modération asynchrone post-génération.",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Streaming Example ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-18 09:13:04 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Réponse streamée: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Code"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " sur"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " l"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "’écran"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " danse"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ","
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ér"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ies"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " de"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " chiffres"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " en"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " boucle"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ","
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "’"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âme"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " d"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'un"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " programme"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "."
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Moderation Example ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-18 09:13:05 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/moderations \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Texte: Bonjour, comment vas-tu?\n",
            "Flaggé: False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-18 09:13:05 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/moderations \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Texte: Enfoiré, je vais te tuer!\n",
            "Flaggé: True\n",
            "Catégories: ['harassment', 'harassment_threatening', 'violence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Checklist Déploiement Production\n",
        "\n",
        "### Sécurité\n",
        "- [ ] API keys stockées dans variables d'environnement (jamais hardcodées)\n",
        "- [ ] Rotation régulière des clés\n",
        "- [ ] Rate limiting côté serveur\n",
        "- [ ] Validation et sanitization des inputs utilisateur\n",
        "- [ ] Modération de contenu activée\n",
        "- [ ] Logs ne contiennent pas de données sensibles\n",
        "\n",
        "### Résilience\n",
        "- [ ] Retry automatique avec backoff exponentiel\n",
        "- [ ] Timeout configurés\n",
        "- [ ] Circuit breaker pour dépendances externes\n",
        "- [ ] Fallback gracieux en cas d'erreur\n",
        "- [ ] Health checks réguliers\n",
        "\n",
        "### Performance\n",
        "- [ ] Cache activé (`store=True`)\n",
        "- [ ] Choix du modèle adapté au cas d'usage\n",
        "- [ ] `max_tokens` configuré pour limiter les coûts\n",
        "- [ ] Streaming pour réponses longues\n",
        "- [ ] Background mode pour tâches longues\n",
        "\n",
        "### Monitoring\n",
        "- [ ] Logging structuré configuré\n",
        "- [ ] Métriques : latence, taux d'erreur, coûts\n",
        "- [ ] Alertes budgétaires configurées\n",
        "- [ ] Dashboards temps réel\n",
        "- [ ] Traçabilité des requêtes (request ID)\n",
        "\n",
        "### Coûts\n",
        "- [ ] Budget mensuel défini\n",
        "- [ ] Alertes à 50%, 80%, 100% du budget\n",
        "- [ ] Audit régulier de l'usage par endpoint/utilisateur\n",
        "- [ ] Optimisation continue des prompts\n",
        "- [ ] Évaluation régulière des modèles (nouveaux modèles moins chers?)\n",
        "\n",
        "### Conformité\n",
        "- [ ] RGPD : consentement utilisateur pour traitement des données\n",
        "- [ ] Politique de rétention des données\n",
        "- [ ] Anonymisation des données personnelles\n",
        "- [ ] Documentation des traitements de données\n",
        "- [ ] DPO informé de l'usage d'IA"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "### Récapitulatif des Patterns\n",
        "\n",
        "| Pattern | Cas d'usage | Complexité |\n",
        "|---------|-------------|------------|\n",
        "| **Responses API + store** | Conversations courtes avec cache | ⭐⭐ |\n",
        "| **Conversations API** | Conversations longue durée | ⭐⭐ |\n",
        "| **Background Mode** | Tâches longues (>30s) | ⭐⭐⭐ |\n",
        "| **Retry + Backoff** | Résilience production | ⭐⭐ |\n",
        "| **Rate Limiter** | Respect limites API | ⭐⭐ |\n",
        "| **Logging structuré** | Monitoring et debug | ⭐⭐ |\n",
        "| **Streaming** | UX temps réel | ⭐⭐ |\n",
        "| **Modération** | Sécurité contenu | ⭐ |\n",
        "\n",
        "### Ressources Supplémentaires\n",
        "\n",
        "**Documentation OpenAI :**\n",
        "- [Responses API](https://platform.openai.com/docs/api-reference/responses)\n",
        "- [Conversations API](https://platform.openai.com/docs/api-reference/conversations)\n",
        "- [Rate Limits](https://platform.openai.com/docs/guides/rate-limits)\n",
        "- [Error Codes](https://platform.openai.com/docs/guides/error-codes)\n",
        "\n",
        "**Bibliothèques Python :**\n",
        "- [tenacity](https://tenacity.readthedocs.io/) : Retry robuste\n",
        "- [openai](https://github.com/openai/openai-python) : SDK officiel\n",
        "- [loguru](https://loguru.readthedocs.io/) : Logging simplifié\n",
        "\n",
        "**Prochaines étapes :**\n",
        "- Implémenter ces patterns dans votre application\n",
        "- Configurer un monitoring complet\n",
        "- Tester la résilience (chaos engineering)\n",
        "- Optimiser les coûts progressivement"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercices Pratiques\n\n### Exercice 1 : Chatbot Multi-Session (30 min)\n\nCréez un chatbot de support client qui :\n1. Utilise la Conversations API pour maintenir le contexte\n2. Persiste les conversations dans un fichier JSON (pour reprise après redémarrage)\n3. Implémente un retry avec backoff\n4. Log toutes les interactions\n\n**Bonus :** Ajouter une commande `/summary` qui résume la conversation en cours.\n\n### Exercice 2 : Système d'Analyse de Documents (40 min)\n\nImplémentez un système qui :\n1. Prend un long document en entrée\n2. Lance l'analyse en background mode\n3. Affiche une barre de progression pendant le traitement\n4. Retourne un rapport structuré (points clés, résumé, recommandations)\n5. Calcule et affiche le coût total de l'analyse\n\n**Bonus :** Comparer les coûts entre gpt-5-mini et gpt-5.\n\n### Exercice 3 : Rate Limiter Multi-Tier (20 min)\n\nAméliorez la classe `RateLimiter` pour supporter :\n1. Des limites par minute ET par jour\n2. Des priorités de requêtes (high/medium/low)\n3. Un mode \"burst\" (permettre 10 requêtes instantanées, puis throttling)\n\n**Bonus :** Ajouter des statistiques (nombre de requêtes dans les dernières 24h, temps moyen d'attente).",
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}