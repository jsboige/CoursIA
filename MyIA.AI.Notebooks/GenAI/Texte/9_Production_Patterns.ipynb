{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0121073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:42:41.287985Z",
     "iopub.status.busy": "2026-02-25T21:42:41.287133Z",
     "iopub.status.idle": "2026-02-25T21:42:41.292063Z",
     "shell.execute_reply": "2026-02-25T21:42:41.291457Z"
    },
    "papermill": {
     "duration": 0.017576,
     "end_time": "2026-02-25T21:42:41.293189",
     "exception": false,
     "start_time": "2026-02-25T21:42:41.275613",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_MODE = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8cea6d",
   "metadata": {
    "papermill": {
     "duration": 0.004059,
     "end_time": "2026-02-25T21:42:41.301621",
     "exception": false,
     "start_time": "2026-02-25T21:42:41.297562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Patterns de Production : APIs Avancées OpenAI\n",
    "\n",
    "Ce notebook couvre les fonctionnalités avancées nécessaires pour des applications en production :\n",
    "- **Conversations API** : Persistance d'état entre sessions\n",
    "- **Background Mode** : Tâches asynchrones longues\n",
    "- **Rate Limiting** : Gestion des limites d'API\n",
    "- **Optimisation** : Réduction des coûts\n",
    "\n",
    "**Objectifs :**\n",
    "- Gérer des conversations multi-sessions\n",
    "- Exécuter des tâches en arrière-plan\n",
    "- Implémenter des patterns de résilience\n",
    "- Optimiser les coûts d'API\n",
    "\n",
    "**Prérequis :** Notebooks 1-4\n",
    "\n",
    "**Durée estimée :** 70 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e533d50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:42:41.311317Z",
     "iopub.status.busy": "2026-02-25T21:42:41.311116Z",
     "iopub.status.idle": "2026-02-25T21:42:43.909580Z",
     "shell.execute_reply": "2026-02-25T21:42:43.908618Z"
    },
    "papermill": {
     "duration": 2.604525,
     "end_time": "2026-02-25T21:42:43.910663",
     "exception": false,
     "start_time": "2026-02-25T21:42:41.306138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client OpenAI initialisé !\n",
      "Modèle par défaut: gpt-5-mini\n",
      "Mode batch: True\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai python-dotenv tenacity\n",
    "\n",
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "load_dotenv('../.env')\n",
    "client = OpenAI()\n",
    "\n",
    "# Modèle par défaut depuis .env\n",
    "DEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\n",
    "BATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\"\n",
    "\n",
    "print(\"Client OpenAI initialisé !\")\n",
    "print(f\"Modèle par défaut: {DEFAULT_MODEL}\")\n",
    "print(f\"Mode batch: {BATCH_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16421dc8",
   "metadata": {
    "papermill": {
     "duration": 0.004109,
     "end_time": "2026-02-25T21:42:43.919121",
     "exception": false,
     "start_time": "2026-02-25T21:42:43.915012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client OpenAI initialisé !\n",
      "Modèle par défaut: gpt-5-mini\n",
      "Mode batch: True\n"
     ]
    }
   ],
   "source": [
    "### Vérification de l'Environnement\n",
    "\n",
    "**Composants installés et chargés :**\n",
    "\n",
    "1. **Bibliothèques Python** :\n",
    "   - `openai` : SDK officiel pour l'API OpenAI\n",
    "   - `python-dotenv` : Chargement sécurisé des variables d'environnement\n",
    "   - `tenacity` : Gestion avancée des retry avec backoff exponentiel\n",
    "\n",
    "2. **Configuration extraite du fichier `.env`** :\n",
    "   - `OPENAI_MODEL` : Modèle par défaut (ex: `gpt-5-mini`)\n",
    "   - `BATCH_MODE` : Active le mode batch pour tests automatisés (skip les inputs interactifs)\n",
    "\n",
    "**Sortie attendue :**\n",
    "\n",
    "```\n",
    "Client OpenAI initialisé !\n",
    "Modèle par défaut: gpt-5-mini\n",
    "Mode batch: False\n",
    "```\n",
    "\n",
    "**Points de validation :**\n",
    "\n",
    "| Élément | Validation | Action si erreur |\n",
    "|---------|------------|------------------|\n",
    "| Client initialisé | ✅ Pas d'exception | Vérifier `OPENAI_API_KEY` dans `.env` |\n",
    "| Modèle détecté | ✅ Affiche un nom valide | Définir `OPENAI_MODEL` dans `.env` |\n",
    "| Mode batch | ✅ `True` ou `False` | Optionnel, par défaut `False` |\n",
    "\n",
    "> **Sécurité** : Ne JAMAIS afficher la clé API dans les logs. Le SDK utilise automatiquement la variable d'environnement `OPENAI_API_KEY` sans exposition dans le code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b884fe",
   "metadata": {
    "papermill": {
     "duration": 0.003681,
     "end_time": "2026-02-25T21:42:43.926539",
     "exception": false,
     "start_time": "2026-02-25T21:42:43.922858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 1. Gestion du Contexte avec Chat Completions\n",
    "\n",
    "La **gestion manuelle du contexte** avec Chat Completions permet de :\n",
    "- **Maintenir l'historique** : Conserver les messages précédents dans une liste\n",
    "- **Chaîner les conversations** : Chaque requête inclut tout le contexte\n",
    "- **Contrôle total** : Gestion explicite de ce qui est envoyé\n",
    "\n",
    "**Cas d'usage :**\n",
    "- Conversations multi-tours\n",
    "- Chatbots avec mémoire\n",
    "- Systèmes de Q&A contextuels\n",
    "\n",
    "**Architecture :**\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"...\"},  # Optionnel\n",
    "    {\"role\": \"user\", \"content\": \"...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"...\"},  # Réponse précédente\n",
    "    {\"role\": \"user\", \"content\": \"...\"}  # Nouvelle question\n",
    "]\n",
    "```\n",
    "\n",
    "**Note importante :** La Responses API avec `store=True` n'est pas disponible dans cette version. Nous utilisons Chat Completions avec gestion manuelle de l'historique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa66074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:42:43.937217Z",
     "iopub.status.busy": "2026-02-25T21:42:43.937005Z",
     "iopub.status.idle": "2026-02-25T21:42:51.905619Z",
     "shell.execute_reply": "2026-02-25T21:42:51.904140Z"
    },
    "papermill": {
     "duration": 7.974787,
     "end_time": "2026-02-25T21:42:51.907120",
     "exception": false,
     "start_time": "2026-02-25T21:42:43.932333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1 ID: chatcmpl-DDGwX5TKGyFGEj9FXBFtiHhx9h3X1\n",
      "Contenu: ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response 2 ID: chatcmpl-DDGwbDCDJYfqvqdjKQyS3xtlykFuf\n",
      "Contenu: \n",
      "\n",
      "Tokens utilisés: 43 input / 200 output\n"
     ]
    }
   ],
   "source": [
    "# Premier message - utilisation de Chat Completions standard\n",
    "# Note: La Responses API avec store n'est pas disponible dans cette version\n",
    "# Nous utilisons Chat Completions avec gestion manuelle du contexte\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Conversation avec historique géré manuellement\n",
    "conversation_history = []\n",
    "\n",
    "# Premier message\n",
    "conversation_history.append({\n",
    "    \"role\": \"user\", \n",
    "    \"content\": \"Je m'appelle Jean et j'habite à Paris. Retiens ces informations.\"\n",
    "})\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=conversation_history,\n",
    "    max_completion_tokens=200\n",
    ")\n",
    "\n",
    "content1 = response1.choices[0].message.content\n",
    "print(f\"Response 1 ID: {response1.id}\")\n",
    "print(f\"Contenu: {content1[:200]}...\")\n",
    "\n",
    "# Ajouter la réponse à l'historique\n",
    "conversation_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": content1\n",
    "})\n",
    "\n",
    "# Deuxième message (contexte préservé via l'historique)\n",
    "conversation_history.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Quel est mon nom et où j'habite?\"\n",
    "})\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=conversation_history,\n",
    "    max_completion_tokens=200\n",
    ")\n",
    "\n",
    "content2 = response2.choices[0].message.content\n",
    "print(f\"\\nResponse 2 ID: {response2.id}\")\n",
    "print(f\"Contenu: {content2}\")\n",
    "\n",
    "# Vérifier les tokens\n",
    "print(f\"\\nTokens utilisés: {response2.usage.prompt_tokens} input / {response2.usage.completion_tokens} output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63aa632",
   "metadata": {
    "papermill": {
     "duration": 0.004139,
     "end_time": "2026-02-25T21:42:51.918792",
     "exception": false,
     "start_time": "2026-02-25T21:42:51.914653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1 ID: resp_04971c529e54bb2a0069957469cc8c81978b57fd930cf7e4cf\n",
      "Contenu: D'accord, Jean ! Tu habites à Paris. Si tu as d'autres informations à partager ou des questions, n'hésite pas !...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response 2 ID: resp_04971c529e54bb2a006995746b5a8481978360d7592ef23f93\n",
      "Contenu: Ton nom est Jean et tu habites à Paris.\n",
      "\n",
      "Tokens utilisés: 71 input / 12 output\n"
     ]
    }
   ],
   "source": [
    "### Interprétation des Résultats\n",
    "\n",
    "**Observations importantes :**\n",
    "\n",
    "1. **Gestion manuelle du contexte** : L'historique est conservé dans une liste `conversation_history`\n",
    "2. **Contexte préservé** : Le modèle se souvient de \"Jean\" et \"Paris\" dans la deuxième requête\n",
    "3. **Consommation de tokens** : Les tokens d'entrée augmentent avec la taille de l'historique\n",
    "\n",
    "**Pattern utilisé :**\n",
    "```python\n",
    "# Ajouter question utilisateur\n",
    "conversation_history.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "# Appel API avec tout l'historique\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=conversation_history\n",
    ")\n",
    "\n",
    "# Ajouter réponse au contexte\n",
    "conversation_history.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "```\n",
    "\n",
    "**Optimisation :** Pour les longues conversations, envisagez de :\n",
    "- Limiter l'historique aux N derniers échanges\n",
    "- Résumer le contexte périodiquement\n",
    "- Utiliser un système RAG pour les contextes très longs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a558f7d",
   "metadata": {
    "papermill": {
     "duration": 0.006231,
     "end_time": "2026-02-25T21:42:51.929050",
     "exception": false,
     "start_time": "2026-02-25T21:42:51.922819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 2. Conversations Multi-Tours\n",
    "\n",
    "La gestion des **conversations multi-tours** avec Chat Completions repose sur :\n",
    "- **Historique cumulatif** : Chaque message s'ajoute à la liste\n",
    "- **Contexte automatique** : Le modèle voit tout l'historique à chaque appel\n",
    "- **Flexibilité** : Possibilité de modifier l'historique (résumé, filtrage)\n",
    "\n",
    "**Architecture :**\n",
    "```\n",
    "messages = [system, user1, assistant1, user2, assistant2, user3, ...]\n",
    "              ↓\n",
    "        Chat Completions API\n",
    "              ↓\n",
    "         assistant3\n",
    "```\n",
    "\n",
    "**Avantages :**\n",
    "- Simple à implémenter\n",
    "- Contrôle total sur le contexte\n",
    "- Compatible avec tous les modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebfed219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:42:51.941245Z",
     "iopub.status.busy": "2026-02-25T21:42:51.941047Z",
     "iopub.status.idle": "2026-02-25T21:43:03.144622Z",
     "shell.execute_reply": "2026-02-25T21:43:03.143757Z"
    },
    "papermill": {
     "duration": 11.211848,
     "end_time": "2026-02-25T21:43:03.145756",
     "exception": false,
     "start_time": "2026-02-25T21:42:51.933908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conversation multi-turn ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: ...\n",
      "\n",
      "6 messages dans la conversation\n",
      "  Contexte: Japon → Tokyo → Meilleure période\n"
     ]
    }
   ],
   "source": [
    "# Simulation de conversation multi-turn avec Chat Completions\n",
    "# Le contexte est géré en conservant l'historique des messages\n",
    "\n",
    "print(\"=== Conversation multi-turn ===\\n\")\n",
    "\n",
    "# Historique de conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant de voyage expert.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Je planifie un voyage au Japon.\"}\n",
    "]\n",
    "\n",
    "# Premier échange\n",
    "resp1 = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=messages,\n",
    "    max_completion_tokens=200\n",
    ")\n",
    "assistant_reply1 = resp1.choices[0].message.content\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_reply1})\n",
    "print(f\"Assistant: {assistant_reply1[:200]}...\")\n",
    "\n",
    "# Deuxième échange (contexte automatiquement préservé via messages)\n",
    "messages.append({\"role\": \"user\", \"content\": \"Quels sont les meilleurs endroits à Tokyo?\"})\n",
    "resp2 = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=messages,\n",
    "    max_completion_tokens=200\n",
    ")\n",
    "assistant_reply2 = resp2.choices[0].message.content\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_reply2})\n",
    "print(f\"\\nAssistant: {assistant_reply2[:200]}...\")\n",
    "\n",
    "# Troisième échange\n",
    "messages.append({\"role\": \"user\", \"content\": \"Quelle est la meilleure période pour y aller?\"})\n",
    "resp3 = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=messages,\n",
    "    max_completion_tokens=200\n",
    ")\n",
    "assistant_reply3 = resp3.choices[0].message.content\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_reply3})\n",
    "print(f\"\\nAssistant: {assistant_reply3[:200]}...\")\n",
    "\n",
    "print(f\"\\n{len([m for m in messages if m['role'] != 'system'])} messages dans la conversation\")\n",
    "print(f\"  Contexte: Japon → Tokyo → Meilleure période\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555afdc",
   "metadata": {
    "papermill": {
     "duration": 0.004562,
     "end_time": "2026-02-25T21:43:03.157328",
     "exception": false,
     "start_time": "2026-02-25T21:43:03.152766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Simulation de conversation multi-turn ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation ID (via Response): resp_0ce0ef616efa361c006995746c4a7c819093d6fcc7ee0cb155\n",
      "\n",
      "Assistant: Génial ! Le Japon est un pays fascinant avec une richesse culturelle, historique et naturelle. Pour t'aider au mieux, j'aurais besoin de quelques détails :\n",
      "\n",
      "1. **Durée de ton séjour** : Combien de jou...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: Tokyo est une ville dynamique avec une multitude d'endroits à explorer. Voici quelques-uns des meilleurs lieux à visiter :\n",
      "\n",
      "### 1. **Shibuya**\n",
      "- **Traversée de Shibuya** : L'une des intersections les ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: La meilleure période pour visiter Tokyo dépend de ce que tu souhaites voir et faire. Voici un aperçu des saisons :\n",
      "\n",
      "### 1. **Printemps (mars à mai)**\n",
      "- **Cerisiers en fleurs (sakura)** : Fin mars à dé...\n",
      "\n",
      "3 messages dans la conversation chaînée\n",
      "  Response IDs: resp_0ce0ef616efa361... -> resp_0ce0ef616efa361... -> resp_0ce0ef616efa361...\n"
     ]
    }
   ],
   "source": [
    "### Analyse de la Conversation Multi-Tour\n",
    "\n",
    "**Points clés observés :**\n",
    "\n",
    "1. **Chaînage automatique** : Chaque `previous_response_id` pointe vers la réponse précédente\n",
    "2. **Contexte cumulatif** : Le modèle comprend le contexte complet :\n",
    "   - Message 1 : Établit le rôle (assistant de voyage) + destination (Japon)\n",
    "   - Message 2 : Peut répondre spécifiquement sur Tokyo (contexte préservé)\n",
    "   - Message 3 : Comprend qu'on parle toujours du Japon\n",
    "\n",
    "3. **Structure de données** :\n",
    "```\n",
    "resp1 (Japon)\n",
    "  └─ resp2 (Tokyo) \n",
    "      └─ resp3 (Meilleure période)\n",
    "```\n",
    "\n",
    "**Cas d'usage production :**\n",
    "- Chatbots de support client (reprendre une conversation après déconnexion)\n",
    "- Assistants multi-sessions (planification, conseil)\n",
    "- Tutoriels interactifs (mémoriser les réponses précédentes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f868db5",
   "metadata": {
    "papermill": {
     "duration": 0.00422,
     "end_time": "2026-02-25T21:43:03.165637",
     "exception": false,
     "start_time": "2026-02-25T21:43:03.161417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Comprendre le Polling\n",
    "\n",
    "**Pourquoi le polling ?**\n",
    "\n",
    "En mode background, le serveur ne peut pas \"pousser\" le résultat vers le client. Le client doit donc **interroger régulièrement** le serveur pour vérifier le statut.\n",
    "\n",
    "**Pattern typique observé :**\n",
    "```\n",
    "t=0s   : status = \"pending\"     → Attendre 5s\n",
    "t=5s   : status = \"processing\"  → Attendre 5s\n",
    "t=10s  : status = \"processing\"  → Attendre 5s\n",
    "t=15s  : status = \"completed\"   → Récupérer le résultat\n",
    "```\n",
    "\n",
    "**Optimisations possibles :**\n",
    "- **Backoff progressif** : Attendre 2s, puis 5s, puis 10s, etc. (éviter surcharge)\n",
    "- **Webhooks** : Le serveur appelle votre API quand c'est terminé (pas supporté par OpenAI actuellement)\n",
    "- **WebSockets** : Connexion persistante pour notifications temps réel\n",
    "\n",
    "**Quand utiliser background mode ?**\n",
    "- ✅ Analyses longues (>30s)\n",
    "- ✅ Génération de rapports complexes\n",
    "- ✅ Tâches où l'utilisateur peut attendre\n",
    "- ❌ Chatbot temps réel (utiliser streaming à la place)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff20780",
   "metadata": {
    "papermill": {
     "duration": 0.00359,
     "end_time": "2026-02-25T21:43:03.173590",
     "exception": false,
     "start_time": "2026-02-25T21:43:03.170000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 3. Streaming pour UX Réactive\n",
    "\n",
    "Le **Streaming** permet d'améliorer l'expérience utilisateur :\n",
    "- **Feedback immédiat** : Les tokens apparaissent progressivement\n",
    "- **Latence perçue réduite** : Premier token en ~200ms\n",
    "- **Annulation possible** : L'utilisateur peut arrêter la génération\n",
    "\n",
    "**Quand utiliser le streaming ?**\n",
    "- ✅ Réponses longues (>100 tokens)\n",
    "- ✅ Chatbots en temps réel\n",
    "- ✅ Génération de contenu (articles, rapports)\n",
    "\n",
    "**Alternative pour tâches longues :**\n",
    "- **API Batch OpenAI** : Pour les traitements asynchrones (délai 24h)\n",
    "- **Architecture async** : Celery, Redis Queue pour le polling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ce47a",
   "metadata": {
    "papermill": {
     "duration": 0.003459,
     "end_time": "2026-02-25T21:43:03.180545",
     "exception": false,
     "start_time": "2026-02-25T21:43:03.177086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Décorticage du Retry\n",
    "\n",
    "**Fonctionnement du décorateur `@retry` :**\n",
    "\n",
    "1. **Première tentative** : Appel normal de la fonction\n",
    "2. **En cas d'échec** : Si l'exception est `RateLimitError` ou `APIError` :\n",
    "   - Attendre 2s (multiplier=1, min=2)\n",
    "   - Tentative 2\n",
    "3. **Nouvel échec** : Attendre 4s (backoff exponentiel)\n",
    "4. **Et ainsi de suite** : jusqu'à 5 tentatives max\n",
    "\n",
    "**Exemple de scénario réel :**\n",
    "```\n",
    "t=0s    : Requête → RateLimitError (429 Too Many Requests)\n",
    "t=2s    : Retry #1 → APIError (503 Service Unavailable)\n",
    "t=6s    : Retry #2 → Succès ✅\n",
    "```\n",
    "\n",
    "**Pourquoi exponentiel ?**\n",
    "- Éviter l'effet \"thundering herd\" : Si 1000 clients retentent simultanément après 2s, le serveur reste surchargé\n",
    "- Avec backoff exponentiel, les requêtes sont étalées : 2s, 4s, 8s, 16s, 32s\n",
    "\n",
    "**Limites :**\n",
    "- ⚠️ Ne résout pas les erreurs permanentes (authentification, prompt invalide)\n",
    "- ⚠️ Peut augmenter la latence totale (jusqu'à 2+4+8+16+32 = 62s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab779be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:43:03.199752Z",
     "iopub.status.busy": "2026-02-25T21:43:03.199220Z",
     "iopub.status.idle": "2026-02-25T21:43:03.205805Z",
     "shell.execute_reply": "2026-02-25T21:43:03.204995Z"
    },
    "papermill": {
     "duration": 0.013298,
     "end_time": "2026-02-25T21:43:03.207229",
     "exception": false,
     "start_time": "2026-02-25T21:43:03.193931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH_MODE] Simulation de streaming:\n",
      "Réponse progressive: Les microservices offrent 3 avantages majeurs...\n"
     ]
    }
   ],
   "source": [
    "# Background Mode non disponible avec Chat Completions standard\n",
    "# Alternative: utiliser l'API Batch d'OpenAI pour les tâches longues\n",
    "\n",
    "if not BATCH_MODE:\n",
    "    print(\"=== Note sur le Background Mode ===\")\n",
    "    print()\n",
    "    print(\"Le Background Mode n'est pas disponible avec l'API Chat Completions standard.\")\n",
    "    print()\n",
    "    print(\"Alternatives pour les tâches longues:\")\n",
    "    print(\"  1. API Batch OpenAI: Pour les traitements asynchrones (24h délai)\")\n",
    "    print(\"  2. Streaming: Pour les réponses longues avec feedback utilisateur\")\n",
    "    print(\"  3. Architecture async: Celery, Redis Queue pour le polling\")\n",
    "    print()\n",
    "    \n",
    "    # Démonstration de streaming comme alternative\n",
    "    print(\"=== Alternative: Streaming pour tâches longues ===\")\n",
    "    stream = client.chat.completions.create(\n",
    "        model=DEFAULT_MODEL,\n",
    "        messages=[{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Analyse en 3 points les avantages des microservices.\"\n",
    "        }],\n",
    "        stream=True,\n",
    "        max_completion_tokens=300\n",
    "    )\n",
    "    \n",
    "    print(\"Réponse progressive: \", end=\"\", flush=True)\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "else:\n",
    "    print(\"[BATCH_MODE] Simulation de streaming:\")\n",
    "    print(\"Réponse progressive: Les microservices offrent 3 avantages majeurs...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f3b15",
   "metadata": {
    "papermill": {
     "duration": 0.004697,
     "end_time": "2026-02-25T21:43:03.219646",
     "exception": false,
     "start_time": "2026-02-25T21:43:03.214949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH_MODE] Simulation de tâche background:\n",
      "Status: pending -> processing -> completed (30s)\n",
      "Résultat: Comparaison de 5 architectures microservices (500 caractères)\n"
     ]
    }
   ],
   "source": [
    "### Comportement du Rate Limiter\n",
    "\n",
    "**Analyse de l'exécution :**\n",
    "\n",
    "Dans cet exemple avec `max_requests_per_minute=10` :\n",
    "- Les 3 requêtes passent **instantanément** (aucune attente)\n",
    "- Pourquoi ? Parce que 3 < 10 (limite non atteinte)\n",
    "\n",
    "**Test avec limite serrée :**\n",
    "\n",
    "Si nous avions configuré `max_requests_per_minute=2` :\n",
    "```\n",
    "t=0s   : Requête 1 → OK (1/2)\n",
    "t=0.5s : Requête 2 → OK (2/2)\n",
    "t=1s   : Requête 3 → ATTENTE de ~59s (fenêtre de 60s)\n",
    "t=60s  : Requête 3 → OK\n",
    "```\n",
    "\n",
    "**Fenêtre glissante vs Fenêtre fixe :**\n",
    "\n",
    "Notre implémentation utilise une **fenêtre glissante** :\n",
    "- ✅ Plus précis : compte exactement les 60 dernières secondes\n",
    "- ✅ Pas d'effet \"reset brutal\" à chaque minute\n",
    "\n",
    "Alternative **fenêtre fixe** :\n",
    "```python\n",
    "# Moins précis mais plus simple\n",
    "if current_minute != last_minute:\n",
    "    counter = 0\n",
    "    last_minute = current_minute\n",
    "counter += 1\n",
    "if counter > max_rpm:\n",
    "    wait()\n",
    "```\n",
    "\n",
    "**En production :**\n",
    "- Combiner rate limiter côté client (courtoisie) + côté serveur (sécurité)\n",
    "- Adapter la limite à votre tier OpenAI (voir dashboard usage limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b10b50",
   "metadata": {
    "papermill": {
     "duration": 0.003495,
     "end_time": "2026-02-25T21:43:03.227101",
     "exception": false,
     "start_time": "2026-02-25T21:43:03.223606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 4. Rate Limiting et Retry\n",
    "\n",
    "En production, il est essentiel de gérer les erreurs transitoires et les limites d'API :\n",
    "\n",
    "### Retry avec Backoff Exponentiel\n",
    "\n",
    "La bibliothèque **tenacity** permet d'implémenter facilement un retry automatique :\n",
    "- **Backoff exponentiel** : Attendre 2s, puis 4s, puis 8s, etc.\n",
    "- **Retry sélectif** : Uniquement sur certaines exceptions\n",
    "- **Limite de tentatives** : Éviter les boucles infinies\n",
    "\n",
    "**Pattern recommandé :**\n",
    "```python\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=60),\n",
    "    retry=retry_if_exception_type((RateLimitError, APIError))\n",
    ")\n",
    "```\n",
    "\n",
    "**Erreurs à gérer :**\n",
    "- `RateLimitError` : Limite de requêtes dépassée (429)\n",
    "- `APIError` : Erreur serveur temporaire (500, 502, 503)\n",
    "- `Timeout` : Délai d'attente dépassé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaaf1eb",
   "metadata": {
    "papermill": {
     "duration": 0.005268,
     "end_time": "2026-02-25T21:43:03.236177",
     "exception": false,
     "start_time": "2026-02-25T21:43:03.230909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Comparaison des Résultats\n",
    "\n",
    "**Analyse coût/bénéfice :**\n",
    "\n",
    "Pour ces deux requêtes similaires (résumé en 3 points), observons :\n",
    "\n",
    "| Métrique | Sans cache | Avec cache | Différence |\n",
    "|----------|-----------|------------|------------|\n",
    "| Tokens input | ~25 | ~25 | 0% (première requête) |\n",
    "| Tokens output | ~60 | ~65 | +8% (variation normale) |\n",
    "| Coût | ~$0.000663 | ~$0.000713 | +7% |\n",
    "| Cache hit | Non | **Dépend du prompt** | - |\n",
    "\n",
    "**Pourquoi pas d'économie ici ?**\n",
    "- Les prompts sont **différents** (\"Python\" vs \"JavaScript\")\n",
    "- Le cache ne fonctionne que sur les **préfixes identiques**\n",
    "\n",
    "**Test optimal pour le cache :**\n",
    "```python\n",
    "# Requête 1\n",
    "resp1 = optimized_completion(\"Contexte: Documentation Python. Question: Qu'est-ce qu'une liste?\")\n",
    "\n",
    "# Requête 2 (même préfixe \"Contexte: Documentation Python.\")\n",
    "resp2 = optimized_completion(\"Contexte: Documentation Python. Question: Qu'est-ce qu'un tuple?\")\n",
    "# → Économie de ~50% sur les tokens du contexte\n",
    "```\n",
    "\n",
    "**Gains typiques du cache :**\n",
    "- Conversations longues : 40-60% économie\n",
    "- RAG avec contexte fixe : 60-80% économie\n",
    "- Requêtes isolées : 0% (aucun préfixe commun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48d928a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:43:03.245664Z",
     "iopub.status.busy": "2026-02-25T21:43:03.245474Z",
     "iopub.status.idle": "2026-02-25T21:43:06.665167Z",
     "shell.execute_reply": "2026-02-25T21:43:06.663478Z"
    },
    "papermill": {
     "duration": 3.426591,
     "end_time": "2026-02-25T21:43:06.667118",
     "exception": false,
     "start_time": "2026-02-25T21:43:03.240527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=60),\n",
    "    retry=retry_if_exception_type((RateLimitError, APIError))\n",
    ")\n",
    "def safe_completion(prompt: str, model: str = None) -> str:\n",
    "    \"\"\"Appel API avec retry automatique sur erreurs transitoires\"\"\"\n",
    "    model = model or DEFAULT_MODEL\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_completion_tokens=200\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test\n",
    "result = safe_completion(\"Dis 'Hello World' en 5 langues.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d6612",
   "metadata": {
    "papermill": {
     "duration": 0.011087,
     "end_time": "2026-02-25T21:43:06.685503",
     "exception": false,
     "start_time": "2026-02-25T21:43:06.674416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici \"Hello World\" dans cinq langues :\n",
      "\n",
      "1. Anglais : Hello World\n",
      "2. Français : Bonjour le monde\n",
      "3. Espagnol : Hola Mundo\n",
      "4. Allemand : Hallo Welt\n",
      "5. Italien : Ciao Mondo\n",
      "\n",
      "Si tu as besoin d'autres traductions, n'hésite pas à demander !\n"
     ]
    }
   ],
   "source": [
    "### Interprétation des Logs\n",
    "\n",
    "**Format de log structuré :**\n",
    "\n",
    "```\n",
    "2026-02-04 15:23:45 | OpenAI_Production | INFO | SUCCESS | Model: gpt-5-mini | Duration: 1.23s | Tokens: 85 (in: 25, out: 60)\n",
    "```\n",
    "\n",
    "**Champs clés :**\n",
    "- `Timestamp` : Horodatage précis (important pour corrélation)\n",
    "- `Level` : INFO (succès) ou ERROR (échec)\n",
    "- `Model` : Tracer quel modèle a été utilisé\n",
    "- `Duration` : Latence end-to-end (objectif : <2s pour chatbot)\n",
    "- `Tokens` : Consommation (surveiller les dépassements)\n",
    "\n",
    "**Analyse des logs :**\n",
    "\n",
    "1. **Log SUCCESS** :\n",
    "   - Durée ~1-2s : Normal pour gpt-5-mini\n",
    "   - 85 tokens total : Petit prompt + réponse courte\n",
    "   - ✅ Requête efficace\n",
    "\n",
    "2. **Log ERROR** :\n",
    "   - `InvalidRequestError` : Modèle invalide\n",
    "   - Capturé et loggé avant propagation\n",
    "   - ✅ Permet diagnostic rapide sans crash\n",
    "\n",
    "**Agrégation recommandée :**\n",
    "```bash\n",
    "# Latence moyenne par modèle (via grep + awk)\n",
    "grep \"SUCCESS\" production.log | awk '{print $8, $12}' | sort\n",
    "\n",
    "# Taux d'erreur sur 1h\n",
    "grep \"ERROR\" production.log | wc -l\n",
    "```\n",
    "\n",
    "**Outils professionnels :**\n",
    "- ELK Stack (Elasticsearch, Logstash, Kibana)\n",
    "- Datadog, Splunk, New Relic\n",
    "- Prometheus + Grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0247598d",
   "metadata": {
    "papermill": {
     "duration": 0.004963,
     "end_time": "2026-02-25T21:43:06.697175",
     "exception": false,
     "start_time": "2026-02-25T21:43:06.692212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Transition vers les Patterns Avancés\n",
    "\n",
    "Jusqu'ici nous avons couvert les **patterns de résilience et d'optimisation** :\n",
    "- ✅ Retry automatique\n",
    "- ✅ Rate limiting\n",
    "- ✅ Logging structuré\n",
    "- ✅ Optimisation des coûts\n",
    "\n",
    "Les deux derniers patterns concernent **l'expérience utilisateur et la sécurité** :\n",
    "\n",
    "1. **Streaming** : Améliorer la perception de latence\n",
    "   - Au lieu d'attendre 5s pour afficher 500 tokens\n",
    "   - Afficher progressivement (premier token en 200ms)\n",
    "   - Utilisateur voit la \"pensée en temps réel\"\n",
    "\n",
    "2. **Modération** : Filtrer le contenu inapproprié\n",
    "   - Avant de traiter : vérifier que l'input est acceptable\n",
    "   - Après génération : s'assurer que l'output est sûr\n",
    "   - Éviter les violations de politique d'usage\n",
    "\n",
    "Ces patterns sont **essentiels** pour toute application destinée aux utilisateurs finaux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515c8bc",
   "metadata": {
    "papermill": {
     "duration": 0.00602,
     "end_time": "2026-02-25T21:43:06.708609",
     "exception": false,
     "start_time": "2026-02-25T21:43:06.702589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Analyse des Résultats\n",
    "\n",
    "**Streaming :**\n",
    "\n",
    "Observez le comportement :\n",
    "- Les tokens apparaissent **progressivement** (un par un ou par petits groupes)\n",
    "- Latence au premier token : ~200-500ms\n",
    "- Latence totale : Identique à une requête non-streamée (~2s)\n",
    "- **Gain perçu** : L'utilisateur voit le début de la réponse immédiatement\n",
    "\n",
    "**Implémentation UX :**\n",
    "```python\n",
    "# Dans une vraie app web\n",
    "async def stream_to_user():\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            await websocket.send(content)  # Envoyer au navigateur en temps réel\n",
    "```\n",
    "\n",
    "**Modération :**\n",
    "\n",
    "Les résultats montrent :\n",
    "1. **Texte neutre** : \"Bonjour, comment vas-tu?\"\n",
    "   - `flagged = False` : Aucun problème détecté\n",
    "   \n",
    "2. **Texte négatif** : \"Je déteste ce produit, c'est nul!\"\n",
    "   - `flagged = False` : Critique légitime (pas de haine/violence)\n",
    "\n",
    "**Cas qui seraient flaggés :**\n",
    "- Contenu haineux, violent, sexuel\n",
    "- Harcèlement, menaces\n",
    "- Auto-mutilation\n",
    "\n",
    "**Pattern de sécurité :**\n",
    "```python\n",
    "# Avant de traiter\n",
    "if moderate(user_input).flagged:\n",
    "    return \"Désolé, ce contenu viole notre politique.\"\n",
    "\n",
    "# Générer\n",
    "response = generate(user_input)\n",
    "\n",
    "# Après génération\n",
    "if moderate(response).flagged:\n",
    "    return \"Désolé, impossible de répondre à cette demande.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d627a6c",
   "metadata": {
    "papermill": {
     "duration": 0.004963,
     "end_time": "2026-02-25T21:43:06.718693",
     "exception": false,
     "start_time": "2026-02-25T21:43:06.713730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Rate Limiter Personnalisé\n",
    "\n",
    "Pour respecter les limites de requêtes par minute (RPM), implémentons un **rate limiter** :\n",
    "- **Fenêtre glissante** : Compte les requêtes sur les 60 dernières secondes\n",
    "- **Attente automatique** : Bloque jusqu'à ce qu'une requête soit autorisée\n",
    "- **Configurable** : Adapter selon votre tier OpenAI\n",
    "\n",
    "**Limites par tier (exemple) :**\n",
    "- Free : 3 RPM\n",
    "- Tier 1 : 500 RPM\n",
    "- Tier 2 : 5000 RPM\n",
    "- Tier 5 : 10000 RPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45e8120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:43:06.729368Z",
     "iopub.status.busy": "2026-02-25T21:43:06.728961Z",
     "iopub.status.idle": "2026-02-25T21:43:19.136777Z",
     "shell.execute_reply": "2026-02-25T21:43:19.134888Z"
    },
    "papermill": {
     "duration": 12.415544,
     "end_time": "2026-02-25T21:43:19.139050",
     "exception": false,
     "start_time": "2026-02-25T21:43:06.723506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requête 1: ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requête 2: ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requête 3: ...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Limite le nombre de requêtes par minute\"\"\"\n",
    "    def __init__(self, max_requests_per_minute: int = 60):\n",
    "        self.max_rpm = max_requests_per_minute\n",
    "        self.requests = deque()\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        now = time.time()\n",
    "        # Nettoyer les anciennes requêtes (plus de 60s)\n",
    "        while self.requests and now - self.requests[0] > 60:\n",
    "            self.requests.popleft()\n",
    "        \n",
    "        # Si limite atteinte, attendre\n",
    "        if len(self.requests) >= self.max_rpm:\n",
    "            wait_time = 60 - (now - self.requests[0])\n",
    "            if wait_time > 0:\n",
    "                print(f\"Rate limit: attente de {wait_time:.1f}s\")\n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        self.requests.append(time.time())\n",
    "\n",
    "# Exemple d'utilisation\n",
    "limiter = RateLimiter(max_requests_per_minute=10)\n",
    "\n",
    "for i in range(3):\n",
    "    limiter.wait_if_needed()\n",
    "    result = safe_completion(f\"Nombre aléatoire #{i+1}\")\n",
    "    print(f\"Requête {i+1}: {result[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e08230",
   "metadata": {
    "papermill": {
     "duration": 0.005064,
     "end_time": "2026-02-25T21:43:19.153357",
     "exception": false,
     "start_time": "2026-02-25T21:43:19.148293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requête 1: Voici un nombre aléatoire : **27**. Si tu veux un ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requête 2: D'accord ! Voici un nom aléatoire : \"Clara Duval\"....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requête 3: Voici un nom aléatoire : **Julien Lefevre**. Si tu...\n"
     ]
    }
   ],
   "source": [
    "### Interprétation du Comportement du Rate Limiter\n",
    "\n",
    "**Observations de l'exécution :**\n",
    "\n",
    "Les 3 requêtes se sont exécutées **sans attente** car la limite (`max_requests_per_minute=10`) n'a pas été atteinte.\n",
    "\n",
    "**Analyse du mécanisme :**\n",
    "\n",
    "1. **Fenêtre glissante** :\n",
    "   ```python\n",
    "   # À t=0s : deque([t0])         → 1/10, OK\n",
    "   # À t=1s : deque([t0, t1])     → 2/10, OK  \n",
    "   # À t=2s : deque([t0, t1, t2]) → 3/10, OK\n",
    "   ```\n",
    "\n",
    "2. **Nettoyage automatique** :\n",
    "   - Les timestamps > 60s sont supprimés du deque\n",
    "   - Garantit que seules les requêtes récentes comptent\n",
    "\n",
    "**Test avec limite stricte :**\n",
    "\n",
    "Si nous avions configuré `max_requests_per_minute=2` :\n",
    "\n",
    "```\n",
    "t=0.0s : Requête #1 → OK (1/2)\n",
    "t=0.5s : Requête #2 → OK (2/2)\n",
    "t=1.0s : Requête #3 → ATTENTE ~59s (limite atteinte)\n",
    "         ↓ Nettoyage à t=60s (requête #1 expire)\n",
    "t=60.5s: Requête #3 → OK (1/2)\n",
    "```\n",
    "\n",
    "**Comparaison des stratégies :**\n",
    "\n",
    "| Approche | Avantages | Inconvénients |\n",
    "|----------|-----------|---------------|\n",
    "| **Fenêtre glissante** (notre impl.) | ✅ Précis, pas d'effet de bord | ⚠️ Mémoire O(n) requêtes |\n",
    "| **Fenêtre fixe** (reset à chaque minute) | ✅ Simple, O(1) mémoire | ⚠️ Burst de 2× limite possible |\n",
    "| **Token bucket** | ✅ Permet les bursts contrôlés | ⚠️ Plus complexe à implémenter |\n",
    "\n",
    "**Recommandations production :**\n",
    "\n",
    "- **Développement** : Fenêtre glissante (notre code)\n",
    "- **Production haute charge** : Token bucket avec Redis\n",
    "- **Edge cases** : Combiner rate limiter client + serveur (double protection)\n",
    "\n",
    "> **Note** : Les limites OpenAI sont par organisation, pas par application. Coordonner le rate limiting si plusieurs services utilisent la même clé API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf53ac",
   "metadata": {
    "papermill": {
     "duration": 0.004254,
     "end_time": "2026-02-25T21:43:19.161525",
     "exception": false,
     "start_time": "2026-02-25T21:43:19.157271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 5. Optimisation des Coûts\n",
    "\n",
    "Stratégies pour réduire les coûts d'API en production :\n",
    "\n",
    "### 1. Utiliser le Cache (store=True)\n",
    "- Économie de 40-80% sur les tokens d'entrée répétés\n",
    "- Activer sur toutes les conversations longues\n",
    "\n",
    "### 2. Choisir le Bon Modèle\n",
    "| Modèle | Prix Input | Prix Output | Cas d'usage |\n",
    "|--------|------------|-------------|-------------|\n",
    "| gpt-5-mini | $0.15/1M | $0.60/1M | Tâches simples, production |\n",
    "| gpt-5 | $2.50/1M | $10.00/1M | Tâches complexes |\n",
    "| o1 | $10.00/1M | $40.00/1M | Raisonnement profond |\n",
    "\n",
    "### 3. Limiter les Tokens\n",
    "- Utiliser `max_tokens` pour contrôler la longueur\n",
    "- Préférer les prompts courts et précis\n",
    "- Éviter les contextes inutilement longs\n",
    "\n",
    "### 4. Batch Processing\n",
    "- Utiliser l'API Batch pour réductions de 50%\n",
    "- Acceptable pour tâches non-temps-réel\n",
    "\n",
    "### 5. Monitoring et Alertes\n",
    "- Suivre les coûts quotidiens/mensuels\n",
    "- Configurer des alertes budgétaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab25a88a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:43:19.170999Z",
     "iopub.status.busy": "2026-02-25T21:43:19.170779Z",
     "iopub.status.idle": "2026-02-25T21:43:27.267298Z",
     "shell.execute_reply": "2026-02-25T21:43:27.266179Z"
    },
    "papermill": {
     "duration": 8.103682,
     "end_time": "2026-02-25T21:43:27.269179",
     "exception": false,
     "start_time": "2026-02-25T21:43:19.165497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1 ===\n",
      "Réponse: ...\n",
      "Tokens: 17 in / 200 out\n",
      "Coût estimé: $0.000123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 2 ===\n",
      "Réponse: ...\n",
      "Tokens: 18 in / 200 out\n",
      "Coût estimé: $0.000123\n"
     ]
    }
   ],
   "source": [
    "def optimized_completion(prompt: str, system_prompt: str = None) -> dict:\n",
    "    \"\"\"Completion optimisée avec métriques de coût\"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=DEFAULT_MODEL,\n",
    "        messages=messages,\n",
    "        max_completion_tokens=200\n",
    "    )\n",
    "    \n",
    "    # Calculer les coûts approximatifs\n",
    "    # Prix gpt-5-mini: ~$0.15/1M input, ~$0.60/1M output\n",
    "    input_tokens = response.usage.prompt_tokens\n",
    "    output_tokens = response.usage.completion_tokens\n",
    "    \n",
    "    cost_input = input_tokens * 0.00000015\n",
    "    cost_output = output_tokens * 0.0000006\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.choices[0].message.content,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"estimated_cost\": cost_input + cost_output\n",
    "    }\n",
    "\n",
    "# Test 1\n",
    "result1 = optimized_completion(\"Résume les avantages de Python en 3 points.\")\n",
    "print(\"=== Test 1 ===\")\n",
    "print(f\"Réponse: {result1['content'][:100]}...\")\n",
    "print(f\"Tokens: {result1['input_tokens']} in / {result1['output_tokens']} out\")\n",
    "print(f\"Coût estimé: ${result1['estimated_cost']:.6f}\")\n",
    "\n",
    "# Test 2\n",
    "result2 = optimized_completion(\"Résume les avantages de JavaScript en 3 points.\")\n",
    "print(\"\\n=== Test 2 ===\")\n",
    "print(f\"Réponse: {result2['content'][:100]}...\")\n",
    "print(f\"Tokens: {result2['input_tokens']} in / {result2['output_tokens']} out\")\n",
    "print(f\"Coût estimé: ${result2['estimated_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b48655",
   "metadata": {
    "papermill": {
     "duration": 0.005064,
     "end_time": "2026-02-25T21:43:27.281572",
     "exception": false,
     "start_time": "2026-02-25T21:43:27.276508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sans cache ===\n",
      "Réponse: Voici trois avantages clés de Python :\n",
      "\n",
      "1. **Facilité d'apprentissage et de lisibilité** : Python a ...\n",
      "Tokens: 18 in / 146 out\n",
      "Coût estimé: $0.000090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Avec cache ===\n",
      "Réponse: Voici trois avantages de JavaScript :\n",
      "\n",
      "1. **Universalité et compatibilité** : JavaScript est pris en...\n",
      "Tokens: 19 in / 141 out\n",
      "Coût estimé: $0.000087\n",
      "Cache utilisé: False\n"
     ]
    }
   ],
   "source": [
    "### Interprétation des Résultats d'Optimisation\n",
    "\n",
    "**Analyse comparative :**\n",
    "\n",
    "Les deux exécutions montrent des coûts similaires car les prompts sont **complètement différents** (pas de préfixe commun pour bénéficier du cache).\n",
    "\n",
    "**Scénario optimal pour le cache :**\n",
    "\n",
    "Imaginons une application de support client avec un contexte fixe :\n",
    "\n",
    "```python\n",
    "# Contexte partagé (200 tokens)\n",
    "context = \"Documentation produit X: [longue description]...\"\n",
    "\n",
    "# Requête 1\n",
    "q1 = context + \" Question: Comment l'installer?\"\n",
    "# → Tokens input: 200 (contexte) + 5 (question) = 205\n",
    "\n",
    "# Requête 2 avec cache\n",
    "q2 = context + \" Question: Comment le configurer?\"\n",
    "# → Tokens input: 5 (seulement la nouvelle question!)\n",
    "# → Cache hit: 200 tokens @ 50% de réduction = économie de $0.000015\n",
    "```\n",
    "\n",
    "**Calcul d'économie réelle :**\n",
    "\n",
    "| Scénario | Sans cache | Avec cache | Économie |\n",
    "|----------|-----------|------------|----------|\n",
    "| **RAG avec contexte 1000 tokens** | $0.00015/req | $0.000075/req | **50%** |\n",
    "| **Conversation 10 tours (500 tokens/tour)** | $0.00375 total | $0.00150 total | **60%** |\n",
    "| **Documentation technique (2000 tokens)** | $0.0003/req | $0.0001/req | **67%** |\n",
    "\n",
    "**Points critiques :**\n",
    "\n",
    "1. ✅ **Activer le cache** : Toujours utiliser `store=True` pour conversations et RAG\n",
    "2. ✅ **Monitorer les hits** : Vérifier que `cached=True` dans les réponses\n",
    "3. ⚠️ **Attention aux modifications** : Changer 1 mot au début du contexte invalide tout le cache\n",
    "4. ⚠️ **Durée de vie** : Cache expire après 30 jours d'inactivité\n",
    "\n",
    "> **Recommandation production** : Pour une app avec 10000 requêtes/jour et contexte moyen de 500 tokens, le cache peut économiser **$20-30/mois** (gpt-5-mini)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592abcc5",
   "metadata": {
    "papermill": {
     "duration": 0.003804,
     "end_time": "2026-02-25T21:43:27.289346",
     "exception": false,
     "start_time": "2026-02-25T21:43:27.285542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 6. Monitoring et Logging\n",
    "\n",
    "Bonnes pratiques pour le monitoring en production :\n",
    "\n",
    "### Logging Structuré\n",
    "- **Timestamp** : Horodatage de chaque requête\n",
    "- **Modèle** : Quel modèle a été utilisé\n",
    "- **Durée** : Temps de réponse\n",
    "- **Tokens** : Consommation de tokens\n",
    "- **Succès/Échec** : Status de la requête\n",
    "- **Erreurs** : Type et message d'erreur\n",
    "\n",
    "### Métriques à Surveiller\n",
    "- **Latence** : p50, p95, p99\n",
    "- **Taux d'erreur** : Pourcentage de requêtes échouées\n",
    "- **Coûts** : Dépenses quotidiennes/mensuelles\n",
    "- **Tokens/requête** : Moyenne et variance\n",
    "- **Rate limiting** : Nombre de requêtes rejetées\n",
    "\n",
    "### Outils Recommandés\n",
    "- **Logging** : Python `logging`, Loguru\n",
    "- **APM** : Datadog, New Relic, OpenTelemetry\n",
    "- **Alerting** : PagerDuty, Opsgenie\n",
    "- **Dashboards** : Grafana, Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6140a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:43:27.298154Z",
     "iopub.status.busy": "2026-02-25T21:43:27.297558Z",
     "iopub.status.idle": "2026-02-25T21:43:31.567207Z",
     "shell.execute_reply": "2026-02-25T21:43:31.566031Z"
    },
    "papermill": {
     "duration": 4.275785,
     "end_time": "2026-02-25T21:43:31.568858",
     "exception": false,
     "start_time": "2026-02-25T21:43:27.293073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test réussi ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 22:43:31 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 22:43:31 | OpenAI_Production | INFO | SUCCESS | Model: gpt-5-mini | Duration: 4.12s | Tokens: 211 (in: 11, out: 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 22:43:31 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 22:43:31 | OpenAI_Production | ERROR | FAILED | Model: gpt-invalid-model | Duration: 0.14s | Error: NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-invalid-model` does not exist or you do not \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse: \n",
      "\n",
      "=== Test avec modèle invalide ===\n",
      "Erreur capturée: NotFoundError\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration du logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(\"OpenAI_Production\")\n",
    "\n",
    "def logged_completion(prompt: str, model: str = None) -> str:\n",
    "    \"\"\"Completion avec logging complet\"\"\"\n",
    "    model = model or DEFAULT_MODEL\n",
    "    start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_completion_tokens=200\n",
    "        )\n",
    "        \n",
    "        duration = (datetime.now() - start).total_seconds()\n",
    "        logger.info(f\"SUCCESS | Model: {model} | Duration: {duration:.2f}s | \"\n",
    "                   f\"Tokens: {response.usage.total_tokens} (in: {response.usage.prompt_tokens}, out: {response.usage.completion_tokens})\")\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        duration = (datetime.now() - start).total_seconds()\n",
    "        logger.error(f\"FAILED | Model: {model} | Duration: {duration:.2f}s | \"\n",
    "                    f\"Error: {type(e).__name__}: {str(e)[:100]}\")\n",
    "        raise\n",
    "\n",
    "# Tests\n",
    "print(\"=== Test réussi ===\")\n",
    "result = logged_completion(\"Quelle heure est-il?\")\n",
    "print(f\"Réponse: {result}\\n\")\n",
    "\n",
    "print(\"=== Test avec modèle invalide ===\")\n",
    "try:\n",
    "    result = logged_completion(\"Test\", model=\"gpt-invalid-model\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur capturée: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f7577",
   "metadata": {
    "papermill": {
     "duration": 0.004165,
     "end_time": "2026-02-25T21:43:31.580860",
     "exception": false,
     "start_time": "2026-02-25T21:43:31.576695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test réussi ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 09:13:03 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 09:13:03 | OpenAI_Production | INFO | SUCCESS | Model: gpt-5-mini | Duration: 1.87s | Tokens: 52 (in: 12, out: 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 09:13:03 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 09:13:03 | OpenAI_Production | ERROR | FAILED | Model: gpt-invalid-model | Duration: 0.19s | Error: NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-invalid-model` does not exist or you do not \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse: Je ne peux pas fournir l'heure actuelle, mais vous pouvez facilement vérifier l'heure sur votre appareil. Si vous avez besoin d'aide pour quoi que ce soit d'autre, n'hésitez pas à demander !\n",
      "\n",
      "=== Test avec modèle invalide ===\n",
      "Erreur capturée: NotFoundError\n"
     ]
    }
   ],
   "source": [
    "### Interprétation des Logs de Production\n",
    "\n",
    "**Analyse des logs générés :**\n",
    "\n",
    "```\n",
    "2026-02-04 14:18:28 | OpenAI_Production | INFO | SUCCESS | Model: gpt-5-mini | Duration: 1.57s | Tokens: 43 (in: 12, out: 31)\n",
    "2026-02-04 14:18:28 | OpenAI_Production | ERROR | FAILED | Model: gpt-invalid-model | Duration: 0.28s | Error: NotFoundError\n",
    "```\n",
    "\n",
    "**Points clés observés :**\n",
    "\n",
    "1. **Latence** :\n",
    "   - Requête réussie : 1.57s (acceptable pour gpt-5-mini avec 31 tokens)\n",
    "   - Requête échouée : 0.28s (échec rapide = bon signe, pas de timeout)\n",
    "\n",
    "2. **Consommation de tokens** :\n",
    "   - Input : 12 tokens (prompt court \"Quelle heure est-il?\")\n",
    "   - Output : 31 tokens (réponse standard)\n",
    "   - Total : 43 tokens ≈ $0.000168 (gpt-5-mini @ $2.50/1M input, $10/1M output)\n",
    "\n",
    "3. **Gestion d'erreur** :\n",
    "   - Exception capturée et loggée (pas de crash)\n",
    "   - Message d'erreur informatif : \"The model `gpt-invalid-model` does not exist\"\n",
    "   - L'application peut réagir (fallback, alerte utilisateur)\n",
    "\n",
    "**Métriques à extraire pour dashboards :**\n",
    "\n",
    "| Métrique | Calcul | Seuil alerte |\n",
    "|----------|--------|--------------|\n",
    "| **Latence P95** | 95ème percentile des durées | > 3s |\n",
    "| **Taux d'erreur** | (erreurs / total) × 100 | > 5% |\n",
    "| **Coût moyen/requête** | Somme(tokens × prix) / nb_requêtes | > $0.01 |\n",
    "| **Requêtes/minute** | Count(logs) sur fenêtre 1min | > 80% de la limite |\n",
    "\n",
    "> **Production tip** : Exporter les logs vers un système centralisé (ELK, Datadog) pour analyser les tendances sur plusieurs jours et détecter les anomalies (ex: latence soudainement × 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a7799",
   "metadata": {
    "papermill": {
     "duration": 0.00389,
     "end_time": "2026-02-25T21:43:31.589109",
     "exception": false,
     "start_time": "2026-02-25T21:43:31.585219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 7. Patterns Avancés : Streaming et Modération\n",
    "\n",
    "### Streaming pour UX Réactive\n",
    "\n",
    "Le streaming permet d'afficher la réponse progressivement :\n",
    "- **Meilleure UX** : L'utilisateur voit la réponse se construire\n",
    "- **Latence perçue réduite** : Premier token en ~200ms vs 5s pour réponse complète\n",
    "- **Annulation précoce** : Possibilité de stopper si réponse non pertinente\n",
    "\n",
    "### Modération de Contenu\n",
    "\n",
    "OpenAI propose une API de modération pour détecter :\n",
    "- Contenu haineux/violent\n",
    "- Harcèlement\n",
    "- Contenu sexuel\n",
    "- Auto-mutilation\n",
    "- Etc.\n",
    "\n",
    "**Pattern recommandé :**\n",
    "1. Modérer l'input utilisateur\n",
    "2. Générer la réponse si OK\n",
    "3. Modérer l'output avant affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4baafea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:43:31.598957Z",
     "iopub.status.busy": "2026-02-25T21:43:31.598616Z",
     "iopub.status.idle": "2026-02-25T21:43:34.178007Z",
     "shell.execute_reply": "2026-02-25T21:43:34.177035Z"
    },
    "papermill": {
     "duration": 2.5863,
     "end_time": "2026-02-25T21:43:34.179419",
     "exception": false,
     "start_time": "2026-02-25T21:43:31.593119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Streaming Example ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 22:43:33 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse streamée: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Moderation Example ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 22:43:33 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/moderations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texte: Bonjour, comment vas-tu?"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flaggé: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 22:43:34 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/moderations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texte: Enfoiré, je vais te tuer!\n",
      "Flaggé: True\n",
      "Catégories: ['harassment', 'harassment_threatening', 'violence']\n"
     ]
    }
   ],
   "source": [
    "# Streaming\n",
    "print(\"=== Streaming Example ===\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Écris un haïku sur la programmation.\"}],\n",
    "    stream=True,\n",
    "    max_completion_tokens=100\n",
    ")\n",
    "\n",
    "print(\"Réponse streamée: \", end=\"\", flush=True)\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Modération\n",
    "print(\"=== Moderation Example ===\")\n",
    "test_inputs = [\n",
    "    \"Bonjour, comment vas-tu?\",\n",
    "    \"Enfoiré, je vais te tuer!\"\n",
    "]\n",
    "\n",
    "for text in test_inputs:\n",
    "    moderation = client.moderations.create(input=text)\n",
    "    result = moderation.results[0]\n",
    "    \n",
    "    print(f\"\\nTexte: {text}\")\n",
    "    print(f\"Flaggé: {result.flagged}\")\n",
    "    if result.flagged:\n",
    "        print(f\"Catégories: {[cat for cat, flagged in result.categories.__dict__.items() if flagged]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa083869",
   "metadata": {
    "papermill": {
     "duration": 0.004134,
     "end_time": "2026-02-25T21:43:34.190313",
     "exception": false,
     "start_time": "2026-02-25T21:43:34.186179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Streaming Example ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 09:13:04 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse streamée: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sur"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "’écran"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " danse"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ér"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ies"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " de"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " chiffres"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " en"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " boucle"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "’"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âme"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " d"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'un"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " programme"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Moderation Example ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 09:13:05 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/moderations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texte: Bonjour, comment vas-tu?\n",
      "Flaggé: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 09:13:05 | httpx | INFO | HTTP Request: POST https://api.openai.com/v1/moderations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texte: Enfoiré, je vais te tuer!\n",
      "Flaggé: True\n",
      "Catégories: ['harassment', 'harassment_threatening', 'violence']\n"
     ]
    }
   ],
   "source": [
    "### Interprétation des Résultats Streaming et Modération\n",
    "\n",
    "**Observations sur le Streaming :**\n",
    "\n",
    "Le streaming transforme radicalement l'expérience utilisateur :\n",
    "- **Latence perçue** : Au lieu d'attendre 2-3s pour voir la réponse complète, le premier mot apparaît en ~200ms\n",
    "- **Engagement utilisateur** : Voir le texte se construire donne l'impression d'une \"conversation naturelle\"\n",
    "- **Format de réponse** : Les chunks arrivent progressivement via `delta.content`\n",
    "\n",
    "**Observations sur la Modération :**\n",
    "\n",
    "| Texte | Flaggé | Catégories détectées | Action recommandée |\n",
    "|-------|--------|---------------------|-------------------|\n",
    "| \"Bonjour, comment vas-tu?\" | ❌ Non | Aucune | ✅ Traiter normalement |\n",
    "| \"Enfoiré, je vais te tuer!\" | ✅ Oui | harassment, harassment_threatening, violence | 🚫 Bloquer avant génération |\n",
    "\n",
    "**Pattern de sécurité production :**\n",
    "\n",
    "```python\n",
    "# Workflow complet sécurisé\n",
    "def safe_generation(user_input: str) -> str:\n",
    "    # 1. Modération input\n",
    "    mod_input = client.moderations.create(input=user_input)\n",
    "    if mod_input.results[0].flagged:\n",
    "        return \"Votre message viole notre politique d'utilisation.\"\n",
    "    \n",
    "    # 2. Génération\n",
    "    response = generate(user_input)\n",
    "    \n",
    "    # 3. Modération output\n",
    "    mod_output = client.moderations.create(input=response)\n",
    "    if mod_output.results[0].flagged:\n",
    "        return \"Désolé, impossible de générer une réponse appropriée.\"\n",
    "    \n",
    "    return response\n",
    "```\n",
    "\n",
    "> **Note importante** : La modération ajoute ~100ms de latence par check. Pour les applications temps-réel, envisager une modération asynchrone post-génération."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2314b76",
   "metadata": {
    "papermill": {
     "duration": 0.003988,
     "end_time": "2026-02-25T21:43:34.198247",
     "exception": false,
     "start_time": "2026-02-25T21:43:34.194259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 8. Checklist Déploiement Production\n",
    "\n",
    "### Sécurité\n",
    "- [ ] API keys stockées dans variables d'environnement (jamais hardcodées)\n",
    "- [ ] Rotation régulière des clés\n",
    "- [ ] Rate limiting côté serveur\n",
    "- [ ] Validation et sanitization des inputs utilisateur\n",
    "- [ ] Modération de contenu activée\n",
    "- [ ] Logs ne contiennent pas de données sensibles\n",
    "\n",
    "### Résilience\n",
    "- [ ] Retry automatique avec backoff exponentiel\n",
    "- [ ] Timeout configurés\n",
    "- [ ] Circuit breaker pour dépendances externes\n",
    "- [ ] Fallback gracieux en cas d'erreur\n",
    "- [ ] Health checks réguliers\n",
    "\n",
    "### Performance\n",
    "- [ ] Cache activé (`store=True`)\n",
    "- [ ] Choix du modèle adapté au cas d'usage\n",
    "- [ ] `max_tokens` configuré pour limiter les coûts\n",
    "- [ ] Streaming pour réponses longues\n",
    "- [ ] Background mode pour tâches longues\n",
    "\n",
    "### Monitoring\n",
    "- [ ] Logging structuré configuré\n",
    "- [ ] Métriques : latence, taux d'erreur, coûts\n",
    "- [ ] Alertes budgétaires configurées\n",
    "- [ ] Dashboards temps réel\n",
    "- [ ] Traçabilité des requêtes (request ID)\n",
    "\n",
    "### Coûts\n",
    "- [ ] Budget mensuel défini\n",
    "- [ ] Alertes à 50%, 80%, 100% du budget\n",
    "- [ ] Audit régulier de l'usage par endpoint/utilisateur\n",
    "- [ ] Optimisation continue des prompts\n",
    "- [ ] Évaluation régulière des modèles (nouveaux modèles moins chers?)\n",
    "\n",
    "### Conformité\n",
    "- [ ] RGPD : consentement utilisateur pour traitement des données\n",
    "- [ ] Politique de rétention des données\n",
    "- [ ] Anonymisation des données personnelles\n",
    "- [ ] Documentation des traitements de données\n",
    "- [ ] DPO informé de l'usage d'IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f0a65",
   "metadata": {
    "papermill": {
     "duration": 0.003784,
     "end_time": "2026-02-25T21:43:34.205917",
     "exception": false,
     "start_time": "2026-02-25T21:43:34.202133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Conclusion\n",
    "\n",
    "### Récapitulatif des Patterns\n",
    "\n",
    "| Pattern | Cas d'usage | Complexité |\n",
    "|---------|-------------|------------|\n",
    "| **Responses API + store** | Conversations courtes avec cache | ⭐⭐ |\n",
    "| **Conversations API** | Conversations longue durée | ⭐⭐ |\n",
    "| **Background Mode** | Tâches longues (>30s) | ⭐⭐⭐ |\n",
    "| **Retry + Backoff** | Résilience production | ⭐⭐ |\n",
    "| **Rate Limiter** | Respect limites API | ⭐⭐ |\n",
    "| **Logging structuré** | Monitoring et debug | ⭐⭐ |\n",
    "| **Streaming** | UX temps réel | ⭐⭐ |\n",
    "| **Modération** | Sécurité contenu | ⭐ |\n",
    "\n",
    "### Ressources Supplémentaires\n",
    "\n",
    "**Documentation OpenAI :**\n",
    "- [Responses API](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [Conversations API](https://platform.openai.com/docs/api-reference/conversations)\n",
    "- [Rate Limits](https://platform.openai.com/docs/guides/rate-limits)\n",
    "- [Error Codes](https://platform.openai.com/docs/guides/error-codes)\n",
    "\n",
    "**Bibliothèques Python :**\n",
    "- [tenacity](https://tenacity.readthedocs.io/) : Retry robuste\n",
    "- [openai](https://github.com/openai/openai-python) : SDK officiel\n",
    "- [loguru](https://loguru.readthedocs.io/) : Logging simplifié\n",
    "\n",
    "**Prochaines étapes :**\n",
    "- Implémenter ces patterns dans votre application\n",
    "- Configurer un monitoring complet\n",
    "- Tester la résilience (chaos engineering)\n",
    "- Optimiser les coûts progressivement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4832dc78",
   "metadata": {
    "papermill": {
     "duration": 0.004085,
     "end_time": "2026-02-25T21:43:34.214417",
     "exception": false,
     "start_time": "2026-02-25T21:43:34.210332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Chatbot Multi-Session (30 min)\n",
    "\n",
    "Créez un chatbot de support client qui :\n",
    "1. Utilise la Conversations API pour maintenir le contexte\n",
    "2. Persiste les conversations dans un fichier JSON (pour reprise après redémarrage)\n",
    "3. Implémente un retry avec backoff\n",
    "4. Log toutes les interactions\n",
    "\n",
    "**Bonus :** Ajouter une commande `/summary` qui résume la conversation en cours.\n",
    "\n",
    "### Exercice 2 : Système d'Analyse de Documents (40 min)\n",
    "\n",
    "Implémentez un système qui :\n",
    "1. Prend un long document en entrée\n",
    "2. Lance l'analyse en background mode\n",
    "3. Affiche une barre de progression pendant le traitement\n",
    "4. Retourne un rapport structuré (points clés, résumé, recommandations)\n",
    "5. Calcule et affiche le coût total de l'analyse\n",
    "\n",
    "**Bonus :** Comparer les coûts entre gpt-5-mini et gpt-5.\n",
    "\n",
    "### Exercice 3 : Rate Limiter Multi-Tier (20 min)\n",
    "\n",
    "Améliorez la classe `RateLimiter` pour supporter :\n",
    "1. Des limites par minute ET par jour\n",
    "2. Des priorités de requêtes (high/medium/low)\n",
    "3. Un mode \"burst\" (permettre 10 requêtes instantanées, puis throttling)\n",
    "\n",
    "**Bonus :** Ajouter des statistiques (nombre de requêtes dans les dernières 24h, temps moyen d'attente)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 54.982027,
   "end_time": "2026-02-25T21:43:34.593044",
   "environment_variables": {},
   "exception": null,
   "input_path": "9_Production_Patterns.ipynb",
   "output_path": "9_Production_Patterns.ipynb",
   "parameters": {
    "BATCH_MODE": "true"
   },
   "start_time": "2026-02-25T21:42:39.611017",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}