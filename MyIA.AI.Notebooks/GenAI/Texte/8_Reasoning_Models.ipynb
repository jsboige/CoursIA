{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles de Raisonnement : o4-mini et GPT-5-thinking\n",
    "\n",
    "Les modèles de raisonnement représentent une évolution majeure des LLMs. Contrairement aux modèles de chat classiques, ils prennent le temps de \"réfléchir\" avant de répondre, ce qui améliore significativement leurs performances sur les tâches complexes.\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre les différences architecturales (thinking time)\n",
    "- Maîtriser `reasoning_effort` (low, medium, high)\n",
    "- Choisir le bon modèle selon la tâche\n",
    "- Analyser les performances et coûts\n",
    "\n",
    "**Prérequis :** Notebook 2 (Prompt Engineering)\n",
    "\n",
    "**Durée estimée :** 60 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Client OpenAI initialisé !\n",
      "Modèle par défaut: gpt-5-mini\n",
      "Mode: INTERACTIF\n"
     ]
    }
   ],
   "source": [
    "%pip install openai python-dotenv --quiet\n",
    "\n",
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "client = OpenAI()\n",
    "\n",
    "# Modèle par défaut depuis .env\n",
    "DEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\n",
    "BATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\"\n",
    "\n",
    "print(f\"Client OpenAI initialisé !\")\n",
    "print(f\"Modèle par défaut: {DEFAULT_MODEL}\")\n",
    "print(f\"Mode: {'BATCH' if BATCH_MODE else 'INTERACTIF'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuration de l'environnement\n",
    "\n",
    "### Imports nécessaires\n",
    "\n",
    "Ce notebook utilise :\n",
    "- **openai** : Bibliothèque officielle OpenAI pour interagir avec l'API\n",
    "- **python-dotenv** : Chargement des variables d'environnement (clés API)\n",
    "- **time** : Mesure des temps de réponse pour comparaison\n",
    "\n",
    "### Structure du notebook\n",
    "\n",
    "Nous allons explorer progressivement :\n",
    "1. Comparaison chat vs reasoning models\n",
    "2. Paramètre `reasoning_effort` (low/medium/high)\n",
    "3. Messages `developer` et formatage\n",
    "4. Génération de code complexe\n",
    "5. Modèles avancés et benchmarks\n",
    "6. Analyse coût/performance\n",
    "\n",
    "**Mode batch** : Si `BATCH_MODE=true` dans `.env`, le notebook s'exécute sans interaction utilisateur (utile pour tests automatisés)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chat Models vs Reasoning Models\n",
    "\n",
    "### Différences fondamentales\n",
    "\n",
    "| Aspect | Chat Models | Reasoning Models |\n",
    "|--------|-------------|------------------|\n",
    "| **Réponse** | Immédiate | Temps de réflexion |\n",
    "| **Optimisation** | Dialogue fluide | Problèmes complexes |\n",
    "| **Exemples** | gpt-4o, gpt-4o-mini | o4-mini, gpt-5-thinking |\n",
    "| **Vitesse** | Rapide (1-3s) | Variable (5-30s+) |\n",
    "| **Précision** | Bonne | Excellente sur tâches complexes |\n",
    "\n",
    "### Quand utiliser quoi ?\n",
    "\n",
    "**Chat Models :**\n",
    "- Conversations naturelles\n",
    "- Questions factuelles simples\n",
    "- Génération de contenu créatif\n",
    "- Traduction, résumé\n",
    "\n",
    "**Reasoning Models :**\n",
    "- Mathématiques et logique\n",
    "- Programmation complexe\n",
    "- Analyse multi-étapes\n",
    "- Problèmes d'optimisation\n",
    "- Déduction et inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparaison Chat vs Reasoning ===\n",
      "\n",
      "gpt-4o-mini (0.72s):\n",
      "  Réponse: Alice a 74 pommes maintenant.\n",
      "\n",
      "gpt-5-mini (4.35s):\n",
      "  Réponse: 74\n",
      "\n",
      "[Réponse correcte: 74 pommes (Alice donne 27, récupère 14, achète 33 → 54 - 27 + 14 + 33 = 74)]\n",
      "\n",
      "Note: Le chat model peut faire une erreur de calcul, le reasoning model analyse pas à pas.\n"
     ]
    }
   ],
   "source": [
    "probleme_math = \"\"\"\n",
    "Alice a 54 pommes. Elle en donne la moitié à Bob.\n",
    "Bob mange 13 pomme puis rend le reste à Alice.\n",
    "Alice achète ensuite 33 pommes de plus.\n",
    "Combien de pommes Alice a-t-elle maintenant?\n",
    "Donne uniquement le nombre final.\n",
    "\"\"\"\n",
    "\n",
    "# Test avec gpt-4o-mini (chat model)\n",
    "start = time.time()\n",
    "response_chat = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": probleme_math}],\n",
    "    max_tokens=100,\n",
    ")\n",
    "time_chat = time.time() - start\n",
    "\n",
    "# Test avec o4-mini (reasoning model)\n",
    "start = time.time()\n",
    "response_reasoning = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": probleme_math}\n",
    "    ],\n",
    "    reasoning_effort=\"medium\"\n",
    ")\n",
    "time_reasoning = time.time() - start\n",
    "\n",
    "print(\"=== Comparaison Chat vs Reasoning ===\")\n",
    "print(f\"\\ngpt-4o-mini ({time_chat:.2f}s):\")\n",
    "print(f\"  Réponse: {response_chat.choices[0].message.content.strip()}\")\n",
    "print(f\"\\ngpt-5-mini ({time_reasoning:.2f}s):\")\n",
    "print(f\"  Réponse: {response_reasoning.choices[0].message.content.strip()}\")\n",
    "print(f\"\\n[Réponse correcte: 74 pommes (Alice donne 27, récupère 14, achète 33 → 54 - 27 + 14 + 33 = 74)]\")\n",
    "print(f\"\\nNote: Le chat model peut faire une erreur de calcul, le reasoning model analyse pas à pas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation des résultats\n",
    "\n",
    "Dans cet exemple, les deux modèles ont donné la bonne réponse (7 pommes), mais observez :\n",
    "\n",
    "**Temps de réponse** :\n",
    "- gpt-4o-mini : ~0.78s (réponse immédiate)\n",
    "- o4-mini : ~4.67s (temps de réflexion inclus)\n",
    "\n",
    "**Précision** :\n",
    "- Sur ce problème simple, les deux modèles réussissent\n",
    "- La différence devient significative sur des problèmes plus complexes nécessitant plusieurs étapes de raisonnement\n",
    "\n",
    "**Quand la différence compte** :\n",
    "- Problèmes avec plusieurs contraintes simultanées\n",
    "- Calculs nécessitant de garder plusieurs valeurs en mémoire\n",
    "- Raisonnement nécessitant de tester plusieurs hypothèses\n",
    "\n",
    "**Trade-off** : Le modèle de raisonnement prend ~6x plus de temps mais offre une meilleure garantie de précision sur les problèmes complexes. À vous de choisir selon vos besoins !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats par niveau\n",
    "\n",
    "**Observations importantes** :\n",
    "\n",
    "1. **Tous les niveaux donnent la bonne réponse** sur ce problème simple\n",
    "   - low (~2.76s) : Raisonnement rapide, réponse correcte\n",
    "   - medium (~4.43s) : Équilibre réflexion/temps\n",
    "   - high (~5.95s) : Analyse approfondie (mais pas nécessaire ici)\n",
    "\n",
    "2. **Différence de temps** :\n",
    "   - low → medium : +60% temps\n",
    "   - medium → high : +34% temps\n",
    "   - Le surcoût augmente de façon non-linéaire\n",
    "\n",
    "3. **Formulation similaire** :\n",
    "   - Les 3 niveaux produisent des explications presque identiques\n",
    "   - Sur un problème simple, la différence est minime\n",
    "\n",
    "**Quand utiliser chaque niveau ?**\n",
    "\n",
    "| Niveau | Durée typique | Cas d'usage |\n",
    "|--------|--------------|-------------|\n",
    "| **low** | 2-10s | Logique simple, calculs directs, questions factuelles avec raisonnement minimal |\n",
    "| **medium** | 5-20s | **Choix par défaut** - équilibre optimal pour la plupart des tâches |\n",
    "| **high** | 10-60s+ | Problèmes très complexes, optimisation multi-contraintes, preuves formelles |\n",
    "\n",
    "**Recommandation** : Commencez toujours par `low` ou `medium`. N'utilisez `high` que si les résultats sont insuffisants.\n",
    "\n",
    "**Impact sur les coûts** : Chaque niveau génère plus de tokens de réflexion cachés (facturés). Sur 1000 requêtes, la différence entre low et high peut représenter des centaines d'euros !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Paramètre `reasoning_effort`\n",
    "\n",
    "Les modèles de raisonnement exposent un paramètre crucial : **`reasoning_effort`**.\n",
    "\n",
    "### Niveaux disponibles\n",
    "\n",
    "| Niveau | Temps de réflexion | Qualité | Usage typique |\n",
    "|--------|-------------------|---------|---------------|\n",
    "| **low** | Minimal (5-10s) | Bonne | Problèmes simples nécessitant un peu de réflexion |\n",
    "| **medium** | Équilibré (10-20s) | Très bonne | Cas d'usage général |\n",
    "| **high** | Approfondi (20-60s+) | Excellente | Problèmes très complexes |\n",
    "\n",
    "### Règles empiriques\n",
    "\n",
    "- **low** : Questions de logique simple, calculs directs\n",
    "- **medium** : Programmation standard, analyses multi-étapes\n",
    "- **high** : Optimisation mathématique, preuve formelle, debugging complexe\n",
    "\n",
    "**Note importante :** Plus de réflexion = plus de coût en tokens. Choisissez judicieusement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Impact du reasoning_effort ===\n",
      "\n",
      "reasoning_effort='low' (2.14s):\n",
      "  Tu te retrouves à la 2ᵉ place, car en dépassant le second tu prends sa position.\n",
      "\n",
      "reasoning_effort='medium' (3.17s):\n",
      "  Tu es à la 2ᵉ place, car en dépassant le coureur qui était 2ᵉ tu prends sa position.\n",
      "\n",
      "reasoning_effort='high' (4.86s):\n",
      "  Tu te retrouves 2ᵉ : en doublant celui qui occupait la 2ᵉ place, tu prends sa position et finis donc deuxième.\n",
      "\n",
      "[Réponse correcte: 2ème position - tu prends la place de celui que tu dépasses]\n",
      "\n",
      "Observation: Les 3 niveaux devraient donner la bonne réponse, mais 'high' peut fournir\n",
      "une explication plus détaillée. La différence est surtout visible sur des problèmes complexes.\n"
     ]
    }
   ],
   "source": [
    "probleme_logique = \"\"\"\n",
    "Dans une course, tu dépasses le 2ème. \n",
    "Quelle est ta position finale?\n",
    "Explique ton raisonnement en une phrase.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Impact du reasoning_effort ===\\n\")\n",
    "\n",
    "for effort in [\"low\", \"medium\", \"high\"]:\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"o4-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "            {\"role\": \"user\", \"content\": probleme_logique}\n",
    "        ],\n",
    "        reasoning_effort=effort\n",
    "    )\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    print(f\"reasoning_effort='{effort}' ({duration:.2f}s):\")\n",
    "    print(f\"  {response.choices[0].message.content.strip()}\")\n",
    "    print()\n",
    "\n",
    "print(\"[Réponse correcte: 2ème position - tu prends la place de celui que tu dépasses]\")\n",
    "print(\"\\nObservation: Les 3 niveaux devraient donner la bonne réponse, mais 'high' peut fournir\")\n",
    "print(\"une explication plus détaillée. La différence est surtout visible sur des problèmes complexes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de la qualité du code généré\n",
    "\n",
    "Le modèle o4-mini avec `reasoning_effort='high'` a produit une solution de **très haute qualité** :\n",
    "\n",
    "**Points forts de l'implémentation** :\n",
    "\n",
    "1. **Algorithme optimal** : Expansion autour du centre (O(n²))\n",
    "   - Considère les palindromes de longueur impaire (centre sur un caractère)\n",
    "   - Considère les palindromes de longueur paire (centre entre deux caractères)\n",
    "   - Complexité théorique minimale pour approche directe\n",
    "\n",
    "2. **Code propre et lisible** :\n",
    "   - Fonction helper `expand_around_center` avec logique claire\n",
    "   - Variables bien nommées (`start`, `max_len`, `curr_len`)\n",
    "   - Commentaires pertinents\n",
    "\n",
    "3. **Tests exhaustifs** :\n",
    "   - Chaîne vide\n",
    "   - Un seul caractère\n",
    "   - Palindrome complet\n",
    "   - Pas de palindrome (> 1)\n",
    "   - Cas ambigus (\"babad\" → \"bab\" ou \"aba\")\n",
    "\n",
    "4. **Robustesse** :\n",
    "   - Gestion des cas limites (n < 2)\n",
    "   - Validation avec assertions\n",
    "   - Message de succès clair\n",
    "\n",
    "**Comparaison avec un chat model** :\n",
    "\n",
    "Un modèle comme gpt-4o-mini aurait probablement :\n",
    "- Fourni une implémentation correcte mais moins optimisée\n",
    "- Tests moins complets\n",
    "- Moins d'explications sur la complexité\n",
    "\n",
    "**Temps de génération** : 17.29s est un excellent compromis pour obtenir du code de cette qualité. En développement professionnel, cela représente un gain de temps considérable.\n",
    "\n",
    "**Alternative possible** : L'algorithme de Manacher (O(n)) existe mais est beaucoup plus complexe. Pour la plupart des usages, O(n²) est suffisant et plus maintenable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Messages `developer` et contrôle du formatage\n",
    "\n",
    "### Rôle `developer`\n",
    "\n",
    "Les modèles de raisonnement (o4-mini, gpt-5-thinking) utilisent un rôle spécial : **`developer`**.\n",
    "\n",
    "- Remplace le rôle `system` des chat models\n",
    "- Définit des instructions méta (comportement, formatage)\n",
    "- Moins strict que `system`, plus flexible\n",
    "\n",
    "### \"Formatting re-enabled\"\n",
    "\n",
    "Par défaut, les modèles raisonnants retournent du texte brut. Pour activer le **markdown** (formules LaTeX, code, listes), ajoutez :\n",
    "\n",
    "```python\n",
    "{\"role\": \"developer\", \"content\": \"Formatting re-enabled\"}\n",
    "```\n",
    "\n",
    "Ceci permet :\n",
    "- Formules mathématiques : `$E = mc^2$`\n",
    "- Blocs de code avec syntax highlighting\n",
    "- Tableaux, listes, titres markdown\n",
    "\n",
    "**Sans ce message**, vous obtenez du texte brut (pas de formatage riche)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Analyse du problème d'optimisation\n\nLe modèle a fourni une **solution complète et rigoureuse** :\n\n**Étapes mathématiques couvertes** :\n\n1. **Variables** : x (largeur), longueur = 100 - 2x\n2. **Contrainte** : Clôture totale = 100m (un côté est le mur)\n3. **Fonction objectif** : S(x) = x(100 - 2x)\n4. **Optimisation** : Dérivée S'(x) = 100 - 4x = 0 → x = 25m\n5. **Vérification** : Dérivée seconde négative (maximum confirmé)\n\n**Ce que le raisonnement apporte** :\n\n- **Structure claire** : Chaque étape est explicite et justifiée\n- **Notation mathématique** : Formules LaTeX bien formatées\n- **Validation** : Vérifie que c'est bien un maximum (pas un minimum)\n- **Résultat concret** : Dimensions finales et surface maximale\n\n**Comparaison avec un chat model** :\n\nUn modèle standard aurait probablement :\n- Donné le bon résultat final\n- Mais sauté certaines étapes de vérification\n- Moins de rigueur dans la démonstration\n\n**Fallback intelligent** : Le code tente plusieurs modèles (gpt-5-thinking → o4-mini → o3-mini → o1-preview) et utilise gpt-5-mini en dernier recours. Cela garantit que le notebook fonctionne même si certains modèles ne sont pas disponibles.\n\n**Cas d'usage similaires** :\n- Optimisation de budget/ressources\n- Problèmes de géométrie (surface, volume)\n- Calcul de marges maximales en finance\n- Conception de systèmes avec contraintes multiples"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sans 'Formatting re-enabled' ===\n",
      "Le théorème de Pythagore concerne uniquement les triangles rectangles (un angle droit, noté 90°). On l’énonce ainsi :\n",
      "\n",
      "1. Énoncé  \n",
      "   Dans un triangle rectangle, le carré de la longueur de l’hypoténuse (le côté opposé à l’angle droit) est égal à la somme des carrés des longueurs des deux autres côtés (les « cathètes »).\n",
      "\n",
      "2. Notations  \n",
      "   • Soit ABC un triangle rectangle en A (angle ∠A = 90°).  \n",
      "   • On note :  \n",
      "     – BC = c (hypoténuse)  \n",
      "     – AB = a (première cathète)  \n",
      "     – AC = b (deuxième cathète)\n",
      "\n",
      "3. Formule principale  \n",
      "   a² + b² = c²\n",
      "\n",
      "4. Exemple numérique  \n",
      "   • Si a = 3 et b = 4 alors :  \n",
      "     a² + b² = 3² + 4² = 9 + 16 = 25  \n",
      "     donc c² = 25 et c = √25 = 5.\n",
      "\n",
      "5. Réciproque  \n",
      "   Si, dans un triangle, on a trois longueurs a, b et c qui vérifient a² + b² = c², alors le triang\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "=== Avec 'Formatting re-enabled' ===\n",
      "Dans un triangle droit (c’est-à-dire un triangle qui a un angle de 90°), le théorème de Pythagore énonce la relation suivante entre les longueurs des côtés :\n",
      "\n",
      "Soit un triangle ABC rectangle en A, avec  \n",
      "- AB = b (un cathète),  \n",
      "- AC = a (l’autre cathète),  \n",
      "- BC = c (l’hypoténuse, le côté opposé à l’angle droit).  \n",
      "\n",
      "Le théorème affirme alors :  \n",
      "\\[ a^2 + b^2 = c^2. \\]\n",
      "\n",
      "Explications et démonstration par découpage géométrique (preuve classique) :\n",
      "\n",
      "1. Construction  \n",
      "   • On construit un carré de côté \\(a + b\\).  \n",
      "   • À l’intérieur, on place quatre fois le même triangle rectangle (côtés \\(a\\), \\(b\\), \\(c\\)), disposés de sorte qu’ils forment un carré central de côté \\(c\\).\n",
      "\n",
      "2. Calcul des aires  \n",
      "   • Aire du grand carré : \\((a + b)^2\\).  \n",
      "   • Aire des quatre triangles : \\(4 \\times (\\tfrac{1}{\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "Différence: Avec formatage, vous obtenez du markdown propre (formules LaTeX, code blocks, etc.)\n"
     ]
    }
   ],
   "source": [
    "# Sans \"Formatting re-enabled\"\n",
    "response_no_format = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explique le théorème de Pythagore avec des formules.\"}],\n",
    "    reasoning_effort=\"low\"\n",
    ")\n",
    "\n",
    "# Avec \"Formatting re-enabled\"\n",
    "response_with_format = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explique le théorème de Pythagore avec des formules.\"}\n",
    "    ],\n",
    "    reasoning_effort=\"low\"\n",
    ")\n",
    "\n",
    "print(\"=== Sans 'Formatting re-enabled' ===\")\n",
    "print(response_no_format.choices[0].message.content[:800])\n",
    "print(\"\\n...\\n\")\n",
    "\n",
    "print(\"\\n=== Avec 'Formatting re-enabled' ===\")\n",
    "print(response_with_format.choices[0].message.content[:800])\n",
    "print(\"\\n...\\n\")\n",
    "\n",
    "print(\"\\nDifférence: Avec formatage, vous obtenez du markdown propre (formules LaTeX, code blocks, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation de la différence de formatage\n",
    "\n",
    "**Sans \"Formatting re-enabled\"** :\n",
    "- Texte brut avec numérotation simple\n",
    "- Pas de balises LaTeX ($...$)\n",
    "- Formules écrites en notation textuelle (ex: `c² = a² + b²`)\n",
    "- Structure simple mais moins élégante\n",
    "\n",
    "**Avec \"Formatting re-enabled\"** :\n",
    "- Markdown riche avec titres, listes numérotées\n",
    "- Formules LaTeX rendues (dans les environnements supportant MathJax)\n",
    "- Blocs de code avec syntax highlighting\n",
    "- Mise en forme professionnelle\n",
    "\n",
    "**Quand activer le formatage ?**\n",
    "\n",
    "| Cas d'usage | Formatting re-enabled ? |\n",
    "|-------------|-------------------------|\n",
    "| **Documentation technique** | ✅ Oui (formules, code) |\n",
    "| **Rapports scientifiques** | ✅ Oui (LaTeX, tableaux) |\n",
    "| **Extraction de données brutes** | ❌ Non (traitement programmatique) |\n",
    "| **API backend** | ❌ Non (texte simple plus facile à parser) |\n",
    "| **Interface utilisateur** | ✅ Oui (présentation soignée) |\n",
    "\n",
    "**Exemple de formules LaTeX** (si le formatage est activé) :\n",
    "\n",
    "```markdown\n",
    "Le théorème de Pythagore : $a^2 + b^2 = c^2$\n",
    "\n",
    "La formule d'Einstein : $E = mc^2$\n",
    "\n",
    "Équation quadratique : $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$\n",
    "```\n",
    "\n",
    "**Astuce** : Pour les notebooks Jupyter, le formatage markdown est automatiquement rendu, ce qui rend les réponses beaucoup plus lisibles !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cas d'usage : Génération de code complexe\n",
    "\n",
    "Les modèles de raisonnement excellent dans la génération de code nécessitant :\n",
    "- Analyse algorithmique\n",
    "- Optimisation de complexité\n",
    "- Tests edge cases\n",
    "- Debugging multi-étapes\n",
    "\n",
    "### Exemple : Algorithme du plus long palindrome\n",
    "\n",
    "Problème classique d'algorithmique :\n",
    "- Approche naïve : O(n³)\n",
    "- Approche optimisée : O(n²) avec expansion autour du centre\n",
    "- Approche avancée : O(n) avec Manacher's algorithm\n",
    "\n",
    "Un modèle raisonnant peut analyser les différentes approches et choisir la meilleure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse approfondie du benchmark\n",
    "\n",
    "**Résultats quantitatifs** :\n",
    "\n",
    "- **Temps moyen gpt-4o-mini** : 1.17s (chat model)\n",
    "- **Temps moyen o4-mini** : 4.41s (reasoning model)\n",
    "- **Ratio** : Le modèle raisonnant est ~3.8x plus lent\n",
    "\n",
    "**Analyse par problème** :\n",
    "\n",
    "1. **Multiplication simple (17 × 23)** :\n",
    "   - Les deux modèles réussissent instantanément\n",
    "   - Différence de temps : 1.61s vs 0.58s\n",
    "   - Conclusion : Sur des calculs directs, le chat model suffit\n",
    "\n",
    "2. **Problème des chats et souris** :\n",
    "   - Piège classique : beaucoup pensent qu'il faut 100 chats\n",
    "   - **Raisonnement correct** : 3 chats attrapent 3 souris en 3 min → 1 chat attrape 1 souris en 3 min → 3 chats attrapent 100 souris en 100 min\n",
    "   - Les deux modèles ont trouvé la bonne réponse\n",
    "   - Temps : 3.02s vs 1.20s\n",
    "\n",
    "3. **Problème des trains** :\n",
    "   - Nécessite plusieurs étapes : distance parcourue par chaque train, rencontre\n",
    "   - gpt-4o-mini : \"10h30\" (approximation)\n",
    "   - o4-mini : \"10h 13min 20s\" (calcul précis)\n",
    "   - **Le reasoning model est plus précis** sur les problèmes multi-étapes\n",
    "\n",
    "**Quand la différence est décisive** :\n",
    "\n",
    "| Type de problème | Chat model | Reasoning model |\n",
    "|------------------|------------|-----------------|\n",
    "| **Calcul direct** | ✅ Suffisant | ⚠️ Overkill |\n",
    "| **Logique simple** | ✅ Bon | ✅ Excellent |\n",
    "| **Multi-étapes** | ⚠️ Approximatif | ✅ Précis |\n",
    "| **Optimisation** | ❌ Erreurs fréquentes | ✅ Fiable |\n",
    "| **Preuve formelle** | ❌ Impossible | ✅ Possible |\n",
    "\n",
    "**Recommandation pratique** :\n",
    "\n",
    "Pour une application en production avec des milliers de requêtes :\n",
    "1. **Classifier les questions** par complexité (simple/moyen/complexe)\n",
    "2. **Router intelligemment** : chat model pour simple, reasoning pour complexe\n",
    "3. **Économie de coûts** : ~70% de réduction si 80% des questions sont simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Génération de code avec o4-mini (reasoning_effort='high') ===\n",
      "\n",
      "Temps de génération: 19.56s\n",
      "\n",
      "Voici une solution en O(n²) (développement autour du centre), qui utilise O(1) d’espace auxiliaire.  \n",
      "On parcourt chaque position comme centre potentiel d’un palindrome (pour les longueurs paires et impaires), on « étire » et on conserve le plus grand trouvé.\n",
      "\n",
      "```python\n",
      "def longest_palindrome(s: str) -> str:\n",
      "    \"\"\"\n",
      "    Retourne la plus longue sous-chaîne de s qui est un palindrome.\n",
      "    Complexité temps : O(n^2), espace auxiliaire : O(1).\n",
      "    \"\"\"\n",
      "    if not s:\n",
      "        return \"\"\n",
      "    \n",
      "    start, end = 0, 0  # indices du plus long palindrome trouvé\n",
      "    n = len(s)\n",
      "    \n",
      "    def expand_center(left: int, right: int) -> int:\n",
      "        \"\"\"\n",
      "        Élargit autour de s[left:right+1] tant que c'est un palindrome.\n",
      "        Retourne la longueur du palindrome.\n",
      "        \"\"\"\n",
      "        while left >= 0 and right < n and s[left] == s[right]:\n",
      "            left -= 1\n",
      "            right += 1\n",
      "        # Quand la boucle casse, (left+1 ... right-1) est un palindrome\n",
      "        return right - left - 1\n",
      "    \n",
      "    for i in range(n):\n",
      "        # 1) palindrome d'une longueur impaire centré en i\n",
      "        len1 = expand_center(i, i)\n",
      "        # 2) palindrome d'une longueur paire centré entre i et i+1\n",
      "        len2 = expand_center(i, i + 1)\n",
      "        max_len = max(len1, len2)\n",
      "        \n",
      "        if max_len > (end - start + 1):\n",
      "            # on recalcule start et end à partir de max_len\n",
      "            start = i - (max_len - 1) // 2\n",
      "            end   = i + max_len // 2\n",
      "    \n",
      "    return s[start:end+1]\n",
      "\n",
      "\n",
      "# ---------- Tests ----------\n",
      "if __name__ == \"__main__\":\n",
      "    tests = [\n",
      "        (\"\", \"\"),                     # chaîne vide\n",
      "        (\"a\", \"a\"),                   # un seul caractère\n",
      "        (\"racecar\", \"racecar\"),       # palindrome complet\n",
      "        (\"abc\", \"a\"),                 # pas de palindrome plus long que 1\n",
      "        (\"cbbd\", \"bb\"),               # palindrome de longueur 2\n",
      "        (\"forgeeksskeegfor\", \"geeksskeeg\"),  # palindrome intermédiaire\n",
      "    ]\n",
      "    \n",
      "    for inp, expected in tests:\n",
      "        out = longest_palindrome(inp)\n",
      "        ok = out == expected\n",
      "        print(f\"Input: {inp!r:20} | Output: {out!r:15} | Expected: {expected!r:15} | {'OK' if ok else 'KO'}\")\n",
      "    \n",
      "    # Cas où plusieurs réponses sont valides :\n",
      "    s = \"babad\"\n",
      "    res = longest_palindrome(s)\n",
      "    print(f\"\\nPour {s!r}, on peut obtenir 'bab' ou 'aba', résultat = {res!r}\")\n",
      "```\n",
      "\n",
      "Explications rapides :\n",
      "- `expand_center` étire deux indices (`left`, `right`) tant que les caractères sont égaux.\n",
      "- On teste à chaque position deux cas : palindrome de taille impaire (même centre) et paire (centre entre deux lettres).\n",
      "- On met à jour les indices `start`, `end` dès qu’on trouve un palindrome plus long.\n",
      "\n",
      "============================================================\n",
      "Note: Le modèle devrait fournir :\n",
      "  1. Une implémentation avec expansion autour du centre (O(n²))\n",
      "  2. Des tests complets couvrant les edge cases\n",
      "  3. Des commentaires expliquant la logique\n"
     ]
    }
   ],
   "source": [
    "probleme_code = \"\"\"\n",
    "Écris une fonction Python qui trouve le plus long palindrome dans une chaîne.\n",
    "La fonction doit être optimisée (O(n²) maximum).\n",
    "Inclus des tests avec plusieurs cas (chaîne vide, un seul caractère, palindrome complet, pas de palindrome).\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Génération de code avec o4-mini (reasoning_effort='high') ===\\n\")\n",
    "\n",
    "start = time.time()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": probleme_code}\n",
    "    ],\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "duration = time.time() - start\n",
    "\n",
    "print(f\"Temps de génération: {duration:.2f}s\\n\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Note: Le modèle devrait fournir :\")\n",
    "print(\"  1. Une implémentation avec expansion autour du centre (O(n²))\")\n",
    "print(\"  2. Des tests complets couvrant les edge cases\")\n",
    "print(\"  3. Des commentaires expliquant la logique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déconstruction du processus de raisonnement\n",
    "\n",
    "Le modèle de raisonnement a identifié le problème de performance avec une **analyse structurée remarquable** :\n",
    "\n",
    "**1. Diagnostic du problème** :\n",
    "- Identifie la récursion redondante\n",
    "- Explique que les mêmes valeurs sont recalculées plusieurs fois\n",
    "- Quantifie l'ampleur du problème (millions d'appels pour n=35-40)\n",
    "\n",
    "**2. Analyse de complexité** :\n",
    "- Formule la complexité O(2ⁿ) explicitement\n",
    "- Explique pourquoi : \"l'arbre d'appels double à chaque niveau\"\n",
    "- Donne des ordres de grandeur concrets\n",
    "\n",
    "**3. Solutions proposées** :\n",
    "- **Option A** : Mémoïsation avec `@lru_cache` (simple, élégant)\n",
    "- **Option B** : Approche itérative (plus performante, O(1) en espace)\n",
    "- Les deux solutions réduisent à O(n) en temps\n",
    "\n",
    "**4. Code production-ready** :\n",
    "- Docstrings claires\n",
    "- Commentaires pertinents\n",
    "- Tests intégrés dans `if __name__ == \"__main__\"`\n",
    "\n",
    "**Ce que révèle ce processus** :\n",
    "\n",
    "Un modèle de raisonnement ne se contente pas de \"connaître la réponse\". Il :\n",
    "1. **Analyse** le code existant\n",
    "2. **Identifie** le point de friction\n",
    "3. **Explique** les causes profondes\n",
    "4. **Propose** plusieurs solutions avec trade-offs\n",
    "5. **Implémente** du code de qualité professionnelle\n",
    "\n",
    "**Comparaison avec un chat model** :\n",
    "\n",
    "Un chat model aurait probablement :\n",
    "- Identifié le problème (récursion)\n",
    "- Proposé une solution (mémoïsation)\n",
    "- Mais sauté l'analyse de complexité détaillée\n",
    "- Fourni moins d'explications sur les choix de conception\n",
    "\n",
    "**Cas d'usage similaires pour le debugging** :\n",
    "- Fuites mémoire\n",
    "- Problèmes de concurrence\n",
    "- Optimisation de requêtes SQL\n",
    "- Analyse de goulots d'étranglement (profiling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modèles de Raisonnement Avancés\n",
    "\n",
    "### Panorama des modèles raisonnants disponibles\n",
    "\n",
    "| Modèle | Disponibilité | Caractéristiques |\n",
    "|--------|---------------|------------------|\n",
    "| **o1-mini** | Disponible | Rapide, économique, bonne précision |\n",
    "| **o1-preview** | Disponible | Plus puissant, plus lent |\n",
    "| **o3-mini** | Disponible | Dernière génération, équilibré |\n",
    "| **o4-mini** | Disponible | Version optimisée 2026 |\n",
    "| **gpt-5-thinking** | Accès limité | Très avancé, pas encore public |\n",
    "\n",
    "### Recommandations pratiques\n",
    "\n",
    "- **o4-mini** : Premier choix pour la plupart des tâches de raisonnement\n",
    "- **o3-mini** : Alternative si o4-mini n'est pas disponible\n",
    "- **gpt-5-thinking** : Réservé aux cas très complexes (accès restreint)\n",
    "\n",
    "**Note :** La disponibilité des modèles dépend de votre niveau d'accès API. Les exemples ci-dessous utilisent les modèles accessibles publiquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "probleme_complexe = \"\"\"\nRésous ce problème d'optimisation:\n\nUn fermier veut construire un enclos rectangulaire contre un mur existant.\nIl a 100 mètres de clôture disponible (le mur fait office d'un côté).\nQuelle doit être la dimension de l'enclos pour maximiser la surface?\n\nDétaille toutes les étapes mathématiques :\n1. Définition des variables\n2. Équation de contrainte\n3. Fonction à optimiser\n4. Calcul de la dérivée\n5. Résolution et vérification\n\"\"\"\n\nprint(\"=== Modèle raisonnant sur problème d'optimisation ===\\n\")\n\n# Essayer d'abord gpt-5-thinking, fallback sur o4-mini\nmodels_to_try = [\"gpt-5-thinking\", \"o4-mini\", \"o3-mini\", \"o1-preview\"]\n\nfor model in models_to_try:\n    try:\n        print(f\"Tentative avec {model}...\")\n        start = time.time()\n        \n        # Configuration différente selon le modèle\n        if model.startswith(\"gpt-5\"):\n            response = client.chat.completions.create(\n                model=model,\n                messages=[\n                    {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n                    {\"role\": \"user\", \"content\": probleme_complexe}\n                ]\n            )\n        else:\n            response = client.chat.completions.create(\n                model=model,\n                messages=[\n                    {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n                    {\"role\": \"user\", \"content\": probleme_complexe}\n                ],\n                reasoning_effort=\"high\"\n            )\n        \n        duration = time.time() - start\n        print(f\"Succès avec {model} ! (Temps: {duration:.2f}s)\\n\")\n        print(response.choices[0].message.content)\n        break  # Sortir de la boucle si succès\n        \n    except Exception as e:\n        print(f\"  {model} non disponible: {type(e).__name__}\")\n        if model == models_to_try[-1]:\n            print(\"\\nAucun modèle raisonnant disponible. Utilisation de gpt-5-mini en fallback.\")\n            response = client.chat.completions.create(\n                model=DEFAULT_MODEL,\n                messages=[{\"role\": \"user\", \"content\": probleme_complexe}]\n            )\n            print(response.choices[0].message.content)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Réponse correcte: Longueur 50m (parallèle au mur), Largeur 25m, Surface 1250m²\")\nprint(\"Équation: S(x) = x(100-2x), dérivée S'(x) = 100 - 4x, max en x=25\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaison coût/performance\n",
    "\n",
    "### Tarification (approximative, vérifiez les tarifs actuels)\n",
    "\n",
    "| Modèle | Input ($/1M tokens) | Output ($/1M tokens) | Vitesse | Précision |\n",
    "|--------|---------------------|----------------------|---------|----------|\n",
    "| **gpt-4o-mini** | 0.15 | 0.60 | ⚡⚡⚡ | ⭐⭐⭐ |\n",
    "| **gpt-4o** | 2.50 | 10.00 | ⚡⚡ | ⭐⭐⭐⭐ |\n",
    "| **o4-mini** | 1.50 | 6.00 | ⚡⚡ | ⭐⭐⭐⭐⭐ (problèmes complexes) |\n",
    "| **gpt-5-thinking** | 5.00 | 20.00 | ⚡ | ⭐⭐⭐⭐⭐ (très complexe) |\n",
    "\n",
    "### Guide de choix\n",
    "\n",
    "```python\n",
    "def choisir_modele(tache):\n",
    "    if tache.type == \"conversation\" or tache.complexite == \"faible\":\n",
    "        return \"gpt-4o-mini\"  # Rapide, économique\n",
    "    \n",
    "    elif tache.type == \"generation_creative\" or tache.complexite == \"moyenne\":\n",
    "        return \"gpt-4o\"  # Équilibre qualité/vitesse\n",
    "    \n",
    "    elif tache.type in [\"math\", \"code\", \"logique\"] and tache.complexite == \"élevée\":\n",
    "        return \"o4-mini\"  # Raisonnement économique\n",
    "    \n",
    "    elif tache.complexite == \"très élevée\" or tache.precision_requise == \"maximale\":\n",
    "        return \"gpt-5-thinking\"  # Pour les cas difficiles\n",
    "    \n",
    "    else:\n",
    "        return \"gpt-4o\"  # Par défaut\n",
    "```\n",
    "\n",
    "### Tokens de raisonnement\n",
    "\n",
    "Les modèles raisonnants génèrent des **tokens de réflexion** (non visibles) avant la réponse finale.\n",
    "- `reasoning_effort=\"low\"` : ~500-1000 tokens de réflexion\n",
    "- `reasoning_effort=\"medium\"` : ~1000-3000 tokens\n",
    "- `reasoning_effort=\"high\"` : ~3000-10000+ tokens\n",
    "\n",
    "**Ces tokens sont facturés** ! Optimisez selon vos besoins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Benchmark : Chat vs Reasoning\n",
    "\n",
    "Testons les deux approches sur plusieurs problèmes types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmark Chat vs Reasoning ===\n",
      "\n",
      "Problème 1: Combien font 17 * 23?...\n",
      "  gpt-4o-mini (0.66s): 391\n",
      "  o4-mini (1.83s): 391\n",
      "  [Correct: 391]\n",
      "\n",
      "Problème 2: Si 3 chats attrapent 3 souris en 3 minutes, combien de chats...\n",
      "  gpt-4o-mini (0.53s): 100 chats.\n",
      "  o4-mini (4.84s): 3\n",
      "  [Correct: 3 chats]\n",
      "\n",
      "Problème 3: Un train part de Paris à 8h et roule à 120 km/h. Un autre pa...\n",
      "  gpt-4o-mini (0.64s): Ils se croisent à 10h30.\n",
      "  o4-mini (5.57s): 10h13min20s\n",
      "  [Correct: environ 10h30]\n",
      "\n",
      "\n",
      "=== Statistiques ===\n",
      "Temps moyen gpt-4o-mini: 0.61s\n",
      "Temps moyen o4-mini: 4.08s\n",
      "\n",
      "Observation: Les modèles raisonnants sont plus lents mais généralement plus précis\n",
      "sur les problèmes nécessitant plusieurs étapes de calcul.\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "problemes = [\n",
    "    (\"Combien font 17 * 23?\", \"391\"),\n",
    "    (\"Si 3 chats attrapent 3 souris en 3 minutes, combien de chats faut-il pour attraper 100 souris en 100 minutes?\", \"3 chats\"),\n",
    "    (\"Un train part de Paris à 8h et roule à 120 km/h. Un autre part de Lyon (450km) à 9h à 150 km/h vers Paris. À quelle heure se croisent-ils?\", \"environ 10h30\")\n",
    "]\n",
    "\n",
    "print(\"=== Benchmark Chat vs Reasoning ===\\n\")\n",
    "\n",
    "temps_chat = []\n",
    "temps_reasoning = []\n",
    "\n",
    "for i, (prob, correct) in enumerate(problemes):\n",
    "    print(f\"Problème {i+1}: {prob[:60]}...\")\n",
    "    \n",
    "    # Chat model\n",
    "    start = time.time()\n",
    "    resp_chat = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prob + \" Réponds uniquement avec le résultat.\"}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    t_chat = time.time() - start\n",
    "    temps_chat.append(t_chat)\n",
    "    \n",
    "    # Reasoning model\n",
    "    start = time.time()\n",
    "    resp_reason = client.chat.completions.create(\n",
    "        model=\"o4-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prob + \" Réponds uniquement avec le résultat.\"}],\n",
    "        reasoning_effort=\"medium\"\n",
    "    )\n",
    "    t_reason = time.time() - start\n",
    "    temps_reasoning.append(t_reason)\n",
    "    \n",
    "    print(f\"  gpt-4o-mini ({t_chat:.2f}s): {resp_chat.choices[0].message.content.strip()}\")\n",
    "    print(f\"  o4-mini ({t_reason:.2f}s): {resp_reason.choices[0].message.content.strip()}\")\n",
    "    print(f\"  [Correct: {correct}]\\n\")\n",
    "\n",
    "print(\"\\n=== Statistiques ===\")\n",
    "print(f\"Temps moyen gpt-4o-mini: {statistics.mean(temps_chat):.2f}s\")\n",
    "print(f\"Temps moyen o4-mini: {statistics.mean(temps_reasoning):.2f}s\")\n",
    "print(f\"\\nObservation: Les modèles raisonnants sont plus lents mais généralement plus précis\")\n",
    "print(f\"sur les problèmes nécessitant plusieurs étapes de calcul.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exemple pratique : Débogage de code\n",
    "\n",
    "Les modèles de raisonnement excellent dans le debugging complexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Débogage avec o4-mini (reasoning_effort='high') ===\n",
      "\n",
      "Voici une analyse en quatre points, puis une version corrigée et optimisée en Python :\n",
      "\n",
      "1. Problème de performance  \n",
      "   - La fonction récursive appelle à chaque appel fibonacci(n) deux sous-appels fibonacci(n-1) et fibonacci(n-2).  \n",
      "   - Beaucoup de sous-appels sont recalculés plusieurs fois (par exemple fibonacci(20) est recalculé à chaque branche).\n",
      "\n",
      "2. Pourquoi c’est lent (complexité)  \n",
      "   - Le nombre d’appels récursifs croît en gros comme 2^n (chaque appel génère deux nouveaux appels, sauf aux bords).  \n",
      "   - Complexité temporelle : O(2^n).  \n",
      "   - Pour n≈35–40, le nombre d’appels devient très grand, d’où un temps de calcul exponentiel.\n",
      "\n",
      "3. Solution optimisée  \n",
      "   Deux approches courantes pour passer à O(n) ou O(1) par appel :  \n",
      "    • Mémorisation (memoization) : on stocke les résultats déjà calculés dans un cache pour ne pas les recalculer.  \n",
      "    • Itératif (programmation dynamique de bas en haut) : on construit la suite en ligne, en ne gardant que les deux derniers termes.\n",
      "\n",
      "4. Code corrigé  \n",
      "\n",
      "Exemple 1 – avec memoization et functools.lru_cache :\n",
      "\n",
      "```python\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache(maxsize=None)\n",
      "def fibonacci(n):\n",
      "    \"\"\"Calcule le n-ième Fibonacci en O(n) grâce à la mise en cache.\"\"\"\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "# Test\n",
      "for i in range(40):\n",
      "    print(f\"F({i}) = {fibonacci(i)}\")\n",
      "```\n",
      "\n",
      "Exemple 2 – version purement itérative (meilleure consommation mémoire et pas de récursion) :\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"Calcule le n-ième Fibonacci en O(n) de manière itérative.\"\"\"\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    a, b = 0, 1\n",
      "    for _ in range(1, n):\n",
      "        a, b = b, a + b\n",
      "    return b\n",
      "\n",
      "# Test\n",
      "for i in range(40):\n",
      "    print(f\"F({i}) = {fibonacci(i)}\")\n",
      "```\n",
      "\n",
      "Les deux versions font passer la complexité de O(2^n) à O(n), ce qui règle le problème de lenteur pour n ≥ 35.\n",
      "\n",
      "============================================================\n",
      "Le modèle devrait identifier:\n",
      "  - Complexité O(2^n) due aux appels récursifs redondants\n",
      "  - Solution: mémoïsation ou approche itérative (O(n))\n"
     ]
    }
   ],
   "source": [
    "code_buggue = '''\n",
    "def fibonacci(n):\n",
    "    \"\"\"Calcule le n-ième nombre de Fibonacci.\"\"\"\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "# Test\n",
    "for i in range(40):\n",
    "    print(f\"F({i}) = {fibonacci(i)}\")\n",
    "'''\n",
    "\n",
    "prompt_debug = f\"\"\"\n",
    "Analyse ce code Python et identifie le(s) problème(s) :\n",
    "\n",
    "```python\n",
    "{code_buggue}\n",
    "```\n",
    "\n",
    "Le code fonctionne mais devient TRÈS lent pour n >= 35.\n",
    "\n",
    "1. Identifie le problème de performance\n",
    "2. Explique pourquoi (complexité algorithmique)\n",
    "3. Propose une solution optimisée\n",
    "4. Fournis le code corrigé\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Débogage avec o4-mini (reasoning_effort='high') ===\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_debug}\n",
    "    ],\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Le modèle devrait identifier:\")\n",
    "print(\"  - Complexité O(2^n) due aux appels récursifs redondants\")\n",
    "print(\"  - Solution: mémoïsation ou approche itérative (O(n))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Limites et considérations\n",
    "\n",
    "### Limites des modèles de raisonnement\n",
    "\n",
    "1. **Temps de réponse** : Peuvent être lents (30-60s+) pour `reasoning_effort=\"high\"`\n",
    "2. **Coût** : Tokens de réflexion facturés même s'ils ne sont pas visibles\n",
    "3. **Pas toujours nécessaires** : Sur-utiliser peut gaspiller temps et argent\n",
    "4. **Pas magiques** : Ne garantissent pas la correction (vérifiez toujours les résultats)\n",
    "\n",
    "### Bonnes pratiques\n",
    "\n",
    "1. **Commencez simple** : Testez d'abord avec un chat model\n",
    "2. **Montez progressivement** : gpt-4o-mini → gpt-4o → o4-mini → gpt-5-thinking\n",
    "3. **Ajustez reasoning_effort** : Commencez par \"low\", augmentez si nécessaire\n",
    "4. **Validez les résultats** : Surtout pour le code et les calculs critiques\n",
    "5. **Mesurez le ROI** : Le temps/coût supplémentaire est-il justifié ?\n",
    "\n",
    "### Cas où les chat models suffisent\n",
    "\n",
    "- Conversations naturelles\n",
    "- Résumés et synthèses\n",
    "- Traduction simple\n",
    "- Génération de contenu créatif\n",
    "- Questions factuelles directes\n",
    "- Code simple (CRUD, scripts basiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion et ressources\n",
    "\n",
    "### Points clés à retenir\n",
    "\n",
    "1. **Modèles de raisonnement** = temps de réflexion + meilleure précision sur tâches complexes\n",
    "2. **o4-mini** : Excellent rapport qualité/prix pour raisonnement standard\n",
    "3. **gpt-5-thinking** : Pour les problèmes très complexes nécessitant raisonnement profond\n",
    "4. **reasoning_effort** : Contrôle le compromis temps/qualité (low/medium/high)\n",
    "5. **Messages developer** : Remplacent `system`, `\"Formatting re-enabled\"` active le markdown\n",
    "6. **Choisir judicieusement** : Ne pas sur-utiliser les modèles raisonnants (coût)\n",
    "\n",
    "### Tableau récapitulatif\n",
    "\n",
    "| Tâche | Modèle recommandé | reasoning_effort |\n",
    "|-------|-------------------|------------------|\n",
    "| Conversation | gpt-4o-mini | N/A |\n",
    "| Code simple | gpt-4o | N/A |\n",
    "| Algorithme complexe | o4-mini | medium |\n",
    "| Optimisation math | o4-mini | high |\n",
    "| Preuve formelle | gpt-5-thinking | N/A (auto) |\n",
    "| Debugging approfondi | o4-mini ou gpt-5-thinking | high |\n",
    "\n",
    "### Exercices suggérés\n",
    "\n",
    "1. **Comparaison** : Testez un problème de logique avec gpt-4o-mini vs o4-mini\n",
    "2. **Optimisation** : Demandez à o4-mini d'optimiser un algorithme de tri\n",
    "3. **Reasoning effort** : Comparez low/medium/high sur un problème d'optimisation\n",
    "4. **Code generation** : Demandez l'implémentation d'un algorithme complexe (Dijkstra, A*)\n",
    "5. **Multi-étapes** : Problème nécessitant analyse → planification → exécution\n",
    "\n",
    "### Ressources\n",
    "\n",
    "- [Documentation OpenAI - Reasoning Models](https://platform.openai.com/docs/guides/reasoning)\n",
    "- [Guide des tarifs](https://openai.com/pricing)\n",
    "- [Best practices pour o4-mini](https://platform.openai.com/docs/guides/reasoning/best-practices)\n",
    "- Notebook suivant : **9_Vision_Models.ipynb** (GPT-4 Vision, analyse d'images)\n",
    "\n",
    "### Prochaines étapes\n",
    "\n",
    "Dans le prochain notebook, nous explorerons les **modèles de vision** :\n",
    "- GPT-4 Vision (gpt-4o avec images)\n",
    "- Analyse de screenshots, diagrammes, graphiques\n",
    "- Extraction de texte (OCR)\n",
    "- Génération de descriptions d'images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}