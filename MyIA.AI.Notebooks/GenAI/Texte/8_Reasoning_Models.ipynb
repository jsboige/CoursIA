{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4216358c",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [3]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0747babf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:42:34.621293Z",
     "iopub.status.busy": "2026-02-25T21:42:34.620739Z",
     "iopub.status.idle": "2026-02-25T21:42:34.625791Z",
     "shell.execute_reply": "2026-02-25T21:42:34.624944Z"
    },
    "papermill": {
     "duration": 0.027997,
     "end_time": "2026-02-25T21:42:34.626970",
     "exception": false,
     "start_time": "2026-02-25T21:42:34.598973",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_MODE = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2bcb04",
   "metadata": {
    "papermill": {
     "duration": 0.0063,
     "end_time": "2026-02-25T21:42:34.644475",
     "exception": false,
     "start_time": "2026-02-25T21:42:34.638175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modèles de Raisonnement : o4-mini et GPT-5-thinking\n",
    "\n",
    "Les modèles de raisonnement représentent une évolution majeure des LLMs. Contrairement aux modèles de chat classiques, ils prennent le temps de \"réfléchir\" avant de répondre, ce qui améliore significativement leurs performances sur les tâches complexes.\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre les différences architecturales (thinking time)\n",
    "- Maîtriser `reasoning_effort` (low, medium, high)\n",
    "- Choisir le bon modèle selon la tâche\n",
    "- Analyser les performances et coûts\n",
    "\n",
    "**Prérequis :** Notebook 2 (Prompt Engineering)\n",
    "\n",
    "**Durée estimée :** 60 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da485373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:42:34.652756Z",
     "iopub.status.busy": "2026-02-25T21:42:34.652560Z",
     "iopub.status.idle": "2026-02-25T21:42:37.190680Z",
     "shell.execute_reply": "2026-02-25T21:42:37.189224Z"
    },
    "papermill": {
     "duration": 2.544032,
     "end_time": "2026-02-25T21:42:37.192329",
     "exception": false,
     "start_time": "2026-02-25T21:42:34.648297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client OpenAI initialisé !\n",
      "Modèle par défaut: gpt-5-mini\n",
      "Mode: BATCH\n"
     ]
    }
   ],
   "source": [
    "%pip install openai python-dotenv --quiet\n",
    "\n",
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "client = OpenAI()\n",
    "\n",
    "# Modèle par défaut depuis .env\n",
    "DEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\n",
    "BATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\"\n",
    "\n",
    "print(f\"Client OpenAI initialisé !\")\n",
    "print(f\"Modèle par défaut: {DEFAULT_MODEL}\")\n",
    "print(f\"Mode: {'BATCH' if BATCH_MODE else 'INTERACTIF'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5833ad",
   "metadata": {
    "papermill": {
     "duration": 0.008573,
     "end_time": "2026-02-25T21:42:37.204947",
     "exception": false,
     "start_time": "2026-02-25T21:42:37.196374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client OpenAI initialisé !\n",
      "Modèle par défaut: gpt-5-mini\n",
      "Mode: BATCH\n"
     ]
    }
   ],
   "source": [
    "## 0. Configuration de l'environnement\n",
    "\n",
    "### Imports nécessaires\n",
    "\n",
    "Ce notebook utilise :\n",
    "- **openai** : Bibliothèque officielle OpenAI pour interagir avec l'API\n",
    "- **python-dotenv** : Chargement des variables d'environnement (clés API)\n",
    "- **time** : Mesure des temps de réponse pour comparaison\n",
    "\n",
    "### Structure du notebook\n",
    "\n",
    "Nous allons explorer progressivement :\n",
    "1. Comparaison chat vs reasoning models\n",
    "2. Paramètre `reasoning_effort` (low/medium/high)\n",
    "3. Messages `developer` et formatage\n",
    "4. Génération de code complexe\n",
    "5. Modèles avancés et benchmarks\n",
    "6. Analyse coût/performance\n",
    "\n",
    "**Mode batch** : Si `BATCH_MODE=true` dans `.env`, le notebook s'exécute sans interaction utilisateur (utile pour tests automatisés)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c492bd",
   "metadata": {
    "papermill": {
     "duration": 0.00419,
     "end_time": "2026-02-25T21:42:37.212814",
     "exception": false,
     "start_time": "2026-02-25T21:42:37.208624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 1. Chat Models vs Reasoning Models\n",
    "\n",
    "### Différences fondamentales\n",
    "\n",
    "| Aspect | Chat Models | Reasoning Models |\n",
    "|--------|-------------|------------------|\n",
    "| **Réponse** | Immédiate | Temps de réflexion |\n",
    "| **Optimisation** | Dialogue fluide | Problèmes complexes |\n",
    "| **Exemples** | gpt-5-mini, gpt-5-mini | o4-mini, o1 |\n",
    "| **Vitesse** | Rapide (1-3s) | Variable (5-30s+) |\n",
    "| **Précision** | Bonne | Excellente sur tâches complexes |\n",
    "\n",
    "### Quand utiliser quoi ?\n",
    "\n",
    "**Chat Models :**\n",
    "- Conversations naturelles\n",
    "- Questions factuelles simples\n",
    "- Génération de contenu créatif\n",
    "- Traduction, résumé\n",
    "\n",
    "**Reasoning Models :**\n",
    "- Mathématiques et logique\n",
    "- Programmation complexe\n",
    "- Analyse multi-étapes\n",
    "- Problèmes d'optimisation\n",
    "- Déduction et inférence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf560c8",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a0121b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T21:42:37.222372Z",
     "iopub.status.busy": "2026-02-25T21:42:37.221885Z",
     "iopub.status.idle": "2026-02-25T21:42:38.082591Z",
     "shell.execute_reply": "2026-02-25T21:42:38.081857Z"
    },
    "papermill": {
     "duration": 0.866916,
     "end_time": "2026-02-25T21:42:38.083419",
     "exception": true,
     "start_time": "2026-02-25T21:42:37.216503",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `gpt-5-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Test avec gpt-5-mini (chat model)\u001b[39;00m\n\u001b[32m     10\u001b[39m start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m response_chat = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-5-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobleme_math\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m time_chat = time.time() - start\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Test avec o4-mini (reasoning model)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'The model `gpt-5-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "probleme_math = \"\"\"\n",
    "Alice a 54 pommes. Elle en donne la moitié à Bob.\n",
    "Bob mange 13 pomme puis rend le reste à Alice.\n",
    "Alice achète ensuite 33 pommes de plus.\n",
    "Combien de pommes Alice a-t-elle maintenant?\n",
    "Donne uniquement le nombre final.\n",
    "\"\"\"\n",
    "\n",
    "# Test avec gpt-5-mini (chat model)\n",
    "start = time.time()\n",
    "response_chat = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": probleme_math}],\n",
    "    max_tokens=100,\n",
    ")\n",
    "time_chat = time.time() - start\n",
    "\n",
    "# Test avec o4-mini (reasoning model)\n",
    "start = time.time()\n",
    "response_reasoning = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": probleme_math}\n",
    "    ],\n",
    "    reasoning_effort=\"medium\"\n",
    ")\n",
    "time_reasoning = time.time() - start\n",
    "\n",
    "print(\"=== Comparaison Chat vs Reasoning ===\")\n",
    "print(f\"\\ngpt-5-mini ({time_chat:.2f}s):\")\n",
    "print(f\"  Réponse: {response_chat.choices[0].message.content.strip()}\")\n",
    "print(f\"\\ngpt-5-mini ({time_reasoning:.2f}s):\")\n",
    "print(f\"  Réponse: {response_reasoning.choices[0].message.content.strip()}\")\n",
    "print(f\"\\n[Réponse correcte: 74 pommes (Alice donne 27, récupère 14, achète 33 → 54 - 27 + 14 + 33 = 74)]\")\n",
    "print(f\"\\nNote: Le chat model peut faire une erreur de calcul, le reasoning model analyse pas à pas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980bed9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparaison Chat vs Reasoning ===\n",
      "\n",
      "gpt-5-mini (1.19s):\n",
      "  Réponse: Alice a maintenant 74 pommes.\n",
      "\n",
      "gpt-5-mini (7.79s):\n",
      "  Réponse: 74\n",
      "\n",
      "[Réponse correcte: 74 pommes (Alice donne 27, récupère 14, achète 33 → 54 - 27 + 14 + 33 = 74)]\n",
      "\n",
      "Note: Le chat model peut faire une erreur de calcul, le reasoning model analyse pas à pas.\n"
     ]
    }
   ],
   "source": [
    "### Interprétation des résultats\n",
    "\n",
    "Dans cet exemple, les deux modèles ont donné la bonne réponse (7 pommes), mais observez :\n",
    "\n",
    "**Temps de réponse** :\n",
    "- gpt-5-mini : ~0.78s (réponse immédiate)\n",
    "- o4-mini : ~4.67s (temps de réflexion inclus)\n",
    "\n",
    "**Précision** :\n",
    "- Sur ce problème simple, les deux modèles réussissent\n",
    "- La différence devient significative sur des problèmes plus complexes nécessitant plusieurs étapes de raisonnement\n",
    "\n",
    "**Quand la différence compte** :\n",
    "- Problèmes avec plusieurs contraintes simultanées\n",
    "- Calculs nécessitant de garder plusieurs valeurs en mémoire\n",
    "- Raisonnement nécessitant de tester plusieurs hypothèses\n",
    "\n",
    "**Trade-off** : Le modèle de raisonnement prend ~6x plus de temps mais offre une meilleure garantie de précision sur les problèmes complexes. À vous de choisir selon vos besoins !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d686b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Analyse des résultats par niveau\n",
    "\n",
    "**Observations importantes** :\n",
    "\n",
    "1. **Tous les niveaux donnent la bonne réponse** sur ce problème simple\n",
    "   - low (~2.76s) : Raisonnement rapide, réponse correcte\n",
    "   - medium (~4.43s) : Équilibre réflexion/temps\n",
    "   - high (~5.95s) : Analyse approfondie (mais pas nécessaire ici)\n",
    "\n",
    "2. **Différence de temps** :\n",
    "   - low → medium : +60% temps\n",
    "   - medium → high : +34% temps\n",
    "   - Le surcoût augmente de façon non-linéaire\n",
    "\n",
    "3. **Formulation similaire** :\n",
    "   - Les 3 niveaux produisent des explications presque identiques\n",
    "   - Sur un problème simple, la différence est minime\n",
    "\n",
    "**Quand utiliser chaque niveau ?**\n",
    "\n",
    "| Niveau | Durée typique | Cas d'usage |\n",
    "|--------|--------------|-------------|\n",
    "| **low** | 2-10s | Logique simple, calculs directs, questions factuelles avec raisonnement minimal |\n",
    "| **medium** | 5-20s | **Choix par défaut** - équilibre optimal pour la plupart des tâches |\n",
    "| **high** | 10-60s+ | Problèmes très complexes, optimisation multi-contraintes, preuves formelles |\n",
    "\n",
    "**Recommandation** : Commencez toujours par `low` ou `medium`. N'utilisez `high` que si les résultats sont insuffisants.\n",
    "\n",
    "**Impact sur les coûts** : Chaque niveau génère plus de tokens de réflexion cachés (facturés). Sur 1000 requêtes, la différence entre low et high peut représenter des centaines d'euros !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3e3af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 2. Paramètre `reasoning_effort`\n",
    "\n",
    "Les modèles de raisonnement exposent un paramètre crucial : **`reasoning_effort`**.\n",
    "\n",
    "### Niveaux disponibles\n",
    "\n",
    "| Niveau | Temps de réflexion | Qualité | Usage typique |\n",
    "|--------|-------------------|---------|---------------|\n",
    "| **low** | Minimal (5-10s) | Bonne | Problèmes simples nécessitant un peu de réflexion |\n",
    "| **medium** | Équilibré (10-20s) | Très bonne | Cas d'usage général |\n",
    "| **high** | Approfondi (20-60s+) | Excellente | Problèmes très complexes |\n",
    "\n",
    "### Règles empiriques\n",
    "\n",
    "- **low** : Questions de logique simple, calculs directs\n",
    "- **medium** : Programmation standard, analyses multi-étapes\n",
    "- **high** : Optimisation mathématique, preuve formelle, debugging complexe\n",
    "\n",
    "**Note importante :** Plus de réflexion = plus de coût en tokens. Choisissez judicieusement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7d969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T19:56:13.481972Z",
     "iopub.status.busy": "2026-02-25T19:56:13.481734Z",
     "iopub.status.idle": "2026-02-25T19:56:21.132259Z",
     "shell.execute_reply": "2026-02-25T19:56:21.131703Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "probleme_logique = \"\"\"\n",
    "Dans une course, tu dépasses le 2ème. \n",
    "Quelle est ta position finale?\n",
    "Explique ton raisonnement en une phrase.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Impact du reasoning_effort ===\\n\")\n",
    "\n",
    "for effort in [\"low\", \"medium\", \"high\"]:\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"o4-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "            {\"role\": \"user\", \"content\": probleme_logique}\n",
    "        ],\n",
    "        reasoning_effort=effort\n",
    "    )\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    print(f\"reasoning_effort='{effort}' ({duration:.2f}s):\")\n",
    "    print(f\"  {response.choices[0].message.content.strip()}\")\n",
    "    print()\n",
    "\n",
    "print(\"[Réponse correcte: 2ème position - tu prends la place de celui que tu dépasses]\")\n",
    "print(\"\\nObservation: Les 3 niveaux devraient donner la bonne réponse, mais 'high' peut fournir\")\n",
    "print(\"une explication plus détaillée. La différence est surtout visible sur des problèmes complexes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176d60e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Impact du reasoning_effort ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning_effort='low' (1.73s):\n",
      "  Tu te retrouves 2ᵉ : en dépassant le concurrent classé 2ᵉ, tu prends sa place.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning_effort='medium' (2.89s):\n",
      "  Tu deviens deuxième, car en dépassant celui qui était à la 2ᵉ place, tu prends sa position.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning_effort='high' (3.62s):\n",
      "  Tu es deuxième : en dépassant le deuxième, tu prends sa place et restes derrière le premier.\n",
      "\n",
      "[Réponse correcte: 2ème position - tu prends la place de celui que tu dépasses]\n",
      "\n",
      "Observation: Les 3 niveaux devraient donner la bonne réponse, mais 'high' peut fournir\n",
      "une explication plus détaillée. La différence est surtout visible sur des problèmes complexes.\n"
     ]
    }
   ],
   "source": [
    "### Analyse de la qualité du code généré\n",
    "\n",
    "Le modèle o4-mini avec `reasoning_effort='high'` a produit une solution de **très haute qualité** :\n",
    "\n",
    "**Points forts de l'implémentation** :\n",
    "\n",
    "1. **Algorithme optimal** : Expansion autour du centre (O(n²))\n",
    "   - Considère les palindromes de longueur impaire (centre sur un caractère)\n",
    "   - Considère les palindromes de longueur paire (centre entre deux caractères)\n",
    "   - Complexité théorique minimale pour approche directe\n",
    "\n",
    "2. **Code propre et lisible** :\n",
    "   - Fonction helper `expand_around_center` avec logique claire\n",
    "   - Variables bien nommées (`start`, `max_len`, `curr_len`)\n",
    "   - Commentaires pertinents\n",
    "\n",
    "3. **Tests exhaustifs** :\n",
    "   - Chaîne vide\n",
    "   - Un seul caractère\n",
    "   - Palindrome complet\n",
    "   - Pas de palindrome (> 1)\n",
    "   - Cas ambigus (\"babad\" → \"bab\" ou \"aba\")\n",
    "\n",
    "4. **Robustesse** :\n",
    "   - Gestion des cas limites (n < 2)\n",
    "   - Validation avec assertions\n",
    "   - Message de succès clair\n",
    "\n",
    "**Comparaison avec un chat model** :\n",
    "\n",
    "Un modèle comme gpt-5-mini aurait probablement :\n",
    "- Fourni une implémentation correcte mais moins optimisée\n",
    "- Tests moins complets\n",
    "- Moins d'explications sur la complexité\n",
    "\n",
    "**Temps de génération** : 17.29s est un excellent compromis pour obtenir du code de cette qualité. En développement professionnel, cela représente un gain de temps considérable.\n",
    "\n",
    "**Alternative possible** : L'algorithme de Manacher (O(n)) existe mais est beaucoup plus complexe. Pour la plupart des usages, O(n²) est suffisant et plus maintenable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c35faa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 3. Messages `developer` et contrôle du formatage\n",
    "\n",
    "### Rôle `developer`\n",
    "\n",
    "Les modèles de raisonnement (o4-mini, o1) utilisent un rôle spécial : **`developer`**.\n",
    "\n",
    "- Remplace le rôle `system` des chat models\n",
    "- Définit des instructions méta (comportement, formatage)\n",
    "- Moins strict que `system`, plus flexible\n",
    "\n",
    "### \"Formatting re-enabled\"\n",
    "\n",
    "Par défaut, les modèles raisonnants retournent du texte brut. Pour activer le **markdown** (formules LaTeX, code, listes), ajoutez :\n",
    "\n",
    "```python\n",
    "{\"role\": \"developer\", \"content\": \"Formatting re-enabled\"}\n",
    "```\n",
    "\n",
    "Ceci permet :\n",
    "- Formules mathématiques : `$E = mc^2$`\n",
    "- Blocs de code avec syntax highlighting\n",
    "- Tableaux, listes, titres markdown\n",
    "\n",
    "**Sans ce message**, vous obtenez du texte brut (pas de formatage riche)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac72e91",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Analyse du problème d'optimisation\n",
    "\n",
    "Le modèle a fourni une **solution complète et rigoureuse** :\n",
    "\n",
    "**Étapes mathématiques couvertes** :\n",
    "\n",
    "1. **Variables** : x (largeur), longueur = 100 - 2x\n",
    "2. **Contrainte** : Clôture totale = 100m (un côté est le mur)\n",
    "3. **Fonction objectif** : S(x) = x(100 - 2x)\n",
    "4. **Optimisation** : Dérivée S'(x) = 100 - 4x = 0 → x = 25m\n",
    "5. **Vérification** : Dérivée seconde négative (maximum confirmé)\n",
    "\n",
    "**Ce que le raisonnement apporte** :\n",
    "\n",
    "- **Structure claire** : Chaque étape est explicite et justifiée\n",
    "- **Notation mathématique** : Formules LaTeX bien formatées\n",
    "- **Validation** : Vérifie que c'est bien un maximum (pas un minimum)\n",
    "- **Résultat concret** : Dimensions finales et surface maximale\n",
    "\n",
    "**Comparaison avec un chat model** :\n",
    "\n",
    "Un modèle standard aurait probablement :\n",
    "- Donné le bon résultat final\n",
    "- Mais sauté certaines étapes de vérification\n",
    "- Moins de rigueur dans la démonstration\n",
    "\n",
    "**Fallback intelligent** : Le code tente plusieurs modèles (o1 → o4-mini → o3-mini → o1-preview) et utilise gpt-5-mini en dernier recours. Cela garantit que le notebook fonctionne même si certains modèles ne sont pas disponibles.\n",
    "\n",
    "**Cas d'usage similaires** :\n",
    "- Optimisation de budget/ressources\n",
    "- Problèmes de géométrie (surface, volume)\n",
    "- Calcul de marges maximales en finance\n",
    "- Conception de systèmes avec contraintes multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57886e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T19:56:21.169243Z",
     "iopub.status.busy": "2026-02-25T19:56:21.168980Z",
     "iopub.status.idle": "2026-02-25T19:56:32.369041Z",
     "shell.execute_reply": "2026-02-25T19:56:32.368317Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sans \"Formatting re-enabled\"\n",
    "response_no_format = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explique le théorème de Pythagore avec des formules.\"}],\n",
    "    reasoning_effort=\"low\"\n",
    ")\n",
    "\n",
    "# Avec \"Formatting re-enabled\"\n",
    "response_with_format = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explique le théorème de Pythagore avec des formules.\"}\n",
    "    ],\n",
    "    reasoning_effort=\"low\"\n",
    ")\n",
    "\n",
    "print(\"=== Sans 'Formatting re-enabled' ===\")\n",
    "print(response_no_format.choices[0].message.content[:800])\n",
    "print(\"\\n...\\n\")\n",
    "\n",
    "print(\"\\n=== Avec 'Formatting re-enabled' ===\")\n",
    "print(response_with_format.choices[0].message.content[:800])\n",
    "print(\"\\n...\\n\")\n",
    "\n",
    "print(\"\\nDifférence: Avec formatage, vous obtenez du markdown propre (formules LaTeX, code blocks, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ddb0bb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sans 'Formatting re-enabled' ===\n",
      "Le théorème de Pythagore porte sur les triangles rectangles (un angle de 90°). Il énonce que, dans tout triangle rectangle, le carré de la longueur de l’hypoténuse (le côté opposé à l’angle droit) est égal à la somme des carrés des longueurs des deux autres côtés (les « côtés adjacents » ou « côtés de l’angle droit »).\n",
      "\n",
      "1. Énoncé  \n",
      "   Soit un triangle ABC rectangle en C. On note :  \n",
      "   – AB = c (l’hypoténuse)  \n",
      "   – AC = b et BC = a (les deux autres côtés)  \n",
      "   Alors :  \n",
      "   a² + b² = c²\n",
      "\n",
      "2. Illustration  \n",
      "   Triangle ABC avec ∠ACB = 90°.  \n",
      "     AC = b  \n",
      "     BC = a  \n",
      "     AB = c  \n",
      "\n",
      "3. Mise en équation  \n",
      "   On pose les longueurs :  \n",
      "   AB = c  \n",
      "   BC = a  \n",
      "   AC = b  \n",
      "   Alors le théorème s’écrit :  \n",
      "   a² + b² = c²\n",
      "\n",
      "4. Exemple numérique  \n",
      "   Si a = 3 et b = 4, alors :  \n",
      "   a² + b² = 3² + 4\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "=== Avec 'Formatting re-enabled' ===\n",
      "Le théorème de Pythagore (XXVIe proposition du livre I des Éléments d’Euclide) s’énonce ainsi :\n",
      "\n",
      "Dans un triangle rectangle, le carré de la longueur de l’hypoténuse est égal à la somme des carrés des longueurs des deux autres côtés.\n",
      "\n",
      "Soit un triangle ABC rectangle en C, avec :  \n",
      "• AB = c (hypoténuse)  \n",
      "• AC = b et BC = a (côtés de l’angle droit)  \n",
      "\n",
      "Alors la relation fondamentale est :\n",
      "\n",
      "1) Formule de base  \n",
      "   a² + b² = c²  \n",
      "\n",
      "2) Exemples numériques  \n",
      "   – Si a = 3 et b = 4, alors c² = 3² + 4² = 9 + 16 = 25 ⇒ c = 5.  \n",
      "   – Si a = 5 et b = 12, alors c² = 5² + 12² = 25 + 144 = 169 ⇒ c = 13.  \n",
      "\n",
      "3) Preuve géométrique (par aire)  \n",
      "   On construit un carré de côté (a + b) et on y place quatre copies du triangle rectangle identiques.  \n",
      "   - L’aire du grand carré est (a + b)².  \n",
      "   - L’aire combinée\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "Différence: Avec formatage, vous obtenez du markdown propre (formules LaTeX, code blocks, etc.)\n"
     ]
    }
   ],
   "source": [
    "### Interprétation de la différence de formatage\n",
    "\n",
    "**Sans \"Formatting re-enabled\"** :\n",
    "- Texte brut avec numérotation simple\n",
    "- Pas de balises LaTeX ($...$)\n",
    "- Formules écrites en notation textuelle (ex: `c² = a² + b²`)\n",
    "- Structure simple mais moins élégante\n",
    "\n",
    "**Avec \"Formatting re-enabled\"** :\n",
    "- Markdown riche avec titres, listes numérotées\n",
    "- Formules LaTeX rendues (dans les environnements supportant MathJax)\n",
    "- Blocs de code avec syntax highlighting\n",
    "- Mise en forme professionnelle\n",
    "\n",
    "**Quand activer le formatage ?**\n",
    "\n",
    "| Cas d'usage | Formatting re-enabled ? |\n",
    "|-------------|-------------------------|\n",
    "| **Documentation technique** | ✅ Oui (formules, code) |\n",
    "| **Rapports scientifiques** | ✅ Oui (LaTeX, tableaux) |\n",
    "| **Extraction de données brutes** | ❌ Non (traitement programmatique) |\n",
    "| **API backend** | ❌ Non (texte simple plus facile à parser) |\n",
    "| **Interface utilisateur** | ✅ Oui (présentation soignée) |\n",
    "\n",
    "**Exemple de formules LaTeX** (si le formatage est activé) :\n",
    "\n",
    "```markdown\n",
    "Le théorème de Pythagore : $a^2 + b^2 = c^2$\n",
    "\n",
    "La formule d'Einstein : $E = mc^2$\n",
    "\n",
    "Équation quadratique : $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$\n",
    "```\n",
    "\n",
    "**Astuce** : Pour les notebooks Jupyter, le formatage markdown est automatiquement rendu, ce qui rend les réponses beaucoup plus lisibles !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37c26e1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 4. Cas d'usage : Génération de code complexe\n",
    "\n",
    "Les modèles de raisonnement excellent dans la génération de code nécessitant :\n",
    "- Analyse algorithmique\n",
    "- Optimisation de complexité\n",
    "- Tests edge cases\n",
    "- Debugging multi-étapes\n",
    "\n",
    "### Exemple : Algorithme du plus long palindrome\n",
    "\n",
    "Problème classique d'algorithmique :\n",
    "- Approche naïve : O(n³)\n",
    "- Approche optimisée : O(n²) avec expansion autour du centre\n",
    "- Approche avancée : O(n) avec Manacher's algorithm\n",
    "\n",
    "Un modèle raisonnant peut analyser les différentes approches et choisir la meilleure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b537fc9e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Analyse approfondie du benchmark\n",
    "\n",
    "**Résultats quantitatifs** :\n",
    "\n",
    "- **Temps moyen gpt-5-mini** : 1.17s (chat model)\n",
    "- **Temps moyen o4-mini** : 4.41s (reasoning model)\n",
    "- **Ratio** : Le modèle raisonnant est ~3.8x plus lent\n",
    "\n",
    "**Analyse par problème** :\n",
    "\n",
    "1. **Multiplication simple (17 × 23)** :\n",
    "   - Les deux modèles réussissent instantanément\n",
    "   - Différence de temps : 1.61s vs 0.58s\n",
    "   - Conclusion : Sur des calculs directs, le chat model suffit\n",
    "\n",
    "2. **Problème des chats et souris** :\n",
    "   - Piège classique : beaucoup pensent qu'il faut 100 chats\n",
    "   - **Raisonnement correct** : 3 chats attrapent 3 souris en 3 min → 1 chat attrape 1 souris en 3 min → 3 chats attrapent 100 souris en 100 min\n",
    "   - Les deux modèles ont trouvé la bonne réponse\n",
    "   - Temps : 3.02s vs 1.20s\n",
    "\n",
    "3. **Problème des trains** :\n",
    "   - Nécessite plusieurs étapes : distance parcourue par chaque train, rencontre\n",
    "   - gpt-5-mini : \"10h30\" (approximation)\n",
    "   - o4-mini : \"10h 13min 20s\" (calcul précis)\n",
    "   - **Le reasoning model est plus précis** sur les problèmes multi-étapes\n",
    "\n",
    "**Quand la différence est décisive** :\n",
    "\n",
    "| Type de problème | Chat model | Reasoning model |\n",
    "|------------------|------------|-----------------|\n",
    "| **Calcul direct** | ✅ Suffisant | ⚠️ Overkill |\n",
    "| **Logique simple** | ✅ Bon | ✅ Excellent |\n",
    "| **Multi-étapes** | ⚠️ Approximatif | ✅ Précis |\n",
    "| **Optimisation** | ❌ Erreurs fréquentes | ✅ Fiable |\n",
    "| **Preuve formelle** | ❌ Impossible | ✅ Possible |\n",
    "\n",
    "**Recommandation pratique** :\n",
    "\n",
    "Pour une application en production avec des milliers de requêtes :\n",
    "1. **Classifier les questions** par complexité (simple/moyen/complexe)\n",
    "2. **Router intelligemment** : chat model pour simple, reasoning pour complexe\n",
    "3. **Économie de coûts** : ~70% de réduction si 80% des questions sont simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c2ecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T19:56:32.401685Z",
     "iopub.status.busy": "2026-02-25T19:56:32.401480Z",
     "iopub.status.idle": "2026-02-25T19:56:50.167661Z",
     "shell.execute_reply": "2026-02-25T19:56:50.167202Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "probleme_code = \"\"\"\n",
    "Écris une fonction Python qui trouve le plus long palindrome dans une chaîne.\n",
    "La fonction doit être optimisée (O(n²) maximum).\n",
    "Inclus des tests avec plusieurs cas (chaîne vide, un seul caractère, palindrome complet, pas de palindrome).\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Génération de code avec o4-mini (reasoning_effort='high') ===\\n\")\n",
    "\n",
    "start = time.time()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": probleme_code}\n",
    "    ],\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "duration = time.time() - start\n",
    "\n",
    "print(f\"Temps de génération: {duration:.2f}s\\n\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Note: Le modèle devrait fournir :\")\n",
    "print(\"  1. Une implémentation avec expansion autour du centre (O(n²))\")\n",
    "print(\"  2. Des tests complets couvrant les edge cases\")\n",
    "print(\"  3. Des commentaires expliquant la logique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8cea6f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Génération de code avec o4-mini (reasoning_effort='high') ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps de génération: 45.09s\n",
      "\n",
      "Voici une solution en Python en O(n²) (approche « expand around center ») avec quelques cas de test :\n",
      "\n",
      "def longest_palindrome(s: str) -> str:\n",
      "    \"\"\"\n",
      "    Retourne le plus long sous‐chaîne palindrome de s.\n",
      "    Complexité : O(n²) en temps, O(1) en espace supplémentaire.\n",
      "    \"\"\"\n",
      "    if len(s) < 2:\n",
      "        return s\n",
      "    start, end = 0, 0\n",
      "    for i in range(len(s)):\n",
      "        # palindrome de longueur impaire centré en i\n",
      "        len1 = _expand_around_center(s, i, i)\n",
      "        # palindrome de longueur paire centré entre i et i+1\n",
      "        len2 = _expand_around_center(s, i, i+1)\n",
      "        curr_max = max(len1, len2)\n",
      "        if curr_max > (end - start + 1):\n",
      "            # on recalcule les bornes du nouveau plus long palindrome\n",
      "            start = i - (curr_max - 1) // 2\n",
      "            end   = i +  curr_max // 2\n",
      "    return s[start:end+1]\n",
      "\n",
      "def _expand_around_center(s: str, left: int, right: int) -> int:\n",
      "    \"\"\"\n",
      "    Étend les pointeurs left/right tant que s[left] == s[right].\n",
      "    Lorsqu'on s'arrête, la longueur du palindrome est right-left-1.\n",
      "    \"\"\"\n",
      "    while left >= 0 and right < len(s) and s[left] == s[right]:\n",
      "        left  -= 1\n",
      "        right += 1\n",
      "    return right - left - 1\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Liste de tests couvrant : chaîne vide, 1 caractère, palindrome complet, pas de palindrome >1, exemples variés.\n",
      "    tests = [\n",
      "        (\"\", \"\"),                    # chaîne vide\n",
      "        (\"a\", \"a\"),                  # un seul caractère\n",
      "        (\"racecar\", \"racecar\"),      # palindrome complet\n",
      "        (\"abcde\", \"a\"),              # pas de palindrome >1\n",
      "        (\"forgeeksskeegfor\", \"geeksskeeg\"),  # palindrome pair\n",
      "        (\"babad\", \"aba\"),            # plusieurs choix possibles (\"aba\" ou \"bab\")\n",
      "        (\"cbbd\", \"bb\"),              # palindrome pair\n",
      "    ]\n",
      "    for s, expected in tests:\n",
      "        res = longest_palindrome(s)\n",
      "        assert res == expected, f\"Erreur pour '{s}' : attendu '{expected}', obtenu '{res}'\"\n",
      "    print(\"Tous les tests sont passés.\")\n",
      "\n",
      "============================================================\n",
      "Note: Le modèle devrait fournir :\n",
      "  1. Une implémentation avec expansion autour du centre (O(n²))\n",
      "  2. Des tests complets couvrant les edge cases\n",
      "  3. Des commentaires expliquant la logique\n"
     ]
    }
   ],
   "source": [
    "### Déconstruction du processus de raisonnement\n",
    "\n",
    "Le modèle de raisonnement a identifié le problème de performance avec une **analyse structurée remarquable** :\n",
    "\n",
    "**1. Diagnostic du problème** :\n",
    "- Identifie la récursion redondante\n",
    "- Explique que les mêmes valeurs sont recalculées plusieurs fois\n",
    "- Quantifie l'ampleur du problème (millions d'appels pour n=35-40)\n",
    "\n",
    "**2. Analyse de complexité** :\n",
    "- Formule la complexité O(2ⁿ) explicitement\n",
    "- Explique pourquoi : \"l'arbre d'appels double à chaque niveau\"\n",
    "- Donne des ordres de grandeur concrets\n",
    "\n",
    "**3. Solutions proposées** :\n",
    "- **Option A** : Mémoïsation avec `@lru_cache` (simple, élégant)\n",
    "- **Option B** : Approche itérative (plus performante, O(1) en espace)\n",
    "- Les deux solutions réduisent à O(n) en temps\n",
    "\n",
    "**4. Code production-ready** :\n",
    "- Docstrings claires\n",
    "- Commentaires pertinents\n",
    "- Tests intégrés dans `if __name__ == \"__main__\"`\n",
    "\n",
    "**Ce que révèle ce processus** :\n",
    "\n",
    "Un modèle de raisonnement ne se contente pas de \"connaître la réponse\". Il :\n",
    "1. **Analyse** le code existant\n",
    "2. **Identifie** le point de friction\n",
    "3. **Explique** les causes profondes\n",
    "4. **Propose** plusieurs solutions avec trade-offs\n",
    "5. **Implémente** du code de qualité professionnelle\n",
    "\n",
    "**Comparaison avec un chat model** :\n",
    "\n",
    "Un chat model aurait probablement :\n",
    "- Identifié le problème (récursion)\n",
    "- Proposé une solution (mémoïsation)\n",
    "- Mais sauté l'analyse de complexité détaillée\n",
    "- Fourni moins d'explications sur les choix de conception\n",
    "\n",
    "**Cas d'usage similaires pour le debugging** :\n",
    "- Fuites mémoire\n",
    "- Problèmes de concurrence\n",
    "- Optimisation de requêtes SQL\n",
    "- Analyse de goulots d'étranglement (profiling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a441da",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 5. Modèles de Raisonnement Avancés\n",
    "\n",
    "### Panorama des modèles raisonnants disponibles\n",
    "\n",
    "| Modèle | Disponibilité | Caractéristiques |\n",
    "|--------|---------------|------------------|\n",
    "| **o1-mini** | Disponible | Rapide, économique, bonne précision |\n",
    "| **o1-preview** | Disponible | Plus puissant, plus lent |\n",
    "| **o3-mini** | Disponible | Dernière génération, équilibré |\n",
    "| **o4-mini** | Disponible | Version optimisée 2026 |\n",
    "| **o1** | Accès limité | Très avancé, pas encore public |\n",
    "\n",
    "### Recommandations pratiques\n",
    "\n",
    "- **o4-mini** : Premier choix pour la plupart des tâches de raisonnement\n",
    "- **o3-mini** : Alternative si o4-mini n'est pas disponible\n",
    "- **o1** : Réservé aux cas très complexes (accès restreint)\n",
    "\n",
    "**Note :** La disponibilité des modèles dépend de votre niveau d'accès API. Les exemples ci-dessous utilisent les modèles accessibles publiquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6260d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T19:56:50.198539Z",
     "iopub.status.busy": "2026-02-25T19:56:50.198311Z",
     "iopub.status.idle": "2026-02-25T19:56:58.726281Z",
     "shell.execute_reply": "2026-02-25T19:56:58.724971Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "probleme_complexe = \"\"\"\n",
    "Résous ce problème d'optimisation:\n",
    "\n",
    "Un fermier veut construire un enclos rectangulaire contre un mur existant.\n",
    "Il a 100 mètres de clôture disponible (le mur fait office d'un côté).\n",
    "Quelle doit être la dimension de l'enclos pour maximiser la surface?\n",
    "\n",
    "Détaille toutes les étapes mathématiques :\n",
    "1. Définition des variables\n",
    "2. Équation de contrainte\n",
    "3. Fonction à optimiser\n",
    "4. Calcul de la dérivée\n",
    "5. Résolution et vérification\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Modèle raisonnant sur problème d'optimisation ===\\n\")\n",
    "\n",
    "# Essayer d'abord o1, fallback sur o4-mini\n",
    "models_to_try = [\"o1\", \"o4-mini\", \"o3-mini\", \"o1-preview\"]\n",
    "\n",
    "for model in models_to_try:\n",
    "    try:\n",
    "        print(f\"Tentative avec {model}...\")\n",
    "        start = time.time()\n",
    "        \n",
    "        # Configuration différente selon le modèle\n",
    "        if model.startswith(\"gpt-5\"):\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "                    {\"role\": \"user\", \"content\": probleme_complexe}\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "                    {\"role\": \"user\", \"content\": probleme_complexe}\n",
    "                ],\n",
    "                reasoning_effort=\"high\"\n",
    "            )\n",
    "        \n",
    "        duration = time.time() - start\n",
    "        print(f\"Succès avec {model} ! (Temps: {duration:.2f}s)\\n\")\n",
    "        print(response.choices[0].message.content)\n",
    "        break  # Sortir de la boucle si succès\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  {model} non disponible: {type(e).__name__}\")\n",
    "        if model == models_to_try[-1]:\n",
    "            print(\"\\nAucun modèle raisonnant disponible. Utilisation de gpt-5-mini en fallback.\")\n",
    "            response = client.chat.completions.create(\n",
    "                model=DEFAULT_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": probleme_complexe}]\n",
    "            )\n",
    "            print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Réponse correcte: Longueur 50m (parallèle au mur), Largeur 25m, Surface 1250m²\")\n",
    "print(\"Équation: S(x) = x(100-2x), dérivée S'(x) = 100 - 4x, max en x=25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9322ecb9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Modèle raisonnant sur problème d'optimisation ===\n",
      "\n",
      "Tentative avec o1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  o1 non disponible: NotFoundError\n",
      "Tentative avec o4-mini...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succès avec o4-mini ! (Temps: 6.30s)\n",
      "\n",
      "Voici une résolution détaillée :\n",
      "\n",
      "1. Définition des variables  \n",
      "   – Soit x la largeur de l’enclos (mesurée perpendiculairement au mur).  \n",
      "   – Soit y la longueur de l’enclos (la face opposée au mur).\n",
      "\n",
      "2. Équation de contrainte (longueur de clôture)  \n",
      "   Seules trois faces nécessitent de la clôture : deux largeurs x et une longueur y.  \n",
      "   Donc : 2·x + y = 100   (toutes les longueurs en mètres)\n",
      "\n",
      "3. Fonction à optimiser (surface A)  \n",
      "   La surface de l’enclos est :  \n",
      "   A(x,y) = x · y\n",
      "\n",
      "4. Réduction à une variable  \n",
      "   Exprimer y en fonction de x via la contrainte :  \n",
      "   y = 100 − 2x  \n",
      "   D’où la surface en fonction de x seul :  \n",
      "   A(x) = x · (100 − 2x) = 100 x − 2 x²\n",
      "\n",
      "5. Calcul de la dérivée et recherche des points critiques  \n",
      "   A′(x) = d/dx (100 x − 2 x²) = 100 − 4 x  \n",
      "   Pour un extremum, on impose A′(x) = 0 ⇒ 100 − 4 x = 0 ⇒ x = 25\n",
      "\n",
      "6. Calcul de y et vérification de maximum  \n",
      "   – y = 100 − 2·25 = 50  \n",
      "   – A″(x) = d/dx (100 − 4 x) = −4 < 0 ⇒ la fonction A est concave et x = 25 m donne un maximum.\n",
      "\n",
      "7. Résultat final  \n",
      "   – Largeur (perpendiculaire au mur) x* = 25 m  \n",
      "   – Longueur (parallèle au mur) y* = 50 m  \n",
      "   – Surface maximale A* = 25 × 50 = 1 250 m²\n",
      "\n",
      "============================================================\n",
      "Réponse correcte: Longueur 50m (parallèle au mur), Largeur 25m, Surface 1250m²\n",
      "Équation: S(x) = x(100-2x), dérivée S'(x) = 100 - 4x, max en x=25\n"
     ]
    }
   ],
   "source": [
    "## 6. Comparaison coût/performance\n",
    "\n",
    "### Tarification (approximative, vérifiez les tarifs actuels)\n",
    "\n",
    "| Modèle | Input ($/1M tokens) | Output ($/1M tokens) | Vitesse | Précision |\n",
    "|--------|---------------------|----------------------|---------|----------|\n",
    "| **gpt-5-mini** | 0.15 | 0.60 | ⚡⚡⚡ | ⭐⭐⭐ |\n",
    "| **gpt-5-mini** | 2.50 | 10.00 | ⚡⚡ | ⭐⭐⭐⭐ |\n",
    "| **o4-mini** | 1.50 | 6.00 | ⚡⚡ | ⭐⭐⭐⭐⭐ (problèmes complexes) |\n",
    "| **o1** | 5.00 | 20.00 | ⚡ | ⭐⭐⭐⭐⭐ (très complexe) |\n",
    "\n",
    "### Guide de choix\n",
    "\n",
    "```python\n",
    "def choisir_modele(tache):\n",
    "    if tache.type == \"conversation\" or tache.complexite == \"faible\":\n",
    "        return \"gpt-5-mini\"  # Rapide, économique\n",
    "    \n",
    "    elif tache.type == \"generation_creative\" or tache.complexite == \"moyenne\":\n",
    "        return \"gpt-5-mini\"  # Équilibre qualité/vitesse\n",
    "    \n",
    "    elif tache.type in [\"math\", \"code\", \"logique\"] and tache.complexite == \"élevée\":\n",
    "        return \"o4-mini\"  # Raisonnement économique\n",
    "    \n",
    "    elif tache.complexite == \"très élevée\" or tache.precision_requise == \"maximale\":\n",
    "        return \"o1\"  # Pour les cas difficiles\n",
    "    \n",
    "    else:\n",
    "        return \"gpt-5-mini\"  # Par défaut\n",
    "```\n",
    "\n",
    "### Tokens de raisonnement\n",
    "\n",
    "Les modèles raisonnants génèrent des **tokens de réflexion** (non visibles) avant la réponse finale.\n",
    "- `reasoning_effort=\"low\"` : ~500-1000 tokens de réflexion\n",
    "- `reasoning_effort=\"medium\"` : ~1000-3000 tokens\n",
    "- `reasoning_effort=\"high\"` : ~3000-10000+ tokens\n",
    "\n",
    "**Ces tokens sont facturés** ! Optimisez selon vos besoins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023d680",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 7. Benchmark : Chat vs Reasoning\n",
    "\n",
    "Testons les deux approches sur plusieurs problèmes types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f6345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T19:56:58.751756Z",
     "iopub.status.busy": "2026-02-25T19:56:58.751568Z",
     "iopub.status.idle": "2026-02-25T19:57:12.742385Z",
     "shell.execute_reply": "2026-02-25T19:57:12.741489Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "problemes = [\n",
    "    (\"Combien font 17 * 23?\", \"391\"),\n",
    "    (\"Si 3 chats attrapent 3 souris en 3 minutes, combien de chats faut-il pour attraper 100 souris en 100 minutes?\", \"3 chats\"),\n",
    "    (\"Un train part de Paris à 8h et roule à 120 km/h. Un autre part de Lyon (450km) à 9h à 150 km/h vers Paris. À quelle heure se croisent-ils?\", \"environ 10h30\")\n",
    "]\n",
    "\n",
    "print(\"=== Benchmark Chat vs Reasoning ===\\n\")\n",
    "\n",
    "temps_chat = []\n",
    "temps_reasoning = []\n",
    "\n",
    "for i, (prob, correct) in enumerate(problemes):\n",
    "    print(f\"Problème {i+1}: {prob[:60]}...\")\n",
    "    \n",
    "    # Chat model\n",
    "    start = time.time()\n",
    "    resp_chat = client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prob + \" Réponds uniquement avec le résultat.\"}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    t_chat = time.time() - start\n",
    "    temps_chat.append(t_chat)\n",
    "    \n",
    "    # Reasoning model\n",
    "    start = time.time()\n",
    "    resp_reason = client.chat.completions.create(\n",
    "        model=\"o4-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prob + \" Réponds uniquement avec le résultat.\"}],\n",
    "        reasoning_effort=\"medium\"\n",
    "    )\n",
    "    t_reason = time.time() - start\n",
    "    temps_reasoning.append(t_reason)\n",
    "    \n",
    "    print(f\"  gpt-5-mini ({t_chat:.2f}s): {resp_chat.choices[0].message.content.strip()}\")\n",
    "    print(f\"  o4-mini ({t_reason:.2f}s): {resp_reason.choices[0].message.content.strip()}\")\n",
    "    print(f\"  [Correct: {correct}]\\n\")\n",
    "\n",
    "print(\"\\n=== Statistiques ===\")\n",
    "print(f\"Temps moyen gpt-5-mini: {statistics.mean(temps_chat):.2f}s\")\n",
    "print(f\"Temps moyen o4-mini: {statistics.mean(temps_reasoning):.2f}s\")\n",
    "print(f\"\\nObservation: Les modèles raisonnants sont plus lents mais généralement plus précis\")\n",
    "print(f\"sur les problèmes nécessitant plusieurs étapes de calcul.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f2983",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmark Chat vs Reasoning ===\n",
      "\n",
      "Problème 1: Combien font 17 * 23?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gpt-5-mini (0.61s): 391\n",
      "  o4-mini (1.38s): 391\n",
      "  [Correct: 391]\n",
      "\n",
      "Problème 2: Si 3 chats attrapent 3 souris en 3 minutes, combien de chats...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gpt-5-mini (0.38s): 33 chats.\n",
      "  o4-mini (2.81s): 3\n",
      "  [Correct: 3 chats]\n",
      "\n",
      "Problème 3: Un train part de Paris à 8h et roule à 120 km/h. Un autre pa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gpt-5-mini (0.47s): Ils se croisent à 10h30.\n",
      "  o4-mini (5.73s): 10h13min20s\n",
      "  [Correct: environ 10h30]\n",
      "\n",
      "\n",
      "=== Statistiques ===\n",
      "Temps moyen gpt-5-mini: 0.49s\n",
      "Temps moyen o4-mini: 3.30s\n",
      "\n",
      "Observation: Les modèles raisonnants sont plus lents mais généralement plus précis\n",
      "sur les problèmes nécessitant plusieurs étapes de calcul.\n"
     ]
    }
   ],
   "source": [
    "## 8. Exemple pratique : Débogage de code\n",
    "\n",
    "Les modèles de raisonnement excellent dans le debugging complexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa65220",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "code_buggue = '''\n",
    "def fibonacci(n):\n",
    "    \"\"\"Calcule le n-ième nombre de Fibonacci.\"\"\"\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "# Test\n",
    "for i in range(40):\n",
    "    print(f\"F({i}) = {fibonacci(i)}\")\n",
    "'''\n",
    "\n",
    "prompt_debug = f\"\"\"\n",
    "Analyse ce code Python et identifie le(s) problème(s) :\n",
    "\n",
    "```python\n",
    "{code_buggue}\n",
    "```\n",
    "\n",
    "Le code fonctionne mais devient TRÈS lent pour n >= 35.\n",
    "\n",
    "1. Identifie le problème de performance\n",
    "2. Explique pourquoi (complexité algorithmique)\n",
    "3. Propose une solution optimisée\n",
    "4. Fournis le code corrigé\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Débogage avec o4-mini (reasoning_effort='high') ===\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_debug}\n",
    "    ],\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Le modèle devrait identifier:\")\n",
    "print(\"  - Complexité O(2^n) due aux appels récursifs redondants\")\n",
    "print(\"  - Solution: mémoïsation ou approche itérative (O(n))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6f92a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Débogage avec o4-mini (reasoning_effort='high') ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici une analyse en 4 points et deux pistes d’optimisation (mémoïsation et itératif) :\n",
      "\n",
      "1. Problème de performance  \n",
      "   – La version récursive naïve refait sans cesse les mêmes calculs (appel à fibonacci(k) multiplié à chaque niveau).  \n",
      "   – Pour n=40, le nombre d’appels explose (≈2⁴⁰ appels), d’où un temps de calcul exponentiel.\n",
      "\n",
      "2. Complexité algorithmique  \n",
      "   – Temps : O(2ⁿ) (chaque appel engendre 2 appels récursifs, sauf pour n≤1).  \n",
      "   – Espace : O(n) en profondeur de pile.\n",
      "\n",
      "3. Solutions optimisées  \n",
      "   a) Mémoïsation (caching) : on stocke le résultat de chaque fibonacci(k) la première fois, puis on le réutilise.  \n",
      "   b) Itératif (programmation dynamique) : on construit la suite de bas en haut en n étapes, O(n) temps et O(1) espace supplémentaire.\n",
      "\n",
      "4. Code corrigé  \n",
      "\n",
      "a) Avec functools.lru_cache  \n",
      "```python\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache(maxsize=None)\n",
      "def fibonacci(n):\n",
      "    \"\"\"Calcule le n-ième nombre de Fibonacci (mémoïsé).\"\"\"\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "# Test\n",
      "for i in range(40):\n",
      "    print(f\"F({i}) = {fibonacci(i)}\")\n",
      "```\n",
      "\n",
      "b) Version itérative (boucle)  \n",
      "```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"Calcule le n-ième nombre de Fibonacci en O(n).\"\"\"\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    a, b = 0, 1\n",
      "    for _ in range(1, n):\n",
      "        a, b = b, a + b\n",
      "    return b\n",
      "\n",
      "# Test\n",
      "for i in range(40):\n",
      "    print(f\"F({i}) = {fibonacci(i)}\")\n",
      "```\n",
      "\n",
      "Les deux versions ramènent le temps de calcul à O(n), et exécuteront très vite même pour n = 10 000 ou plus.\n",
      "\n",
      "============================================================\n",
      "Le modèle devrait identifier:\n",
      "  - Complexité O(2^n) due aux appels récursifs redondants\n",
      "  - Solution: mémoïsation ou approche itérative (O(n))\n"
     ]
    }
   ],
   "source": [
    "## 9. Limites et considérations\n",
    "\n",
    "### Limites des modèles de raisonnement\n",
    "\n",
    "1. **Temps de réponse** : Peuvent être lents (30-60s+) pour `reasoning_effort=\"high\"`\n",
    "2. **Coût** : Tokens de réflexion facturés même s'ils ne sont pas visibles\n",
    "3. **Pas toujours nécessaires** : Sur-utiliser peut gaspiller temps et argent\n",
    "4. **Pas magiques** : Ne garantissent pas la correction (vérifiez toujours les résultats)\n",
    "\n",
    "### Bonnes pratiques\n",
    "\n",
    "1. **Commencez simple** : Testez d'abord avec un chat model\n",
    "2. **Montez progressivement** : gpt-5-mini → gpt-5-mini → o4-mini → o1\n",
    "3. **Ajustez reasoning_effort** : Commencez par \"low\", augmentez si nécessaire\n",
    "4. **Validez les résultats** : Surtout pour le code et les calculs critiques\n",
    "5. **Mesurez le ROI** : Le temps/coût supplémentaire est-il justifié ?\n",
    "\n",
    "### Cas où les chat models suffisent\n",
    "\n",
    "- Conversations naturelles\n",
    "- Résumés et synthèses\n",
    "- Traduction simple\n",
    "- Génération de contenu créatif\n",
    "- Questions factuelles directes\n",
    "- Code simple (CRUD, scripts basiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e613377",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 10. Conclusion et ressources\n",
    "\n",
    "### Points clés à retenir\n",
    "\n",
    "1. **Modèles de raisonnement** = temps de réflexion + meilleure précision sur tâches complexes\n",
    "2. **o4-mini** : Excellent rapport qualité/prix pour raisonnement standard\n",
    "3. **o1** : Pour les problèmes très complexes nécessitant raisonnement profond\n",
    "4. **reasoning_effort** : Contrôle le compromis temps/qualité (low/medium/high)\n",
    "5. **Messages developer** : Remplacent `system`, `\"Formatting re-enabled\"` active le markdown\n",
    "6. **Choisir judicieusement** : Ne pas sur-utiliser les modèles raisonnants (coût)\n",
    "\n",
    "### Tableau récapitulatif\n",
    "\n",
    "| Tâche | Modèle recommandé | reasoning_effort |\n",
    "|-------|-------------------|------------------|\n",
    "| Conversation | gpt-5-mini | N/A |\n",
    "| Code simple | gpt-5-mini | N/A |\n",
    "| Algorithme complexe | o4-mini | medium |\n",
    "| Optimisation math | o4-mini | high |\n",
    "| Preuve formelle | o1 | N/A (auto) |\n",
    "| Debugging approfondi | o4-mini ou o1 | high |\n",
    "\n",
    "### Exercices suggérés\n",
    "\n",
    "1. **Comparaison** : Testez un problème de logique avec gpt-5-mini vs o4-mini\n",
    "2. **Optimisation** : Demandez à o4-mini d'optimiser un algorithme de tri\n",
    "3. **Reasoning effort** : Comparez low/medium/high sur un problème d'optimisation\n",
    "4. **Code generation** : Demandez l'implémentation d'un algorithme complexe (Dijkstra, A*)\n",
    "5. **Multi-étapes** : Problème nécessitant analyse → planification → exécution\n",
    "\n",
    "### Ressources\n",
    "\n",
    "- [Documentation OpenAI - Reasoning Models](https://platform.openai.com/docs/guides/reasoning)\n",
    "- [Guide des tarifs](https://openai.com/pricing)\n",
    "- [Best practices pour o4-mini](https://platform.openai.com/docs/guides/reasoning/best-practices)\n",
    "- Notebook suivant : **9_Vision_Models.ipynb** (GPT-4 Vision, analyse d'images)\n",
    "\n",
    "### Prochaines étapes\n",
    "\n",
    "Dans le prochain notebook, nous explorerons les **modèles de vision** :\n",
    "- GPT-4 Vision (gpt-5-mini avec images)\n",
    "- Analyse de screenshots, diagrammes, graphiques\n",
    "- Extraction de texte (OCR)\n",
    "- Génération de descriptions d'images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.485613,
   "end_time": "2026-02-25T21:42:38.441804",
   "environment_variables": {},
   "exception": true,
   "input_path": "8_Reasoning_Models.ipynb",
   "output_path": "8_Reasoning_Models.ipynb",
   "parameters": {
    "BATCH_MODE": "true"
   },
   "start_time": "2026-02-25T21:42:32.956191",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}