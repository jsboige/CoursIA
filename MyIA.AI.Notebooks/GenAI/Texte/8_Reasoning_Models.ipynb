{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles de Raisonnement : o4-mini et GPT-5-thinking\n",
    "\n",
    "Les modèles de raisonnement représentent une évolution majeure des LLMs. Contrairement aux modèles de chat classiques, ils prennent le temps de \"réfléchir\" avant de répondre, ce qui améliore significativement leurs performances sur les tâches complexes.\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre les différences architecturales (thinking time)\n",
    "- Maîtriser `reasoning_effort` (low, medium, high)\n",
    "- Choisir le bon modèle selon la tâche\n",
    "- Analyser les performances et coûts\n",
    "\n",
    "**Prérequis :** Notebook 2 (Prompt Engineering)\n",
    "\n",
    "**Durée estimée :** 60 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%pip install openai python-dotenv --quiet\n\nimport os\nimport time\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv('../.env')\nclient = OpenAI()\n\n# Modèle par défaut depuis .env\nDEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\nBATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\"\n\nprint(f\"Client OpenAI initialisé !\")\nprint(f\"Modèle par défaut: {DEFAULT_MODEL}\")\nprint(f\"Mode: {'BATCH' if BATCH_MODE else 'INTERACTIF'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chat Models vs Reasoning Models\n",
    "\n",
    "### Différences fondamentales\n",
    "\n",
    "| Aspect | Chat Models | Reasoning Models |\n",
    "|--------|-------------|------------------|\n",
    "| **Réponse** | Immédiate | Temps de réflexion |\n",
    "| **Optimisation** | Dialogue fluide | Problèmes complexes |\n",
    "| **Exemples** | gpt-4o, gpt-4o-mini | o4-mini, gpt-5-thinking |\n",
    "| **Vitesse** | Rapide (1-3s) | Variable (5-30s+) |\n",
    "| **Précision** | Bonne | Excellente sur tâches complexes |\n",
    "\n",
    "### Quand utiliser quoi ?\n",
    "\n",
    "**Chat Models :**\n",
    "- Conversations naturelles\n",
    "- Questions factuelles simples\n",
    "- Génération de contenu créatif\n",
    "- Traduction, résumé\n",
    "\n",
    "**Reasoning Models :**\n",
    "- Mathématiques et logique\n",
    "- Programmation complexe\n",
    "- Analyse multi-étapes\n",
    "- Problèmes d'optimisation\n",
    "- Déduction et inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparaison Chat vs Reasoning ===\n",
      "\n",
      "gpt-4o-mini (0.78s):\n",
      "  Réponse: Alice a maintenant 7 pommes.\n",
      "\n",
      "o4-mini (4.67s):\n",
      "  Réponse: 7\n",
      "\n",
      "[Réponse correcte: 5.5 pommes (Alice donne 2.5, récupère 1.5, achète 3 → 5 + 1.5 + 3 - 2.5 = 7)]\n",
      "\n",
      "Note: Le chat model peut faire une erreur de calcul, le reasoning model analyse pas à pas.\n"
     ]
    }
   ],
   "source": [
    "probleme_math = \"\"\"\n",
    "Alice a 5 pommes. Elle en donne la moitié à Bob.\n",
    "Bob mange 1 pomme puis rend le reste à Alice.\n",
    "Alice achète ensuite 3 pommes de plus.\n",
    "Combien de pommes Alice a-t-elle maintenant?\n",
    "Donne uniquement le nombre final.\n",
    "\"\"\"\n",
    "\n",
    "# Test avec gpt-4o-mini (chat model)\n",
    "start = time.time()\n",
    "response_chat = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": probleme_math}],\n",
    "    max_tokens=100\n",
    ")\n",
    "time_chat = time.time() - start\n",
    "\n",
    "# Test avec o4-mini (reasoning model)\n",
    "start = time.time()\n",
    "response_reasoning = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": probleme_math}\n",
    "    ],\n",
    "    reasoning_effort=\"medium\"\n",
    ")\n",
    "time_reasoning = time.time() - start\n",
    "\n",
    "print(\"=== Comparaison Chat vs Reasoning ===\")\n",
    "print(f\"\\ngpt-4o-mini ({time_chat:.2f}s):\")\n",
    "print(f\"  Réponse: {response_chat.choices[0].message.content.strip()}\")\n",
    "print(f\"\\no4-mini ({time_reasoning:.2f}s):\")\n",
    "print(f\"  Réponse: {response_reasoning.choices[0].message.content.strip()}\")\n",
    "print(f\"\\n[Réponse correcte: 5.5 pommes (Alice donne 2.5, récupère 1.5, achète 3 → 5 + 1.5 + 3 - 2.5 = 7)]\")\n",
    "print(f\"\\nNote: Le chat model peut faire une erreur de calcul, le reasoning model analyse pas à pas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Paramètre `reasoning_effort`\n",
    "\n",
    "Les modèles de raisonnement exposent un paramètre crucial : **`reasoning_effort`**.\n",
    "\n",
    "### Niveaux disponibles\n",
    "\n",
    "| Niveau | Temps de réflexion | Qualité | Usage typique |\n",
    "|--------|-------------------|---------|---------------|\n",
    "| **low** | Minimal (5-10s) | Bonne | Problèmes simples nécessitant un peu de réflexion |\n",
    "| **medium** | Équilibré (10-20s) | Très bonne | Cas d'usage général |\n",
    "| **high** | Approfondi (20-60s+) | Excellente | Problèmes très complexes |\n",
    "\n",
    "### Règles empiriques\n",
    "\n",
    "- **low** : Questions de logique simple, calculs directs\n",
    "- **medium** : Programmation standard, analyses multi-étapes\n",
    "- **high** : Optimisation mathématique, preuve formelle, debugging complexe\n",
    "\n",
    "**Note importante :** Plus de réflexion = plus de coût en tokens. Choisissez judicieusement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Impact du reasoning_effort ===\n",
      "\n",
      "reasoning_effort='low' (2.76s):\n",
      "  Tu te retrouves à la 2ᵉ place, car en dépassant le coureur classé 2ᵉ tu prends sa position.\n",
      "\n",
      "reasoning_effort='medium' (4.43s):\n",
      "  Ta position finale est 2ᵉ : en dépassant celui qui était deuxième, tu prends sa place.\n",
      "\n",
      "reasoning_effort='high' (5.95s):\n",
      "  Tu termines 2e : en dépassant le 2e coureur, tu prends sa place.\n",
      "\n",
      "[Réponse correcte: 2ème position - tu prends la place de celui que tu dépasses]\n",
      "\n",
      "Observation: Les 3 niveaux devraient donner la bonne réponse, mais 'high' peut fournir\n",
      "une explication plus détaillée. La différence est surtout visible sur des problèmes complexes.\n"
     ]
    }
   ],
   "source": [
    "probleme_logique = \"\"\"\n",
    "Dans une course, tu dépasses le 2ème. \n",
    "Quelle est ta position finale?\n",
    "Explique ton raisonnement en une phrase.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Impact du reasoning_effort ===\\n\")\n",
    "\n",
    "for effort in [\"low\", \"medium\", \"high\"]:\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"o4-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "            {\"role\": \"user\", \"content\": probleme_logique}\n",
    "        ],\n",
    "        reasoning_effort=effort\n",
    "    )\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    print(f\"reasoning_effort='{effort}' ({duration:.2f}s):\")\n",
    "    print(f\"  {response.choices[0].message.content.strip()}\")\n",
    "    print()\n",
    "\n",
    "print(\"[Réponse correcte: 2ème position - tu prends la place de celui que tu dépasses]\")\n",
    "print(\"\\nObservation: Les 3 niveaux devraient donner la bonne réponse, mais 'high' peut fournir\")\n",
    "print(\"une explication plus détaillée. La différence est surtout visible sur des problèmes complexes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Messages `developer` et contrôle du formatage\n",
    "\n",
    "### Rôle `developer`\n",
    "\n",
    "Les modèles de raisonnement (o4-mini, gpt-5-thinking) utilisent un rôle spécial : **`developer`**.\n",
    "\n",
    "- Remplace le rôle `system` des chat models\n",
    "- Définit des instructions méta (comportement, formatage)\n",
    "- Moins strict que `system`, plus flexible\n",
    "\n",
    "### \"Formatting re-enabled\"\n",
    "\n",
    "Par défaut, les modèles raisonnants retournent du texte brut. Pour activer le **markdown** (formules LaTeX, code, listes), ajoutez :\n",
    "\n",
    "```python\n",
    "{\"role\": \"developer\", \"content\": \"Formatting re-enabled\"}\n",
    "```\n",
    "\n",
    "Ceci permet :\n",
    "- Formules mathématiques : `$E = mc^2$`\n",
    "- Blocs de code avec syntax highlighting\n",
    "- Tableaux, listes, titres markdown\n",
    "\n",
    "**Sans ce message**, vous obtenez du texte brut (pas de formatage riche)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sans 'Formatting re-enabled' ===\n",
      "Énoncé du théorème  \n",
      "Dans un triangle rectangle, le carré de la longueur de l’hypoténuse (le côté opposé à l’angle droit) est égal à la somme des carrés des longueurs des deux autres côtés.\n",
      "\n",
      "Soit un triangle ABC rectangle en C, avec :  \n",
      "- AB = c (hypoténuse)  \n",
      "- BC = a  \n",
      "- AC = b  \n",
      "\n",
      "Alors le théorème de Pythagore s’écrit :  \n",
      "(1) c² = a² + b²  \n",
      "\n",
      "Démonstrations succinctes  \n",
      "\n",
      "1. Preuve par découpage \n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "=== Avec 'Formatting re-enabled' ===\n",
      "Le théorème de Pythagore, valable dans un triangle rectangle, énonce que :\n",
      "\n",
      "Soit un triangle ABC rectangle en A (l’angle en A vaut 90°). On note :\n",
      "- BC = c (l’hypoténuse, côté opposé à l’angle droit)  \n",
      "- AB = b  \n",
      "- AC = a  \n",
      "\n",
      "Alors la relation entre les longueurs des côtés est :\n",
      "\n",
      "1) Formule principale  \n",
      "   a² + b² = c²  \n",
      "\n",
      "2) Interprétation géométrique  \n",
      "   – L’aire du carré de côté a (donc a²) plus\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "Différence: Avec formatage, vous obtenez du markdown propre (formules LaTeX, code blocks, etc.)\n"
     ]
    }
   ],
   "source": [
    "# Sans \"Formatting re-enabled\"\n",
    "response_no_format = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explique le théorème de Pythagore avec des formules.\"}],\n",
    "    reasoning_effort=\"low\"\n",
    ")\n",
    "\n",
    "# Avec \"Formatting re-enabled\"\n",
    "response_with_format = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explique le théorème de Pythagore avec des formules.\"}\n",
    "    ],\n",
    "    reasoning_effort=\"low\"\n",
    ")\n",
    "\n",
    "print(\"=== Sans 'Formatting re-enabled' ===\")\n",
    "print(response_no_format.choices[0].message.content[:400])\n",
    "print(\"\\n...\\n\")\n",
    "\n",
    "print(\"\\n=== Avec 'Formatting re-enabled' ===\")\n",
    "print(response_with_format.choices[0].message.content[:400])\n",
    "print(\"\\n...\\n\")\n",
    "\n",
    "print(\"\\nDifférence: Avec formatage, vous obtenez du markdown propre (formules LaTeX, code blocks, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cas d'usage : Génération de code complexe\n",
    "\n",
    "Les modèles de raisonnement excellent dans la génération de code nécessitant :\n",
    "- Analyse algorithmique\n",
    "- Optimisation de complexité\n",
    "- Tests edge cases\n",
    "- Debugging multi-étapes\n",
    "\n",
    "### Exemple : Algorithme du plus long palindrome\n",
    "\n",
    "Problème classique d'algorithmique :\n",
    "- Approche naïve : O(n³)\n",
    "- Approche optimisée : O(n²) avec expansion autour du centre\n",
    "- Approche avancée : O(n) avec Manacher's algorithm\n",
    "\n",
    "Un modèle raisonnant peut analyser les différentes approches et choisir la meilleure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Génération de code avec o4-mini (reasoning_effort='high') ===\n",
      "\n",
      "Temps de génération: 17.29s\n",
      "\n",
      "Voici une solution en O(n²) qui, pour chaque position, « étend » un palindrome de longueur impaire et paire.  \n",
      "\n",
      "```python\n",
      "def longest_palindrome(s: str) -> str:\n",
      "    \"\"\"\n",
      "    Retourne le plus long sous-palindrome de s.\n",
      "    Complexité time O(n^2), space O(1) (hors sortie).\n",
      "    \"\"\"\n",
      "    n = len(s)\n",
      "    if n < 2:\n",
      "        return s  # \"\" ou 1 caractère est déjà un palindrome\n",
      "\n",
      "    start, max_len = 0, 1\n",
      "\n",
      "    def expand_around_center(left: int, right: int):\n",
      "        nonlocal start, max_len\n",
      "        # étend tant que les bornes sont valides et que les caractères correspondent\n",
      "        while left >= 0 and right < n and s[left] == s[right]:\n",
      "            curr_len = right - left + 1\n",
      "            if curr_len > max_len:\n",
      "                start, max_len = left, curr_len\n",
      "            left -= 1\n",
      "            right += 1\n",
      "\n",
      "    for i in range(n):\n",
      "        # cas palindrome de longueur impaire (centre sur i)\n",
      "        expand_around_center(i, i)\n",
      "        # cas palindrome de longueur paire (centre entre i et i+1)\n",
      "        expand_around_center(i, i + 1)\n",
      "\n",
      "    return s[start:start + max_len]\n",
      "\n",
      "\n",
      "# --- Tests ---------------------------------------------------\n",
      "if __name__ == \"__main__\":\n",
      "    tests = [\n",
      "        (\"\", \"\"),                    # chaîne vide\n",
      "        (\"x\", \"x\"),                  # un seul caractère\n",
      "        (\"racecar\", \"racecar\"),      # palindrome complet\n",
      "        (\"abcdef\", \"a\"),             # pas de palindrome > 1, on renvoie le 1er\n",
      "        (\"babad\", \"bab\"),            # deux choix possibles (ici \"bab\")\n",
      "    ]\n",
      "\n",
      "    for inp, expected in tests:\n",
      "        out = longest_palindrome(inp)\n",
      "        print(f\"entrée: {inp!r:10}  → sortie: {out!r:10}  (attendu: {expected!r:10})\")\n",
      "        assert out == expected, f\"Échec pour {inp!r}: attendu {expected!r}, obtenu {out!r}\"\n",
      "\n",
      "    print(\"Tous les tests sont passés ✅\")\n",
      "```\n",
      "\n",
      "============================================================\n",
      "Note: Le modèle devrait fournir :\n",
      "  1. Une implémentation avec expansion autour du centre (O(n²))\n",
      "  2. Des tests complets couvrant les edge cases\n",
      "  3. Des commentaires expliquant la logique\n"
     ]
    }
   ],
   "source": [
    "probleme_code = \"\"\"\n",
    "Écris une fonction Python qui trouve le plus long palindrome dans une chaîne.\n",
    "La fonction doit être optimisée (O(n²) maximum).\n",
    "Inclus des tests avec plusieurs cas (chaîne vide, un seul caractère, palindrome complet, pas de palindrome).\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Génération de code avec o4-mini (reasoning_effort='high') ===\\n\")\n",
    "\n",
    "start = time.time()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": probleme_code}\n",
    "    ],\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "duration = time.time() - start\n",
    "\n",
    "print(f\"Temps de génération: {duration:.2f}s\\n\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Note: Le modèle devrait fournir :\")\n",
    "print(\"  1. Une implémentation avec expansion autour du centre (O(n²))\")\n",
    "print(\"  2. Des tests complets couvrant les edge cases\")\n",
    "print(\"  3. Des commentaires expliquant la logique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Modèles de Raisonnement Avancés\n\n### Panorama des modèles raisonnants disponibles\n\n| Modèle | Disponibilité | Caractéristiques |\n|--------|---------------|------------------|\n| **o1-mini** | Disponible | Rapide, économique, bonne précision |\n| **o1-preview** | Disponible | Plus puissant, plus lent |\n| **o3-mini** | Disponible | Dernière génération, équilibré |\n| **o4-mini** | Disponible | Version optimisée 2026 |\n| **gpt-5-thinking** | Accès limité | Très avancé, pas encore public |\n\n### Recommandations pratiques\n\n- **o4-mini** : Premier choix pour la plupart des tâches de raisonnement\n- **o3-mini** : Alternative si o4-mini n'est pas disponible\n- **gpt-5-thinking** : Réservé aux cas très complexes (accès restreint)\n\n**Note :** La disponibilité des modèles dépend de votre niveau d'accès API. Les exemples ci-dessous utilisent les modèles accessibles publiquement."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "probleme_complexe = \"\"\"\nRésous ce problème d'optimisation:\n\nUn fermier veut construire un enclos rectangulaire contre un mur existant.\nIl a 100 mètres de clôture disponible (le mur fait office d'un côté).\nQuelle doit être la dimension de l'enclos pour maximiser la surface?\n\nDétaille toutes les étapes mathématiques :\n1. Définition des variables\n2. Équation de contrainte\n3. Fonction à optimiser\n4. Calcul de la dérivée\n5. Résolution et vérification\n\"\"\"\n\nprint(\"=== Modèle raisonnant sur problème d'optimisation ===\\n\")\n\n# Essayer d'abord gpt-5-thinking, fallback sur o4-mini\nmodels_to_try = [\"gpt-5-thinking\", \"o4-mini\", \"o3-mini\", \"o1-preview\"]\n\nfor model in models_to_try:\n    try:\n        print(f\"Tentative avec {model}...\")\n        start = time.time()\n        \n        # Configuration différente selon le modèle\n        if model.startswith(\"gpt-5\"):\n            response = client.chat.completions.create(\n                model=model,\n                messages=[\n                    {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n                    {\"role\": \"user\", \"content\": probleme_complexe}\n                ]\n            )\n        else:\n            response = client.chat.completions.create(\n                model=model,\n                messages=[\n                    {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n                    {\"role\": \"user\", \"content\": probleme_complexe}\n                ],\n                reasoning_effort=\"high\"\n            )\n        \n        duration = time.time() - start\n        print(f\"Succès avec {model} ! (Temps: {duration:.2f}s)\\n\")\n        print(response.choices[0].message.content)\n        break  # Sortir de la boucle si succès\n        \n    except Exception as e:\n        print(f\"  {model} non disponible: {type(e).__name__}\")\n        if model == models_to_try[-1]:\n            print(\"\\nAucun modèle raisonnant disponible. Utilisation de gpt-4o en fallback.\")\n            response = client.chat.completions.create(\n                model=DEFAULT_MODEL,\n                messages=[{\"role\": \"user\", \"content\": probleme_complexe}]\n            )\n            print(response.choices[0].message.content)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Réponse correcte: Longueur 50m (parallèle au mur), Largeur 25m, Surface 1250m²\")\nprint(\"Équation: S(x) = x(100-2x), dérivée S'(x) = 100 - 4x, max en x=25\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaison coût/performance\n",
    "\n",
    "### Tarification (approximative, vérifiez les tarifs actuels)\n",
    "\n",
    "| Modèle | Input ($/1M tokens) | Output ($/1M tokens) | Vitesse | Précision |\n",
    "|--------|---------------------|----------------------|---------|----------|\n",
    "| **gpt-4o-mini** | 0.15 | 0.60 | ⚡⚡⚡ | ⭐⭐⭐ |\n",
    "| **gpt-4o** | 2.50 | 10.00 | ⚡⚡ | ⭐⭐⭐⭐ |\n",
    "| **o4-mini** | 1.50 | 6.00 | ⚡⚡ | ⭐⭐⭐⭐⭐ (problèmes complexes) |\n",
    "| **gpt-5-thinking** | 5.00 | 20.00 | ⚡ | ⭐⭐⭐⭐⭐ (très complexe) |\n",
    "\n",
    "### Guide de choix\n",
    "\n",
    "```python\n",
    "def choisir_modele(tache):\n",
    "    if tache.type == \"conversation\" or tache.complexite == \"faible\":\n",
    "        return \"gpt-4o-mini\"  # Rapide, économique\n",
    "    \n",
    "    elif tache.type == \"generation_creative\" or tache.complexite == \"moyenne\":\n",
    "        return \"gpt-4o\"  # Équilibre qualité/vitesse\n",
    "    \n",
    "    elif tache.type in [\"math\", \"code\", \"logique\"] and tache.complexite == \"élevée\":\n",
    "        return \"o4-mini\"  # Raisonnement économique\n",
    "    \n",
    "    elif tache.complexite == \"très élevée\" or tache.precision_requise == \"maximale\":\n",
    "        return \"gpt-5-thinking\"  # Pour les cas difficiles\n",
    "    \n",
    "    else:\n",
    "        return \"gpt-4o\"  # Par défaut\n",
    "```\n",
    "\n",
    "### Tokens de raisonnement\n",
    "\n",
    "Les modèles raisonnants génèrent des **tokens de réflexion** (non visibles) avant la réponse finale.\n",
    "- `reasoning_effort=\"low\"` : ~500-1000 tokens de réflexion\n",
    "- `reasoning_effort=\"medium\"` : ~1000-3000 tokens\n",
    "- `reasoning_effort=\"high\"` : ~3000-10000+ tokens\n",
    "\n",
    "**Ces tokens sont facturés** ! Optimisez selon vos besoins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Benchmark : Chat vs Reasoning\n",
    "\n",
    "Testons les deux approches sur plusieurs problèmes types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmark Chat vs Reasoning ===\n",
      "\n",
      "Problème 1: Combien font 17 * 23?...\n",
      "  gpt-4o-mini (0.58s): 391\n",
      "  o4-mini (1.61s): 391\n",
      "  [Correct: 391]\n",
      "\n",
      "Problème 2: Si 3 chats attrapent 3 souris en 3 minutes, combien de chats...\n",
      "  gpt-4o-mini (1.20s): Il faut 3 chats.\n",
      "  o4-mini (3.02s): 3\n",
      "  [Correct: 3 chats]\n",
      "\n",
      "Problème 3: Un train part de Paris à 8h et roule à 120 km/h. Un autre pa...\n",
      "  gpt-4o-mini (1.73s): Ils se croisent à 10h30.\n",
      "  o4-mini (8.60s): 10 h 13 min 20 s\n",
      "  [Correct: environ 10h30]\n",
      "\n",
      "\n",
      "=== Statistiques ===\n",
      "Temps moyen gpt-4o-mini: 1.17s\n",
      "Temps moyen o4-mini: 4.41s\n",
      "\n",
      "Observation: Les modèles raisonnants sont plus lents mais généralement plus précis\n",
      "sur les problèmes nécessitant plusieurs étapes de calcul.\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "problemes = [\n",
    "    (\"Combien font 17 * 23?\", \"391\"),\n",
    "    (\"Si 3 chats attrapent 3 souris en 3 minutes, combien de chats faut-il pour attraper 100 souris en 100 minutes?\", \"3 chats\"),\n",
    "    (\"Un train part de Paris à 8h et roule à 120 km/h. Un autre part de Lyon (450km) à 9h à 150 km/h vers Paris. À quelle heure se croisent-ils?\", \"environ 10h30\")\n",
    "]\n",
    "\n",
    "print(\"=== Benchmark Chat vs Reasoning ===\\n\")\n",
    "\n",
    "temps_chat = []\n",
    "temps_reasoning = []\n",
    "\n",
    "for i, (prob, correct) in enumerate(problemes):\n",
    "    print(f\"Problème {i+1}: {prob[:60]}...\")\n",
    "    \n",
    "    # Chat model\n",
    "    start = time.time()\n",
    "    resp_chat = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prob + \" Réponds uniquement avec le résultat.\"}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    t_chat = time.time() - start\n",
    "    temps_chat.append(t_chat)\n",
    "    \n",
    "    # Reasoning model\n",
    "    start = time.time()\n",
    "    resp_reason = client.chat.completions.create(\n",
    "        model=\"o4-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prob + \" Réponds uniquement avec le résultat.\"}],\n",
    "        reasoning_effort=\"medium\"\n",
    "    )\n",
    "    t_reason = time.time() - start\n",
    "    temps_reasoning.append(t_reason)\n",
    "    \n",
    "    print(f\"  gpt-4o-mini ({t_chat:.2f}s): {resp_chat.choices[0].message.content.strip()}\")\n",
    "    print(f\"  o4-mini ({t_reason:.2f}s): {resp_reason.choices[0].message.content.strip()}\")\n",
    "    print(f\"  [Correct: {correct}]\\n\")\n",
    "\n",
    "print(\"\\n=== Statistiques ===\")\n",
    "print(f\"Temps moyen gpt-4o-mini: {statistics.mean(temps_chat):.2f}s\")\n",
    "print(f\"Temps moyen o4-mini: {statistics.mean(temps_reasoning):.2f}s\")\n",
    "print(f\"\\nObservation: Les modèles raisonnants sont plus lents mais généralement plus précis\")\n",
    "print(f\"sur les problèmes nécessitant plusieurs étapes de calcul.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exemple pratique : Débogage de code\n",
    "\n",
    "Les modèles de raisonnement excellent dans le debugging complexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Débogage avec o4-mini (reasoning_effort='high') ===\n",
      "\n",
      "Voici une analyse point par point et une version optimisée :\n",
      "\n",
      "1. Problème de performance  \n",
      "   - La fonction fibonacci fait deux appels récursifs à chaque niveau.  \n",
      "   - Elle recalcule sans cesse les mêmes valeurs de Fibonacci.\n",
      "\n",
      "2. Pourquoi c’est lent (complexité)  \n",
      "   - L’arbre d’appels récursifs double quasiment à chaque niveau.  \n",
      "   - En notation Big-O, la complexité est O(2ⁿ).  \n",
      "   - Pour n = 35–40, le nombre d’appels explose (plusieurs dizaines de millions).\n",
      "\n",
      "3. Solution optimisée  \n",
      "   - utiliser un cache (mémoisation) pour ne calculer chaque F(k) qu’une seule fois, ou  \n",
      "   - passer à une version itérative en O(n) et O(1) d’espace supplémentaire.\n",
      "\n",
      "4. Code corrigé  \n",
      "\n",
      "Option A – avec mémoisation automatique :\n",
      "\n",
      "```python\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache(maxsize=None)\n",
      "def fibonacci(n):\n",
      "    \"\"\"Calcule le n-ième nombre de Fibonacci en mémoisant.\"\"\"\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "# Test\n",
      "for i in range(40):\n",
      "    print(f\"F({i}) = {fibonacci(i)}\")\n",
      "```\n",
      "\n",
      "Option B – version itérative (la plus simple et la plus rapide) :\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"Calcule le n-ième nombre de Fibonacci en O(n).\"\"\"\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    a, b = 0, 1\n",
      "    for _ in range(1, n):\n",
      "        a, b = b, a + b\n",
      "    return b\n",
      "\n",
      "# Test\n",
      "for i in range(40):\n",
      "    print(f\"F({i}) = {fibonacci(i)}\")\n",
      "```\n",
      "\n",
      "Les deux approches réduisent la complexité à O(n) et sont instantanées même pour n très grand (jusqu’à plusieurs millions sans problème de temps, sous réserve de la taille des entiers).\n",
      "\n",
      "============================================================\n",
      "Le modèle devrait identifier:\n",
      "  - Complexité O(2^n) due aux appels récursifs redondants\n",
      "  - Solution: mémoïsation ou approche itérative (O(n))\n"
     ]
    }
   ],
   "source": [
    "code_buggue = '''\n",
    "def fibonacci(n):\n",
    "    \"\"\"Calcule le n-ième nombre de Fibonacci.\"\"\"\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "# Test\n",
    "for i in range(40):\n",
    "    print(f\"F({i}) = {fibonacci(i)}\")\n",
    "'''\n",
    "\n",
    "prompt_debug = f\"\"\"\n",
    "Analyse ce code Python et identifie le(s) problème(s) :\n",
    "\n",
    "```python\n",
    "{code_buggue}\n",
    "```\n",
    "\n",
    "Le code fonctionne mais devient TRÈS lent pour n >= 35.\n",
    "\n",
    "1. Identifie le problème de performance\n",
    "2. Explique pourquoi (complexité algorithmique)\n",
    "3. Propose une solution optimisée\n",
    "4. Fournis le code corrigé\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Débogage avec o4-mini (reasoning_effort='high') ===\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Formatting re-enabled\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_debug}\n",
    "    ],\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Le modèle devrait identifier:\")\n",
    "print(\"  - Complexité O(2^n) due aux appels récursifs redondants\")\n",
    "print(\"  - Solution: mémoïsation ou approche itérative (O(n))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Limites et considérations\n",
    "\n",
    "### Limites des modèles de raisonnement\n",
    "\n",
    "1. **Temps de réponse** : Peuvent être lents (30-60s+) pour `reasoning_effort=\"high\"`\n",
    "2. **Coût** : Tokens de réflexion facturés même s'ils ne sont pas visibles\n",
    "3. **Pas toujours nécessaires** : Sur-utiliser peut gaspiller temps et argent\n",
    "4. **Pas magiques** : Ne garantissent pas la correction (vérifiez toujours les résultats)\n",
    "\n",
    "### Bonnes pratiques\n",
    "\n",
    "1. **Commencez simple** : Testez d'abord avec un chat model\n",
    "2. **Montez progressivement** : gpt-4o-mini → gpt-4o → o4-mini → gpt-5-thinking\n",
    "3. **Ajustez reasoning_effort** : Commencez par \"low\", augmentez si nécessaire\n",
    "4. **Validez les résultats** : Surtout pour le code et les calculs critiques\n",
    "5. **Mesurez le ROI** : Le temps/coût supplémentaire est-il justifié ?\n",
    "\n",
    "### Cas où les chat models suffisent\n",
    "\n",
    "- Conversations naturelles\n",
    "- Résumés et synthèses\n",
    "- Traduction simple\n",
    "- Génération de contenu créatif\n",
    "- Questions factuelles directes\n",
    "- Code simple (CRUD, scripts basiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion et ressources\n",
    "\n",
    "### Points clés à retenir\n",
    "\n",
    "1. **Modèles de raisonnement** = temps de réflexion + meilleure précision sur tâches complexes\n",
    "2. **o4-mini** : Excellent rapport qualité/prix pour raisonnement standard\n",
    "3. **gpt-5-thinking** : Pour les problèmes très complexes nécessitant raisonnement profond\n",
    "4. **reasoning_effort** : Contrôle le compromis temps/qualité (low/medium/high)\n",
    "5. **Messages developer** : Remplacent `system`, `\"Formatting re-enabled\"` active le markdown\n",
    "6. **Choisir judicieusement** : Ne pas sur-utiliser les modèles raisonnants (coût)\n",
    "\n",
    "### Tableau récapitulatif\n",
    "\n",
    "| Tâche | Modèle recommandé | reasoning_effort |\n",
    "|-------|-------------------|------------------|\n",
    "| Conversation | gpt-4o-mini | N/A |\n",
    "| Code simple | gpt-4o | N/A |\n",
    "| Algorithme complexe | o4-mini | medium |\n",
    "| Optimisation math | o4-mini | high |\n",
    "| Preuve formelle | gpt-5-thinking | N/A (auto) |\n",
    "| Debugging approfondi | o4-mini ou gpt-5-thinking | high |\n",
    "\n",
    "### Exercices suggérés\n",
    "\n",
    "1. **Comparaison** : Testez un problème de logique avec gpt-4o-mini vs o4-mini\n",
    "2. **Optimisation** : Demandez à o4-mini d'optimiser un algorithme de tri\n",
    "3. **Reasoning effort** : Comparez low/medium/high sur un problème d'optimisation\n",
    "4. **Code generation** : Demandez l'implémentation d'un algorithme complexe (Dijkstra, A*)\n",
    "5. **Multi-étapes** : Problème nécessitant analyse → planification → exécution\n",
    "\n",
    "### Ressources\n",
    "\n",
    "- [Documentation OpenAI - Reasoning Models](https://platform.openai.com/docs/guides/reasoning)\n",
    "- [Guide des tarifs](https://openai.com/pricing)\n",
    "- [Best practices pour o4-mini](https://platform.openai.com/docs/guides/reasoning/best-practices)\n",
    "- Notebook suivant : **9_Vision_Models.ipynb** (GPT-4 Vision, analyse d'images)\n",
    "\n",
    "### Prochaines étapes\n",
    "\n",
    "Dans le prochain notebook, nous explorerons les **modèles de vision** :\n",
    "- GPT-4 Vision (gpt-4o avec images)\n",
    "- Analyse de screenshots, diagrammes, graphiques\n",
    "- Extraction de texte (OCR)\n",
    "- Génération de descriptions d'images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}