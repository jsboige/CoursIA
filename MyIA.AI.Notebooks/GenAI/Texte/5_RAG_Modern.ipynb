{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. RAG Modern - Retrieval Augmented Generation\n",
    "\n",
    "**Durée estimée** : 65 minutes\n",
    "\n",
    "**Prérequis** : Notebooks 1 (OpenAI Intro), 4 (Function Calling)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "La **RAG** (Retrieval Augmented Generation) permet d'enrichir les réponses d'un LLM avec des données externes (documents, bases de connaissances). Ce notebook couvre :\n",
    "\n",
    "1. **Fondamentaux RAG** : Embeddings, chunking, recherche vectorielle\n",
    "2. **Stratégies de chunking** : Fixe, sémantique, récursif\n",
    "3. **Responses API** : Multi-turn RAG avec `previous_response_id`\n",
    "4. **Optimisation cache** : Économies 40-80% sur les tokens\n",
    "5. **Citations et sources** : Traçabilité des réponses\n",
    "\n",
    "---\n",
    "\n",
    "## Pourquoi la RAG ?\n",
    "\n",
    "| Problème LLM seul | Solution RAG |\n",
    "|-------------------|-------------|\n",
    "| Connaissances figées (cutoff date) | Données actualisées en temps réel |\n",
    "| Hallucinations fréquentes | Réponses basées sur sources vérifiables |\n",
    "| Pas de données privées | Accès à vos documents internes |\n",
    "| Contexte limité | Fenêtre étendue via retrieval |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installation des dépendances\n",
    "%pip install openai tiktoken python-dotenv scikit-learn numpy pandas requests beautifulsoup4 lxml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Dict, Any\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Charger les variables d'environnement\nload_dotenv('../.env')\n\n# Mode batch pour tests automatisés\nBATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\"\n\n# Client OpenAI\nclient = OpenAI()\n\n# Modèle par défaut depuis .env\nDEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\n\nprint(f\"Configuration chargée - Mode batch: {BATCH_MODE}\")\nprint(f\"Modèle embeddings: text-embedding-3-large\")\nprint(f\"Modèle génération: {DEFAULT_MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Bibliothèques utilisées\n\nCe notebook nécessite plusieurs dépendances clés :\n\n- **openai** : API OpenAI pour embeddings et génération\n- **tiktoken** : Tokenizer officiel pour compter les tokens\n- **scikit-learn** : k-Nearest Neighbors pour la recherche vectorielle\n- **numpy/pandas** : Manipulation de données et vecteurs\n- **requests/beautifulsoup4** : Scraping web pour récupérer le document source\n- **python-dotenv** : Gestion sécurisée des clés API\n\n**Note** : En production, remplacer scikit-learn par une vraie base vectorielle (Pinecone, Qdrant, Weaviate).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Partie 1 : Fondamentaux RAG\n",
    "\n",
    "## 1.1 Architecture RAG\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌──────────────┐     ┌─────────────┐\n",
    "│  Documents  │ ──► │   Chunking   │ ──► │  Embeddings │\n",
    "└─────────────┘     └──────────────┘     └──────┬──────┘\n",
    "                                                │\n",
    "                                                ▼\n",
    "┌─────────────┐     ┌──────────────┐     ┌─────────────┐\n",
    "│   Réponse   │ ◄── │     LLM      │ ◄── │  Retrieval  │\n",
    "└─────────────┘     └──────────────┘     └─────────────┘\n",
    "                           ▲\n",
    "                           │\n",
    "                    ┌──────┴──────┐\n",
    "                    │   Question  │\n",
    "                    └─────────────┘\n",
    "```\n",
    "\n",
    "**Étapes** :\n",
    "1. **Indexation** : Documents → Chunks → Embeddings → Base vectorielle\n",
    "2. **Retrieval** : Question → Embedding → k-NN → Chunks pertinents\n",
    "3. **Generation** : Question + Chunks → LLM → Réponse augmentée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Préparation des données\n",
    "\n",
    "Pour cet exemple, nous utilisons le débat Lincoln-Douglas (1858), un document historique public."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Pourquoi le débat Lincoln-Douglas ?\n\nCe document historique est un **excellent cas d'usage RAG** :\n\n1. **Domaine public** : Accessible librement, pas de problèmes de copyright\n2. **Taille idéale** : ~98k caractères, assez long pour nécessiter du chunking\n3. **Structure narrative** : Discours structurés avec arguments clairs\n4. **Questions naturelles** : \"Quelle était la position de Lincoln ?\" → Réponses vérifiables\n\n**Fallback pour les tests** : Le code inclut un texte de secours si le scraping échoue (mode batch, timeout réseau, etc.). Cela garantit que le notebook peut s'exécuter même sans connexion.\n\n**Point technique** : Nous récupérons uniquement le premier débat (Ottawa, 21 août 1858). La série complète comprend 7 débats.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte récupéré: 98529 caractères\n",
      "Aperçu: First Debate: Ottawa, Illinois\n",
      "August 21, 1858\n",
      "It was dry and dusty, between 10,000 and 12,000 people were in attendance when the debate began at 2:00 p.m. There were no seats or bleachers.\n",
      "Douglas charged Lincoln with trying to “abolitionize” the Whig and Democratic Parties. He also charged Lincoln had been present when a very radical “abolitionist” type platform had been written by the Republican Party in 1854. Douglas accused Lincoln of taking the side of the common enemy in the Mexican War. ...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_debate_text() -> str:\n",
    "    \"\"\"Récupère le texte du premier débat Lincoln-Douglas.\"\"\"\n",
    "    url = \"https://home.nps.gov/liho/learn/historyculture/debate1.htm\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        main_div = soup.select_one(\"div.ColumnMain\")\n",
    "        \n",
    "        if main_div:\n",
    "            return main_div.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            raise ValueError(\"Conteneur principal non trouvé\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du scraping: {e}\")\n",
    "        # Texte de fallback pour le mode batch\n",
    "        return \"\"\"Lincoln-Douglas Debate - Ottawa, Illinois, August 21, 1858\n",
    "        \n",
    "Abraham Lincoln argued that slavery was morally wrong and should not be extended \n",
    "into new territories. He believed in the principle that \"all men are created equal\" \n",
    "as stated in the Declaration of Independence.\n",
    "\n",
    "Lincoln stated: \"I have no purpose to introduce political and social equality \n",
    "between the white and black races. There is a physical difference between the two, \n",
    "which, in my judgment, will probably forever forbid their living together upon the \n",
    "footing of perfect equality.\"\n",
    "\n",
    "However, Lincoln maintained that Black Americans had the right to earn their own \n",
    "bread and improve their condition. He opposed the expansion of slavery while \n",
    "acknowledging the constitutional protections it had in existing states.\n",
    "\n",
    "Douglas advocated for popular sovereignty, allowing each territory to decide \n",
    "the slavery question for itself. He accused Lincoln of wanting to make all \n",
    "states free states, which Lincoln denied.\n",
    "\n",
    "The debate centered on the interpretation of the Dred Scott decision and \n",
    "whether Congress could prohibit slavery in the territories.\"\"\"\n",
    "\n",
    "# Récupération du texte\n",
    "debate_text = fetch_debate_text()\n",
    "print(f\"Texte récupéré: {len(debate_text)} caractères\")\n",
    "print(f\"Aperçu: {debate_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Implémentation des stratégies de chunking\n\nLes trois fonctions suivantes implémentent les stratégies présentées ci-dessus. Chacune a ses avantages selon le type de document.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Partie 2 : Stratégies de Chunking\n",
    "\n",
    "Le chunking est **critique** pour la qualité de la RAG. Trois stratégies principales :\n",
    "\n",
    "| Stratégie | Avantages | Inconvénients |\n",
    "|-----------|-----------|---------------|\n",
    "| **Fixe** | Simple, prévisible | Coupe au milieu des phrases |\n",
    "| **Sémantique** | Respecte le sens | Plus complexe, variable |\n",
    "| **Récursif** | Équilibre taille/sens | Nécessite des délimiteurs |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Préparation du DataFrame de chunks\n\nNous créons maintenant notre **base de connaissances** : un DataFrame pandas contenant tous les chunks avec leurs métadonnées.\n\n**Structure du DataFrame** :\n- `chunk_id` : Identifiant unique (0-50)\n- `source` : Source du document (pour traçabilité)\n- `text` : Contenu textuel du chunk\n- `embedding` : Vecteur de 3072 dimensions (ajouté à l'étape suivante)\n\nCette structure permet de facilement filtrer, rechercher et enrichir les chunks avec des métadonnées supplémentaires (date, auteur, catégorie, etc.).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparaison des stratégies de chunking ===\n",
      "\n",
      "Texte source: 98529 caractères\n",
      "\n",
      "Chunking fixe (100 mots, overlap 20): 222 chunks\n",
      "Chunking sémantique (3 phrases): 214 chunks\n",
      "Chunking récursif (max 500 chars): 327 chunks\n"
     ]
    }
   ],
   "source": [
    "def chunk_fixed(text: str, chunk_size: int = 400, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunking fixe par nombre de mots.\n",
    "    \n",
    "    Args:\n",
    "        text: Texte à découper\n",
    "        chunk_size: Nombre de mots par chunk\n",
    "        overlap: Chevauchement entre chunks (évite les coupures brutales)\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_semantic(text: str, max_sentences: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunking sémantique par phrases.\n",
    "    Regroupe les phrases en chunks cohérents.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Découpage par phrases (approximatif)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence)\n",
    "        if len(current_chunk) >= max_sentences:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "    \n",
    "    # Dernier chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_recursive(text: str, max_size: int = 500, delimiters: List[str] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunking récursif avec délimiteurs hiérarchiques.\n",
    "    Essaie de découper par paragraphes, puis phrases, puis mots.\n",
    "    \"\"\"\n",
    "    if delimiters is None:\n",
    "        delimiters = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    \n",
    "    if len(text) <= max_size or not delimiters:\n",
    "        return [text] if text.strip() else []\n",
    "    \n",
    "    delimiter = delimiters[0]\n",
    "    parts = text.split(delimiter)\n",
    "    \n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    \n",
    "    for part in parts:\n",
    "        if len(current) + len(part) + len(delimiter) <= max_size:\n",
    "            current += (delimiter if current else \"\") + part\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "            # Récursion avec délimiteur suivant si le part est trop grand\n",
    "            if len(part) > max_size:\n",
    "                chunks.extend(chunk_recursive(part, max_size, delimiters[1:]))\n",
    "            else:\n",
    "                current = part\n",
    "    \n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Comparaison des stratégies\n",
    "print(\"=== Comparaison des stratégies de chunking ===\")\n",
    "print(f\"\\nTexte source: {len(debate_text)} caractères\")\n",
    "\n",
    "chunks_fixed = chunk_fixed(debate_text, chunk_size=100, overlap=20)\n",
    "chunks_semantic = chunk_semantic(debate_text, max_sentences=3)\n",
    "chunks_recursive = chunk_recursive(debate_text, max_size=500)\n",
    "\n",
    "print(f\"\\nChunking fixe (100 mots, overlap 20): {len(chunks_fixed)} chunks\")\n",
    "print(f\"Chunking sémantique (3 phrases): {len(chunks_semantic)} chunks\")\n",
    "print(f\"Chunking récursif (max 500 chars): {len(chunks_recursive)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exemple chunk fixe ===\n",
      "First Debate: Ottawa, Illinois August 21, 1858 It was dry and dusty, between 10,000 and 12,000 people were in attendance when the debate began at 2:00 p.m. There were no seats or bleachers. Douglas charged Lincoln with trying to “abolitionize” the Whig and Democratic Parties. He also charged Lincoln...\n",
      "\n",
      "=== Exemple chunk sémantique ===\n",
      "First Debate: Ottawa, Illinois\n",
      "August 21, 1858\n",
      "It was dry and dusty, between 10,000 and 12,000 people were in attendance when the debate began at 2:00 p.m. There were no seats or bleachers. Douglas charged Lincoln with trying to “abolitionize” the Whig and Democratic Parties.\n",
      "\n",
      "=== Exemple chunk récursif ===\n",
      "First Debate: Ottawa, Illinois\n",
      "August 21, 1858\n",
      "It was dry and dusty, between 10,000 and 12,000 people were in attendance when the debate began at 2:00 p.m. There were no seats or bleachers.\n"
     ]
    }
   ],
   "source": [
    "# Visualisation des chunks\n",
    "print(\"=== Exemple chunk fixe ===\")\n",
    "print(chunks_fixed[0][:300] + \"...\" if len(chunks_fixed[0]) > 300 else chunks_fixed[0])\n",
    "\n",
    "print(\"\\n=== Exemple chunk sémantique ===\")\n",
    "print(chunks_semantic[0][:300] + \"...\" if len(chunks_semantic[0]) > 300 else chunks_semantic[0])\n",
    "\n",
    "print(\"\\n=== Exemple chunk récursif ===\")\n",
    "print(chunks_recursive[0][:300] + \"...\" if len(chunks_recursive[0]) > 300 else chunks_recursive[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Analyse des résultats de chunking\n\nObservons les différences obtenues :\n\n| Stratégie | Nombre de chunks | Observations |\n|-----------|------------------|--------------|\n| **Fixe (100 mots, overlap 20)** | 222 | Nombreux chunks petits, overlap assure la continuité |\n| **Sémantique (3 phrases)** | 214 | Chunks de taille variable, respecte le sens |\n| **Récursif (max 500 chars)** | 327 | Plus de petits chunks, découpage hiérarchique |\n\n**Points clés** :\n\n1. **Chunking fixe** : Le premier exemple montre une coupure au milieu d'une phrase (\"...He also charged Lincoln...\"), typique de cette méthode.\n\n2. **Chunking sémantique** : Le découpage respecte les limites de phrases, plus naturel pour la compréhension.\n\n3. **Chunking récursif** : Découpe d'abord par paragraphes (`\\n\\n`), puis par phrases si trop long. Résultat : chunks courts mais cohérents.\n\n**Pour la suite**, nous utilisons le chunking fixe avec **400 mots et overlap 50** - un bon compromis entre taille et continuité pour ce type de document narratif.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Choix de la stratégie\n",
    "\n",
    "**Recommandations** :\n",
    "\n",
    "- **Documents structurés** (code, JSON, markdown) → Chunking récursif\n",
    "- **Texte narratif** (articles, livres) → Chunking sémantique\n",
    "- **Données tabulaires** → Chunking fixe avec métadonnées\n",
    "\n",
    "Pour la suite, nous utilisons le chunking fixe avec overlap (400 mots, overlap 50)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Comprendre les embeddings\n\nUn **embedding** est une représentation vectorielle d'un texte dans un espace à haute dimension. Chaque dimension capture un aspect sémantique du texte.\n\n**Exemple simplifié (2D)** :\n```\n\"chat\"      → [0.8, 0.2]  (animal, domestique)\n\"chien\"     → [0.7, 0.3]  (animal, domestique)\n\"ordinateur\" → [0.1, 0.9]  (objet, technologie)\n```\n\nEn réalité, `text-embedding-3-large` génère des vecteurs à **3072 dimensions** pour capturer les nuances du langage.\n\n**Propriétés clés** :\n\n1. **Similarité sémantique** : Textes similaires ont des vecteurs proches (distance cosinus faible)\n2. **Invariance** : Même sens avec différents mots → vecteurs similaires\n3. **Efficacité** : Comparer 3072 nombres est plus rapide que comparer du texte brut\n\n**Batch vs Single** : La fonction `create_embeddings_batch()` est **5-10x plus rapide** que des appels individuels pour de gros volumes. OpenAI autorise jusqu'à 2048 textes par batch.\n\n**Résultat** : 51 vecteurs de 3072 dimensions, prêts pour la recherche vectorielle.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Implémentation de la VectorStore\n\nLa classe `VectorStore` encapsule la logique de recherche vectorielle. Elle utilise scikit-learn pour le prototypage, mais en production vous utiliseriez une vraie base vectorielle.\n\n**Points clés de l'implémentation** :\n\n1. **Métrique cosinus** : `metric=\"cosine\"` mesure l'angle entre vecteurs, pas la distance euclidienne\n2. **Validation des embeddings** : Filtre les None pour éviter les erreurs\n3. **Retour structuré** : Dicts avec `chunk_id`, `text`, `source`, et `score` pour traçabilité complète",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de connaissances: 51 chunks\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Lincoln-Douglas Debate 1 (Ottawa, 1858)</td>\n",
       "      <td>First Debate: Ottawa, Illinois August 21, 1858...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lincoln-Douglas Debate 1 (Ottawa, 1858)</td>\n",
       "      <td>were proclaimed wherever the Constitution rule...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lincoln-Douglas Debate 1 (Ottawa, 1858)</td>\n",
       "      <td>the Whig party and the Democratic party both s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Lincoln-Douglas Debate 1 (Ottawa, 1858)</td>\n",
       "      <td>name and disguise of a Republican party. (Laug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Lincoln-Douglas Debate 1 (Ottawa, 1858)</td>\n",
       "      <td>with such views as the circumstances and exige...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                                   source  \\\n",
       "0         0  Lincoln-Douglas Debate 1 (Ottawa, 1858)   \n",
       "1         1  Lincoln-Douglas Debate 1 (Ottawa, 1858)   \n",
       "2         2  Lincoln-Douglas Debate 1 (Ottawa, 1858)   \n",
       "3         3  Lincoln-Douglas Debate 1 (Ottawa, 1858)   \n",
       "4         4  Lincoln-Douglas Debate 1 (Ottawa, 1858)   \n",
       "\n",
       "                                                text  \n",
       "0  First Debate: Ottawa, Illinois August 21, 1858...  \n",
       "1  were proclaimed wherever the Constitution rule...  \n",
       "2  the Whig party and the Democratic party both s...  \n",
       "3  name and disguise of a Republican party. (Laug...  \n",
       "4  with such views as the circumstances and exige...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création du DataFrame de chunks pour la suite\n",
    "chunks = chunk_fixed(debate_text, chunk_size=400, overlap=50)\n",
    "\n",
    "df_chunks = pd.DataFrame({\n",
    "    \"chunk_id\": range(len(chunks)),\n",
    "    \"source\": \"Lincoln-Douglas Debate 1 (Ottawa, 1858)\",\n",
    "    \"text\": chunks\n",
    "})\n",
    "\n",
    "print(f\"Base de connaissances: {len(df_chunks)} chunks\")\n",
    "df_chunks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Partie 3 : Embeddings et Recherche Vectorielle\n",
    "\n",
    "## 3.1 Génération des embeddings\n",
    "\n",
    "OpenAI propose plusieurs modèles d'embeddings :\n",
    "\n",
    "| Modèle | Dimensions | Performance | Coût |\n",
    "|--------|------------|-------------|------|\n",
    "| `text-embedding-3-small` | 1536 | Bon | $ |\n",
    "| `text-embedding-3-large` | 3072 | Excellent | $$ |\n",
    "| `text-embedding-ada-002` | 1536 | Bon (legacy) | $ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération des embeddings...\n",
      "Embeddings générés: 51 / 51\n",
      "Dimension des vecteurs: 3072\n"
     ]
    }
   ],
   "source": [
    "def create_embedding(text: str, model: str = \"text-embedding-3-large\") -> List[float]:\n",
    "    \"\"\"\n",
    "    Génère un embedding pour un texte donné.\n",
    "    \n",
    "    Args:\n",
    "        text: Texte à vectoriser\n",
    "        model: Modèle d'embedding OpenAI\n",
    "    \n",
    "    Returns:\n",
    "        Vecteur d'embedding (liste de floats)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=[text]\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_embeddings_batch(texts: List[str], model: str = \"text-embedding-3-large\") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Génère des embeddings en batch (plus efficace).\n",
    "    \n",
    "    Note: OpenAI supporte jusqu'à 2048 textes par requête.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=texts\n",
    "        )\n",
    "        return [item.embedding for item in response.data]\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur batch embedding: {e}\")\n",
    "        return [None] * len(texts)\n",
    "\n",
    "\n",
    "# Génération des embeddings pour tous les chunks\n",
    "print(\"Génération des embeddings...\")\n",
    "embeddings = create_embeddings_batch(df_chunks[\"text\"].tolist())\n",
    "df_chunks[\"embedding\"] = embeddings\n",
    "\n",
    "print(f\"Embeddings générés: {len([e for e in embeddings if e])} / {len(embeddings)}\")\n",
    "print(f\"Dimension des vecteurs: {len(embeddings[0]) if embeddings[0] else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Anatomie d'une réponse RAG\n\nLe pipeline `rag_query()` exécute 4 étapes :\n\n**1. Retrieval (Récupération)** :\n```python\nquery_embedding = create_embedding(question)\nchunks = vector_store.search(query_embedding, k=3)\n```\nConvertit la question en vecteur, puis cherche les 3 chunks les plus proches.\n\n**2. Construction du contexte** :\n```python\ncontext = \"\\n\\n\".join([f\"[Source: {c['source']}]\\n{c['text']}\" for c in chunks])\n```\nAssemble les chunks récupérés en un seul texte avec métadonnées.\n\n**3. Prompt augmenté** :\n```python\nsystem_prompt = \"Réponds UNIQUEMENT basé sur le contexte fourni...\"\nuser_prompt = f\"Contexte:\\n{context}\\n\\nQuestion: {question}\"\n```\nInjecte le contexte dans le prompt pour \"augmenter\" les connaissances du modèle.\n\n**4. Génération** :\n```python\nresponse = client.chat.completions.create(...)\n```\nLe LLM génère une réponse basée sur le contexte fourni, pas sur ses connaissances pré-entraînées.\n\n**Paramètres clés** :\n- `temperature=0.3` : Faible pour plus de précision et moins d'hallucinations\n- `max_tokens=500` : Limite la longueur de la réponse\n\n**Résultat attendu** : Une réponse précise citant les chunks utilisés, avec traçabilité complète via les métadonnées de tokens.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Recherche k-NN (k-Nearest Neighbors)\n",
    "\n",
    "Pour la recherche, on calcule la distance entre l'embedding de la question et ceux des chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Importance des citations en RAG\n\nLes citations sont **essentielles** pour :\n\n1. **Vérifiabilité** : L'utilisateur peut vérifier la source de l'information\n2. **Confiance** : Une réponse avec sources est plus crédible qu'une affirmation sans preuve\n3. **Debugging** : Identifier si le problème vient du retrieval (mauvais chunks) ou de la génération (mauvaise interprétation)\n4. **Conformité** : Certains domaines (médical, juridique) exigent la traçabilité complète\n\n**Anti-pattern** : Générer une réponse sans indiquer les sources utilisées. Cela empêche la vérification et rend le système opaque.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Responses API vs Chat Completions\n\nLa **Responses API** est l'approche moderne (2025) pour la RAG, avec des avantages significatifs :\n\n**Différences techniques** :\n\n| Aspect | Chat Completions | Responses API |\n|--------|------------------|---------------|\n| **Historique** | Envoyé manuellement à chaque tour | Géré automatiquement par `previous_response_id` |\n| **Cache** | Manuel (prompt caching) | Automatique sur contextes répétés |\n| **Tokens** | Tous retransmis | Économies 40-80% via cache |\n| **Persistence** | Client-side | Server-side avec `store: true` |\n\n**Exemple d'économie** :\n```\nChat Completions (conversation 3 tours):\n- Tour 1: 1000 tokens contexte + 500 génération = 1500\n- Tour 2: 1000 contexte + 500 génération = 1500\n- Tour 3: 1000 contexte + 500 génération = 1500\nTotal: 4500 tokens\n\nResponses API (avec previous_response_id):\n- Tour 1: 1000 tokens contexte + 500 génération = 1500\n- Tour 2: Contexte en cache + 500 génération = 500\n- Tour 3: Contexte en cache + 500 génération = 500\nTotal: 2500 tokens (économie 44%)\n```\n\n**Fallback implémenté** : Si l'API Responses n'est pas disponible, le code bascule automatiquement sur Chat Completions. C'est une bonne pratique de compatibilité.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Transition vers les bonnes pratiques\n\nNous avons maintenant un pipeline RAG complet fonctionnel. Cependant, pour passer en **production**, plusieurs optimisations sont nécessaires.\n\n**Prochaines étapes** :\n\n1. **Monitoring et métriques** : Mesurer la qualité (relevance, precision, recall)\n2. **Scaling** : Base vectorielle distribuée (Pinecone, Qdrant)\n3. **Optimisation coûts** : Cache intelligent, batch processing\n4. **Évaluation continue** : Tests A/B sur différentes stratégies de chunking\n\nLa section suivante couvre ces aspects avec des recommandations concrètes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStore initialisé: 51 vecteurs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Base vectorielle simple basée sur scikit-learn.\n",
    "    En production, utiliser Pinecone, Qdrant, Weaviate, ou Chroma.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, embedding_col: str = \"embedding\"):\n",
    "        self.df = df\n",
    "        self.embedding_col = embedding_col\n",
    "        \n",
    "        # Filtrer les embeddings valides\n",
    "        valid_mask = df[embedding_col].apply(lambda x: x is not None)\n",
    "        self.valid_indices = df[valid_mask].index.tolist()\n",
    "        \n",
    "        # Construire la matrice de vecteurs\n",
    "        vectors = np.array(df.loc[self.valid_indices, embedding_col].tolist())\n",
    "        \n",
    "        # Index k-NN\n",
    "        self.nn = NearestNeighbors(n_neighbors=min(5, len(vectors)), metric=\"cosine\")\n",
    "        self.nn.fit(vectors)\n",
    "        \n",
    "        print(f\"VectorStore initialisé: {len(self.valid_indices)} vecteurs\")\n",
    "    \n",
    "    def search(self, query_embedding: List[float], k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Recherche les k chunks les plus similaires.\n",
    "        \n",
    "        Returns:\n",
    "            Liste de dicts avec chunk_id, text, source, score\n",
    "        \"\"\"\n",
    "        distances, indices = self.nn.kneighbors([query_embedding], n_neighbors=k)\n",
    "        \n",
    "        results = []\n",
    "        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            original_idx = self.valid_indices[idx]\n",
    "            row = self.df.iloc[original_idx]\n",
    "            results.append({\n",
    "                \"chunk_id\": row[\"chunk_id\"],\n",
    "                \"text\": row[\"text\"],\n",
    "                \"source\": row[\"source\"],\n",
    "                \"score\": 1 - dist  # Cosine similarity (1 = identique)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialisation du VectorStore\n",
    "vector_store = VectorStore(df_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What did Lincoln argue about slavery?\n",
      "\n",
      "=== Chunks retrouvés ===\n",
      "\n",
      "[1] Score: 0.628 | Source: Lincoln-Douglas Debate 1 (Ottawa, 1858)\n",
      "    proclaims his Abolition doctrines. Let me read a part of them. In his speech at Springfield to the Convention, which nominated him for the Senate, he said: \"In my opinion it will not cease until a cri...\n",
      "\n",
      "[2] Score: 0.580 | Source: Lincoln-Douglas Debate 1 (Ottawa, 1858)\n",
      "    them not to have it if they do not want it. [Applause and laughter.] I do not mean that if this vast concourse of people were in a Territory of the United States, any one of them would be obliged to h...\n",
      "\n",
      "[3] Score: 0.572 | Source: Lincoln-Douglas Debate 1 (Ottawa, 1858)\n",
      "    it not exist on the same principles on which our fathers made it? (\"It can.\")The knew when they framed the Constitution that in a country as wide and broad as this, with such a variety of climate, pro...\n"
     ]
    }
   ],
   "source": [
    "# Test de recherche\n",
    "question = \"What did Lincoln argue about slavery?\"\n",
    "\n",
    "# Embedding de la question\n",
    "query_embedding = create_embedding(question)\n",
    "\n",
    "# Recherche des chunks pertinents\n",
    "results = vector_store.search(query_embedding, k=3)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"=== Chunks retrouvés ===\")\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Score: {r['score']:.3f} | Source: {r['source']}\")\n",
    "    print(f\"    {r['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Interprétation des scores de similarité\n\nLes résultats de recherche montrent :\n\n| Rang | Score | Interprétation |\n|------|-------|----------------|\n| 1 | 0.628 | **Bonne correspondance** - Le chunk contient directement la doctrine d'abolition de Lincoln |\n| 2 | 0.580 | **Pertinence moyenne** - Discussion sur les droits territoriaux liés à l'esclavage |\n| 3 | 0.572 | **Pertinence moyenne** - Contexte historique sur les principes des pères fondateurs |\n\n**Analyse du score** :\n\n- **Score > 0.7** : Excellent match, très pertinent\n- **0.5 < Score < 0.7** : Pertinent, mais contexte indirect\n- **Score < 0.5** : Faible pertinence, risque de bruit\n\nIci, le **chunk #1 avec score 0.628** est le plus pertinent. Il mentionne explicitement les \"Abolition doctrines\" de Lincoln dans son discours à Springfield, répondant directement à la question \"What did Lincoln argue about slavery?\".\n\n**Point important** : Même un score de 0.628 (62.8% de similarité) est considéré comme **bon** en RAG. Les scores parfaits (>0.9) ne s'obtiennent que pour des textes quasi-identiques.\n\n**Distance cosinus** : Le score affiché est `1 - distance`, donc 1 = identique, 0 = orthogonal (aucune similarité).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Partie 4 : RAG avec Chat Completions\n",
    "\n",
    "## 4.1 Pipeline RAG classique"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Qualité des citations : critères d'évaluation\n\nUne bonne réponse RAG avec citations doit respecter plusieurs critères :\n\n**1. Précision des références** :\n- Chaque affirmation doit être associée à une source [1], [2], etc.\n- Les citations multiples sont encouragées si plusieurs sources confirment la même information\n\n**2. Format structuré** :\n```\nRÉPONSE: Lincoln argued that... [1] while Douglas maintained... [2]\nSOURCES UTILISÉES: [1, 2]\nCONFIANCE: haute\n```\n\n**3. Niveaux de confiance** :\n- **Haute** : Information présente dans plusieurs sources, cohérente\n- **Moyenne** : Information dans une seule source, claire\n- **Basse** : Information implicite, nécessite inférence\n\n**Exemple de sortie attendue** :\n```\nRÉPONSE: Les points clés de désaccord étaient :\n1. L'extension de l'esclavage [1, 2]\n2. La souveraineté populaire vs restriction fédérale [1, 3]\n3. L'interprétation de la Déclaration d'Indépendance [2]\n\nSOURCES UTILISÉES: [1, 2, 3]\nCONFIANCE: haute\n```\n\n**Vérification de qualité** : Comparer les extraits des chunks récupérés avec les citations dans la réponse pour s'assurer de la fidélité.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rag_query(question: str, vector_store: VectorStore, k: int = 3) -> Dict[str, Any]:\n    \"\"\"\n    Pipeline RAG complet avec Chat Completions.\n    \n    Args:\n        question: Question de l'utilisateur\n        vector_store: Base vectorielle\n        k: Nombre de chunks à récupérer\n    \n    Returns:\n        Dict avec réponse, sources, et métadonnées\n    \"\"\"\n    # 1. Retrieval\n    query_embedding = create_embedding(question)\n    chunks = vector_store.search(query_embedding, k=k)\n    \n    # 2. Construction du contexte\n    context = \"\\n\\n\".join([\n        f\"[Source: {c['source']} | Chunk {c['chunk_id']}]\\n{c['text']}\"\n        for c in chunks\n    ])\n    \n    # 3. Prompt augmenté\n    system_prompt = \"\"\"Tu es un assistant expert en histoire américaine.\nRéponds aux questions en te basant UNIQUEMENT sur le contexte fourni.\nSi l'information n'est pas dans le contexte, dis-le clairement.\nCite tes sources avec le format [Chunk X].\"\"\"\n    \n    user_prompt = f\"\"\"Contexte:\n{context}\n\nQuestion: {question}\n\nRéponds de manière précise en citant les sources.\"\"\"\n    \n    # 4. Génération\n    response = client.chat.completions.create(\n        model=DEFAULT_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0.3,  # Faible pour plus de précision\n        max_tokens=500\n    )\n    \n    return {\n        \"question\": question,\n        \"answer\": response.choices[0].message.content,\n        \"sources\": chunks,\n        \"model\": DEFAULT_MODEL,\n        \"tokens\": {\n            \"prompt\": response.usage.prompt_tokens,\n            \"completion\": response.usage.completion_tokens\n        }\n    }\n\n\n# Test du pipeline RAG\nresult = rag_query(\n    \"What were Lincoln's main arguments about slavery in the debate?\",\n    vector_store\n)\n\nprint(\"=== Réponse RAG ===\")\nprint(f\"\\nQuestion: {result['question']}\")\nprint(f\"\\nRéponse:\\n{result['answer']}\")\nprint(f\"\\nTokens utilisés: {result['tokens']}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Reranking : filtrer le bruit\n\nLe **reranking** est une étape critique pour améliorer la précision de la RAG :\n\n**Problème** : k-NN récupère les k chunks les **plus proches**, pas forcément les plus **pertinents**. Un chunk avec score 0.3 (30% de similarité) peut contenir du bruit.\n\n**Solution** : Filtrer par score minimum (threshold).\n\n**Impact du filtrage** :\n\n| Scénario | Chunks k=5 | Après filtrage (min_score=0.3) | Résultat |\n|----------|------------|-------------------------------|----------|\n| Question très spécifique | [0.8, 0.7, 0.6, 0.4, 0.3] | [0.8, 0.7, 0.6, 0.4, 0.3] | 5 chunks conservés |\n| Question hors sujet | [0.4, 0.3, 0.2, 0.1, 0.05] | [0.4, 0.3] | 3 chunks éliminés |\n| Question ambiguë | [0.5, 0.5, 0.4, 0.2, 0.1] | [0.5, 0.5, 0.4] | 2 chunks éliminés |\n\n**Seuils recommandés** :\n- **0.5+** : Très strict, seulement réponses haute confiance\n- **0.3-0.5** : Équilibré (recommandé)\n- **0.2-0.3** : Permissif, accepte plus de contexte\n\n**Techniques avancées** :\n- **Reranking cross-encoder** : Utiliser un modèle spécialisé (ex: `ms-marco-MiniLM-L-6-v2`) pour réévaluer les paires (question, chunk)\n- **Diversité maximale** : Éviter les chunks redondants avec clustering\n- **Temporal decay** : Préférer les chunks récents pour des données temporelles\n\n**Note** : Le simple filtrage par score est déjà très efficace et coûte 0 token supplémentaire.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Partie 5 : RAG avec Responses API (Moderne)\n",
    "\n",
    "La **Responses API** (2025) offre des avantages significatifs pour la RAG :\n",
    "\n",
    "| Fonctionnalité | Avantage |\n",
    "|----------------|----------|\n",
    "| `store: true` | Persistence automatique des conversations |\n",
    "| `previous_response_id` | Multi-turn sans renvoyer l'historique |\n",
    "| Cache automatique | Économies 40-80% sur tokens répétés |\n",
    "| Outils intégrés | Web search, file search, code interpreter |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rag_query_responses_api(\n    question: str,\n    vector_store: VectorStore,\n    previous_response_id: str = None,\n    k: int = 3\n) -> Dict[str, Any]:\n    \"\"\"\n    Pipeline RAG avec Responses API.\n    \n    Avantages:\n    - Conversations multi-turn avec previous_response_id\n    - Cache automatique des contextes répétés\n    - Persistence côté serveur\n    \"\"\"\n    # 1. Retrieval\n    query_embedding = create_embedding(question)\n    chunks = vector_store.search(query_embedding, k=k)\n    \n    # 2. Construction du contexte (format texte simple pour Responses API)\n    context = \"\\n\\n\".join([\n        f\"[Source: {c['source']} | Chunk {c['chunk_id']}]\\n{c['text']}\"\n        for c in chunks\n    ])\n    \n    # 3. Prompt complet\n    full_prompt = f\"\"\"Tu es un assistant expert. Réponds en citant les sources [Chunk X].\n\nContexte:\n{context}\n\nQuestion: {question}\"\"\"\n    \n    # 4. Appel Responses API\n    try:\n        kwargs = {\n            \"model\": DEFAULT_MODEL,\n            \"input\": full_prompt,\n            \"store\": True\n        }\n        if previous_response_id:\n            kwargs[\"previous_response_id\"] = previous_response_id\n        \n        response = client.responses.create(**kwargs)\n        \n        # Extraire le texte de la réponse\n        answer = \"\"\n        if response.output:\n            for item in response.output:\n                if hasattr(item, 'content'):\n                    for content in item.content:\n                        if hasattr(content, 'text'):\n                            answer += content.text\n        \n        return {\n            \"question\": question,\n            \"answer\": answer if answer else \"Pas de réponse\",\n            \"response_id\": response.id,  # Pour le chaînage\n            \"sources\": chunks,\n            \"api\": \"responses\"\n        }\n        \n    except Exception as e:\n        # Fallback sur Chat Completions si Responses API non disponible\n        print(f\"Note: Responses API non disponible ({e}), utilisation de Chat Completions\")\n        return rag_query(question, vector_store, k)\n\n\n# Test Responses API\nprint(\"=== Test RAG avec Responses API ===\")\nresult1 = rag_query_responses_api(\n    \"What did Lincoln say about slavery?\",\n    vector_store\n)\n\nprint(f\"\\nQuestion 1: {result1['question']}\")\nprint(f\"Réponse: {result1['answer'][:500]}...\")\nprint(f\"Response ID: {result1.get('response_id', 'N/A')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conversation multi-turn ===\n",
      "Responses API non disponible pour le multi-turn\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn avec previous_response_id\n",
    "print(\"=== Conversation multi-turn ===\")\n",
    "\n",
    "if result1.get('response_id'):\n",
    "    # Question de suivi utilisant le contexte précédent\n",
    "    result2 = rag_query_responses_api(\n",
    "        \"And what was Douglas's counter-argument?\",\n",
    "        vector_store,\n",
    "        previous_response_id=result1['response_id']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuestion 2 (suivi): {result2['question']}\")\n",
    "    print(f\"Réponse: {result2['answer'][:500]}...\")\n",
    "    print(\"\\nNote: Le contexte de la question 1 est automatiquement inclus via previous_response_id\")\n",
    "else:\n",
    "    print(\"Responses API non disponible pour le multi-turn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Avantages du Multi-turn RAG\n",
    "\n",
    "Avec `previous_response_id`, le modèle conserve automatiquement :\n",
    "\n",
    "1. **Le contexte précédent** : Pas besoin de renvoyer tous les chunks\n",
    "2. **L'historique de conversation** : Questions/réponses précédentes\n",
    "3. **Cache des embeddings** : Économies sur les tokens répétés\n",
    "\n",
    "**Cas d'usage** :\n",
    "- Conversations de recherche documentaire\n",
    "- Analyse progressive de documents\n",
    "- Q&A avec follow-up questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Partie 6 : Citations et Traçabilité\n",
    "\n",
    "Un système RAG de qualité doit permettre de **vérifier les sources**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rag_with_citations(\n    question: str,\n    vector_store: VectorStore,\n    k: int = 3\n) -> Dict[str, Any]:\n    \"\"\"\n    RAG avec génération structurée de citations.\n    \"\"\"\n    # Retrieval\n    query_embedding = create_embedding(question)\n    chunks = vector_store.search(query_embedding, k=k)\n    \n    # Contexte numéroté pour citations\n    context_parts = []\n    for i, c in enumerate(chunks):\n        context_parts.append(f\"[{i+1}] {c['text'][:500]}\")\n    context = \"\\n\\n\".join(context_parts)\n    \n    # Prompt demandant des citations explicites\n    prompt = f\"\"\"Basé sur ces sources:\n\n{context}\n\nQuestion: {question}\n\nInstructions:\n1. Réponds à la question en citant les sources entre crochets [1], [2], etc.\n2. Si plusieurs sources confirment une information, cite-les toutes.\n3. Si l'information n'est pas dans les sources, indique-le.\n\nFormat de réponse:\nRÉPONSE: [ta réponse avec citations]\nSOURCES UTILISÉES: [liste des numéros de sources]\nCONFIANCE: [haute/moyenne/basse]\"\"\"\n    \n    response = client.chat.completions.create(\n        model=DEFAULT_MODEL,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.2\n    )\n    \n    return {\n        \"question\": question,\n        \"response\": response.choices[0].message.content,\n        \"retrieved_chunks\": [\n            {\"id\": i+1, \"score\": c[\"score\"], \"preview\": c[\"text\"][:200]}\n            for i, c in enumerate(chunks)\n        ]\n    }\n\n\n# Test avec citations\nresult_cited = rag_with_citations(\n    \"What were the key points of disagreement between Lincoln and Douglas?\",\n    vector_store\n)\n\nprint(\"=== RAG avec Citations ===\")\nprint(f\"\\nQuestion: {result_cited['question']}\")\nprint(f\"\\n{result_cited['response']}\")\nprint(\"\\n--- Chunks récupérés ---\")\nfor chunk in result_cited['retrieved_chunks']:\n    print(f\"[{chunk['id']}] Score: {chunk['score']:.3f} - {chunk['preview'][:100]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Partie 7 : Bonnes Pratiques\n",
    "\n",
    "## 7.1 Checklist RAG Production\n",
    "\n",
    "| Aspect | Recommandation |\n",
    "|--------|----------------|\n",
    "| **Chunking** | Tester plusieurs stratégies, mesurer la qualité |\n",
    "| **Embeddings** | text-embedding-3-large pour la précision |\n",
    "| **k (retrieval)** | 3-5 chunks, augmenter si précision insuffisante |\n",
    "| **Reranking** | Filtrer les chunks peu pertinents (score < 0.7) |\n",
    "| **Prompt** | Demander explicitement des citations |\n",
    "| **Cache** | Utiliser Responses API avec `store: true` |\n",
    "| **Monitoring** | Logger les questions sans bonne réponse |\n",
    "\n",
    "## 7.2 Bases vectorielles en production\n",
    "\n",
    "| Solution | Type | Avantages |\n",
    "|----------|------|----------|\n",
    "| **Pinecone** | Cloud | Scalable, serverless |\n",
    "| **Qdrant** | Self-hosted/Cloud | Performant, filtres avancés |\n",
    "| **Weaviate** | Self-hosted/Cloud | Hybrid search, modules |\n",
    "| **Chroma** | Local | Simple, bon pour prototypage |\n",
    "| **pgvector** | PostgreSQL | Intégration SQL existante |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exemple de filtrage par score (reranking simple)\ndef rag_with_reranking(\n    question: str,\n    vector_store: VectorStore,\n    k: int = 5,\n    min_score: float = 0.3\n) -> Dict[str, Any]:\n    \"\"\"\n    RAG avec reranking: filtre les chunks peu pertinents.\n    \"\"\"\n    query_embedding = create_embedding(question)\n    all_chunks = vector_store.search(query_embedding, k=k)\n    \n    # Filtrage par score minimum\n    filtered_chunks = [c for c in all_chunks if c['score'] >= min_score]\n    \n    print(f\"Chunks récupérés: {len(all_chunks)}, après filtrage (score >= {min_score}): {len(filtered_chunks)}\")\n    \n    if not filtered_chunks:\n        return {\"answer\": \"Aucun contexte pertinent trouvé pour cette question.\"}\n    \n    # Génération avec chunks filtrés\n    context = \"\\n\\n\".join([c['text'] for c in filtered_chunks])\n    \n    response = client.chat.completions.create(\n        model=DEFAULT_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": \"Réponds uniquement basé sur le contexte fourni.\"},\n            {\"role\": \"user\", \"content\": f\"Contexte:\\n{context}\\n\\nQuestion: {question}\"}\n        ]\n    )\n    \n    return {\n        \"answer\": response.choices[0].message.content,\n        \"chunks_used\": len(filtered_chunks),\n        \"chunks_filtered_out\": len(all_chunks) - len(filtered_chunks)\n    }\n\n\n# Test reranking\nresult_reranked = rag_with_reranking(\n    \"What was the outcome of the debate?\",\n    vector_store,\n    k=5,\n    min_score=0.3\n)\n\nprint(f\"\\nRéponse: {result_reranked['answer'][:500]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "## Points clés\n",
    "\n",
    "1. **Chunking** : Stratégie critique - fixe, sémantique, ou récursif selon le cas\n",
    "2. **Embeddings** : `text-embedding-3-large` offre la meilleure précision\n",
    "3. **Retrieval** : k-NN avec score minimum pour filtrer le bruit\n",
    "4. **Responses API** : `previous_response_id` pour multi-turn efficace\n",
    "5. **Citations** : Toujours demander des références vérifiables\n",
    "\n",
    "## Prochaines étapes\n",
    "\n",
    "- **Notebook 6** : PDF et Web Search intégrés\n",
    "- **Notebook 7** : Code Interpreter pour analyse de données\n",
    "- **Notebook 9** : Patterns de production (batch, retry, monitoring)\n",
    "\n",
    "## Ressources\n",
    "\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [Responses API Documentation](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [RAG Best Practices](https://platform.openai.com/docs/guides/retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fonctions RAG disponibles ===\n",
      "\n",
      "Chunking:\n",
      "  - chunk_fixed(text, chunk_size, overlap)\n",
      "  - chunk_semantic(text, max_sentences)\n",
      "  - chunk_recursive(text, max_size, delimiters)\n",
      "\n",
      "Embeddings:\n",
      "  - create_embedding(text, model)\n",
      "  - create_embeddings_batch(texts, model)\n",
      "\n",
      "Recherche:\n",
      "  - VectorStore.search(query_embedding, k)\n",
      "\n",
      "RAG:\n",
      "  - rag_query(question, vector_store, k)           # Chat Completions\n",
      "  - rag_query_responses_api(question, ...)         # Responses API (multi-turn)\n",
      "  - rag_with_citations(question, vector_store, k)  # Avec sources\n",
      "  - rag_with_reranking(question, ..., min_score)   # Avec filtrage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Résumé des fonctions créées\n",
    "print(\"=== Fonctions RAG disponibles ===\")\n",
    "print(\"\"\"\n",
    "Chunking:\n",
    "  - chunk_fixed(text, chunk_size, overlap)\n",
    "  - chunk_semantic(text, max_sentences)\n",
    "  - chunk_recursive(text, max_size, delimiters)\n",
    "\n",
    "Embeddings:\n",
    "  - create_embedding(text, model)\n",
    "  - create_embeddings_batch(texts, model)\n",
    "\n",
    "Recherche:\n",
    "  - VectorStore.search(query_embedding, k)\n",
    "\n",
    "RAG:\n",
    "  - rag_query(question, vector_store, k)           # Chat Completions\n",
    "  - rag_query_responses_api(question, ...)         # Responses API (multi-turn)\n",
    "  - rag_with_citations(question, vector_store, k)  # Avec sources\n",
    "  - rag_with_reranking(question, ..., min_score)   # Avec filtrage\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}