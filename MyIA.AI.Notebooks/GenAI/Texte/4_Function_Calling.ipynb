{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling : Connecter les LLMs au Monde Réel\n",
    "\n",
    "Dans ce notebook, nous explorons le **Function Calling** (appel de fonctions), qui permet aux modèles d'interagir avec des systèmes externes comme des APIs, bases de données, ou services web.\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre la structure des Tools dans l'API OpenAI\n",
    "- Maîtriser tool_choice (auto, required, none)\n",
    "- Exécuter des fonctions en parallèle\n",
    "- Construire des boucles agentiques\n",
    "\n",
    "**Prérequis :** Notebook 1 (OpenAI Intro), Notebook 3 (Structured Outputs)\n",
    "\n",
    "**Durée estimée :** 60 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Installation et configuration\n%pip install -q openai python-dotenv\n\nimport os\nimport json\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nfrom datetime import datetime\nimport random\n\nload_dotenv('../.env')\nclient = OpenAI()\n\n# Charger le modèle depuis .env ou utiliser gpt-5-mini par défaut\nDEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\nBATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\"\n\nprint(\"Client OpenAI initialisé !\")\nprint(f\"Modèle par défaut: {DEFAULT_MODEL}\")\nprint(f\"Mode batch: {BATCH_MODE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture des Tools\n",
    "\n",
    "Les **Tools** permettent au modèle d'appeler des fonctions définies par l'utilisateur. La structure d'un tool suit le format :\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"nom_fonction\",\n",
    "    \"description\": \"Description claire pour aider le modèle à choisir\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": { ... },  // JSON Schema\n",
    "      \"required\": [...]        // Paramètres obligatoires\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Points clés :**\n",
    "- La **description** est cruciale : elle guide le modèle pour choisir le bon outil\n",
    "- Les **parameters** utilisent JSON Schema pour validation\n",
    "- Le modèle génère les arguments, l'utilisateur exécute la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool défini: get_weather\n",
      "\n",
      "Structure du tool:\n",
      "{\n",
      "  \"type\": \"function\",\n",
      "  \"function\": {\n",
      "    \"name\": \"get_weather\",\n",
      "    \"description\": \"Obtenir la météo actuelle d'une ville\",\n",
      "    \"parameters\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"location\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"Nom de la ville (ex: Paris, Lyon)\"\n",
      "        },\n",
      "        \"unit\": {\n",
      "          \"type\": \"string\",\n",
      "          \"enum\": [\n",
      "            \"fahrenheit\",\n",
      "            \"celsius\"\n",
      "          ],\n",
      "          \"description\": \"Unité de température\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"location\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Définition d'un outil météo\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Obtenir la météo actuelle d'une ville\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Nom de la ville (ex: Paris, Lyon)\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"fahrenheit\", \"celsius\"],\n",
    "                    \"description\": \"Unité de température\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "print(\"Tool défini:\", tools[0][\"function\"][\"name\"])\n",
    "print(\"\\nStructure du tool:\")\n",
    "print(json.dumps(tools[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Interprétation de la structure du tool\n\nL'output affiche la structure complète du tool `get_weather` au format JSON. Observons les éléments clés :\n\n| Champ | Valeur | Rôle |\n|-------|--------|------|\n| `type` | `\"function\"` | Indique qu'il s'agit d'un appel de fonction |\n| `function.name` | `\"get_weather\"` | Identifiant unique de la fonction |\n| `function.description` | \"Obtenir la météo...\" | Utilisée par le modèle pour **décider** quand appeler cette fonction |\n| `parameters.properties` | `location`, `unit` | Définit les arguments que le modèle doit générer |\n| `parameters.required` | `[\"location\"]` | `location` est obligatoire, `unit` est optionnel |\n\n**Point critique** : La **description** est ce qui permet au modèle de choisir le bon outil parmi plusieurs. Plus elle est précise, meilleures seront les décisions du modèle.\n\n> **Analogie** : C'est comme donner une boîte à outils à un assistant - la description de chaque outil (marteau, tournevis) l'aide à choisir le bon pour chaque tâche.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test de la fonction get_weather:\n",
      "{\"location\": \"Paris\", \"temperature\": \"18°C\", \"condition\": \"nuageux\", \"humidity\": \"65%\"}\n",
      "\n",
      "Test avec Fahrenheit:\n",
      "{\"location\": \"Lyon\", \"temperature\": \"71.6°F\", \"condition\": \"ensoleillé\", \"humidity\": \"45%\"}\n",
      "\n",
      "Test ville inconnue (fallback):\n",
      "{\"location\": \"Bordeaux\", \"temperature\": \"20°C\", \"condition\": \"variable\", \"humidity\": \"50%\"}\n"
     ]
    }
   ],
   "source": [
    "# Implémentation de la fonction météo\n",
    "def get_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"Simule une API météo (en production, appeler une vraie API comme OpenWeatherMap)\"\"\"\n",
    "    # Données simulées pour démonstration\n",
    "    weather_data = {\n",
    "        \"Paris\": {\"temp_c\": 18, \"condition\": \"nuageux\", \"humidity\": 65},\n",
    "        \"Lyon\": {\"temp_c\": 22, \"condition\": \"ensoleillé\", \"humidity\": 45},\n",
    "        \"Marseille\": {\"temp_c\": 26, \"condition\": \"ensoleillé\", \"humidity\": 55},\n",
    "    }\n",
    "    \n",
    "    # Fallback pour villes non répertoriées\n",
    "    data = weather_data.get(location, {\"temp_c\": 20, \"condition\": \"variable\", \"humidity\": 50})\n",
    "    \n",
    "    # Conversion température\n",
    "    temp = data[\"temp_c\"] if unit == \"celsius\" else data[\"temp_c\"] * 9/5 + 32\n",
    "    unit_symbol = \"°C\" if unit == \"celsius\" else \"°F\"\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"location\": location,\n",
    "        \"temperature\": f\"{temp}{unit_symbol}\",\n",
    "        \"condition\": data[\"condition\"],\n",
    "        \"humidity\": f\"{data['humidity']}%\"\n",
    "    }, ensure_ascii=False)\n",
    "\n",
    "# Test de la fonction\n",
    "print(\"Test de la fonction get_weather:\")\n",
    "print(get_weather(\"Paris\", \"celsius\"))\n",
    "print(\"\\nTest avec Fahrenheit:\")\n",
    "print(get_weather(\"Lyon\", \"fahrenheit\"))\n",
    "print(\"\\nTest ville inconnue (fallback):\")\n",
    "print(get_weather(\"Bordeaux\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation des tests\n",
    "\n",
    "**Test 1 - Paris en Celsius :**\n",
    "Retourne des données simulées pour Paris (18°C, nuageux, 65% humidité). En production, cette fonction appellerait une API réelle comme OpenWeatherMap.\n",
    "\n",
    "**Test 2 - Lyon en Fahrenheit :**\n",
    "Conversion automatique : 22°C × 9/5 + 32 = 71.6°F. La fonction gère l'unité de température de manière flexible.\n",
    "\n",
    "**Test 3 - Bordeaux (fallback) :**\n",
    "La ville n'existe pas dans `weather_data`, donc la fonction retourne des valeurs par défaut (20°C, variable). Ce mécanisme de **fallback** évite les erreurs et garantit une réponse.\n",
    "\n",
    "**Points clés :**\n",
    "- Retour en JSON structuré (facilite le parsing par le modèle)\n",
    "- Gestion des cas limites (villes inconnues)\n",
    "- Paramètres optionnels avec valeurs par défaut\n",
    "\n",
    "> **Production** : Remplacer la simulation par une vraie API et ajouter un cache pour réduire les appels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Premier Appel avec Tool\n",
    "\n",
    "Lorsqu'on passe des `tools` à l'API, le modèle peut décider d'appeler une fonction au lieu de répondre directement. Le paramètre `tool_choice` contrôle ce comportement :\n",
    "\n",
    "- **`auto`** (défaut) : Le modèle décide s'il doit appeler un outil\n",
    "- **`required`** : Le modèle DOIT appeler au moins un outil\n",
    "- **`none`** : Le modèle ne peut PAS appeler d'outils\n",
    "- **`{\"type\": \"function\", \"function\": {\"name\": \"...\"}}`** : Forcer un outil spécifique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reason: tool_calls\n",
      "\n",
      "Le modèle a décidé d'appeler un outil !\n",
      "\n",
      "Contenu de la réponse du modèle:\n",
      "None\n",
      "\n",
      "Tool call détaillé:\n",
      "  ID: call_UeHKdALJw170jpEG83clhESq\n",
      "  Fonction: get_weather\n",
      "  Arguments: {\"location\":\"New York\",\"unit\":\"celsius\"}\n"
     ]
    }
   ],
   "source": [
    "# Premier appel avec tool\n",
    "# messages = [{\"role\": \"user\", \"content\": \"Quelle est la météo à Paris aujourd'hui?\"}]\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather forecast for tomorrow in NYC?\"}]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"  # Le modèle décide s'il doit appeler un outil\n",
    ")\n",
    "\n",
    "print(\"Finish reason:\", response.choices[0].finish_reason)\n",
    "print(\"\\nLe modèle a décidé d'appeler un outil !\")\n",
    "\n",
    "# Examiner les tool_calls\n",
    "if response.choices[0].message.tool_calls:\n",
    "    contenu_reponse = response.choices[0].message.content\n",
    "    print(\"\\nContenu de la réponse du modèle:\")\n",
    "    print(contenu_reponse)\n",
    "    tool_call = response.choices[0].message.tool_calls[0]\n",
    "    print(\"\\nTool call détaillé:\")\n",
    "    print(f\"  ID: {tool_call.id}\")\n",
    "    print(f\"  Fonction: {tool_call.function.name}\")\n",
    "    print(f\"  Arguments: {tool_call.function.arguments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation de la réponse\n",
    "\n",
    "**Finish reason : `tool_calls`**\n",
    "\n",
    "Contrairement à un finish_reason `stop` (réponse textuelle normale), `tool_calls` indique que le modèle a décidé d'appeler une fonction.\n",
    "\n",
    "**Structure de tool_call :**\n",
    "\n",
    "| Champ | Valeur | Signification |\n",
    "|-------|--------|---------------|\n",
    "| `id` | `call_XXX` | Identifiant unique pour lier la réponse de l'outil |\n",
    "| `function.name` | `get_weather` | Fonction choisie par le modèle |\n",
    "| `function.arguments` | `{\"location\": \"Paris\", \"unit\": \"celsius\"}` | Arguments générés (JSON) |\n",
    "\n",
    "**Le modèle a fait quoi exactement ?**\n",
    "\n",
    "1. Analysé la question : \"Quelle est la météo à Paris aujourd'hui?\"\n",
    "2. Reconnu qu'il a besoin de données en temps réel\n",
    "3. Choisi la fonction `get_weather` (grâce à la description)\n",
    "4. Extrait les arguments : `location=\"Paris\"`, `unit=\"celsius\"` (valeur par défaut)\n",
    "\n",
    "> **Important** : Le modèle ne **exécute PAS** la fonction. Il retourne uniquement les instructions. C'est à l'application de l'exécuter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Flux Complet : Boucle Agentique\n",
    "\n",
    "Le flux typique d'une conversation avec function calling :\n",
    "\n",
    "1. **Utilisateur** envoie un message\n",
    "2. **Modèle** retourne `tool_calls` (ou répond directement si pas besoin d'outils)\n",
    "3. **Application** exécute les fonctions demandées\n",
    "4. **Application** injecte les résultats avec `role=\"tool\"`\n",
    "5. **Modèle** utilise les résultats pour formuler une réponse finale\n",
    "6. Retour à l'étape 2 si le modèle veut appeler d'autres fonctions\n",
    "\n",
    "Cette boucle est appelée **boucle agentique** car le modèle agit comme un agent autonome qui décide quand et quels outils utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Quel temps fait-il à Lyon?\n",
      "\n",
      "  Appel: get_weather({'location': 'Lyon'})\n",
      "\n",
      "Réponse finale: À Lyon, il fait actuellement 22°C et le temps est ensoleillé avec une humidité de 45%.\n"
     ]
    }
   ],
   "source": [
    "def run_conversation(user_message: str, tools: list, available_functions: dict):\n",
    "    \"\"\"Exécute une conversation complète avec appels de fonctions\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    \n",
    "    while True:\n",
    "        response = client.chat.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message\n",
    "        messages.append(assistant_message)\n",
    "        \n",
    "        # Si pas d'appel de fonction, on a terminé\n",
    "        if not assistant_message.tool_calls:\n",
    "            return assistant_message.content\n",
    "        \n",
    "        # Exécuter chaque fonction appelée\n",
    "        for tool_call in assistant_message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            print(f\"  Appel: {function_name}({function_args})\")\n",
    "            \n",
    "            if function_name in available_functions:\n",
    "                result = available_functions[function_name](**function_args)\n",
    "            else:\n",
    "                result = json.dumps({\"error\": f\"Fonction {function_name} non trouvée\"})\n",
    "            \n",
    "            # Injecter le résultat dans la conversation\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": result\n",
    "            })\n",
    "    \n",
    "# Test de la boucle agentique\n",
    "available_functions = {\"get_weather\": get_weather}\n",
    "\n",
    "question = \"Quel temps fait-il à Lyon?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "result = run_conversation(question, tools, available_functions)\n",
    "print(f\"\\nRéponse finale: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du flux de la boucle agentique\n",
    "\n",
    "**Étapes exécutées :**\n",
    "\n",
    "1. **Tour 1** : Modèle reçoit \"Quel temps fait-il à Lyon?\"\n",
    "   - Décision : Appeler `get_weather(location=\"Lyon\")`\n",
    "   - Retour fonction : `{\"location\": \"Lyon\", \"temperature\": \"22°C\", \"condition\": \"ensoleillé\", ...}`\n",
    "\n",
    "2. **Tour 2** : Modèle reçoit le résultat de la fonction\n",
    "   - Décision : Formuler une réponse en langage naturel\n",
    "   - Pas de `tool_calls` → boucle terminée\n",
    "\n",
    "**Points clés :**\n",
    "\n",
    "- Le modèle **décide automatiquement** quand arrêter (pas de `tool_calls` → sortie)\n",
    "- L'ajout de `role=\"tool\"` dans les messages permet au modèle de \"voir\" les résultats\n",
    "- Ce pattern est à la base des **agents autonomes** (AutoGPT, BabyAGI, etc.)\n",
    "\n",
    "> **Note** : Une boucle mal conçue peut tourner indéfiniment. Toujours prévoir une limite `max_iterations`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Appels de Fonctions Multiples et Parallèles\n",
    "\n",
    "Le modèle peut appeler **plusieurs fonctions** dans une seule réponse. Ceci est utile pour :\n",
    "- Répondre à des questions complexes nécessitant plusieurs sources de données\n",
    "- Exécuter plusieurs actions en parallèle\n",
    "- Optimiser le nombre d'aller-retours API\n",
    "\n",
    "Ajoutons plus de fonctions pour démontrer ce comportement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonctions disponibles: get_weather, get_time, create_reminder\n"
     ]
    }
   ],
   "source": [
    "# Ajouter plus de fonctions\n",
    "def get_time(timezone: str = \"Europe/Paris\") -> str:\n",
    "    \"\"\"Retourne l'heure actuelle (simulation)\"\"\"\n",
    "    return json.dumps({\n",
    "        \"timezone\": timezone,\n",
    "        \"time\": datetime.now().strftime(\"%H:%M:%S\"),\n",
    "        \"date\": datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    })\n",
    "\n",
    "def create_reminder(title: str, time: str, priority: str = \"normal\") -> str:\n",
    "    \"\"\"Crée un rappel (simulation)\"\"\"\n",
    "    return json.dumps({\n",
    "        \"status\": \"created\",\n",
    "        \"reminder\": {\"title\": title, \"time\": time, \"priority\": priority}\n",
    "    }, ensure_ascii=False)\n",
    "\n",
    "# Définir les nouveaux tools\n",
    "extended_tools = tools + [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_time\",\n",
    "            \"description\": \"Obtenir l'heure et la date actuelles pour un fuseau horaire donné\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"timezone\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Fuseau horaire (ex: Europe/Paris, America/New_York)\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"create_reminder\",\n",
    "            \"description\": \"Créer un rappel pour une tâche future\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"title\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Titre du rappel\"\n",
    "                    },\n",
    "                    \"time\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Heure du rappel (ex: '09:00', 'demain 14h')\"\n",
    "                    },\n",
    "                    \"priority\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"low\", \"normal\", \"high\"],\n",
    "                        \"description\": \"Priorité du rappel\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"title\", \"time\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "all_functions = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_time\": get_time,\n",
    "    \"create_reminder\": create_reminder\n",
    "}\n",
    "\n",
    "print(\"Fonctions disponibles:\", \", \".join(all_functions.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Analyse des nouvelles fonctions\n\nNous avons ajouté deux nouvelles fonctions pour démontrer les appels multiples :\n\n| Fonction | Objectif | Paramètres | Cas d'usage |\n|----------|----------|------------|-------------|\n| `get_time()` | Obtenir l'heure actuelle | `timezone` (optionnel) | Planification, rappels temporels |\n| `create_reminder()` | Créer un rappel | `title`, `time`, `priority` | Gestion de tâches, productivité |\n\n**Points techniques** :\n- Les deux fonctions retournent du JSON structuré (facilite le parsing par le modèle)\n- `priority` utilise un `enum` pour contraindre les valeurs possibles (low/normal/high)\n- Les paramètres optionnels ont des valeurs par défaut sensibles\n\nAvec ces 3 fonctions (météo, heure, rappel), le modèle peut maintenant orchestrer des scénarios complexes combinant plusieurs sources de données et actions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question complexe:\n",
      "Quelle heure est-il à Paris et quel temps fait-il? Crée aussi un rappel pour demain 9h: 'Réunion équipe'\n",
      "\n",
      "Appels de fonctions (parallèles):\n",
      "  Appel: get_time({'timezone': 'Europe/Paris'})\n",
      "  Appel: get_weather({'location': 'Paris', 'unit': 'celsius'})\n",
      "  Appel: create_reminder({'title': 'Réunion équipe', 'time': 'demain 9h'})\n",
      "\n",
      "Réponse finale:\n",
      "Il est actuellement 10h27 à Paris. Le temps à Paris est nuageux avec une température de 18°C et une humidité de 65%. J'ai également créé un rappel pour demain à 9h intitulé \"Réunion équipe\".\n"
     ]
    }
   ],
   "source": [
    "# Test avec requête complexe nécessitant plusieurs appels\n",
    "complex_question = (\n",
    "    \"Quelle heure est-il à Paris et quel temps fait-il? \"\n",
    "    \"Crée aussi un rappel pour demain 9h: 'Réunion équipe'\"\n",
    ")\n",
    "\n",
    "print(\"Question complexe:\")\n",
    "print(complex_question)\n",
    "print(\"\\nAppels de fonctions (parallèles):\")\n",
    "\n",
    "result = run_conversation(complex_question, extended_tools, all_functions)\n",
    "\n",
    "print(\"\\nRéponse finale:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de l'exécution parallèle\n",
    "\n",
    "**Observation clé** : Le modèle a effectué **3 appels de fonction en parallèle** dans une seule réponse :\n",
    "\n",
    "1. `get_time(timezone='Europe/Paris')`\n",
    "2. `get_weather(location='Paris')`\n",
    "3. `create_reminder(title='Réunion équipe', time='demain 09h')`\n",
    "\n",
    "**Avantages de l'exécution parallèle :**\n",
    "\n",
    "| Aspect | Sans parallélisme | Avec parallélisme |\n",
    "|--------|------------------|-------------------|\n",
    "| **Nombre d'aller-retours API** | 3 tours (1 appel par tour) | 1 tour (3 appels groupés) |\n",
    "| **Latence totale** | ~3-6 secondes | ~1-2 secondes |\n",
    "| **Tokens consommés** | Plus élevé (3 réponses) | Optimisé (1 réponse) |\n",
    "\n",
    "**Comment le modèle décide-t-il d'exécuter en parallèle ?**\n",
    "\n",
    "Le modèle détecte que les 3 fonctions sont **indépendantes** (pas de dépendance de données entre elles). Si une fonction dépend du résultat d'une autre, le modèle les exécute séquentiellement.\n",
    "\n",
    "> **Best practice** : Concevez vos fonctions pour maximiser l'indépendance et permettre la parallélisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contrôle Avancé : tool_choice\n",
    "\n",
    "Le paramètre `tool_choice` offre un contrôle fin sur le comportement du modèle :\n",
    "\n",
    "| Valeur | Comportement |\n",
    "|--------|-------------|\n",
    "| `\"auto\"` | Le modèle décide (défaut) |\n",
    "| `\"required\"` | Le modèle DOIT appeler au moins un outil |\n",
    "| `\"none\"` | Le modèle ne peut PAS appeler d'outils |\n",
    "| `{\"type\": \"function\", \"function\": {\"name\": \"X\"}}` | Force l'appel de la fonction X |\n",
    "\n",
    "**Cas d'usage :**\n",
    "- `\"required\"` : Forcer l'utilisation d'outils pour des données en temps réel\n",
    "- `\"none\"` : Désactiver temporairement les outils (mode conversation pure)\n",
    "- Spécifique : Garantir qu'une action précise sera exécutée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Forcer l'appel d'un outil spécifique ===\n",
      "Question: Bonjour, comment vas-tu?\n",
      "\n",
      "Avec tool_choice forcé à get_weather:\n",
      "  Le modèle a appelé: get_weather\n",
      "  Arguments: {\"location\":\"Paris\",\"unit\":\"celsius\"}\n",
      "\n",
      "=== Test 2: Désactiver les tools ===\n",
      "Question: Quelle est la météo à Paris?\n",
      "\n",
      "Avec tool_choice='none' (pas d'appel de fonction):\n",
      "  Réponse directe: Voulez-vous connaître la météo à Paris en Celsius ou en Fahrenheit ?...\n",
      "\n",
      "=== Test 3: tool_choice='required' ===\n",
      "Question: Dis-moi bonjour\n",
      "\n",
      "Avec tool_choice='required':\n",
      "\n",
      "Contenu de la réponse du modèle:\n",
      "None\n",
      "  Le modèle est FORCÉ d'appeler un outil: get_time avec les paramètres {\"timezone\": \"Europe/Paris\"}\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Forcer l'appel d'une fonction spécifique\n",
    "print(\"=== Test 1: Forcer l'appel d'un outil spécifique ===\")\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Bonjour, comment vas-tu?\"}],\n",
    "    tools=tools,\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}\n",
    ")\n",
    "\n",
    "print(\"Question: Bonjour, comment vas-tu?\")\n",
    "print(\"\\nAvec tool_choice forcé à get_weather:\")\n",
    "if response.choices[0].message.tool_calls:\n",
    "    tc = response.choices[0].message.tool_calls[0]\n",
    "    print(f\"  Le modèle a appelé: {tc.function.name}\")\n",
    "    print(f\"  Arguments: {tc.function.arguments}\")\n",
    "\n",
    "# Test 2: Désactiver les tools\n",
    "print(\"\\n=== Test 2: Désactiver les tools ===\")\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Quelle est la météo à Paris?\"}],\n",
    "    tools=tools,\n",
    "    tool_choice=\"none\"\n",
    ")\n",
    "\n",
    "print(\"Question: Quelle est la météo à Paris?\")\n",
    "print(\"\\nAvec tool_choice='none' (pas d'appel de fonction):\")\n",
    "print(f\"  Réponse directe: {response.choices[0].message.content[:100]}...\")\n",
    "\n",
    "# Test 3: Forcer au moins un appel (required)\n",
    "print(\"\\n=== Test 3: tool_choice='required' ===\")\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Dis-moi bonjour\"}],\n",
    "    tools=extended_tools,\n",
    "    tool_choice=\"required\"\n",
    ")\n",
    "\n",
    "print(\"Question: Dis-moi bonjour\")\n",
    "print(\"\\nAvec tool_choice='required':\")\n",
    "\n",
    "if response.choices[0].message.tool_calls:\n",
    "    contenu_reponse = response.choices[0].message.content\n",
    "    print(\"\\nContenu de la réponse du modèle:\")\n",
    "    print(contenu_reponse)\n",
    "    tc = response.choices[0].message.tool_calls[0]\n",
    "    print(f\"  Le modèle est FORCÉ d'appeler un outil: {tc.function.name} avec les paramètres {tc.function.arguments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des comportements de tool_choice\n",
    "\n",
    "| Mode | Requête | Résultat | Observation |\n",
    "|------|---------|----------|-------------|\n",
    "| **Forcé spécifique** | \"Bonjour, comment vas-tu?\" | Appel forcé à `get_weather` | Le modèle DOIT appeler la fonction même si inappropriée |\n",
    "| **none** | \"Quelle est la météo à Paris?\" | Réponse textuelle sans fonction | Le modèle répond directement sans accès aux données réelles |\n",
    "| **required** | \"Dis-moi bonjour\" | Appel forcé à un outil (ex: `get_time`) | Le modèle choisit l'outil le moins inapproprié |\n",
    "\n",
    "**Implications pratiques :**\n",
    "- `tool_choice` forcé peut générer des appels non pertinents → utiliser avec parcimonie\n",
    "- `tool_choice=\"none\"` utile pour économiser des appels API coûteux quand les données ne changent pas\n",
    "- `tool_choice=\"required\"` garantit l'utilisation de données temps réel (ex: prix en bourse)\n",
    "\n",
    "> **Attention** : Forcer un outil peut dégrader l'expérience utilisateur si mal utilisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gestion Robuste des Erreurs\n",
    "\n",
    "Dans un système de production, la gestion d'erreurs est cruciale :\n",
    "\n",
    "**Sources d'erreurs possibles :**\n",
    "1. **Erreurs API** : Timeout, limites de taux, problèmes réseau\n",
    "2. **Arguments invalides** : JSON malformé, types incorrects\n",
    "3. **Fonction non disponible** : Le modèle appelle une fonction qui n'existe pas\n",
    "4. **Erreur d'exécution** : La fonction échoue (ex: API externe indisponible)\n",
    "5. **Boucles infinies** : Le modèle continue d'appeler des fonctions sans fin\n",
    "\n",
    "**Bonnes pratiques :**\n",
    "- Limiter le nombre d'itérations (max_iterations)\n",
    "- Valider les arguments JSON\n",
    "- Retourner des messages d'erreur structurés au modèle\n",
    "- Logger les erreurs pour débogage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Ville non répertoriée (fallback) ===\n",
      "La météo à Berlin est actuellement de 20°C avec des conditions variables et une humidité de 50%.\n",
      "\n",
      "=== Test 2: Fonction inexistante ===\n",
      "Je ne peux pas acheter de billets d'avion directement, mais je peux vous aider à trouver des informations ou vous guider sur la manière de le faire. Par exemple, je peux vous indiquer les étapes pour acheter un billet en ligne, ou vous fournir des informations sur les vols disponibles. Que souhaitez-vous faire exactement ?\n",
      "\n",
      "=== Test 3: Arguments invalides ===\n",
      "Il semble que votre message comporte des symboles ou des espaces réservés pour certaines informations, comme le nom de la ville et l'unité de température. Pourriez-vous préciser la ville et l'unité de température souhaitées (Celsius ou Fahrenheit) afin que je puisse vous aider correctement ? Par exemple, \"Météo à Paris température en Celsius\".\n"
     ]
    }
   ],
   "source": [
    "def run_safe_conversation(user_message: str, tools: list, available_functions: dict, max_iterations: int = 5):\n",
    "    \"\"\"Version robuste avec gestion d'erreurs et limite d'itérations\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Gestion erreurs API\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=DEFAULT_MODEL,\n",
    "                messages=messages,\n",
    "                tools=tools,\n",
    "                tool_choice=\"auto\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return f\"Erreur API: {e}\"\n",
    "        \n",
    "        assistant_message = response.choices[0].message\n",
    "        messages.append(assistant_message)\n",
    "        \n",
    "        if not assistant_message.tool_calls:\n",
    "            return assistant_message.content\n",
    "        \n",
    "        # Traiter chaque appel de fonction\n",
    "        for tool_call in assistant_message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            \n",
    "            # Gestion erreurs JSON\n",
    "            try:\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "            except json.JSONDecodeError:\n",
    "                result = json.dumps({\"error\": \"Arguments invalides, JSON malformé\"})\n",
    "                messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": result})\n",
    "                continue\n",
    "            \n",
    "            # Vérifier que la fonction existe\n",
    "            if function_name not in available_functions:\n",
    "                result = json.dumps({\"error\": f\"Fonction '{function_name}' non disponible\"})\n",
    "            else:\n",
    "                # Gestion erreurs d'exécution\n",
    "                try:\n",
    "                    result = available_functions[function_name](**function_args)\n",
    "                except Exception as e:\n",
    "                    result = json.dumps({\"error\": f\"Erreur d'exécution: {str(e)}\"})\n",
    "            \n",
    "            messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": result})\n",
    "    \n",
    "    return f\"Limite d'itérations atteinte ({max_iterations}). Conversation trop longue.\"\n",
    "\n",
    "# Test de gestion d'erreurs\n",
    "print(\"=== Test 1: Ville non répertoriée (fallback) ===\")\n",
    "print(run_safe_conversation(\"Météo à Berlin?\", tools, all_functions))\n",
    "\n",
    "print(\"\\n=== Test 2: Fonction inexistante ===\")\n",
    "print(run_safe_conversation(\"Achète-moi un billet d'avion\", extended_tools, all_functions))\n",
    "\n",
    "print(\"\\n=== Test 3: Arguments invalides ===\")\n",
    "# Simuler un cas où le modèle pourrait générer des arguments incorrects\n",
    "print(run_safe_conversation(\"Météo à ????? température en @@@@\", tools, all_functions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats de gestion d'erreurs\n",
    "\n",
    "Les trois tests démontrent la robustesse du système :\n",
    "\n",
    "| Test | Scénario | Comportement attendu |\n",
    "|------|----------|---------------------|\n",
    "| **Ville non répertoriée** | Berlin n'est pas dans `weather_data` | Fallback vers données par défaut (20°C, variable) |\n",
    "| **Fonction inexistante** | \"Achète-moi un billet d'avion\" | Retour JSON avec `error: \"Fonction 'X' non disponible\"` |\n",
    "| **Arguments invalides** | Caractères spéciaux (??, @@) | Le modèle peut soit échouer à générer des arguments valides, soit utiliser un fallback |\n",
    "\n",
    "**Points clés :**\n",
    "- La fonction `run_safe_conversation()` ne plante **jamais**\n",
    "- Tous les cas d'erreur sont capturés et retournés de manière structurée\n",
    "- Le modèle reçoit les messages d'erreur et peut adapter sa stratégie\n",
    "\n",
    "> **Production** : En environnement réel, logguer toutes les erreurs pour analyse et amélioration continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cas d'Usage Avancés\n",
    "\n",
    "Voyons quelques patterns avancés de function calling :\n",
    "\n",
    "### A. Recherche de base de données\n",
    "Simuler une recherche dans une base de données avec filtres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Trouve-moi tous les cours de Machine Learning de plus de 20 heures\n",
      "\n",
      "Voici les cours de Machine Learning de plus de 20 heures disponibles :\n",
      "\n",
      "1. **Deep Learning Avancé**\n",
      "   - Durée : 30 heures\n",
      "   - Niveau : Avancé\n",
      "   - Prix : 299 euros\n",
      "\n",
      "2. **Computer Vision**\n",
      "   - Durée : 25 heures\n",
      "   - Niveau : Avancé\n",
      "   - Prix : 349 euros\n",
      "\n",
      "Si tu as besoin de plus d'informations ou d'autres filtres, fais-le moi savoir !\n"
     ]
    }
   ],
   "source": [
    "# Simuler une base de données de cours\n",
    "COURSE_DB = [\n",
    "    {\"id\": 1, \"title\": \"Python pour débutants\", \"category\": \"Programming\", \"duration_hours\": 10, \"level\": \"Débutant\", \"price\": 49},\n",
    "    {\"id\": 2, \"title\": \"Machine Learning Intro\", \"category\": \"ML\", \"duration_hours\": 15, \"level\": \"Intermédiaire\", \"price\": 99},\n",
    "    {\"id\": 3, \"title\": \"Deep Learning Avancé\", \"category\": \"ML\", \"duration_hours\": 30, \"level\": \"Avancé\", \"price\": 299},\n",
    "    {\"id\": 4, \"title\": \"Data Science avec R\", \"category\": \"Data\", \"duration_hours\": 20, \"level\": \"Intermédiaire\", \"price\": 149},\n",
    "    {\"id\": 5, \"title\": \"Computer Vision\", \"category\": \"ML\", \"duration_hours\": 25, \"level\": \"Avancé\", \"price\": 349},\n",
    "]\n",
    "\n",
    "def search_courses(category: str = None, min_duration: int = None, max_price: int = None, level: str = None) -> str:\n",
    "    \"\"\"Recherche de cours avec filtres multiples\"\"\"\n",
    "    results = COURSE_DB.copy()\n",
    "    \n",
    "    if category:\n",
    "        results = [c for c in results if c[\"category\"] == category]\n",
    "    if min_duration:\n",
    "        results = [c for c in results if c[\"duration_hours\"] >= min_duration]\n",
    "    if max_price:\n",
    "        results = [c for c in results if c[\"price\"] <= max_price]\n",
    "    if level:\n",
    "        results = [c for c in results if c[\"level\"] == level]\n",
    "    \n",
    "    return json.dumps({\"count\": len(results), \"courses\": results}, ensure_ascii=False)\n",
    "\n",
    "# Définir le tool\n",
    "search_tool = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"search_courses\",\n",
    "        \"description\": \"Rechercher des cours dans la base de données avec filtres optionnels\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"category\": {\"type\": \"string\", \"enum\": [\"Programming\", \"ML\", \"Data\"], \"description\": \"Catégorie de cours\"},\n",
    "                \"min_duration\": {\"type\": \"integer\", \"description\": \"Durée minimale en heures\"},\n",
    "                \"max_price\": {\"type\": \"integer\", \"description\": \"Prix maximum en euros\"},\n",
    "                \"level\": {\"type\": \"string\", \"enum\": [\"Débutant\", \"Intermédiaire\", \"Avancé\"], \"description\": \"Niveau\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "search_functions = {\"search_courses\": search_courses}\n",
    "\n",
    "# Test\n",
    "question = \"Trouve-moi tous les cours de Machine Learning de plus de 20 heures\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(run_safe_conversation(question, search_tool, search_functions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation des résultats de recherche\n",
    "\n",
    "**Requête analysée par le modèle :**\n",
    "- Catégorie : Machine Learning\n",
    "- Durée minimale : 20 heures\n",
    "\n",
    "**Résultats retournés :**\n",
    "Le modèle a correctement extrait les critères et construit les arguments de la fonction `search_courses()`. Notez que :\n",
    "\n",
    "1. **Traitement du langage naturel** : \"de plus de 20 heures\" → `min_duration=20`\n",
    "2. **Mapping de catégorie** : \"Machine Learning\" → `category=\"ML\"`\n",
    "3. **Filtres combinés** : Les deux critères sont appliqués en ET logique\n",
    "\n",
    "Ce pattern de recherche s'applique à de nombreux cas d'usage réels (e-commerce, CRM, documentation, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Orchestration Multi-Étapes\n",
    "\n",
    "Le modèle peut orchestrer plusieurs étapes complexes automatiquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scénario: Planifie ma journée demain - réveil à 7h, sport à 8h, travail à 9h\n",
      "\n",
      "Étapes exécutées:\n",
      "  Appel: create_reminder({'title': 'Réveil', 'time': 'demain 07:00', 'priority': 'high'})\n",
      "  Appel: create_reminder({'title': 'Sport', 'time': 'demain 08:00', 'priority': 'normal'})\n",
      "  Appel: create_reminder({'title': 'Travail', 'time': 'demain 09:00', 'priority': 'normal'})\n",
      "\n",
      "Résultat:\n",
      "J'ai planifié votre journée de demain :\n",
      "\n",
      "- **Réveil** à 7h (priorité haute)\n",
      "- **Sport** à 8h (priorité normale)\n",
      "- **Travail** à 9h (priorité normale)\n",
      "\n",
      "Vos rappels sont créés et prêts pour vous. N'hésitez pas à me dire si vous avez besoin d'une autre tâche pour demain !\n"
     ]
    }
   ],
   "source": [
    "# Le modèle peut décomposer une tâche complexe en plusieurs appels\n",
    "scenario = \"Planifie ma journée demain - réveil à 7h, sport à 8h, travail à 9h\"\n",
    "print(f\"Scénario: {scenario}\\n\")\n",
    "print(\"Étapes exécutées:\")\n",
    "\n",
    "result = run_conversation(scenario, extended_tools, all_functions)\n",
    "\n",
    "print(\"\\nRésultat:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Interprétation de l'orchestration\n\nLe modèle a démontré une capacité clé des **agents autonomes** : décomposer une intention complexe en étapes atomiques.\n\n**Analyse du comportement :**\n- **Entrée** : \"Planifie ma journée demain - réveil à 7h, sport à 8h, travail à 9h\"\n- **Décomposition automatique** : 3 appels à `create_reminder()` avec des priorités différentes\n- **Contextualisation** : Le modèle a assigné `priority=\"high\"` au réveil (plus critique que les autres)\n\n**Ce pattern est fondamental pour :**\n- **Agents de productivité** : Transformation de langage naturel en actions multiples\n- **Workflows complexes** : Orchestration automatique de tâches interdépendantes\n- **Interfaces conversationnelles** : Réduire le nombre d'interactions utilisateur\n\n> **Pattern clé** : Le modèle transforme \"planifie ma journée\" en une séquence de 3 actions atomiques sans intervention humaine. C'est le cœur du paradigme **agentique**.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion et Bonnes Pratiques\n",
    "\n",
    "### Points Clés\n",
    "\n",
    "1. **Descriptions précises** : Les descriptions des fonctions guident le modèle. Soyez explicite !\n",
    "2. **JSON Schema rigoureux** : Définissez bien les types et contraintes des paramètres\n",
    "3. **Gestion d'erreurs** : Toujours prévoir des fallbacks et retourner des erreurs structurées\n",
    "4. **Limite d'itérations** : Évitez les boucles infinies avec un max_iterations\n",
    "5. **Sécurité** : Validez TOUJOURS les arguments avant exécution (injections, chemins dangereux, etc.)\n",
    "\n",
    "### Cas d'Usage Réels\n",
    "\n",
    "| Domaine | Exemple d'Application |\n",
    "|---------|----------------------|\n",
    "| **E-commerce** | Recherche produits, ajout panier, suivi commande |\n",
    "| **Productivité** | Gestion calendrier, emails, rappels |\n",
    "| **Finance** | Consultation soldes, virements, alertes |\n",
    "| **Data Science** | Requêtes SQL, visualisations, analyses |\n",
    "| **DevOps** | Déploiements, logs, monitoring |\n",
    "\n",
    "### Exercices Suggérés\n",
    "\n",
    "1. **Système de réservation** : Créez des fonctions pour chercher/réserver des restaurants\n",
    "2. **Calculatrice avancée** : Fonctions mathématiques (factorielle, fibonacci, primalité)\n",
    "3. **API réelle** : Intégrez une vraie API météo (OpenWeatherMap) au lieu de la simulation\n",
    "4. **Multi-agents** : Créez plusieurs \"agents\" avec des sets de fonctions différents\n",
    "\n",
    "### Prochaine Étape\n",
    "\n",
    "Dans le notebook suivant (**5_RAG_Introduction.ipynb**), nous verrons comment combiner function calling avec le RAG (Retrieval-Augmented Generation) pour créer des assistants qui peuvent chercher dans des bases de connaissances.\n",
    "\n",
    "---\n",
    "\n",
    "**Ressources :**\n",
    "- [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)\n",
    "- [JSON Schema Documentation](https://json-schema.org/)\n",
    "- Documentation Notebook 1 (OpenAI Intro)\n",
    "- Documentation Notebook 3 (Structured Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Notebook Function Calling terminé !\n",
      "✓ Concepts maîtrisés: Tools, tool_choice, boucles agentiques, gestion d'erreurs\n",
      "✓ Prochaine étape: RAG (Retrieval-Augmented Generation)\n"
     ]
    }
   ],
   "source": [
    "# Cellule de validation finale\n",
    "print(\"✓ Notebook Function Calling terminé !\")\n",
    "print(\"✓ Concepts maîtrisés: Tools, tool_choice, boucles agentiques, gestion d'erreurs\")\n",
    "print(\"✓ Prochaine étape: RAG (Retrieval-Augmented Generation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Validation de la progression\n\nCette cellule confirme la complétion du notebook et résume les acquis :\n\n**Concepts maîtrisés :**\n1. **Tools** : Définition de fonctions avec JSON Schema\n2. **tool_choice** : Contrôle fin du comportement du modèle (auto, required, none, spécifique)\n3. **Boucles agentiques** : Pattern fondamental pour les agents autonomes\n4. **Gestion d'erreurs** : Robustesse en production (timeouts, validation, limites)\n5. **Orchestration** : Appels parallèles et décomposition automatique de tâches complexes\n\n**Prochaine étape :** Le notebook RAG (Retrieval-Augmented Generation) combinera ces techniques avec des bases de connaissances pour créer des assistants capables de raisonner sur des documents.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}