{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# Whisper Local - Transcription GPU avec faster-whisper\n",
    "\n",
    "**Module :** 01-Audio-Foundation  \n",
    "**Niveau :** Intermediaire  \n",
    "**Technologies :** faster-whisper (CTranslate2), transformers  \n",
    "**Duree estimee :** 45 minutes  \n",
    "**VRAM :** ~10 GB  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Installer et configurer faster-whisper\n",
    "- [ ] Comprendre les tailles de modeles et leurs compromis\n",
    "- [ ] Transcrire de l'audio avec WhisperModel en local\n",
    "- [ ] Obtenir des segments detailles avec timestamps\n",
    "- [ ] Detecter la langue et la probabilite de detection\n",
    "- [ ] Effectuer une transcription batch de plusieurs fichiers\n",
    "- [ ] Comparer les performances local vs API (vitesse, qualite, cout)\n",
    "- [ ] Surveiller l'utilisation VRAM\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- GPU NVIDIA avec au moins 4 GB VRAM (10 GB recommande pour large-v3)\n",
    "- CUDA Toolkit installe\n",
    "- `pip install faster-whisper`\n",
    "- Echantillons audio (generes dans les notebooks precedents ou fournis)\n",
    "\n",
    "**Navigation :** [Index](../README.md) | [<< Precedent](01-3-Basic-Audio-Operations.ipynb) | [Suivant >>](01-5-Kokoro-TTS-Local.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-params",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres Whisper local\n",
    "model_size = \"large-v3-turbo\"      # \"tiny\", \"base\", \"small\", \"medium\", \"large-v3\", \"large-v3-turbo\"\n",
    "device = \"cuda\"                    # \"cuda\" ou \"cpu\"\n",
    "compute_type = \"float16\"           # \"float16\", \"int8\", \"int8_float16\"\n",
    "\n",
    "# Configuration\n",
    "generate_test_audio = True         # Generer des echantillons via TTS\n",
    "compare_model_sizes = True         # Comparer differentes tailles de modeles\n",
    "compare_with_api = True            # Comparer avec l'API OpenAI\n",
    "batch_transcribe = True            # Tester la transcription batch\n",
    "monitor_vram = True                # Surveiller l'utilisation VRAM\n",
    "save_results = True                # Sauvegarder les resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.audio_helpers import transcribe_local, load_audio, play_audio_file\n",
    "        print(\"Helpers audio importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers audio non disponibles - mode autonome\")\n",
    "\n",
    "# Repertoires\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'whisper_local'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAMPLES_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'samples'\n",
    "SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('whisper_local')\n",
    "\n",
    "# Verification GPU\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_vram = torch.cuda.get_device_properties(0).total_mem / (1024**3)\n",
    "        print(f\"GPU : {gpu_name} ({gpu_vram:.1f} GB VRAM)\")\n",
    "    else:\n",
    "        print(\"GPU non disponible - utilisation CPU\")\n",
    "        device = \"cpu\"\n",
    "        compute_type = \"int8\"\n",
    "except ImportError:\n",
    "    print(\"torch non disponible - verification GPU ignoree\")\n",
    "    gpu_available = False\n",
    "\n",
    "print(f\"\\nWhisper Local - Transcription GPU\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Modele : {model_size}, Device : {device}, Compute : {compute_type}\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et preparation des echantillons\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "# Preparation des echantillons audio\n",
    "test_files = {}\n",
    "\n",
    "# Verifier les fichiers existants (generes par les notebooks precedents)\n",
    "for sample_file in SAMPLES_DIR.glob(\"sample_*.mp3\"):\n",
    "    lang = sample_file.stem.split('_')[1]\n",
    "    test_files[lang] = sample_file\n",
    "    print(f\"Echantillon existant : {sample_file.name}\")\n",
    "\n",
    "# Generer si necessaire\n",
    "if generate_test_audio and len(test_files) == 0:\n",
    "    openai_key = os.getenv('OPENAI_API_KEY')\n",
    "    if openai_key:\n",
    "        from openai import OpenAI\n",
    "        client_api = OpenAI(api_key=openai_key)\n",
    "\n",
    "        texts = {\n",
    "            \"fr\": \"La reconnaissance vocale locale permet de transcrire sans connexion internet.\",\n",
    "            \"en\": \"Local speech recognition enables transcription without internet connection.\",\n",
    "            \"de\": \"Die lokale Spracherkennung ermoeglicht die Transkription ohne Internetverbindung.\"\n",
    "        }\n",
    "\n",
    "        for lang, text in texts.items():\n",
    "            print(f\"Generation echantillon '{lang}'...\")\n",
    "            response = client_api.audio.speech.create(\n",
    "                model=\"tts-1\", voice=\"nova\", input=text, response_format=\"mp3\"\n",
    "            )\n",
    "            filepath = SAMPLES_DIR / f\"sample_{lang}.mp3\"\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            test_files[lang] = filepath\n",
    "    else:\n",
    "        print(\"OPENAI_API_KEY non disponible pour generer les echantillons\")\n",
    "\n",
    "print(f\"\\n{len(test_files)} echantillons prets pour transcription\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-intro",
   "metadata": {},
   "source": [
    "## Section 1 : Modeles Whisper et compromis\n",
    "\n",
    "faster-whisper utilise CTranslate2 pour une inference optimisee des modeles Whisper.\n",
    "\n",
    "### Tailles de modeles\n",
    "\n",
    "| Modele | Parametres | VRAM (float16) | Vitesse relative | Qualite |\n",
    "|--------|-----------|----------------|-------------------|---------|\n",
    "| `tiny` | 39 M | ~1 GB | 32x | Basique |\n",
    "| `base` | 74 M | ~1 GB | 16x | Correcte |\n",
    "| `small` | 244 M | ~2 GB | 6x | Bonne |\n",
    "| `medium` | 769 M | ~5 GB | 2x | Tres bonne |\n",
    "| `large-v3` | 1.55 B | ~10 GB | 1x (reference) | Excellente |\n",
    "| `large-v3-turbo` | 809 M | ~6 GB | 3x | Excellente |\n",
    "\n",
    "### Types de calcul\n",
    "\n",
    "| Type | Precision | VRAM | Vitesse |\n",
    "|------|-----------|------|--------|\n",
    "| `float16` | Haute | Standard | Standard |\n",
    "| `int8_float16` | Bonne | Reduite ~50% | Plus rapide |\n",
    "| `int8` | Acceptable | Minimale | Plus rapide |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modele Whisper\n",
    "print(\"CHARGEMENT DU MODELE WHISPER\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Mesure VRAM avant chargement\n",
    "vram_before = 0\n",
    "if monitor_vram and gpu_available:\n",
    "    vram_before = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"VRAM avant chargement : {vram_before:.2f} GB\")\n",
    "\n",
    "print(f\"\\nChargement du modele '{model_size}' sur '{device}'...\")\n",
    "print(f\"Type de calcul : {compute_type}\")\n",
    "\n",
    "start_time = time.time()\n",
    "model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"Modele charge en {load_time:.1f}s\")\n",
    "\n",
    "# Mesure VRAM apres chargement\n",
    "if monitor_vram and gpu_available:\n",
    "    vram_after = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    vram_used = vram_after - vram_before\n",
    "    print(f\"VRAM apres chargement : {vram_after:.2f} GB\")\n",
    "    print(f\"VRAM utilisee par le modele : {vram_used:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-intro",
   "metadata": {},
   "source": [
    "## Section 2 : Transcription detaillee\n",
    "\n",
    "faster-whisper retourne deux objets :\n",
    "- `segments` : generateur de segments avec timestamps, texte et confiance\n",
    "- `info` : metadonnees (langue detectee, probabilite, duree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcription detaillee\n",
    "print(\"TRANSCRIPTION DETAILLEE\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if test_files:\n",
    "    # Transcription du premier echantillon\n",
    "    lang_key = list(test_files.keys())[0]\n",
    "    sample_path = test_files[lang_key]\n",
    "\n",
    "    print(f\"Fichier : {sample_path.name}\")\n",
    "    print(f\"\\nTranscription en cours...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    segments, info = model.transcribe(\n",
    "        str(sample_path),\n",
    "        beam_size=5,\n",
    "        word_timestamps=True\n",
    "    )\n",
    "\n",
    "    # Collecter les segments (le generateur est consomme une seule fois)\n",
    "    segments_list = list(segments)\n",
    "    transcribe_time = time.time() - start_time\n",
    "\n",
    "    # Informations de detection\n",
    "    print(f\"\\n--- Metadonnees ---\")\n",
    "    print(f\"  Langue detectee : {info.language} (probabilite : {info.language_probability:.2%})\")\n",
    "    print(f\"  Duree audio : {info.duration:.1f}s\")\n",
    "    print(f\"  Temps de transcription : {transcribe_time:.2f}s\")\n",
    "    print(f\"  Ratio temps reel : {info.duration / transcribe_time:.1f}x\")\n",
    "\n",
    "    # Texte complet\n",
    "    full_text = \" \".join([s.text.strip() for s in segments_list])\n",
    "    print(f\"\\n--- Texte transcrit ---\")\n",
    "    print(f\"  {full_text}\")\n",
    "\n",
    "    # Segments detailles\n",
    "    print(f\"\\n--- Segments ({len(segments_list)}) ---\")\n",
    "    for seg in segments_list:\n",
    "        print(f\"  [{seg.start:.2f}s - {seg.end:.2f}s] (conf: {seg.avg_logprob:.3f}) {seg.text.strip()}\")\n",
    "\n",
    "    # Mots avec timestamps\n",
    "    all_words = []\n",
    "    for seg in segments_list:\n",
    "        if seg.words:\n",
    "            all_words.extend(seg.words)\n",
    "\n",
    "    if all_words:\n",
    "        print(f\"\\n--- Mots avec timestamps ({len(all_words)}) ---\")\n",
    "        for w in all_words[:10]:\n",
    "            print(f\"  [{w.start:.2f}s - {w.end:.2f}s] (p={w.probability:.2f}) {w.word}\")\n",
    "        if len(all_words) > 10:\n",
    "            print(f\"  ... ({len(all_words) - 10} mots supplementaires)\")\n",
    "\n",
    "    # Sauvegarde\n",
    "    if save_results:\n",
    "        result = {\n",
    "            \"model\": model_size,\n",
    "            \"device\": device,\n",
    "            \"compute_type\": compute_type,\n",
    "            \"language\": info.language,\n",
    "            \"language_probability\": info.language_probability,\n",
    "            \"duration\": info.duration,\n",
    "            \"transcription_time\": transcribe_time,\n",
    "            \"text\": full_text,\n",
    "            \"segments\": [\n",
    "                {\"start\": s.start, \"end\": s.end, \"text\": s.text.strip(),\n",
    "                 \"avg_logprob\": s.avg_logprob}\n",
    "                for s in segments_list\n",
    "            ]\n",
    "        }\n",
    "        result_file = OUTPUT_DIR / f\"transcription_{lang_key}_{model_size}.json\"\n",
    "        with open(result_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nResultat sauvegarde : {result_file.name}\")\n",
    "else:\n",
    "    print(\"Aucun echantillon audio disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Transcription locale\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Ratio temps reel | >1x = plus rapide que le temps reel | Plus le ratio est eleve, meilleure est la performance |\n",
    "| avg_logprob | Proche de 0 = haute confiance | Valeurs < -1.0 indiquent une incertitude |\n",
    "| language_probability | Proche de 1.0 | Detection de langue fiable |\n",
    "\n",
    "**Points cles** :\n",
    "1. `word_timestamps=True` active l'alignement mot par mot\n",
    "2. `beam_size=5` ameliore la qualite au prix d'une latence accrue\n",
    "3. Le generateur `segments` est consomme une seule fois - le convertir en liste pour reutilisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-intro",
   "metadata": {},
   "source": [
    "## Section 3 : Transcription batch\n",
    "\n",
    "Pour traiter plusieurs fichiers audio, on itere sur les fichiers tout en conservant le modele en memoire. Le cout marginal d'une transcription supplementaire est minime une fois le modele charge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcription batch\n",
    "print(\"TRANSCRIPTION BATCH\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if batch_transcribe and len(test_files) > 0:\n",
    "    batch_results = {}\n",
    "\n",
    "    for lang_key, filepath in test_files.items():\n",
    "        print(f\"\\nTranscription '{lang_key}' : {filepath.name}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        segments, info = model.transcribe(str(filepath))\n",
    "        segments_list = list(segments)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        text = \" \".join([s.text.strip() for s in segments_list])\n",
    "\n",
    "        batch_results[lang_key] = {\n",
    "            \"text\": text,\n",
    "            \"language\": info.language,\n",
    "            \"probability\": info.language_probability,\n",
    "            \"duration\": info.duration,\n",
    "            \"time\": elapsed\n",
    "        }\n",
    "\n",
    "        print(f\"  Langue : {info.language} ({info.language_probability:.0%})\")\n",
    "        print(f\"  Texte : {text[:80]}...\")\n",
    "        print(f\"  Temps : {elapsed:.2f}s (ratio : {info.duration/elapsed:.1f}x)\")\n",
    "\n",
    "    # Tableau recapitulatif\n",
    "    print(f\"\\nRecapitulatif batch :\")\n",
    "    print(f\"{'Fichier':<12} {'Langue':<8} {'Proba':<8} {'Duree':<8} {'Temps':<8} {'Ratio':<8}\")\n",
    "    print(\"-\" * 52)\n",
    "    for lang_key, data in batch_results.items():\n",
    "        ratio = data['duration'] / data['time'] if data['time'] > 0 else 0\n",
    "        print(f\"{lang_key:<12} {data['language']:<8} {data['probability']:<8.0%} \"\n",
    "              f\"{data['duration']:<8.1f} {data['time']:<8.2f} {ratio:<8.1f}\")\n",
    "\n",
    "    total_duration = sum(d['duration'] for d in batch_results.values())\n",
    "    total_time = sum(d['time'] for d in batch_results.values())\n",
    "    print(f\"\\nTotal : {total_duration:.1f}s d'audio transcrits en {total_time:.1f}s\")\n",
    "else:\n",
    "    print(\"Transcription batch desactivee ou pas d'echantillons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-intro",
   "metadata": {},
   "source": [
    "## Section 4 : Comparaison local vs API\n",
    "\n",
    "Analyse comparative entre la transcription locale (faster-whisper) et l'API OpenAI Whisper.\n",
    "\n",
    "| Critere | Local (faster-whisper) | API (OpenAI) |\n",
    "|---------|----------------------|---------------|\n",
    "| Cout | Gratuit (hardware) | $0.006/minute |\n",
    "| Latence | Depend du GPU | ~1-3s par requete |\n",
    "| Confidentialite | Donnees locales | Envoi a OpenAI |\n",
    "| Disponibilite | Pas de connexion requise | Internet requis |\n",
    "| Maintenance | Mises a jour manuelles | Geree par OpenAI |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison local vs API\n",
    "print(\"COMPARAISON LOCAL VS API\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if compare_with_api and len(test_files) > 0:\n",
    "    openai_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "    if openai_key:\n",
    "        from openai import OpenAI\n",
    "        client_api = OpenAI(api_key=openai_key)\n",
    "\n",
    "        lang_key = list(test_files.keys())[0]\n",
    "        filepath = test_files[lang_key]\n",
    "\n",
    "        # --- Transcription locale ---\n",
    "        print(f\"\\n--- Local ({model_size}) ---\")\n",
    "        start_local = time.time()\n",
    "        segments, info = model.transcribe(str(filepath))\n",
    "        text_local = \" \".join([s.text.strip() for s in segments])\n",
    "        time_local = time.time() - start_local\n",
    "        print(f\"  Texte : {text_local[:80]}...\")\n",
    "        print(f\"  Temps : {time_local:.2f}s\")\n",
    "\n",
    "        # --- Transcription API ---\n",
    "        print(f\"\\n--- API (whisper-1) ---\")\n",
    "        start_api = time.time()\n",
    "        with open(filepath, 'rb') as audio_file:\n",
    "            transcript = client_api.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio_file,\n",
    "                response_format=\"json\"\n",
    "            )\n",
    "        text_api = transcript.text\n",
    "        time_api = time.time() - start_api\n",
    "        print(f\"  Texte : {text_api[:80]}...\")\n",
    "        print(f\"  Temps : {time_api:.2f}s\")\n",
    "\n",
    "        # --- Analyse ---\n",
    "        print(f\"\\n--- Analyse comparative ---\")\n",
    "        print(f\"{'Critere':<25} {'Local':<20} {'API':<20}\")\n",
    "        print(\"-\" * 65)\n",
    "        print(f\"{'Temps':<25} {time_local:<20.2f} {time_api:<20.2f}\")\n",
    "        print(f\"{'Longueur texte':<25} {len(text_local):<20} {len(text_api):<20}\")\n",
    "\n",
    "        cost_api = info.duration / 60 * 0.006\n",
    "        print(f\"{'Cout':<25} {'$0.00 (local)':<20} {f'${cost_api:.4f}':<20}\")\n",
    "\n",
    "        # Estimation cout pour 1 heure\n",
    "        cost_1h = 60 * 0.006\n",
    "        print(f\"\\nCout pour 1 heure d'audio :\")\n",
    "        print(f\"  Local : $0.00 (cout electricite uniquement)\")\n",
    "        print(f\"  API   : ${cost_1h:.2f}\")\n",
    "    else:\n",
    "        print(\"OPENAI_API_KEY non disponible pour la comparaison\")\n",
    "else:\n",
    "    print(\"Comparaison desactivee ou pas d'echantillons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"MODE INTERACTIF\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nEntrez le chemin d'un fichier audio a transcrire :\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "\n",
    "    try:\n",
    "        user_path = input(\"\\nChemin du fichier audio : \")\n",
    "\n",
    "        if user_path.strip():\n",
    "            user_file = Path(user_path.strip())\n",
    "            if user_file.exists():\n",
    "                print(f\"\\nTranscription de {user_file.name}...\")\n",
    "                start_time = time.time()\n",
    "                segments, info = model.transcribe(\n",
    "                    str(user_file), word_timestamps=True\n",
    "                )\n",
    "                segments_list = list(segments)\n",
    "                elapsed = time.time() - start_time\n",
    "\n",
    "                text = \" \".join([s.text.strip() for s in segments_list])\n",
    "                print(f\"\\nLangue : {info.language} ({info.language_probability:.0%})\")\n",
    "                print(f\"Duree : {info.duration:.1f}s\")\n",
    "                print(f\"Temps : {elapsed:.2f}s\")\n",
    "                print(f\"\\nTexte : {text}\")\n",
    "            else:\n",
    "                print(f\"Fichier non trouve : {user_file}\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type} - {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-practices",
   "metadata": {},
   "source": [
    "## Bonnes pratiques et optimisation\n",
    "\n",
    "### Choix du modele\n",
    "\n",
    "| Scenario | Modele recommande | Raison |\n",
    "|----------|-------------------|--------|\n",
    "| Prototypage rapide | `small` ou `base` | Chargement rapide, VRAM minimale |\n",
    "| Production | `large-v3-turbo` | Meilleur compromis qualite/vitesse |\n",
    "| Qualite maximale | `large-v3` | Precision maximale, plus lent |\n",
    "| GPU limite (4 GB) | `small` + `int8` | Fonctionne sur la plupart des GPUs |\n",
    "\n",
    "### Optimisation des performances\n",
    "\n",
    "| Technique | Impact | Description |\n",
    "|-----------|--------|-------------|\n",
    "| `int8_float16` | VRAM -50% | Quantification mixte |\n",
    "| `beam_size=1` | Vitesse +50% | Perte de qualite minime |\n",
    "| Specifier la langue | Vitesse +20% | Evite la detection |\n",
    "| `condition_on_previous_text=False` | Evite les repetitions | Utile pour longs fichiers |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"STATISTIQUES DE SESSION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Modele : {model_size}\")\n",
    "print(f\"Device : {device}, Compute : {compute_type}\")\n",
    "print(f\"Fichiers transcrits : {len(test_files)}\")\n",
    "\n",
    "if monitor_vram and gpu_available:\n",
    "    vram_current = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    vram_peak = torch.cuda.max_memory_allocated(0) / (1024**3)\n",
    "    print(f\"VRAM actuelle : {vram_current:.2f} GB\")\n",
    "    print(f\"VRAM pic : {vram_peak:.2f} GB\")\n",
    "\n",
    "if save_results:\n",
    "    saved = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"Fichiers sauvegardes : {len(saved)} dans {OUTPUT_DIR}\")\n",
    "\n",
    "# Liberation memoire\n",
    "print(f\"\\nLiberation du modele...\")\n",
    "del model\n",
    "gc.collect()\n",
    "if gpu_available:\n",
    "    torch.cuda.empty_cache()\n",
    "print(f\"Memoire liberee\")\n",
    "\n",
    "print(f\"\\nPROCHAINES ETAPES\")\n",
    "print(f\"1. Essayer le TTS local avec Kokoro (01-5-Kokoro-TTS-Local)\")\n",
    "print(f\"2. Decouvrir le voice cloning avec XTTS (02-2)\")\n",
    "print(f\"3. Comparer tous les modeles STT (03-1)\")\n",
    "print(f\"4. Construire un pipeline STT->LLM->TTS (03-2)\")\n",
    "\n",
    "print(f\"\\nNotebook Whisper Local termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}