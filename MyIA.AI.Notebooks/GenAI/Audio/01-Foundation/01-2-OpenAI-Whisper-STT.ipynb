{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# OpenAI Whisper STT - Reconnaissance Vocale par API\n",
    "\n",
    "**Module :** 01-Audio-Foundation  \n",
    "**Niveau :** Debutant  \n",
    "**Technologies :** OpenAI Whisper API, GPT-4o-Transcribe  \n",
    "**Duree estimee :** 35 minutes  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Generer un echantillon audio de test avec l'API TTS\n",
    "- [ ] Transcrire de l'audio avec `client.audio.transcriptions.create`\n",
    "- [ ] Comprendre les formats de reponse (json, text, srt, vtt, verbose_json)\n",
    "- [ ] Obtenir des timestamps au niveau des mots\n",
    "- [ ] Utiliser la detection automatique de langue\n",
    "- [ ] Traduire de l'audio vers l'anglais avec l'endpoint translation\n",
    "- [ ] Comparer Whisper-1 et GPT-4o-Transcribe\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Environment Setup (module 00) complete\n",
    "- Cle API OpenAI configuree (`OPENAI_API_KEY` dans `.env`)\n",
    "- Notebook 01-1 (TTS) recommande mais pas obligatoire\n",
    "\n",
    "**Navigation :** [Index](../README.md) | [<< Precedent](01-1-OpenAI-TTS-Intro.ipynb) | [Suivant >>](01-3-Basic-Audio-Operations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-params",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres STT\n",
    "stt_model = \"whisper-1\"            # \"whisper-1\" ou \"gpt-4o-transcribe\"\n",
    "language = None                    # Code langue ISO 639-1 (None = auto-detection)\n",
    "response_format = \"verbose_json\"   # \"json\", \"text\", \"srt\", \"vtt\", \"verbose_json\"\n",
    "\n",
    "# Configuration\n",
    "generate_test_audio = True         # Generer un fichier audio de test via TTS\n",
    "test_translation = True            # Tester l'endpoint de traduction\n",
    "compare_models = True              # Comparer whisper-1 et gpt-4o-transcribe\n",
    "save_results = True                # Sauvegarder les resultats de transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from io import BytesIO\n",
    "import logging\n",
    "\n",
    "# Lecture audio dans Jupyter\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.audio_helpers import transcribe_openai, synthesize_openai\n",
    "        print(\"Helpers audio importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers audio non disponibles - mode autonome\")\n",
    "\n",
    "# Repertoires\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'stt'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAMPLES_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'samples'\n",
    "SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('openai_whisper')\n",
    "\n",
    "print(f\"OpenAI Whisper STT - Reconnaissance Vocale\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}, Modele STT : {stt_model}\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la configuration et validation API\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Recherche du .env dans les parents\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve dans l'arborescence\")\n",
    "\n",
    "# Verification cle API OpenAI\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not openai_key:\n",
    "    if notebook_mode == \"batch\" and not generate_test_audio:\n",
    "        print(\"Mode batch sans generation : cle API ignoree\")\n",
    "        openai_key = \"dummy_key_for_validation\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"OPENAI_API_KEY manquante dans .env\\n\"\n",
    "            \"Obtenez votre cle sur : https://platform.openai.com/api-keys\"\n",
    "        )\n",
    "\n",
    "# Initialisation client OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=openai_key)\n",
    "\n",
    "# Test de connexion\n",
    "if openai_key != \"dummy_key_for_validation\":\n",
    "    try:\n",
    "        models = client.models.list()\n",
    "        whisper_models = [m.id for m in models if 'whisper' in m.id or 'transcribe' in m.id]\n",
    "        print(f\"Connexion API reussie\")\n",
    "        print(f\"Modeles STT detectes : {whisper_models}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur connexion : {str(e)[:100]}\")\n",
    "\n",
    "print(f\"\\nConfiguration STT :\")\n",
    "print(f\"  Modele : {stt_model}\")\n",
    "print(f\"  Langue : {language or 'auto-detection'}\")\n",
    "print(f\"  Format reponse : {response_format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-intro",
   "metadata": {},
   "source": [
    "## Section 1 : Generation d'un echantillon de test\n",
    "\n",
    "Avant de transcrire, nous avons besoin d'un fichier audio. Nous utilisons l'API TTS pour generer un echantillon controle, ce qui nous permettra de verifier la precision de la transcription.\n",
    "\n",
    "Cette approche \"round-trip\" (TTS puis STT) est utile pour :\n",
    "- Valider le pipeline audio de bout en bout\n",
    "- Mesurer la fidelite de la transcription\n",
    "- Avoir un texte de reference pour comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-generate-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation d'echantillons audio de test\n",
    "print(\"GENERATION DES ECHANTILLONS DE TEST\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Textes de test dans differentes langues\n",
    "test_texts = {\n",
    "    \"fr\": (\n",
    "        \"La reconnaissance vocale permet de convertir la parole en texte. \"\n",
    "        \"Cette technologie utilise des reseaux de neurones profonds pour \"\n",
    "        \"analyser le signal audio et identifier les mots prononces.\"\n",
    "    ),\n",
    "    \"en\": (\n",
    "        \"Speech recognition converts spoken language into written text. \"\n",
    "        \"Modern systems use deep neural networks trained on thousands \"\n",
    "        \"of hours of audio data to achieve human-level accuracy.\"\n",
    "    ),\n",
    "    \"multi\": (\n",
    "        \"Bonjour, je parle francais. Now I switch to English. \"\n",
    "        \"Et je reviens au francais pour terminer.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "audio_samples = {}\n",
    "\n",
    "if generate_test_audio:\n",
    "    for lang_key, text in test_texts.items():\n",
    "        print(f\"\\nGeneration echantillon '{lang_key}'...\")\n",
    "        print(f\"  Texte : {text[:80]}...\")\n",
    "\n",
    "        response = client.audio.speech.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=\"nova\",\n",
    "            input=text,\n",
    "            response_format=\"mp3\"\n",
    "        )\n",
    "\n",
    "        audio_data = response.content\n",
    "        filepath = SAMPLES_DIR / f\"sample_{lang_key}.mp3\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(audio_data)\n",
    "\n",
    "        audio_samples[lang_key] = {\n",
    "            \"path\": filepath,\n",
    "            \"text_original\": text,\n",
    "            \"size_kb\": len(audio_data) / 1024\n",
    "        }\n",
    "\n",
    "        print(f\"  Fichier : {filepath.name} ({len(audio_data)/1024:.1f} KB)\")\n",
    "        display(Audio(data=audio_data, autoplay=False))\n",
    "\n",
    "    print(f\"\\n{len(audio_samples)} echantillons generes\")\n",
    "else:\n",
    "    # Chercher des fichiers existants\n",
    "    for lang_key in test_texts:\n",
    "        filepath = SAMPLES_DIR / f\"sample_{lang_key}.mp3\"\n",
    "        if filepath.exists():\n",
    "            audio_samples[lang_key] = {\n",
    "                \"path\": filepath,\n",
    "                \"text_original\": test_texts[lang_key],\n",
    "                \"size_kb\": filepath.stat().st_size / 1024\n",
    "            }\n",
    "    print(f\"{len(audio_samples)} echantillons existants trouves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-intro",
   "metadata": {},
   "source": [
    "## Section 2 : Transcription avec Whisper API\n",
    "\n",
    "L'API Whisper accepte un fichier audio et retourne la transcription. Les parametres principaux :\n",
    "\n",
    "| Parametre | Description | Valeurs |\n",
    "|-----------|-------------|--------|\n",
    "| `model` | Modele STT | `whisper-1`, `gpt-4o-transcribe` |\n",
    "| `file` | Fichier audio | MP3, MP4, WAV, FLAC, etc. (max 25 MB) |\n",
    "| `language` | Langue source | Code ISO 639-1 (`fr`, `en`, etc.) ou None |\n",
    "| `response_format` | Format de sortie | `json`, `text`, `srt`, `vtt`, `verbose_json` |\n",
    "| `timestamp_granularities` | Precision timestamps | `[\"word\"]`, `[\"segment\"]`, `[\"word\", \"segment\"]` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcription de base\n",
    "print(\"TRANSCRIPTION WHISPER\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if \"fr\" in audio_samples:\n",
    "    sample = audio_samples[\"fr\"]\n",
    "    print(f\"Fichier source : {sample['path'].name}\")\n",
    "    print(f\"Texte original : {sample['text_original'][:80]}...\")\n",
    "\n",
    "    # --- Format JSON simple ---\n",
    "    print(f\"\\n--- Transcription (format json) ---\")\n",
    "    with open(sample['path'], 'rb') as audio_file:\n",
    "        transcript_json = client.audio.transcriptions.create(\n",
    "            model=stt_model,\n",
    "            file=audio_file,\n",
    "            response_format=\"json\"\n",
    "        )\n",
    "    print(f\"Resultat : {transcript_json.text}\")\n",
    "\n",
    "    # --- Format verbose_json (avec metadonnees) ---\n",
    "    print(f\"\\n--- Transcription (format verbose_json) ---\")\n",
    "    with open(sample['path'], 'rb') as audio_file:\n",
    "        transcript_verbose = client.audio.transcriptions.create(\n",
    "            model=stt_model,\n",
    "            file=audio_file,\n",
    "            response_format=\"verbose_json\",\n",
    "            timestamp_granularities=[\"word\", \"segment\"]\n",
    "        )\n",
    "\n",
    "    print(f\"Texte : {transcript_verbose.text}\")\n",
    "    print(f\"Langue detectee : {transcript_verbose.language}\")\n",
    "    print(f\"Duree : {transcript_verbose.duration:.1f}s\")\n",
    "\n",
    "    # Affichage des segments\n",
    "    if hasattr(transcript_verbose, 'segments') and transcript_verbose.segments:\n",
    "        print(f\"\\nSegments ({len(transcript_verbose.segments)}) :\")\n",
    "        for seg in transcript_verbose.segments:\n",
    "            print(f\"  [{seg.start:.1f}s - {seg.end:.1f}s] {seg.text.strip()}\")\n",
    "\n",
    "    # Affichage des mots avec timestamps\n",
    "    if hasattr(transcript_verbose, 'words') and transcript_verbose.words:\n",
    "        print(f\"\\nMots avec timestamps ({len(transcript_verbose.words)}) :\")\n",
    "        for word in transcript_verbose.words[:15]:  # Premiers 15 mots\n",
    "            print(f\"  [{word.start:.2f}s - {word.end:.2f}s] {word.word}\")\n",
    "        if len(transcript_verbose.words) > 15:\n",
    "            print(f\"  ... ({len(transcript_verbose.words) - 15} mots supplementaires)\")\n",
    "\n",
    "    # Sauvegarde\n",
    "    if save_results:\n",
    "        result_file = OUTPUT_DIR / \"transcription_fr.json\"\n",
    "        with open(result_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"text\": transcript_verbose.text,\n",
    "                \"language\": transcript_verbose.language,\n",
    "                \"duration\": transcript_verbose.duration,\n",
    "                \"model\": stt_model\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nResultat sauvegarde : {result_file.name}\")\n",
    "else:\n",
    "    print(\"Aucun echantillon audio disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Transcription Whisper\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Fidelite | Tres elevee | Whisper reproduit le texte original avec precision |\n",
    "| Detection de langue | Automatique | Whisper detecte la langue sans parametre explicite |\n",
    "| Timestamps | Niveau mot et segment | Utile pour sous-titrage, karaoke, synchronisation |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le format `verbose_json` fournit les metadonnees les plus completes\n",
    "2. Les timestamps au niveau mot necessitent `timestamp_granularities=[\"word\"]`\n",
    "3. La detection de langue est fiable pour les langues courantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-intro",
   "metadata": {},
   "source": [
    "## Section 3 : Formats de sortie specialises\n",
    "\n",
    "Whisper supporte des formats standards pour le sous-titrage :\n",
    "\n",
    "| Format | Description | Usage |\n",
    "|--------|-------------|-------|\n",
    "| `srt` | SubRip Subtitle | Lecteurs video, YouTube |\n",
    "| `vtt` | WebVTT | Navigateurs web, HTML5 video |\n",
    "| `text` | Texte brut | Traitement de texte, NLP |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-subtitle-formats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formats de sous-titrage\n",
    "print(\"FORMATS DE SOUS-TITRAGE\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if \"fr\" in audio_samples:\n",
    "    sample = audio_samples[\"fr\"]\n",
    "    subtitle_formats = [\"text\", \"srt\", \"vtt\"]\n",
    "\n",
    "    for fmt in subtitle_formats:\n",
    "        print(f\"\\n--- Format : {fmt.upper()} ---\")\n",
    "        with open(sample['path'], 'rb') as audio_file:\n",
    "            result = client.audio.transcriptions.create(\n",
    "                model=stt_model,\n",
    "                file=audio_file,\n",
    "                response_format=fmt\n",
    "            )\n",
    "\n",
    "        # Le resultat est une chaine pour text/srt/vtt\n",
    "        output_text = result if isinstance(result, str) else str(result)\n",
    "        print(output_text[:300])\n",
    "\n",
    "        # Sauvegarde\n",
    "        if save_results:\n",
    "            ext = fmt\n",
    "            result_file = OUTPUT_DIR / f\"transcription_fr.{ext}\"\n",
    "            with open(result_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(output_text)\n",
    "            print(f\"Sauvegarde : {result_file.name}\")\n",
    "\n",
    "    print(f\"\\nTous les formats generes avec succes\")\n",
    "else:\n",
    "    print(\"Aucun echantillon audio disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-intro",
   "metadata": {},
   "source": [
    "## Section 4 : Traduction et detection multilingue\n",
    "\n",
    "L'API Whisper offre un endpoint de traduction (`translations`) qui traduit l'audio en anglais, quelle que soit la langue source.\n",
    "\n",
    "| Endpoint | Entree | Sortie |\n",
    "|----------|--------|--------|\n",
    "| `transcriptions` | Audio (n'importe quelle langue) | Texte dans la langue source |\n",
    "| `translations` | Audio (n'importe quelle langue) | Texte en anglais |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traduction et detection multilingue\n",
    "print(\"TRADUCTION ET DETECTION MULTILINGUE\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if test_translation and \"fr\" in audio_samples:\n",
    "    sample_fr = audio_samples[\"fr\"]\n",
    "\n",
    "    # --- Transcription (langue source) ---\n",
    "    print(\"\\n--- Transcription (francais -> francais) ---\")\n",
    "    with open(sample_fr['path'], 'rb') as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=stt_model,\n",
    "            file=audio_file,\n",
    "            response_format=\"json\"\n",
    "        )\n",
    "    print(f\"Resultat : {transcript.text}\")\n",
    "\n",
    "    # --- Traduction (langue source -> anglais) ---\n",
    "    print(\"\\n--- Traduction (francais -> anglais) ---\")\n",
    "    with open(sample_fr['path'], 'rb') as audio_file:\n",
    "        translation = client.audio.translations.create(\n",
    "            model=stt_model,\n",
    "            file=audio_file,\n",
    "            response_format=\"json\"\n",
    "        )\n",
    "    print(f\"Resultat : {translation.text}\")\n",
    "\n",
    "    # --- Test multilingue ---\n",
    "    if \"multi\" in audio_samples:\n",
    "        print(\"\\n--- Detection multilingue ---\")\n",
    "        sample_multi = audio_samples[\"multi\"]\n",
    "        print(f\"Texte original : {sample_multi['text_original']}\")\n",
    "\n",
    "        with open(sample_multi['path'], 'rb') as audio_file:\n",
    "            transcript_multi = client.audio.transcriptions.create(\n",
    "                model=stt_model,\n",
    "                file=audio_file,\n",
    "                response_format=\"verbose_json\"\n",
    "            )\n",
    "        print(f\"Transcription : {transcript_multi.text}\")\n",
    "        print(f\"Langue detectee : {transcript_multi.language}\")\n",
    "\n",
    "    print(f\"\\nTraduction et detection terminees\")\n",
    "else:\n",
    "    print(\"Traduction desactivee ou echantillons manquants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Traduction et multilingue\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Traduction | Francais vers anglais | Whisper traduit directement sans etape intermediaire |\n",
    "| Multilingue | Detection langue dominante | Whisper identifie la langue principale du fichier |\n",
    "| Code-switching | Gere partiellement | Le melange de langues peut reduire la precision |\n",
    "\n",
    "> **Note technique** : L'endpoint `translations` ne supporte que la traduction vers l'anglais. Pour d'autres langues cibles, combiner Whisper STT + un LLM de traduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-intro",
   "metadata": {},
   "source": [
    "## Section 5 : Comparaison Whisper-1 vs GPT-4o-Transcribe\n",
    "\n",
    "OpenAI propose deux modeles de transcription :\n",
    "\n",
    "| Modele | Architecture | Forces | Limitations |\n",
    "|--------|-------------|--------|-------------|\n",
    "| `whisper-1` | Whisper V2 | Rapide, stable, multilingue | Precision moderee sur accents forts |\n",
    "| `gpt-4o-transcribe` | GPT-4o | Meilleure comprehension contextuelle | Plus lent, plus cher |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des modeles STT\n",
    "print(\"COMPARAISON MODELES STT\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "stt_models_to_test = [\"whisper-1\"]\n",
    "if compare_models:\n",
    "    stt_models_to_test.append(\"gpt-4o-transcribe\")\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "if \"fr\" in audio_samples:\n",
    "    sample = audio_samples[\"fr\"]\n",
    "\n",
    "    for model in stt_models_to_test:\n",
    "        print(f\"\\n--- Modele : {model} ---\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            with open(sample['path'], 'rb') as audio_file:\n",
    "                transcript = client.audio.transcriptions.create(\n",
    "                    model=model,\n",
    "                    file=audio_file,\n",
    "                    response_format=\"verbose_json\"\n",
    "                )\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            comparison_results[model] = {\n",
    "                \"text\": transcript.text,\n",
    "                \"time\": elapsed,\n",
    "                \"language\": transcript.language,\n",
    "                \"duration\": transcript.duration\n",
    "            }\n",
    "\n",
    "            print(f\"  Texte : {transcript.text}\")\n",
    "            print(f\"  Langue : {transcript.language}\")\n",
    "            print(f\"  Duree audio : {transcript.duration:.1f}s\")\n",
    "            print(f\"  Temps API : {elapsed:.2f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur : {str(e)[:100]}\")\n",
    "            comparison_results[model] = {\"error\": str(e)}\n",
    "\n",
    "    # Tableau comparatif\n",
    "    if len(comparison_results) > 1:\n",
    "        print(f\"\\nTableau comparatif :\")\n",
    "        print(f\"{'Modele':<25} {'Temps API':<12} {'Langue':<10}\")\n",
    "        print(\"-\" * 47)\n",
    "        for model, data in comparison_results.items():\n",
    "            if \"error\" not in data:\n",
    "                print(f\"{model:<25} {data['time']:<12.2f} {data['language']:<10}\")\n",
    "            else:\n",
    "                print(f\"{model:<25} {'ERREUR':<12} {'-':<10}\")\n",
    "else:\n",
    "    print(\"Aucun echantillon audio disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"MODE INTERACTIF\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nEntrez un texte a synthetiser puis transcrire (round-trip) :\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "\n",
    "    try:\n",
    "        user_text = input(\"\\nVotre texte : \")\n",
    "\n",
    "        if user_text.strip():\n",
    "            # Generation TTS\n",
    "            print(f\"\\n1. Generation TTS...\")\n",
    "            tts_response = client.audio.speech.create(\n",
    "                model=\"tts-1\",\n",
    "                voice=\"nova\",\n",
    "                input=user_text,\n",
    "                response_format=\"mp3\"\n",
    "            )\n",
    "            print(f\"   Audio genere ({len(tts_response.content)/1024:.1f} KB)\")\n",
    "            display(Audio(data=tts_response.content, autoplay=False))\n",
    "\n",
    "            # Sauvegarde temporaire pour transcription\n",
    "            temp_path = OUTPUT_DIR / \"interactive_sample.mp3\"\n",
    "            with open(temp_path, 'wb') as f:\n",
    "                f.write(tts_response.content)\n",
    "\n",
    "            # Transcription\n",
    "            print(f\"\\n2. Transcription STT...\")\n",
    "            with open(temp_path, 'rb') as audio_file:\n",
    "                transcript = client.audio.transcriptions.create(\n",
    "                    model=stt_model,\n",
    "                    file=audio_file,\n",
    "                    response_format=\"json\"\n",
    "                )\n",
    "\n",
    "            print(f\"   Original    : {user_text}\")\n",
    "            print(f\"   Transcrit   : {transcript.text}\")\n",
    "\n",
    "            # Comparaison\n",
    "            match = user_text.lower().strip() == transcript.text.lower().strip()\n",
    "            print(f\"   Correspondance exacte : {'Oui' if match else 'Non'}\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type} - {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-practices",
   "metadata": {},
   "source": [
    "## Bonnes pratiques et analyse des couts\n",
    "\n",
    "### Optimisation de la transcription\n",
    "\n",
    "| Strategie | Description | Impact |\n",
    "|-----------|-------------|--------|\n",
    "| Specifier la langue | `language=\"fr\"` | Precision accrue, latence reduite |\n",
    "| Audio propre | Reduire le bruit de fond | Meilleure qualite de transcription |\n",
    "| Segmenter les longs fichiers | Decouper en segments < 25 MB | Evite les timeouts |\n",
    "| Utiliser verbose_json | Exploiter les timestamps | Utile pour le sous-titrage |\n",
    "\n",
    "### Grille tarifaire (Janvier 2025)\n",
    "\n",
    "| Modele | Cout | Equivalent |\n",
    "|--------|------|------------|\n",
    "| `whisper-1` | $0.006 / minute | ~$0.36 / heure |\n",
    "| `gpt-4o-transcribe` | $0.006 / minute | ~$0.36 / heure |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"STATISTIQUES DE SESSION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Modele STT : {stt_model}\")\n",
    "print(f\"Format reponse : {response_format}\")\n",
    "print(f\"Echantillons generes : {len(audio_samples)}\")\n",
    "\n",
    "if comparison_results:\n",
    "    print(f\"Modeles compares : {list(comparison_results.keys())}\")\n",
    "\n",
    "if save_results:\n",
    "    saved = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"Fichiers sauvegardes : {len(saved)} dans {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nPROCHAINES ETAPES\")\n",
    "print(f\"1. Decouvrir les operations audio de base (01-3-Basic-Audio-Operations)\")\n",
    "print(f\"2. Tester Whisper en local avec GPU (01-4-Whisper-Local)\")\n",
    "print(f\"3. Essayer le TTS local avec Kokoro (01-5-Kokoro-TTS-Local)\")\n",
    "print(f\"4. Explorer la comparaison multi-modeles (03-1)\")\n",
    "\n",
    "print(f\"\\nNotebook Whisper STT termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}