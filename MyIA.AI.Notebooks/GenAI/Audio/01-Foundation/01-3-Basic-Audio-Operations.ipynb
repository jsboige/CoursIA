{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# Operations de Base sur l'Audio\n",
    "\n",
    "**Module :** 01-Audio-Foundation  \n",
    "**Niveau :** Debutant  \n",
    "**Technologies :** librosa, soundfile, pydub, matplotlib  \n",
    "**Duree estimee :** 35 minutes  \n",
    "**VRAM :** 0 (CPU uniquement)  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Charger et analyser des fichiers audio avec librosa\n",
    "- [ ] Visualiser des formes d'onde (waveform)\n",
    "- [ ] Generer et interpreter des spectrogrammes (mel, STFT)\n",
    "- [ ] Comprendre les MFCC (Mel-Frequency Cepstral Coefficients)\n",
    "- [ ] Manipuler l'audio avec pydub (conversion, trimming, concatenation)\n",
    "- [ ] Extraire des caracteristiques audio (tempo, pitch, energie)\n",
    "- [ ] Utiliser les fonctions du module audio_helpers.py\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Environment Setup (module 00) complete\n",
    "- Notions de base en Python et numpy\n",
    "- Cle API OpenAI recommandee (pour generer un echantillon) mais pas obligatoire\n",
    "\n",
    "**Navigation :** [Index](../README.md) | [<< Precedent](01-2-OpenAI-Whisper-STT.ipynb) | [Suivant >>](01-4-Whisper-Local.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-params",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres audio\n",
    "sample_duration = 5                # Duree de l'echantillon de test (secondes)\n",
    "sample_rate = 22050                # Taux d'echantillonnage par defaut\n",
    "\n",
    "# Configuration\n",
    "generate_sample_via_tts = True     # Generer un echantillon avec OpenAI TTS\n",
    "show_visualizations = True         # Afficher les visualisations\n",
    "test_pydub_ops = True              # Tester les operations pydub\n",
    "save_results = True                # Sauvegarder les resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from io import BytesIO\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Imports audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "# Lecture audio dans Jupyter\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.audio_helpers import (\n",
    "            plot_waveform, plot_spectrogram, plot_mfcc,\n",
    "            load_audio, save_audio, play_audio, get_audio_info\n",
    "        )\n",
    "        HELPERS_AVAILABLE = True\n",
    "        print(\"Helpers audio importes\")\n",
    "    except ImportError:\n",
    "        HELPERS_AVAILABLE = False\n",
    "        print(\"Helpers audio non disponibles - mode autonome\")\n",
    "else:\n",
    "    HELPERS_AVAILABLE = False\n",
    "\n",
    "# Repertoires\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'operations'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAMPLES_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'samples'\n",
    "SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('audio_ops')\n",
    "\n",
    "print(f\"Operations de Base sur l'Audio\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"librosa : {librosa.__version__}\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et preparation de l'echantillon audio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "# Preparation de l'echantillon audio\n",
    "sample_path = SAMPLES_DIR / \"sample_fr.mp3\"\n",
    "audio_data = None\n",
    "sr = sample_rate\n",
    "\n",
    "if sample_path.exists():\n",
    "    print(f\"Echantillon existant trouve : {sample_path.name}\")\n",
    "    audio_data, sr = librosa.load(str(sample_path), sr=sample_rate)\n",
    "    print(f\"  Duree : {len(audio_data)/sr:.1f}s, Sample rate : {sr} Hz\")\n",
    "\n",
    "elif generate_sample_via_tts:\n",
    "    openai_key = os.getenv('OPENAI_API_KEY')\n",
    "    if openai_key:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=openai_key)\n",
    "\n",
    "        tts_text = (\n",
    "            \"Le traitement numerique du signal audio permet d'analyser \"\n",
    "            \"les sons en termes de frequences, d'amplitude et de temps. \"\n",
    "            \"Les spectrogrammes revelent la structure frequentielle \"\n",
    "            \"des signaux audio.\"\n",
    "        )\n",
    "\n",
    "        print(\"Generation de l'echantillon via TTS...\")\n",
    "        response = client.audio.speech.create(\n",
    "            model=\"tts-1\", voice=\"nova\",\n",
    "            input=tts_text, response_format=\"mp3\"\n",
    "        )\n",
    "        with open(sample_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        audio_data, sr = librosa.load(str(sample_path), sr=sample_rate)\n",
    "        print(f\"Echantillon genere : {sample_path.name}\")\n",
    "        print(f\"  Duree : {len(audio_data)/sr:.1f}s, Sample rate : {sr} Hz\")\n",
    "        display(Audio(data=audio_data, rate=sr))\n",
    "    else:\n",
    "        print(\"OPENAI_API_KEY non disponible, generation d'un signal synthetique\")\n",
    "\n",
    "# Fallback : signal synthetique\n",
    "if audio_data is None:\n",
    "    print(\"Generation d'un signal synthetique de test...\")\n",
    "    t = np.linspace(0, sample_duration, sample_duration * sample_rate, endpoint=False)\n",
    "    # Signal compose : fondamentale 440 Hz + harmoniques\n",
    "    audio_data = (\n",
    "        0.5 * np.sin(2 * np.pi * 440 * t) +\n",
    "        0.3 * np.sin(2 * np.pi * 880 * t) +\n",
    "        0.1 * np.sin(2 * np.pi * 1320 * t)\n",
    "    ).astype(np.float32)\n",
    "    sr = sample_rate\n",
    "    print(f\"Signal synthetique : {sample_duration}s, {sr} Hz, 440 Hz + harmoniques\")\n",
    "\n",
    "print(f\"\\nEchantillon pret : {len(audio_data)} echantillons, {len(audio_data)/sr:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-intro",
   "metadata": {},
   "source": [
    "## Section 1 : Forme d'onde (Waveform)\n",
    "\n",
    "La forme d'onde est la representation la plus directe d'un signal audio : l'amplitude en fonction du temps.\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| Amplitude | Intensite du signal a un instant donne (-1.0 a 1.0 en normalise) |\n",
    "| Sample rate | Nombre d'echantillons par seconde (ex: 22050 Hz, 44100 Hz) |\n",
    "| Duree | Nombre d'echantillons / sample rate |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-waveform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la forme d'onde\n",
    "print(\"FORME D'ONDE (WAVEFORM)\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if show_visualizations:\n",
    "    # Utiliser les helpers si disponibles\n",
    "    if HELPERS_AVAILABLE:\n",
    "        plot_waveform(audio_data, sr, title=\"Forme d'onde - Echantillon de test\")\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(12, 3))\n",
    "        librosa.display.waveshow(audio_data, sr=sr, ax=ax)\n",
    "        ax.set_title(\"Forme d'onde - Echantillon de test\")\n",
    "        ax.set_xlabel(\"Temps (s)\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Statistiques du signal\n",
    "    print(f\"\\nStatistiques du signal :\")\n",
    "    print(f\"  Duree : {len(audio_data)/sr:.2f}s\")\n",
    "    print(f\"  Sample rate : {sr} Hz\")\n",
    "    print(f\"  Echantillons : {len(audio_data):,}\")\n",
    "    print(f\"  Amplitude max : {np.max(np.abs(audio_data)):.4f}\")\n",
    "    print(f\"  Amplitude moyenne : {np.mean(np.abs(audio_data)):.4f}\")\n",
    "    print(f\"  RMS : {np.sqrt(np.mean(audio_data**2)):.4f}\")\n",
    "    print(f\"  Dynamique (dB) : {20 * np.log10(np.max(np.abs(audio_data)) / (np.sqrt(np.mean(audio_data**2)) + 1e-10)):.1f} dB\")\n",
    "else:\n",
    "    print(\"Visualisations desactivees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-intro",
   "metadata": {},
   "source": [
    "## Section 2 : Spectrogrammes\n",
    "\n",
    "Un spectrogramme montre comment les frequences evoluent dans le temps. C'est l'outil fondamental de l'analyse audio.\n",
    "\n",
    "### Types de spectrogrammes\n",
    "\n",
    "| Type | Description | Usage |\n",
    "|------|-------------|-------|\n",
    "| STFT | Transformee de Fourier a court terme | Analyse frequentielle brute |\n",
    "| Mel | Echelle mel (perception humaine) | STT, classification audio |\n",
    "| Chromagramme | Notes musicales (C, D, E...) | Analyse musicale |\n",
    "\n",
    "L'echelle mel compresse les hautes frequences, imitant la perception humaine : nous distinguons mieux les differences dans les basses frequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-spectrograms",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogrammes\n",
    "print(\"SPECTROGRAMMES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if show_visualizations:\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    gs = GridSpec(3, 1, figure=fig, hspace=0.4)\n",
    "\n",
    "    # 1. Spectrogramme STFT\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    D = librosa.stft(audio_data)\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    img1 = librosa.display.specshow(S_db, x_axis='time', y_axis='hz', sr=sr, ax=ax1)\n",
    "    fig.colorbar(img1, ax=ax1, format='%+2.0f dB')\n",
    "    ax1.set_title(\"Spectrogramme STFT (echelle lineaire)\")\n",
    "\n",
    "    # 2. Spectrogramme Mel\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    S_mel = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_mels=128)\n",
    "    S_mel_db = librosa.power_to_db(S_mel, ref=np.max)\n",
    "    img2 = librosa.display.specshow(S_mel_db, x_axis='time', y_axis='mel', sr=sr, ax=ax2)\n",
    "    fig.colorbar(img2, ax=ax2, format='%+2.0f dB')\n",
    "    ax2.set_title(\"Spectrogramme Mel (echelle perceptuelle)\")\n",
    "\n",
    "    # 3. Chromagramme\n",
    "    ax3 = fig.add_subplot(gs[2])\n",
    "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n",
    "    img3 = librosa.display.specshow(chroma, x_axis='time', y_axis='chroma', sr=sr, ax=ax3)\n",
    "    fig.colorbar(img3, ax=ax3)\n",
    "    ax3.set_title(\"Chromagramme (notes musicales)\")\n",
    "\n",
    "    plt.suptitle(\"Analyse spectrale de l'echantillon audio\", fontsize=14, fontweight='bold', y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nDimensions spectrogramme STFT : {S_db.shape} (frequences x frames)\")\n",
    "    print(f\"Dimensions spectrogramme Mel : {S_mel_db.shape} (mels x frames)\")\n",
    "    print(f\"Dimensions chromagramme : {chroma.shape} (notes x frames)\")\n",
    "else:\n",
    "    print(\"Visualisations desactivees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Spectrogrammes\n",
    "\n",
    "| Type | Axes | Ce qu'on observe |\n",
    "|------|------|------------------|\n",
    "| STFT | Frequence (Hz) vs Temps | Structure frequentielle detaillee |\n",
    "| Mel | Mel bins vs Temps | Representation perceptuelle, concentree sur les basses |\n",
    "| Chroma | Note (C-B) vs Temps | Contenu harmonique, tonalite |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le spectrogramme Mel est le format d'entree standard pour les modeles de reconnaissance vocale (Whisper inclus)\n",
    "2. La voix humaine occupe principalement la bande 80 Hz - 8000 Hz\n",
    "3. Les zones claires (haute energie) correspondent aux sons vocaliques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-intro",
   "metadata": {},
   "source": [
    "## Section 3 : MFCC (Mel-Frequency Cepstral Coefficients)\n",
    "\n",
    "Les MFCC sont la representation la plus utilisee en reconnaissance vocale. Ils capturent l'enveloppe spectrale du signal, qui encode l'information phonetique.\n",
    "\n",
    "### Pipeline de calcul\n",
    "\n",
    "1. **Pre-emphasis** : Amplifier les hautes frequences\n",
    "2. **Fenetrage** : Decouper en trames de ~25ms\n",
    "3. **FFT** : Calcul du spectre de puissance\n",
    "4. **Banc de filtres Mel** : Application de filtres triangulaires\n",
    "5. **Log** : Passage en echelle logarithmique\n",
    "6. **DCT** : Transformee en cosinus discrete\n",
    "\n",
    "On garde typiquement 13 a 20 coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-mfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCC\n",
    "print(\"MFCC (Mel-Frequency Cepstral Coefficients)\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if show_visualizations:\n",
    "    n_mfcc = 13\n",
    "\n",
    "    # Calcul des MFCC\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "    # Utiliser les helpers si disponibles\n",
    "    if HELPERS_AVAILABLE:\n",
    "        plot_mfcc(audio_data, sr, n_mfcc=n_mfcc, title=\"MFCC - Echantillon de test\")\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "        img = librosa.display.specshow(mfccs, x_axis='time', ax=ax)\n",
    "        fig.colorbar(img, ax=ax)\n",
    "        ax.set_title(\"MFCC - Echantillon de test\")\n",
    "        ax.set_ylabel(\"Coefficient MFCC\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Statistiques MFCC\n",
    "    print(f\"\\nStatistiques MFCC :\")\n",
    "    print(f\"  Nombre de coefficients : {n_mfcc}\")\n",
    "    print(f\"  Frames temporelles : {mfccs.shape[1]}\")\n",
    "    print(f\"  Dimensions : {mfccs.shape}\")\n",
    "    print(f\"\\n  Moyenne par coefficient :\")\n",
    "    for i in range(min(n_mfcc, 5)):\n",
    "        print(f\"    MFCC[{i}] : moy={np.mean(mfccs[i]):.2f}, std={np.std(mfccs[i]):.2f}\")\n",
    "    if n_mfcc > 5:\n",
    "        print(f\"    ... ({n_mfcc - 5} coefficients supplementaires)\")\n",
    "\n",
    "    # Delta MFCC (vitesse de changement)\n",
    "    mfcc_delta = librosa.feature.delta(mfccs)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n",
    "    for ax, data, title in zip(axes, [mfccs, mfcc_delta, mfcc_delta2],\n",
    "                                [\"MFCC\", \"Delta MFCC (vitesse)\", \"Delta-Delta MFCC (acceleration)\"]):\n",
    "        img = librosa.display.specshow(data, x_axis='time', ax=ax)\n",
    "        fig.colorbar(img, ax=ax)\n",
    "        ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nLes Delta-MFCC capturent la dynamique temporelle du signal\")\n",
    "else:\n",
    "    print(\"Visualisations desactivees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-intro",
   "metadata": {},
   "source": [
    "## Section 4 : Manipulations avec pydub\n",
    "\n",
    "pydub est une bibliotheque de haut niveau pour manipuler l'audio :\n",
    "\n",
    "| Operation | Description | Methode |\n",
    "|-----------|-------------|--------|\n",
    "| Conversion de format | MP3 -> WAV, FLAC, etc. | `audio.export(format=...)` |\n",
    "| Trimming | Extraire un segment | `audio[start_ms:end_ms]` |\n",
    "| Concatenation | Assembler des segments | `audio1 + audio2` |\n",
    "| Volume | Ajuster le volume | `audio + dB` ou `audio - dB` |\n",
    "| Fade | Fondu en entree/sortie | `audio.fade_in(ms).fade_out(ms)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pydub",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulations avec pydub\n",
    "print(\"MANIPULATIONS PYDUB\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if test_pydub_ops:\n",
    "    from pydub import AudioSegment\n",
    "\n",
    "    # Charger l'echantillon\n",
    "    sample_file = SAMPLES_DIR / \"sample_fr.mp3\"\n",
    "    if sample_file.exists():\n",
    "        audio_seg = AudioSegment.from_file(str(sample_file))\n",
    "    else:\n",
    "        # Creer un segment depuis les donnees numpy\n",
    "        audio_int16 = (audio_data * 32767).astype(np.int16)\n",
    "        audio_seg = AudioSegment(\n",
    "            data=audio_int16.tobytes(),\n",
    "            sample_width=2, frame_rate=sr, channels=1\n",
    "        )\n",
    "\n",
    "    print(f\"Audio charge :\")\n",
    "    print(f\"  Duree : {len(audio_seg) / 1000:.1f}s\")\n",
    "    print(f\"  Channels : {audio_seg.channels}\")\n",
    "    print(f\"  Sample rate : {audio_seg.frame_rate} Hz\")\n",
    "    print(f\"  Sample width : {audio_seg.sample_width} bytes\")\n",
    "    print(f\"  dBFS : {audio_seg.dBFS:.1f} dB\")\n",
    "\n",
    "    # --- Trimming ---\n",
    "    print(f\"\\n--- Trimming ---\")\n",
    "    first_half = audio_seg[:len(audio_seg)//2]\n",
    "    last_2s = audio_seg[-2000:]\n",
    "    print(f\"  Premiere moitie : {len(first_half)/1000:.1f}s\")\n",
    "    print(f\"  Derniers 2s : {len(last_2s)/1000:.1f}s\")\n",
    "\n",
    "    # --- Volume ---\n",
    "    print(f\"\\n--- Ajustement volume ---\")\n",
    "    louder = audio_seg + 6    # +6 dB\n",
    "    quieter = audio_seg - 6   # -6 dB\n",
    "    print(f\"  Original : {audio_seg.dBFS:.1f} dBFS\")\n",
    "    print(f\"  +6 dB    : {louder.dBFS:.1f} dBFS\")\n",
    "    print(f\"  -6 dB    : {quieter.dBFS:.1f} dBFS\")\n",
    "\n",
    "    # --- Fade in/out ---\n",
    "    print(f\"\\n--- Fade in/out ---\")\n",
    "    faded = audio_seg.fade_in(500).fade_out(500)\n",
    "    print(f\"  Fade in : 500ms, Fade out : 500ms\")\n",
    "\n",
    "    # --- Concatenation ---\n",
    "    print(f\"\\n--- Concatenation ---\")\n",
    "    silence = AudioSegment.silent(duration=500)  # 500ms de silence\n",
    "    concatenated = first_half + silence + last_2s\n",
    "    print(f\"  Resultat : {len(concatenated)/1000:.1f}s (premiere moitie + silence + fin)\")\n",
    "\n",
    "    # --- Conversion de format ---\n",
    "    print(f\"\\n--- Conversion de format ---\")\n",
    "    formats = {\"wav\": \"wav\", \"flac\": \"flac\"}\n",
    "    for fmt_name, fmt_ext in formats.items():\n",
    "        output_path = OUTPUT_DIR / f\"converted.{fmt_ext}\"\n",
    "        audio_seg.export(str(output_path), format=fmt_ext)\n",
    "        size_kb = output_path.stat().st_size / 1024\n",
    "        print(f\"  {fmt_name.upper()} : {output_path.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "    # Ecoute du resultat concatene\n",
    "    print(f\"\\nEcoute du resultat concatene :\")\n",
    "    concat_path = OUTPUT_DIR / \"concatenated.wav\"\n",
    "    concatenated.export(str(concat_path), format=\"wav\")\n",
    "    concat_data, concat_sr = librosa.load(str(concat_path), sr=None)\n",
    "    display(Audio(data=concat_data, rate=concat_sr))\n",
    "else:\n",
    "    print(\"Operations pydub desactivees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-intro",
   "metadata": {},
   "source": [
    "## Section 5 : Extraction de caracteristiques audio\n",
    "\n",
    "librosa permet d'extraire des caracteristiques de haut niveau :\n",
    "\n",
    "| Caracteristique | Description | Unite |\n",
    "|-----------------|-------------|-------|\n",
    "| Tempo | Vitesse rythmique | BPM |\n",
    "| Zero-Crossing Rate | Frequence de passage par zero | Hz |\n",
    "| Spectral Centroid | Centre de gravite spectral | Hz |\n",
    "| Spectral Bandwidth | Largeur spectrale | Hz |\n",
    "| RMS Energy | Energie quadratique moyenne | - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction de caracteristiques\n",
    "print(\"EXTRACTION DE CARACTERISTIQUES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Tempo\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=audio_data, sr=sr)\n",
    "tempo_value = float(tempo) if np.isscalar(tempo) else float(tempo[0])\n",
    "print(f\"  Tempo estime : {tempo_value:.1f} BPM\")\n",
    "\n",
    "# Zero-Crossing Rate\n",
    "zcr = librosa.feature.zero_crossing_rate(audio_data)\n",
    "print(f\"  Zero-Crossing Rate (moy) : {np.mean(zcr):.4f}\")\n",
    "\n",
    "# Spectral Centroid\n",
    "spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=sr)\n",
    "print(f\"  Spectral Centroid (moy) : {np.mean(spectral_centroid):.1f} Hz\")\n",
    "\n",
    "# Spectral Bandwidth\n",
    "spectral_bw = librosa.feature.spectral_bandwidth(y=audio_data, sr=sr)\n",
    "print(f\"  Spectral Bandwidth (moy) : {np.mean(spectral_bw):.1f} Hz\")\n",
    "\n",
    "# Spectral Rolloff\n",
    "spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sr)\n",
    "print(f\"  Spectral Rolloff (moy) : {np.mean(spectral_rolloff):.1f} Hz\")\n",
    "\n",
    "# RMS Energy\n",
    "rms = librosa.feature.rms(y=audio_data)\n",
    "print(f\"  RMS Energy (moy) : {np.mean(rms):.4f}\")\n",
    "\n",
    "# Visualisation des caracteristiques\n",
    "if show_visualizations:\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
    "    frames = range(len(spectral_centroid[0]))\n",
    "    t_frames = librosa.frames_to_time(list(frames), sr=sr)\n",
    "\n",
    "    axes[0].plot(t_frames, spectral_centroid[0], color='blue')\n",
    "    axes[0].set_title(\"Spectral Centroid\")\n",
    "    axes[0].set_ylabel(\"Hz\")\n",
    "\n",
    "    axes[1].plot(t_frames, spectral_bw[0], color='green')\n",
    "    axes[1].set_title(\"Spectral Bandwidth\")\n",
    "    axes[1].set_ylabel(\"Hz\")\n",
    "\n",
    "    t_rms = librosa.frames_to_time(list(range(len(rms[0]))), sr=sr)\n",
    "    axes[2].plot(t_rms, rms[0], color='red')\n",
    "    axes[2].set_title(\"RMS Energy\")\n",
    "    axes[2].set_ylabel(\"Amplitude\")\n",
    "\n",
    "    t_zcr = librosa.frames_to_time(list(range(len(zcr[0]))), sr=sr)\n",
    "    axes[3].plot(t_zcr, zcr[0], color='purple')\n",
    "    axes[3].set_title(\"Zero-Crossing Rate\")\n",
    "    axes[3].set_ylabel(\"Rate\")\n",
    "    axes[3].set_xlabel(\"Temps (s)\")\n",
    "\n",
    "    plt.suptitle(\"Caracteristiques audio extraites\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Caracteristiques audio\n",
    "\n",
    "| Caracteristique | Valeur basse | Valeur haute |\n",
    "|-----------------|-------------|---------------|\n",
    "| Spectral Centroid | Sons graves, sombres | Sons aigus, brillants |\n",
    "| Spectral Bandwidth | Signal tonal pur | Signal bruyanf, riche |\n",
    "| ZCR | Voix grave, voyelles | Consonnes fricatives, bruit |\n",
    "| RMS | Silence, pauses | Parole forte, musique |\n",
    "\n",
    "> **Note technique** : Ces caracteristiques sont utilisees dans les systemes de classification audio (genre musical, detection de parole vs musique, reconnaissance d'emotions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"MODE INTERACTIF\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nEntrez le chemin d'un fichier audio a analyser :\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "\n",
    "    try:\n",
    "        user_path = input(\"\\nChemin du fichier audio : \")\n",
    "\n",
    "        if user_path.strip():\n",
    "            user_file = Path(user_path.strip())\n",
    "            if user_file.exists():\n",
    "                user_audio, user_sr = librosa.load(str(user_file), sr=None)\n",
    "                print(f\"\\nFichier charge : {user_file.name}\")\n",
    "                print(f\"  Duree : {len(user_audio)/user_sr:.1f}s\")\n",
    "                print(f\"  Sample rate : {user_sr} Hz\")\n",
    "\n",
    "                # Visualisations\n",
    "                if HELPERS_AVAILABLE:\n",
    "                    plot_waveform(user_audio, user_sr, title=f\"Forme d'onde - {user_file.name}\")\n",
    "                    plot_spectrogram(user_audio, user_sr, title=f\"Spectrogramme - {user_file.name}\")\n",
    "                    plot_mfcc(user_audio, user_sr, title=f\"MFCC - {user_file.name}\")\n",
    "                display(Audio(data=user_audio, rate=user_sr))\n",
    "            else:\n",
    "                print(f\"Fichier non trouve : {user_file}\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type} - {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-practices",
   "metadata": {},
   "source": [
    "## Bonnes pratiques et recapitulatif\n",
    "\n",
    "### Guide de selection des outils\n",
    "\n",
    "| Tache | Outil recommande | Raison |\n",
    "|-------|-----------------|--------|\n",
    "| Analyse spectrale | librosa | API complete, visualisations integrees |\n",
    "| Lecture/ecriture formats | soundfile | Support natif WAV, FLAC, OGG |\n",
    "| Conversion, montage | pydub | API de haut niveau, simple |\n",
    "| Visualisation | matplotlib + librosa.display | Integration parfaite |\n",
    "| Lecture dans Jupyter | IPython.display.Audio | Lecteur integre |\n",
    "\n",
    "### Representations cles pour le ML\n",
    "\n",
    "| Representation | Usage ML | Dimensions typiques |\n",
    "|----------------|----------|--------------------|\n",
    "| Mel Spectrogram | Entree Whisper, classification | (128, T) |\n",
    "| MFCC | Classification traditionnelle | (13-20, T) |\n",
    "| Raw waveform | WaveNet, modeles end-to-end | (N,) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"STATISTIQUES DE SESSION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Duree echantillon analyse : {len(audio_data)/sr:.1f}s\")\n",
    "print(f\"Sample rate : {sr} Hz\")\n",
    "print(f\"Helpers audio : {'disponibles' if HELPERS_AVAILABLE else 'non disponibles'}\")\n",
    "\n",
    "if save_results:\n",
    "    saved = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"Fichiers sauvegardes : {len(saved)} dans {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nOperations couvertes :\")\n",
    "print(f\"  1. Forme d'onde (waveform) et statistiques temporelles\")\n",
    "print(f\"  2. Spectrogrammes (STFT, Mel, Chromagramme)\")\n",
    "print(f\"  3. MFCC et Delta-MFCC\")\n",
    "print(f\"  4. Manipulations pydub (trim, concat, volume, conversion)\")\n",
    "print(f\"  5. Extraction de caracteristiques (tempo, centroid, etc.)\")\n",
    "\n",
    "print(f\"\\nPROCHAINES ETAPES\")\n",
    "print(f\"1. Tester Whisper en local avec GPU (01-4-Whisper-Local)\")\n",
    "print(f\"2. Essayer le TTS local avec Kokoro (01-5-Kokoro-TTS-Local)\")\n",
    "print(f\"3. Explorer le voice cloning (02-2-XTTS-Voice-Cloning)\")\n",
    "print(f\"4. Decouvrir la separation de sources (02-4-Demucs)\")\n",
    "\n",
    "print(f\"\\nNotebook Operations Audio termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}