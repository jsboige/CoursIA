{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# Orchestration de Pipelines Audio\n",
    "\n",
    "**Module :** 03-Audio-Orchestration  \n",
    "**Niveau :** Avance  \n",
    "**Technologies :** STT->LLM->TTS pipelines, faster-whisper, OpenAI GPT, Kokoro/Chatterbox, ~14 GB VRAM  \n",
    "**Duree estimee :** 60 minutes  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Construire un pipeline STT->LLM->TTS (question audio -> transcription -> reponse GPT -> synthese vocale)\n",
    "- [ ] Implementer un pipeline de traduction audio (FR audio -> EN texte -> EN speech)\n",
    "- [ ] Generer un podcast multi-voix a partir d'un sujet (2+ voix discutant d'un theme)\n",
    "- [ ] Gerer les erreurs et les retries dans les pipelines\n",
    "- [ ] Profiler les performances par etape du pipeline\n",
    "- [ ] Identifier les goulots d'etranglement et optimiser\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Notebook 03-1 recommande (comparaison des modeles)\n",
    "- GPU NVIDIA avec au moins 14 GB VRAM\n",
    "- Cle API OpenAI configuree (`OPENAI_API_KEY` dans `.env`)\n",
    "- `pip install faster-whisper openai soundfile kokoro`\n",
    "\n",
    "**Navigation :** [<< 03-1](03-1-Multi-Model-Audio-Comparison.ipynb) | [Index](../README.md) | [Suivant >>](03-3-Realtime-Voice-API.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-params",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres pipeline\n",
    "pipeline_mode = \"stt_llm_tts\"      # \"stt_llm_tts\", \"translation\", \"podcast\"\n",
    "llm_model = \"gpt-4o-mini\"          # Modele LLM pour le traitement\n",
    "tts_voice = \"alloy\"                # Voix OpenAI pour le TTS\n",
    "device = \"cuda\"                    # \"cuda\" ou \"cpu\"\n",
    "\n",
    "# Configuration\n",
    "save_results = True                # Sauvegarder les fichiers generes\n",
    "max_retries = 3                    # Nombre max de retries par etape\n",
    "generate_podcast = True            # Generer le podcast multi-voix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.audio_helpers import play_audio, save_audio\n",
    "        print(\"Helpers audio importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers audio non disponibles - mode autonome\")\n",
    "\n",
    "# Repertoires\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'pipelines'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('audio_pipelines')\n",
    "\n",
    "# Verification GPU\n",
    "gpu_available = False\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_vram = torch.cuda.get_device_properties(0).total_mem / (1024**3)\n",
    "        print(f\"GPU : {gpu_name} ({gpu_vram:.1f} GB VRAM)\")\n",
    "    else:\n",
    "        print(\"GPU non disponible - pipelines locaux seront plus lents\")\n",
    "        if device == \"cuda\":\n",
    "            device = \"cpu\"\n",
    "            print(\"Fallback vers CPU\")\n",
    "except ImportError:\n",
    "    print(\"torch non installe\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"\\nOrchestration de Pipelines Audio\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}, Device : {device}\")\n",
    "print(f\"Pipeline : {pipeline_mode}\")\n",
    "print(f\"LLM : {llm_model}, TTS voice : {tts_voice}\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et validation API\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Validation cle OpenAI (necessaire pour LLM et TTS API)\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_available = False\n",
    "\n",
    "if openai_key:\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=openai_key)\n",
    "        openai_available = True\n",
    "        print(f\"OPENAI_API_KEY valide - API disponible\")\n",
    "        print(f\"LLM : {llm_model}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur OpenAI : {str(e)[:100]}\")\n",
    "else:\n",
    "    if notebook_mode == \"batch\":\n",
    "        print(\"Mode batch sans API : validation structurelle uniquement\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"OPENAI_API_KEY manquante dans .env\\n\"\n",
    "            \"Les pipelines necessitent l'API OpenAI pour le LLM et le TTS.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-intro",
   "metadata": {},
   "source": [
    "## Section 1 : Architecture des pipelines\n",
    "\n",
    "Un pipeline audio orchestre plusieurs modeles en sequence. Chaque etape transforme les donnees et les passe a la suivante.\n",
    "\n",
    "### Pipeline STT -> LLM -> TTS\n",
    "\n",
    "| Etape | Entree | Modele | Sortie |\n",
    "|-------|--------|--------|--------|\n",
    "| 1. STT | Audio WAV | faster-whisper | Texte transcrit |\n",
    "| 2. LLM | Texte (question) | GPT-4o-mini | Texte (reponse) |\n",
    "| 3. TTS | Texte (reponse) | OpenAI TTS | Audio WAV |\n",
    "\n",
    "### Pipeline de traduction\n",
    "\n",
    "| Etape | Entree | Modele | Sortie |\n",
    "|-------|--------|--------|--------|\n",
    "| 1. STT | Audio FR | faster-whisper | Texte FR |\n",
    "| 2. LLM | Texte FR | GPT-4o-mini | Texte EN |\n",
    "| 3. TTS | Texte EN | OpenAI TTS | Audio EN |\n",
    "\n",
    "### Gestion des erreurs\n",
    "\n",
    "| Type d'erreur | Strategie | Retries |\n",
    "|--------------|-----------|--------|\n",
    "| Timeout API | Retry exponentiel | 3 |\n",
    "| Erreur reseau | Retry avec backoff | 3 |\n",
    "| GPU OOM | Fallback CPU | 1 |\n",
    "| Audio corrompu | Skip avec log | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pipeline-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework de pipeline avec gestion d'erreurs\n",
    "print(\"FRAMEWORK DE PIPELINE\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineStep:\n",
    "    \"\"\"Resultat d'une etape du pipeline.\"\"\"\n",
    "    name: str\n",
    "    input_type: str\n",
    "    output_type: str\n",
    "    duration_s: float = 0.0\n",
    "    success: bool = False\n",
    "    error: Optional[str] = None\n",
    "    output_data: Any = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineResult:\n",
    "    \"\"\"Resultat complet d'un pipeline.\"\"\"\n",
    "    name: str\n",
    "    steps: List[PipelineStep] = field(default_factory=list)\n",
    "    total_duration_s: float = 0.0\n",
    "    success: bool = False\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        lines = [f\"Pipeline: {self.name}\"]\n",
    "        lines.append(f\"Statut: {'Succes' if self.success else 'Echec'}\")\n",
    "        lines.append(f\"Duree totale: {self.total_duration_s:.2f}s\")\n",
    "        for step in self.steps:\n",
    "            status = 'OK' if step.success else f'ERREUR: {step.error}'\n",
    "            lines.append(f\"  {step.name}: {step.duration_s:.2f}s [{status}]\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def retry_with_backoff(func, max_retries: int = 3, base_delay: float = 1.0):\n",
    "    \"\"\"Executer une fonction avec retry exponentiel.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "            logger.warning(f\"Tentative {attempt+1}/{max_retries} echouee: {e}. Retry dans {delay}s\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "\n",
    "# Statistiques globales\n",
    "pipeline_results: List[PipelineResult] = []\n",
    "\n",
    "print(\"Framework initialise\")\n",
    "print(f\"Max retries : {max_retries}\")\n",
    "print(f\"Gestion d'erreurs : retry exponentiel + fallback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-intro",
   "metadata": {},
   "source": [
    "## Section 2 : Pipeline STT -> LLM -> TTS\n",
    "\n",
    "Ce pipeline permet de poser une question en audio et d'obtenir une reponse vocale generee par un LLM. C'est la base d'un assistant vocal.\n",
    "\n",
    "| Composant | Modele | Role |\n",
    "|-----------|--------|------|\n",
    "| STT | faster-whisper large-v3-turbo | Transcription de la question |\n",
    "| LLM | GPT-4o-mini | Generation de la reponse |\n",
    "| TTS | OpenAI tts-1 | Synthese vocale de la reponse |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stt-llm-tts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline STT -> LLM -> TTS\n",
    "print(\"PIPELINE STT -> LLM -> TTS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Generer un audio de test (question)\n",
    "question_text = \"What are the main advantages of using local AI models compared to cloud APIs?\"\n",
    "question_path = OUTPUT_DIR / \"question.wav\"\n",
    "\n",
    "if openai_available:\n",
    "    result = PipelineResult(name=\"stt_llm_tts\")\n",
    "\n",
    "    # 0. Generer l'audio de la question\n",
    "    print(\"Etape 0 : Generation de la question audio...\")\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\", voice=\"nova\", input=question_text, response_format=\"wav\"\n",
    "    )\n",
    "    with open(question_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"  Question : {question_text}\")\n",
    "    display(Audio(data=response.content, autoplay=False))\n",
    "\n",
    "    # --- ETAPE 1 : STT ---\n",
    "    print(f\"\\nEtape 1 : STT (transcription)...\")\n",
    "    step1 = PipelineStep(name=\"STT\", input_type=\"audio\", output_type=\"text\")\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Essayer faster-whisper local d'abord\n",
    "        stt_used = \"API\"\n",
    "        try:\n",
    "            from faster_whisper import WhisperModel\n",
    "            stt_model = WhisperModel(\"large-v3-turbo\", device=device,\n",
    "                                     compute_type=\"float16\" if device == \"cuda\" else \"int8\")\n",
    "            segments, info = stt_model.transcribe(str(question_path), beam_size=5)\n",
    "            transcribed_text = \" \".join([s.text.strip() for s in segments])\n",
    "            stt_used = \"local (faster-whisper)\"\n",
    "            del stt_model\n",
    "            gc.collect()\n",
    "            if gpu_available:\n",
    "                torch.cuda.empty_cache()\n",
    "        except ImportError:\n",
    "            # Fallback vers API Whisper\n",
    "            with open(question_path, \"rb\") as f:\n",
    "                transcribed_text = client.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\", file=f, response_format=\"text\"\n",
    "                )\n",
    "            stt_used = \"API (whisper-1)\"\n",
    "\n",
    "        step1.duration_s = time.time() - start_time\n",
    "        step1.success = True\n",
    "        step1.output_data = transcribed_text\n",
    "        print(f\"  Methode : {stt_used}\")\n",
    "        print(f\"  Transcription : {transcribed_text}\")\n",
    "        print(f\"  Duree : {step1.duration_s:.2f}s\")\n",
    "    except Exception as e:\n",
    "        step1.error = str(e)[:100]\n",
    "        print(f\"  Erreur STT : {step1.error}\")\n",
    "    result.steps.append(step1)\n",
    "\n",
    "    # --- ETAPE 2 : LLM ---\n",
    "    print(f\"\\nEtape 2 : LLM (generation reponse)...\")\n",
    "    step2 = PipelineStep(name=\"LLM\", input_type=\"text\", output_type=\"text\")\n",
    "\n",
    "    if step1.success:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            def call_llm():\n",
    "                return client.chat.completions.create(\n",
    "                    model=llm_model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Answer concisely in 2-3 sentences.\"},\n",
    "                        {\"role\": \"user\", \"content\": transcribed_text}\n",
    "                    ],\n",
    "                    max_tokens=200\n",
    "                )\n",
    "\n",
    "            llm_response = retry_with_backoff(call_llm, max_retries=max_retries)\n",
    "            answer_text = llm_response.choices[0].message.content\n",
    "\n",
    "            step2.duration_s = time.time() - start_time\n",
    "            step2.success = True\n",
    "            step2.output_data = answer_text\n",
    "            print(f\"  Modele : {llm_model}\")\n",
    "            print(f\"  Reponse : {answer_text}\")\n",
    "            print(f\"  Tokens : {llm_response.usage.total_tokens}\")\n",
    "            print(f\"  Duree : {step2.duration_s:.2f}s\")\n",
    "        except Exception as e:\n",
    "            step2.error = str(e)[:100]\n",
    "            print(f\"  Erreur LLM : {step2.error}\")\n",
    "    else:\n",
    "        step2.error = \"Etape precedente en echec\"\n",
    "        print(f\"  Skipped : STT en echec\")\n",
    "    result.steps.append(step2)\n",
    "\n",
    "    # --- ETAPE 3 : TTS ---\n",
    "    print(f\"\\nEtape 3 : TTS (synthese vocale)...\")\n",
    "    step3 = PipelineStep(name=\"TTS\", input_type=\"text\", output_type=\"audio\")\n",
    "\n",
    "    if step2.success:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            def call_tts():\n",
    "                return client.audio.speech.create(\n",
    "                    model=\"tts-1\", voice=tts_voice,\n",
    "                    input=answer_text, response_format=\"wav\"\n",
    "                )\n",
    "\n",
    "            tts_response = retry_with_backoff(call_tts, max_retries=max_retries)\n",
    "\n",
    "            step3.duration_s = time.time() - start_time\n",
    "            step3.success = True\n",
    "            step3.output_data = tts_response.content\n",
    "\n",
    "            answer_path = OUTPUT_DIR / \"pipeline_answer.wav\"\n",
    "            with open(answer_path, 'wb') as f:\n",
    "                f.write(tts_response.content)\n",
    "\n",
    "            print(f\"  Voix : {tts_voice}\")\n",
    "            print(f\"  Taille : {len(tts_response.content)/1024:.1f} KB\")\n",
    "            print(f\"  Duree : {step3.duration_s:.2f}s\")\n",
    "            print(f\"\\n  Reponse audio :\")\n",
    "            display(Audio(data=tts_response.content, autoplay=False))\n",
    "        except Exception as e:\n",
    "            step3.error = str(e)[:100]\n",
    "            print(f\"  Erreur TTS : {step3.error}\")\n",
    "    else:\n",
    "        step3.error = \"Etape precedente en echec\"\n",
    "        print(f\"  Skipped : LLM en echec\")\n",
    "    result.steps.append(step3)\n",
    "\n",
    "    # Resume du pipeline\n",
    "    result.total_duration_s = sum(s.duration_s for s in result.steps)\n",
    "    result.success = all(s.success for s in result.steps)\n",
    "    pipeline_results.append(result)\n",
    "\n",
    "    print(f\"\\n{result.summary()}\")\n",
    "else:\n",
    "    print(\"API OpenAI non disponible - pipeline desactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Pipeline STT -> LLM -> TTS\n",
    "\n",
    "| Etape | Latence typique | Goulot d'etranglement |\n",
    "|-------|----------------|----------------------|\n",
    "| STT (local) | 0.5-2s | GPU / taille audio |\n",
    "| STT (API) | 1-3s | Reseau |\n",
    "| LLM | 1-3s | Generation de tokens |\n",
    "| TTS | 1-2s | Reseau / synthese |\n",
    "| **Total** | **3-8s** | **LLM souvent le plus lent** |\n",
    "\n",
    "**Points cles** :\n",
    "1. La latence totale du pipeline est la somme des latences individuelles\n",
    "2. Le LLM est souvent le goulot d'etranglement (generation autoregressive)\n",
    "3. Le retry exponentiel protege contre les erreurs transitoires\n",
    "4. Pour du temps reel, privilegier des modeles plus petits (gpt-4o-mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-intro",
   "metadata": {},
   "source": [
    "## Section 3 : Pipeline de traduction audio\n",
    "\n",
    "Ce pipeline traduit un message vocal d'une langue a une autre en combinant STT, traduction LLM et TTS.\n",
    "\n",
    "| Etape | Description | Modele |\n",
    "|-------|-------------|--------|\n",
    "| 1. Transcription | Audio FR -> Texte FR | faster-whisper |\n",
    "| 2. Traduction | Texte FR -> Texte EN | GPT-4o-mini |\n",
    "| 3. Synthese | Texte EN -> Audio EN | OpenAI TTS |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-translation-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de traduction audio FR -> EN\n",
    "print(\"PIPELINE DE TRADUCTION AUDIO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "french_text = (\n",
    "    \"L'intelligence artificielle permet aujourd'hui de traduire automatiquement \"\n",
    "    \"la parole d'une langue a une autre en temps quasi reel. \"\n",
    "    \"Cette technologie ouvre des perspectives pour la communication internationale.\"\n",
    ")\n",
    "\n",
    "if openai_available:\n",
    "    result = PipelineResult(name=\"translation_fr_en\")\n",
    "\n",
    "    # 0. Generer l'audio FR\n",
    "    print(\"Etape 0 : Generation de l'audio source (FR)...\")\n",
    "    fr_response = client.audio.speech.create(\n",
    "        model=\"tts-1\", voice=\"nova\", input=french_text, response_format=\"wav\"\n",
    "    )\n",
    "    fr_path = OUTPUT_DIR / \"translation_source_fr.wav\"\n",
    "    with open(fr_path, 'wb') as f:\n",
    "        f.write(fr_response.content)\n",
    "    print(f\"  Texte FR : {french_text[:80]}...\")\n",
    "    display(Audio(data=fr_response.content, autoplay=False))\n",
    "\n",
    "    # --- ETAPE 1 : STT (FR) ---\n",
    "    print(f\"\\nEtape 1 : Transcription (FR)...\")\n",
    "    step1 = PipelineStep(name=\"STT-FR\", input_type=\"audio-fr\", output_type=\"text-fr\")\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        with open(fr_path, \"rb\") as f:\n",
    "            fr_transcript = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\", file=f, response_format=\"text\", language=\"fr\"\n",
    "            )\n",
    "        step1.duration_s = time.time() - start_time\n",
    "        step1.success = True\n",
    "        step1.output_data = fr_transcript\n",
    "        print(f\"  Transcription FR : {fr_transcript}\")\n",
    "        print(f\"  Duree : {step1.duration_s:.2f}s\")\n",
    "    except Exception as e:\n",
    "        step1.error = str(e)[:100]\n",
    "        print(f\"  Erreur : {step1.error}\")\n",
    "    result.steps.append(step1)\n",
    "\n",
    "    # --- ETAPE 2 : Traduction LLM (FR -> EN) ---\n",
    "    print(f\"\\nEtape 2 : Traduction (FR -> EN)...\")\n",
    "    step2 = PipelineStep(name=\"LLM-Translate\", input_type=\"text-fr\", output_type=\"text-en\")\n",
    "\n",
    "    if step1.success:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            translation = client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Translate the following French text to English. Return only the translation, no explanations.\"},\n",
    "                    {\"role\": \"user\", \"content\": fr_transcript}\n",
    "                ],\n",
    "                max_tokens=300\n",
    "            )\n",
    "            en_text = translation.choices[0].message.content\n",
    "            step2.duration_s = time.time() - start_time\n",
    "            step2.success = True\n",
    "            step2.output_data = en_text\n",
    "            print(f\"  Traduction EN : {en_text}\")\n",
    "            print(f\"  Duree : {step2.duration_s:.2f}s\")\n",
    "        except Exception as e:\n",
    "            step2.error = str(e)[:100]\n",
    "            print(f\"  Erreur : {step2.error}\")\n",
    "    else:\n",
    "        step2.error = \"STT en echec\"\n",
    "    result.steps.append(step2)\n",
    "\n",
    "    # --- ETAPE 3 : TTS (EN) ---\n",
    "    print(f\"\\nEtape 3 : Synthese vocale (EN)...\")\n",
    "    step3 = PipelineStep(name=\"TTS-EN\", input_type=\"text-en\", output_type=\"audio-en\")\n",
    "\n",
    "    if step2.success:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            en_response = client.audio.speech.create(\n",
    "                model=\"tts-1\", voice=tts_voice,\n",
    "                input=en_text, response_format=\"wav\"\n",
    "            )\n",
    "            step3.duration_s = time.time() - start_time\n",
    "            step3.success = True\n",
    "            step3.output_data = en_response.content\n",
    "\n",
    "            en_path = OUTPUT_DIR / \"translation_result_en.wav\"\n",
    "            with open(en_path, 'wb') as f:\n",
    "                f.write(en_response.content)\n",
    "\n",
    "            print(f\"  Taille : {len(en_response.content)/1024:.1f} KB\")\n",
    "            print(f\"  Duree : {step3.duration_s:.2f}s\")\n",
    "            print(f\"\\n  Audio traduit (EN) :\")\n",
    "            display(Audio(data=en_response.content, autoplay=False))\n",
    "        except Exception as e:\n",
    "            step3.error = str(e)[:100]\n",
    "            print(f\"  Erreur : {step3.error}\")\n",
    "    else:\n",
    "        step3.error = \"Traduction en echec\"\n",
    "    result.steps.append(step3)\n",
    "\n",
    "    result.total_duration_s = sum(s.duration_s for s in result.steps)\n",
    "    result.success = all(s.success for s in result.steps)\n",
    "    pipeline_results.append(result)\n",
    "\n",
    "    print(f\"\\n{result.summary()}\")\n",
    "else:\n",
    "    print(\"API OpenAI non disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Pipeline de traduction\n",
    "\n",
    "| Aspect | Observation | Signification |\n",
    "|--------|-------------|---------------|\n",
    "| Qualite STT FR | Bonne avec Whisper | Le modele gere bien le francais |\n",
    "| Qualite traduction | Excellente via GPT | LLM performant pour la traduction |\n",
    "| Qualite TTS EN | Naturelle | OpenAI TTS produit un anglais naturel |\n",
    "| Latence totale | 3-8s | Acceptable pour du quasi temps reel |\n",
    "\n",
    "**Points cles** :\n",
    "1. La chaine STT->Traduction->TTS fonctionne bien pour des textes courts a moyens\n",
    "2. Les erreurs de transcription peuvent se propager dans la traduction\n",
    "3. Pour de la traduction en temps reel, l'API Realtime d'OpenAI est plus adaptee (03-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-intro",
   "metadata": {},
   "source": [
    "## Section 4 : Generation de podcast multi-voix\n",
    "\n",
    "Ce pipeline genere un podcast ou deux locuteurs discutent d'un sujet. Le LLM genere le script, puis chaque replique est synthetisee avec une voix differente.\n",
    "\n",
    "| Etape | Description |\n",
    "|-------|-------------|\n",
    "| 1. Generation du script | LLM cree un dialogue structure |\n",
    "| 2. Synthese par replique | Chaque replique est synthetisee avec sa voix |\n",
    "| 3. Assemblage | Les segments audio sont concatenes avec pauses |\n",
    "\n",
    "### Attribution des voix\n",
    "\n",
    "| Locuteur | Voix OpenAI | Caractere |\n",
    "|----------|-------------|----------|\n",
    "| Host | alloy | Neutre, professionnel |\n",
    "| Expert | onyx | Grave, autoritaire |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-podcast-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation de podcast multi-voix\n",
    "print(\"GENERATION DE PODCAST MULTI-VOIX\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "podcast_topic = \"the future of local AI models versus cloud-based AI services\"\n",
    "voice_mapping = {\n",
    "    \"Host\": \"alloy\",\n",
    "    \"Expert\": \"onyx\"\n",
    "}\n",
    "\n",
    "if openai_available and generate_podcast:\n",
    "    result = PipelineResult(name=\"podcast_generation\")\n",
    "\n",
    "    # --- ETAPE 1 : Generation du script ---\n",
    "    print(f\"Etape 1 : Generation du script...\")\n",
    "    step1 = PipelineStep(name=\"Script-LLM\", input_type=\"topic\", output_type=\"json-script\")\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        script_response = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": (\n",
    "                    \"Generate a short podcast script as a JSON array of objects. \"\n",
    "                    \"Each object has 'speaker' (either 'Host' or 'Expert') and 'text' (the dialogue line). \"\n",
    "                    \"Generate exactly 6 exchanges (3 per speaker). Keep each line under 40 words. \"\n",
    "                    \"Return ONLY the JSON array, no markdown formatting.\"\n",
    "                )},\n",
    "                {\"role\": \"user\", \"content\": f\"Topic: {podcast_topic}\"}\n",
    "            ],\n",
    "            max_tokens=800\n",
    "        )\n",
    "\n",
    "        script_text = script_response.choices[0].message.content\n",
    "        # Nettoyer le JSON (enlever les backticks markdown si presents)\n",
    "        script_text = script_text.strip()\n",
    "        if script_text.startswith(\"```\"):\n",
    "            script_text = script_text.split(\"\\n\", 1)[1]\n",
    "            script_text = script_text.rsplit(\"```\", 1)[0]\n",
    "        script_lines = json.loads(script_text)\n",
    "\n",
    "        step1.duration_s = time.time() - start_time\n",
    "        step1.success = True\n",
    "        step1.output_data = script_lines\n",
    "\n",
    "        print(f\"  Script genere ({len(script_lines)} repliques) :\")\n",
    "        for line in script_lines:\n",
    "            print(f\"    [{line['speaker']}] {line['text'][:80]}\")\n",
    "        print(f\"  Duree : {step1.duration_s:.2f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        step1.error = str(e)[:150]\n",
    "        print(f\"  Erreur : {step1.error}\")\n",
    "        script_lines = []\n",
    "    result.steps.append(step1)\n",
    "\n",
    "    # --- ETAPE 2 : Synthese TTS par replique ---\n",
    "    print(f\"\\nEtape 2 : Synthese vocale par replique...\")\n",
    "    step2 = PipelineStep(name=\"TTS-Multi\", input_type=\"json-script\", output_type=\"audio-segments\")\n",
    "\n",
    "    audio_segments = []\n",
    "\n",
    "    if step1.success and script_lines:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            for idx, line in enumerate(script_lines):\n",
    "                speaker = line[\"speaker\"]\n",
    "                voice = voice_mapping.get(speaker, \"alloy\")\n",
    "                text = line[\"text\"]\n",
    "\n",
    "                tts_resp = client.audio.speech.create(\n",
    "                    model=\"tts-1\", voice=voice,\n",
    "                    input=text, response_format=\"wav\"\n",
    "                )\n",
    "\n",
    "                # Sauvegarder le segment\n",
    "                seg_path = OUTPUT_DIR / f\"podcast_seg_{idx:02d}_{speaker.lower()}.wav\"\n",
    "                with open(seg_path, 'wb') as f:\n",
    "                    f.write(tts_resp.content)\n",
    "\n",
    "                audio_segments.append({\n",
    "                    \"speaker\": speaker,\n",
    "                    \"path\": seg_path,\n",
    "                    \"size_kb\": len(tts_resp.content) / 1024\n",
    "                })\n",
    "                print(f\"  Segment {idx+1}/{len(script_lines)} [{speaker}:{voice}] - {len(tts_resp.content)/1024:.1f} KB\")\n",
    "\n",
    "            step2.duration_s = time.time() - start_time\n",
    "            step2.success = True\n",
    "            step2.output_data = audio_segments\n",
    "            print(f\"  Duree totale TTS : {step2.duration_s:.2f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            step2.error = str(e)[:100]\n",
    "            print(f\"  Erreur : {step2.error}\")\n",
    "    else:\n",
    "        step2.error = \"Script non genere\"\n",
    "    result.steps.append(step2)\n",
    "\n",
    "    # --- ETAPE 3 : Assemblage ---\n",
    "    print(f\"\\nEtape 3 : Assemblage du podcast...\")\n",
    "    step3 = PipelineStep(name=\"Assembly\", input_type=\"audio-segments\", output_type=\"audio-podcast\")\n",
    "\n",
    "    if step2.success and audio_segments:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            all_audio = []\n",
    "            target_sr = None\n",
    "\n",
    "            for seg in audio_segments:\n",
    "                data, sr = sf.read(str(seg[\"path\"]))\n",
    "                if target_sr is None:\n",
    "                    target_sr = sr\n",
    "                all_audio.append(data)\n",
    "                # Pause de 0.5s entre les repliques\n",
    "                pause = np.zeros(int(sr * 0.5)) if data.ndim == 1 else np.zeros((int(sr * 0.5), data.shape[1]))\n",
    "                all_audio.append(pause)\n",
    "\n",
    "            podcast_audio = np.concatenate(all_audio)\n",
    "            podcast_path = OUTPUT_DIR / \"podcast_final.wav\"\n",
    "            sf.write(str(podcast_path), podcast_audio, target_sr)\n",
    "\n",
    "            step3.duration_s = time.time() - start_time\n",
    "            step3.success = True\n",
    "            podcast_duration = len(podcast_audio) / target_sr\n",
    "\n",
    "            print(f\"  Duree podcast : {podcast_duration:.1f}s\")\n",
    "            print(f\"  Sample rate : {target_sr} Hz\")\n",
    "            print(f\"  Fichier : {podcast_path.name}\")\n",
    "            print(f\"\\n  Podcast final :\")\n",
    "            display(Audio(data=podcast_audio, rate=target_sr))\n",
    "\n",
    "        except Exception as e:\n",
    "            step3.error = str(e)[:100]\n",
    "            print(f\"  Erreur : {step3.error}\")\n",
    "    else:\n",
    "        step3.error = \"Segments non generes\"\n",
    "    result.steps.append(step3)\n",
    "\n",
    "    result.total_duration_s = sum(s.duration_s for s in result.steps)\n",
    "    result.success = all(s.success for s in result.steps)\n",
    "    pipeline_results.append(result)\n",
    "\n",
    "    print(f\"\\n{result.summary()}\")\n",
    "else:\n",
    "    print(\"Generation de podcast desactivee ou API non disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Podcast multi-voix\n",
    "\n",
    "| Etape | Duree typique | Qualite |\n",
    "|-------|--------------|--------|\n",
    "| Script LLM | 2-5s | Depend du prompt et du modele |\n",
    "| TTS multi-voix | 5-15s (6 segments) | Voix distinctes et naturelles |\n",
    "| Assemblage | < 1s | Deterministe, rapide |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le prompt du LLM doit etre precis pour obtenir un JSON valide\n",
    "2. Les voix alloy et onyx offrent un bon contraste pour un dialogue\n",
    "3. La pause de 0.5s entre les repliques donne un rythme naturel\n",
    "4. Pour un podcast plus long, augmenter le nombre d'echanges et varier les voix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-intro",
   "metadata": {},
   "source": [
    "## Section 5 : Profilage des performances\n",
    "\n",
    "Analysons la repartition du temps dans chaque pipeline pour identifier les goulots d'etranglement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-profiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profilage des performances\n",
    "print(\"PROFILAGE DES PERFORMANCES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if pipeline_results:\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(pipeline_results), figsize=(7 * len(pipeline_results), 5))\n",
    "        if len(pipeline_results) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        colors_map = {\n",
    "            'STT': '#3498db', 'STT-FR': '#3498db',\n",
    "            'LLM': '#e74c3c', 'LLM-Translate': '#e74c3c', 'Script-LLM': '#e74c3c',\n",
    "            'TTS': '#2ecc71', 'TTS-EN': '#2ecc71', 'TTS-Multi': '#2ecc71',\n",
    "            'Assembly': '#f39c12'\n",
    "        }\n",
    "\n",
    "        for idx, result in enumerate(pipeline_results):\n",
    "            ax = axes[idx]\n",
    "            names = [s.name for s in result.steps if s.success]\n",
    "            durations = [s.duration_s for s in result.steps if s.success]\n",
    "            colors = [colors_map.get(n, '#95a5a6') for n in names]\n",
    "\n",
    "            if durations:\n",
    "                bars = ax.bar(names, durations, color=colors, alpha=0.8)\n",
    "                for bar, dur in zip(bars, durations):\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
    "                           f'{dur:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "            ax.set_ylabel('Duree (s)')\n",
    "            ax.set_title(f'{result.name}\\n(total: {result.total_duration_s:.2f}s)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(OUTPUT_DIR / \"pipeline_profiling.png\"), dpi=100, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Graphique sauvegarde : pipeline_profiling.png\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"matplotlib non disponible\")\n",
    "\n",
    "    # Tableau recapitulatif\n",
    "    print(f\"\\nRecapitulatif des pipelines :\")\n",
    "    print(f\"{'Pipeline':<25} {'Duree (s)':<12} {'Statut':<10} {'Etapes':<8}\")\n",
    "    print(\"-\" * 55)\n",
    "    for r in pipeline_results:\n",
    "        status = \"Succes\" if r.success else \"Echec\"\n",
    "        ok_steps = sum(1 for s in r.steps if s.success)\n",
    "        print(f\"{r.name:<25} {r.total_duration_s:<12.2f} {status:<10} {ok_steps}/{len(r.steps)}\")\n",
    "else:\n",
    "    print(\"Aucun pipeline execute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "te9yxzs4tt",
   "source": "### Interpretation : Profilage\n\n| Pipeline | Etape la plus lente | Optimisation possible |\n|----------|--------------------|-----------------------|\n| STT->LLM->TTS | LLM (generation) | Utiliser gpt-4o-mini, reduire max_tokens |\n| Traduction | Comparable STT/LLM/TTS | Paralleliser STT et TTS si possible |\n| Podcast | TTS (N segments) | Paralleliser les appels TTS |\n\n**Points cles** :\n1. Le profilage revele que le LLM est souvent le goulot d'etranglement\n2. La generation du podcast est dominee par les appels TTS sequentiels\n3. L'assemblage est negligeable par rapport aux appels API",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif - Pipeline personnalise\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"MODE INTERACTIF - PIPELINE PERSONNALISE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nFournissez un fichier audio pour le pipeline STT->LLM->TTS :\")\n",
    "    print(\"L'audio sera transcrit, traite par le LLM, et la reponse sera synthetisee.\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "\n",
    "    try:\n",
    "        user_path = input(\"\\nChemin du fichier audio : \")\n",
    "\n",
    "        if user_path.strip() and openai_available:\n",
    "            user_file = Path(user_path.strip())\n",
    "            if user_file.exists():\n",
    "                print(f\"\\nTraitement de : {user_file.name}\")\n",
    "\n",
    "                # STT\n",
    "                with open(user_file, \"rb\") as f:\n",
    "                    user_transcript = client.audio.transcriptions.create(\n",
    "                        model=\"whisper-1\", file=f, response_format=\"text\"\n",
    "                    )\n",
    "                print(f\"  Transcription : {user_transcript}\")\n",
    "\n",
    "                # LLM\n",
    "                llm_resp = client.chat.completions.create(\n",
    "                    model=llm_model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"Answer concisely in 2-3 sentences.\"},\n",
    "                        {\"role\": \"user\", \"content\": user_transcript}\n",
    "                    ],\n",
    "                    max_tokens=200\n",
    "                )\n",
    "                answer = llm_resp.choices[0].message.content\n",
    "                print(f\"  Reponse LLM : {answer}\")\n",
    "\n",
    "                # TTS\n",
    "                tts_resp = client.audio.speech.create(\n",
    "                    model=\"tts-1\", voice=tts_voice,\n",
    "                    input=answer, response_format=\"wav\"\n",
    "                )\n",
    "                print(f\"  Audio genere :\")\n",
    "                display(Audio(data=tts_resp.content, autoplay=False))\n",
    "\n",
    "                if save_results:\n",
    "                    out_path = OUTPUT_DIR / f\"user_pipeline_answer.wav\"\n",
    "                    with open(out_path, 'wb') as f:\n",
    "                        f.write(tts_resp.content)\n",
    "                    print(f\"  Sauvegarde : {out_path.name}\")\n",
    "            else:\n",
    "                print(f\"Fichier non trouve : {user_file}\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type} - {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-practices",
   "metadata": {},
   "source": [
    "## Bonnes pratiques pour les pipelines audio\n",
    "\n",
    "### Optimisation de la latence\n",
    "\n",
    "| Technique | Impact | Description |\n",
    "|-----------|--------|-------------|\n",
    "| STT local (faster-whisper) | -1 a 2s | Elimine la latence reseau |\n",
    "| Modele LLM leger (gpt-4o-mini) | -0.5 a 1s | Generation plus rapide |\n",
    "| TTS streaming | -50% latence | Premier byte audio plus tot |\n",
    "| Parallelisation | Variable | Traiter plusieurs requetes simultanement |\n",
    "\n",
    "### Gestion des erreurs\n",
    "\n",
    "| Pattern | Quand l'utiliser |\n",
    "|---------|------------------|\n",
    "| Retry exponentiel | Erreurs reseau / timeout API |\n",
    "| Fallback local | GPU OOM, API indisponible |\n",
    "| Circuit breaker | Erreurs repetees (eviter les cascades) |\n",
    "| Dead letter queue | Erreurs non recuperables (audit) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"STATISTIQUES DE SESSION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Device : {device}\")\n",
    "print(f\"LLM : {llm_model}\")\n",
    "print(f\"TTS voice : {tts_voice}\")\n",
    "\n",
    "if gpu_available:\n",
    "    vram_current = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"VRAM actuelle : {vram_current:.2f} GB\")\n",
    "\n",
    "# Resume des pipelines\n",
    "if pipeline_results:\n",
    "    print(f\"\\nPipelines executes : {len(pipeline_results)}\")\n",
    "    total_time = sum(r.total_duration_s for r in pipeline_results)\n",
    "    success_count = sum(1 for r in pipeline_results if r.success)\n",
    "    print(f\"Succes : {success_count}/{len(pipeline_results)}\")\n",
    "    print(f\"Temps total : {total_time:.2f}s\")\n",
    "\n",
    "    for r in pipeline_results:\n",
    "        print(f\"\\n  {r.name} :\")\n",
    "        for s in r.steps:\n",
    "            status = 'OK' if s.success else f'ERREUR'\n",
    "            print(f\"    {s.name:<15} : {s.duration_s:.2f}s [{status}]\")\n",
    "\n",
    "if save_results:\n",
    "    saved = list(OUTPUT_DIR.glob('*'))\n",
    "    total_size = sum(f.stat().st_size for f in saved if f.is_file()) / (1024*1024)\n",
    "    print(f\"\\nFichiers sauvegardes : {len(saved)} ({total_size:.1f} MB) dans {OUTPUT_DIR}\")\n",
    "\n",
    "# Liberation memoire\n",
    "gc.collect()\n",
    "if gpu_available:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Memoire GPU liberee\")\n",
    "\n",
    "print(f\"\\nPROCHAINES ETAPES\")\n",
    "print(f\"1. Explorer l'API Realtime d'OpenAI pour la voix en temps reel (03-3)\")\n",
    "print(f\"2. Creer du contenu educatif audio automatise (04-1)\")\n",
    "print(f\"3. Combiner audio et generation d'images (04-2)\")\n",
    "\n",
    "print(f\"\\nNotebook Orchestration de Pipelines Audio termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}