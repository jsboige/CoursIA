{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# OpenAI Realtime Voice API\n",
    "\n",
    "**Module :** 03-Audio-Orchestration  \n",
    "**Niveau :** Avance  \n",
    "**Technologies :** OpenAI Realtime API, WebSocket, gpt-4o-realtime-preview  \n",
    "**Duree estimee :** 45 minutes  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Comprendre l'architecture de l'API Realtime d'OpenAI (WebSocket, events)\n",
    "- [ ] Etablir une connexion WebSocket et configurer une session\n",
    "- [ ] Envoyer de l'audio en streaming et recevoir des reponses vocales\n",
    "- [ ] Configurer la voix, la detection de tour de parole et les instructions\n",
    "- [ ] Implementer le function calling dans les conversations vocales\n",
    "- [ ] Gerer l'historique de conversation et les tours de parole\n",
    "- [ ] Analyser les couts du temps reel vs batch\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Notebooks 03-1 et 03-2 recommandes (comparaison et pipelines)\n",
    "- Cle API OpenAI configuree (`OPENAI_API_KEY` dans `.env`)\n",
    "- `pip install websockets openai`\n",
    "- Pas de GPU necessaire (API cloud)\n",
    "\n",
    "**Navigation :** [<< 03-2](03-2-Audio-Pipeline-Orchestration.ipynb) | [Index](../README.md) | [Suivant >>](../04-Applications/04-1-Educational-Audio-Content.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-params",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres Realtime API\n",
    "realtime_model = \"gpt-4o-realtime-preview\"  # Modele Realtime\n",
    "voice = \"alloy\"                    # \"alloy\", \"echo\", \"shimmer\", \"ash\", \"ballad\", \"coral\", \"sage\", \"verse\"\n",
    "enable_function_calling = True     # Activer le function calling\n",
    "\n",
    "# Configuration\n",
    "save_results = True                # Sauvegarder les fichiers generes\n",
    "simulate_realtime = True           # Simuler via texte (pas de micro dans notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import struct\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.audio_helpers import play_audio, save_audio\n",
    "        print(\"Helpers audio importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers audio non disponibles - mode autonome\")\n",
    "\n",
    "# Repertoires\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'realtime'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('realtime_api')\n",
    "\n",
    "print(f\"OpenAI Realtime Voice API\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Modele : {realtime_model}\")\n",
    "print(f\"Voix : {voice}\")\n",
    "print(f\"Function calling : {enable_function_calling}\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et validation API\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Validation cle OpenAI\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_available = False\n",
    "\n",
    "if openai_key:\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=openai_key)\n",
    "        openai_available = True\n",
    "        print(f\"OPENAI_API_KEY valide - API disponible\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur OpenAI : {str(e)[:100]}\")\n",
    "else:\n",
    "    if notebook_mode == \"batch\":\n",
    "        print(\"Mode batch sans API : validation structurelle uniquement\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"OPENAI_API_KEY manquante dans .env\\n\"\n",
    "            \"L'API Realtime necessite une cle API OpenAI.\"\n",
    "        )\n",
    "\n",
    "# Verifier la disponibilite de websockets\n",
    "websockets_available = False\n",
    "try:\n",
    "    import websockets\n",
    "    websockets_available = True\n",
    "    print(f\"websockets disponible (v{websockets.__version__})\")\n",
    "except ImportError:\n",
    "    print(\"websockets non installe - pip install websockets\")\n",
    "\n",
    "print(f\"\\nConfiguration :\")\n",
    "print(f\"  API : {'OK' if openai_available else 'NON'}\")\n",
    "print(f\"  WebSocket : {'OK' if websockets_available else 'NON'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-intro",
   "metadata": {},
   "source": [
    "## Section 1 : Architecture de l'API Realtime\n",
    "\n",
    "L'API Realtime d'OpenAI permet des conversations vocales bidirectionnelles en temps reel via WebSocket. Contrairement au pipeline STT->LLM->TTS, tout se fait dans un seul flux.\n",
    "\n",
    "### Comparaison des approches\n",
    "\n",
    "| Aspect | Pipeline STT->LLM->TTS | API Realtime |\n",
    "|--------|----------------------|---------------|\n",
    "| Latence | 3-8s (somme des etapes) | 300-800ms (bout-en-bout) |\n",
    "| Protocole | HTTP REST (3 appels) | WebSocket (1 connexion) |\n",
    "| Streaming | Non (sauf TTS) | Oui (entree et sortie) |\n",
    "| Detection parole | Manuelle | Automatique (VAD) |\n",
    "| Interruption | Non | Oui (barge-in) |\n",
    "| Function calling | Via LLM uniquement | Integre nativement |\n",
    "| Cout | Variable par etape | Par token audio |\n",
    "\n",
    "### Protocole WebSocket\n",
    "\n",
    "| Direction | Type d'event | Description |\n",
    "|-----------|-------------|-------------|\n",
    "| Client -> Serveur | `session.update` | Configurer la session |\n",
    "| Client -> Serveur | `input_audio_buffer.append` | Envoyer de l'audio (PCM16, 24kHz) |\n",
    "| Client -> Serveur | `input_audio_buffer.commit` | Finaliser l'entree audio |\n",
    "| Client -> Serveur | `conversation.item.create` | Ajouter un message texte |\n",
    "| Client -> Serveur | `response.create` | Demander une reponse |\n",
    "| Serveur -> Client | `response.audio.delta` | Chunk audio de reponse (base64) |\n",
    "| Serveur -> Client | `response.audio_transcript.delta` | Transcription en temps reel |\n",
    "| Serveur -> Client | `response.function_call_arguments.delta` | Arguments function call |\n",
    "| Serveur -> Client | `response.done` | Fin de reponse |\n",
    "\n",
    "### Format audio\n",
    "\n",
    "| Parametre | Valeur |\n",
    "|-----------|--------|\n",
    "| Format | PCM 16-bit signed little-endian |\n",
    "| Sample rate | 24000 Hz |\n",
    "| Canaux | 1 (mono) |\n",
    "| Encodage WebSocket | base64 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-session-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de session Realtime API\n",
    "print(\"CONFIGURATION DE SESSION REALTIME\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# URL WebSocket de l'API Realtime\n",
    "REALTIME_URL = f\"wss://api.openai.com/v1/realtime?model={realtime_model}\"\n",
    "\n",
    "# Configuration de la session\n",
    "session_config = {\n",
    "    \"type\": \"session.update\",\n",
    "    \"session\": {\n",
    "        \"modalities\": [\"text\", \"audio\"],\n",
    "        \"instructions\": (\n",
    "            \"You are a helpful AI assistant specialized in explaining AI concepts. \"\n",
    "            \"Keep your answers concise and educational. \"\n",
    "            \"When asked about technical topics, provide clear explanations with examples.\"\n",
    "        ),\n",
    "        \"voice\": voice,\n",
    "        \"input_audio_format\": \"pcm16\",\n",
    "        \"output_audio_format\": \"pcm16\",\n",
    "        \"input_audio_transcription\": {\n",
    "            \"model\": \"whisper-1\"\n",
    "        },\n",
    "        \"turn_detection\": {\n",
    "            \"type\": \"server_vad\",\n",
    "            \"threshold\": 0.5,\n",
    "            \"prefix_padding_ms\": 300,\n",
    "            \"silence_duration_ms\": 500\n",
    "        },\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_response_output_tokens\": 500\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function calling tools\n",
    "tools_config = []\n",
    "if enable_function_calling:\n",
    "    tools_config = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"get_model_info\",\n",
    "            \"description\": \"Get information about an AI model (parameters, VRAM, license)\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"model_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Name of the AI model\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"model_name\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"get_current_time\",\n",
    "            \"description\": \"Get the current date and time\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    session_config[\"session\"][\"tools\"] = tools_config\n",
    "    session_config[\"session\"][\"tool_choice\"] = \"auto\"\n",
    "\n",
    "print(f\"URL : {REALTIME_URL}\")\n",
    "print(f\"Voix : {voice}\")\n",
    "print(f\"VAD threshold : {session_config['session']['turn_detection']['threshold']}\")\n",
    "print(f\"Silence duration : {session_config['session']['turn_detection']['silence_duration_ms']}ms\")\n",
    "print(f\"Tools : {len(tools_config)} fonctions\")\n",
    "print(f\"\\nConfiguration de session :\")\n",
    "print(json.dumps(session_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-intro",
   "metadata": {},
   "source": [
    "## Section 2 : Communication WebSocket\n",
    "\n",
    "Dans un notebook Jupyter, nous ne pouvons pas maintenir une connexion WebSocket interactive avec microphone. Nous simulons la conversation en envoyant des messages texte et en recevant des reponses audio.\n",
    "\n",
    "### Flux de communication\n",
    "\n",
    "| Etape | Client | Serveur |\n",
    "|-------|--------|--------|\n",
    "| 1 | `session.update` (config) | `session.created` |\n",
    "| 2 | `conversation.item.create` (texte) | - |\n",
    "| 3 | `response.create` | - |\n",
    "| 4 | - | `response.audio.delta` (chunks) |\n",
    "| 5 | - | `response.audio_transcript.delta` |\n",
    "| 6 | - | `response.done` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-websocket-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation via Realtime API (simulation texte)\n",
    "print(\"CONVERSATION REALTIME API\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "import asyncio\n",
    "\n",
    "conversation_log = []\n",
    "\n",
    "\n",
    "def handle_function_call(name: str, arguments: str) -> str:\n",
    "    \"\"\"Gerer les appels de fonction.\"\"\"\n",
    "    args = json.loads(arguments) if arguments else {}\n",
    "\n",
    "    if name == \"get_model_info\":\n",
    "        model_db = {\n",
    "            \"whisper\": {\"params\": \"1.5B\", \"vram\": \"~6 GB\", \"license\": \"MIT\", \"type\": \"STT\"},\n",
    "            \"chatterbox\": {\"params\": \"~300M\", \"vram\": \"~8 GB\", \"license\": \"MIT\", \"type\": \"TTS\"},\n",
    "            \"kokoro\": {\"params\": \"82M\", \"vram\": \"~2 GB\", \"license\": \"MIT\", \"type\": \"TTS\"},\n",
    "            \"gpt-4o\": {\"params\": \"Non publie\", \"vram\": \"N/A (API)\", \"license\": \"Proprietaire\", \"type\": \"LLM\"},\n",
    "        }\n",
    "        model_name = args.get(\"model_name\", \"\").lower()\n",
    "        for key, info in model_db.items():\n",
    "            if key in model_name:\n",
    "                return json.dumps(info)\n",
    "        return json.dumps({\"error\": f\"Model '{model_name}' not found in database\"})\n",
    "\n",
    "    elif name == \"get_current_time\":\n",
    "        return json.dumps({\"datetime\": datetime.now().isoformat(), \"timezone\": \"local\"})\n",
    "\n",
    "    return json.dumps({\"error\": f\"Unknown function: {name}\"})\n",
    "\n",
    "\n",
    "async def realtime_conversation(messages: List[str]) -> List[Dict]:\n",
    "    \"\"\"Conduire une conversation via l'API Realtime.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    if not websockets_available or not openai_available:\n",
    "        print(\"WebSocket ou API non disponible - simulation locale\")\n",
    "        for msg in messages:\n",
    "            results.append({\n",
    "                \"input\": msg,\n",
    "                \"transcript\": f\"[Simulated response to: {msg}]\",\n",
    "                \"audio_chunks\": 0,\n",
    "                \"duration_s\": 0.0\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {openai_key}\",\n",
    "        \"OpenAI-Beta\": \"realtime=v1\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        async with websockets.connect(REALTIME_URL, additional_headers=headers) as ws:\n",
    "            # Attendre session.created\n",
    "            welcome = json.loads(await asyncio.wait_for(ws.recv(), timeout=10))\n",
    "            print(f\"  Session creee : {welcome.get('type', 'unknown')}\")\n",
    "\n",
    "            # Configurer la session\n",
    "            await ws.send(json.dumps(session_config))\n",
    "            config_resp = json.loads(await asyncio.wait_for(ws.recv(), timeout=10))\n",
    "            print(f\"  Session configuree : {config_resp.get('type', 'unknown')}\")\n",
    "\n",
    "            for msg_idx, message in enumerate(messages):\n",
    "                print(f\"\\n  --- Message {msg_idx+1}/{len(messages)} ---\")\n",
    "                print(f\"  User: {message}\")\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Envoyer le message texte\n",
    "                await ws.send(json.dumps({\n",
    "                    \"type\": \"conversation.item.create\",\n",
    "                    \"item\": {\n",
    "                        \"type\": \"message\",\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"type\": \"input_text\", \"text\": message}]\n",
    "                    }\n",
    "                }))\n",
    "\n",
    "                # Demander une reponse\n",
    "                await ws.send(json.dumps({\"type\": \"response.create\"}))\n",
    "\n",
    "                # Collecter la reponse\n",
    "                audio_chunks = []\n",
    "                transcript_parts = []\n",
    "                function_call_name = \"\"\n",
    "                function_call_args = \"\"\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    try:\n",
    "                        event_str = await asyncio.wait_for(ws.recv(), timeout=30)\n",
    "                        event = json.loads(event_str)\n",
    "                        event_type = event.get(\"type\", \"\")\n",
    "\n",
    "                        if event_type == \"response.audio.delta\":\n",
    "                            audio_b64 = event.get(\"delta\", \"\")\n",
    "                            if audio_b64:\n",
    "                                audio_chunks.append(base64.b64decode(audio_b64))\n",
    "\n",
    "                        elif event_type == \"response.audio_transcript.delta\":\n",
    "                            transcript_parts.append(event.get(\"delta\", \"\"))\n",
    "\n",
    "                        elif event_type == \"response.function_call_arguments.delta\":\n",
    "                            function_call_args += event.get(\"delta\", \"\")\n",
    "\n",
    "                        elif event_type == \"response.output_item.added\":\n",
    "                            item = event.get(\"item\", {})\n",
    "                            if item.get(\"type\") == \"function_call\":\n",
    "                                function_call_name = item.get(\"name\", \"\")\n",
    "                                print(f\"  Function call: {function_call_name}\")\n",
    "\n",
    "                        elif event_type == \"response.done\":\n",
    "                            done = True\n",
    "\n",
    "                        elif event_type == \"error\":\n",
    "                            print(f\"  Erreur API: {event.get('error', {})}\")\n",
    "                            done = True\n",
    "\n",
    "                    except asyncio.TimeoutError:\n",
    "                        print(f\"  Timeout en attente de reponse\")\n",
    "                        done = True\n",
    "\n",
    "                # Gerer le function calling\n",
    "                if function_call_name:\n",
    "                    print(f\"  Execution de {function_call_name}({function_call_args})\")\n",
    "                    fn_result = handle_function_call(function_call_name, function_call_args)\n",
    "                    print(f\"  Resultat : {fn_result}\")\n",
    "\n",
    "                    # Envoyer le resultat au modele\n",
    "                    await ws.send(json.dumps({\n",
    "                        \"type\": \"conversation.item.create\",\n",
    "                        \"item\": {\n",
    "                            \"type\": \"function_call_output\",\n",
    "                            \"call_id\": event.get(\"item\", {}).get(\"call_id\", \"\"),\n",
    "                            \"output\": fn_result\n",
    "                        }\n",
    "                    }))\n",
    "                    await ws.send(json.dumps({\"type\": \"response.create\"}))\n",
    "\n",
    "                    # Attendre la reponse post function call\n",
    "                    done2 = False\n",
    "                    while not done2:\n",
    "                        try:\n",
    "                            event_str = await asyncio.wait_for(ws.recv(), timeout=30)\n",
    "                            event = json.loads(event_str)\n",
    "                            et = event.get(\"type\", \"\")\n",
    "                            if et == \"response.audio.delta\":\n",
    "                                ab = event.get(\"delta\", \"\")\n",
    "                                if ab:\n",
    "                                    audio_chunks.append(base64.b64decode(ab))\n",
    "                            elif et == \"response.audio_transcript.delta\":\n",
    "                                transcript_parts.append(event.get(\"delta\", \"\"))\n",
    "                            elif et == \"response.done\":\n",
    "                                done2 = True\n",
    "                            elif et == \"error\":\n",
    "                                done2 = True\n",
    "                        except asyncio.TimeoutError:\n",
    "                            done2 = True\n",
    "\n",
    "                elapsed = time.time() - start_time\n",
    "                transcript = \"\".join(transcript_parts)\n",
    "\n",
    "                # Assembler l'audio\n",
    "                if audio_chunks:\n",
    "                    audio_bytes = b\"\".join(audio_chunks)\n",
    "                    # PCM16 mono 24kHz -> numpy\n",
    "                    audio_samples = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "                    audio_duration = len(audio_samples) / 24000\n",
    "\n",
    "                    print(f\"  Assistant: {transcript}\")\n",
    "                    print(f\"  Audio: {len(audio_chunks)} chunks, {audio_duration:.1f}s\")\n",
    "                    print(f\"  Latence totale: {elapsed:.2f}s\")\n",
    "                    display(Audio(data=audio_samples, rate=24000))\n",
    "\n",
    "                    if save_results:\n",
    "                        out_path = OUTPUT_DIR / f\"realtime_response_{msg_idx:02d}.wav\"\n",
    "                        sf.write(str(out_path), audio_samples, 24000)\n",
    "                else:\n",
    "                    print(f\"  Assistant (texte): {transcript}\")\n",
    "                    audio_duration = 0\n",
    "\n",
    "                results.append({\n",
    "                    \"input\": message,\n",
    "                    \"transcript\": transcript,\n",
    "                    \"audio_chunks\": len(audio_chunks),\n",
    "                    \"duration_s\": elapsed,\n",
    "                    \"audio_duration_s\": audio_duration\n",
    "                })\n",
    "                conversation_log.append({\"role\": \"user\", \"content\": message})\n",
    "                conversation_log.append({\"role\": \"assistant\", \"content\": transcript})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur WebSocket : {type(e).__name__} - {str(e)[:200]}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Messages de test\n",
    "test_messages = [\n",
    "    \"What is the Realtime API and how does it differ from the standard Chat API?\",\n",
    "    \"What are the main advantages of using local AI models?\",\n",
    "]\n",
    "\n",
    "if enable_function_calling:\n",
    "    test_messages.append(\"What can you tell me about the Whisper model? Use your tools.\")\n",
    "\n",
    "print(f\"Messages de test : {len(test_messages)}\")\n",
    "for i, msg in enumerate(test_messages):\n",
    "    print(f\"  {i+1}. {msg}\")\n",
    "\n",
    "# Executer la conversation\n",
    "print(f\"\\nDemarrage de la conversation...\")\n",
    "try:\n",
    "    # Utiliser asyncio.run() ou le event loop existant\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        conversation_results = asyncio.run(realtime_conversation(test_messages))\n",
    "    except RuntimeError:\n",
    "        conversation_results = asyncio.run(realtime_conversation(test_messages))\n",
    "except Exception as e:\n",
    "    print(f\"Erreur : {type(e).__name__} - {str(e)[:150]}\")\n",
    "    print(\"\\nFallback : simulation locale de la conversation\")\n",
    "    conversation_results = []\n",
    "    for msg in test_messages:\n",
    "        conversation_results.append({\n",
    "            \"input\": msg,\n",
    "            \"transcript\": f\"[Simulation - reponse non disponible sans connexion API]\",\n",
    "            \"audio_chunks\": 0,\n",
    "            \"duration_s\": 0.0,\n",
    "            \"audio_duration_s\": 0.0\n",
    "        })\n",
    "        print(f\"  User: {msg}\")\n",
    "        print(f\"  Assistant: [Simulation]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Conversation Realtime\n",
    "\n",
    "| Metrique | Valeur typique | Signification |\n",
    "|----------|---------------|---------------|\n",
    "| Latence premier chunk | 300-800ms | Temps avant le premier byte audio |\n",
    "| Latence totale | 1-3s | Temps de reponse complete |\n",
    "| Chunks audio | 10-50 | Nombre de segments audio recus |\n",
    "| Format audio | PCM16 24kHz | Qualite CD (non compresse) |\n",
    "\n",
    "**Points cles** :\n",
    "1. L'API Realtime est significativement plus rapide que le pipeline STT->LLM->TTS\n",
    "2. Le streaming audio permet de commencer l'ecoute avant la fin de la generation\n",
    "3. Le function calling fonctionne de maniere transparente dans le flux vocal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-intro",
   "metadata": {},
   "source": [
    "## Section 3 : Envoi d'audio et streaming\n",
    "\n",
    "Pour une interaction vocale complete, on envoie de l'audio (PCM16, 24kHz) plutot que du texte. Voici comment convertir et envoyer un fichier audio.\n",
    "\n",
    "| Etape | Description |\n",
    "|-------|-------------|\n",
    "| 1. Charger l'audio | Lire le fichier WAV |\n",
    "| 2. Convertir | Resample en 24kHz mono PCM16 |\n",
    "| 3. Encoder | Convertir en base64 |\n",
    "| 4. Envoyer | `input_audio_buffer.append` par chunks |\n",
    "| 5. Commit | `input_audio_buffer.commit` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-audio-streaming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation et envoi d'audio vers l'API Realtime\n",
    "print(\"PREPARATION AUDIO POUR STREAMING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "\n",
    "def audio_to_pcm16_base64(audio_path: str, target_sr: int = 24000) -> List[str]:\n",
    "    \"\"\"Convertir un fichier audio en chunks PCM16 base64 pour l'API Realtime.\"\"\"\n",
    "    # Charger l'audio\n",
    "    data, sr = sf.read(audio_path)\n",
    "\n",
    "    # Convertir en mono si stereo\n",
    "    if data.ndim > 1:\n",
    "        data = data.mean(axis=1)\n",
    "\n",
    "    # Resample si necessaire\n",
    "    if sr != target_sr:\n",
    "        # Resample simple par interpolation\n",
    "        duration = len(data) / sr\n",
    "        new_length = int(duration * target_sr)\n",
    "        indices = np.linspace(0, len(data) - 1, new_length)\n",
    "        data = np.interp(indices, np.arange(len(data)), data)\n",
    "\n",
    "    # Convertir en PCM16\n",
    "    pcm16 = (data * 32767).astype(np.int16)\n",
    "    pcm_bytes = pcm16.tobytes()\n",
    "\n",
    "    # Decouper en chunks de ~100ms (4800 samples = 9600 bytes)\n",
    "    chunk_size = 9600  # 100ms a 24kHz en PCM16\n",
    "    chunks = []\n",
    "    for i in range(0, len(pcm_bytes), chunk_size):\n",
    "        chunk = pcm_bytes[i:i + chunk_size]\n",
    "        chunks.append(base64.b64encode(chunk).decode('utf-8'))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def pcm16_base64_to_events(chunks: List[str]) -> List[Dict]:\n",
    "    \"\"\"Convertir des chunks en events WebSocket.\"\"\"\n",
    "    events = []\n",
    "    for chunk in chunks:\n",
    "        events.append({\n",
    "            \"type\": \"input_audio_buffer.append\",\n",
    "            \"audio\": chunk\n",
    "        })\n",
    "    events.append({\"type\": \"input_audio_buffer.commit\"})\n",
    "    return events\n",
    "\n",
    "\n",
    "# Demonstration avec un audio synthetique\n",
    "print(\"Creation d'un audio de demonstration...\")\n",
    "demo_sr = 24000\n",
    "demo_duration = 2.0\n",
    "t = np.linspace(0, demo_duration, int(demo_sr * demo_duration), endpoint=False)\n",
    "# Signal vocal simule\n",
    "demo_audio = 0.5 * np.sin(2 * np.pi * (200 + 80 * np.sin(2 * np.pi * 3 * t)) * t)\n",
    "demo_audio *= (1 + 0.3 * np.sin(2 * np.pi * 1.5 * t))\n",
    "\n",
    "# Sauvegarder\n",
    "demo_path = OUTPUT_DIR / \"demo_input.wav\"\n",
    "sf.write(str(demo_path), demo_audio, demo_sr)\n",
    "\n",
    "# Convertir en chunks\n",
    "chunks = audio_to_pcm16_base64(str(demo_path))\n",
    "events = pcm16_base64_to_events(chunks)\n",
    "\n",
    "print(f\"Audio source : {demo_duration}s, {demo_sr} Hz\")\n",
    "print(f\"Chunks generes : {len(chunks)}\")\n",
    "print(f\"Taille par chunk : ~{len(chunks[0]) if chunks else 0} chars (base64)\")\n",
    "print(f\"Events WebSocket : {len(events)} (dont 1 commit)\")\n",
    "print(f\"\\nExemple d'event :\")\n",
    "print(json.dumps({\"type\": events[0][\"type\"], \"audio\": events[0][\"audio\"][:50] + \"...\"}, indent=2))\n",
    "\n",
    "display(Audio(data=demo_audio, rate=demo_sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Preparation audio\n",
    "\n",
    "| Parametre | Valeur | Raison |\n",
    "|-----------|--------|--------|\n",
    "| Sample rate | 24000 Hz | Impose par l'API Realtime |\n",
    "| Format | PCM16 little-endian | Format brut sans compression |\n",
    "| Chunk size | 100ms (9600 bytes) | Equilibre latence/overhead |\n",
    "| Encodage | base64 | Transport via JSON/WebSocket |\n",
    "\n",
    "**Points cles** :\n",
    "1. L'audio doit etre en mono 24kHz PCM16 (pas de MP3, pas de stereo)\n",
    "2. Les chunks de 100ms offrent un bon compromis pour le streaming\n",
    "3. Le base64 augmente la taille de ~33% mais est necessaire pour le transport JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-intro",
   "metadata": {},
   "source": [
    "## Section 4 : Analyse des couts\n",
    "\n",
    "L'API Realtime a une tarification differente des API standard. Elle facture par token audio (entree et sortie).\n",
    "\n",
    "### Tarification Realtime API (Janvier 2025)\n",
    "\n",
    "| Composant | Cout | Unite |\n",
    "|-----------|------|-------|\n",
    "| Audio input | $0.06 | par minute |\n",
    "| Audio output | $0.24 | par minute |\n",
    "| Text input | $5.00 | par 1M tokens |\n",
    "| Text output | $20.00 | par 1M tokens |\n",
    "\n",
    "### Comparaison avec le pipeline\n",
    "\n",
    "| Approche | Cout pour 1 min conversation | Latence |\n",
    "|----------|----------------------------|---------|\n",
    "| Pipeline STT+LLM+TTS | ~$0.02-0.05 | 3-8s |\n",
    "| Realtime API | ~$0.30 | 0.3-0.8s |\n",
    "| Ratio | **6-15x plus cher** | **4-26x plus rapide** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cost-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des couts Realtime vs Pipeline\n",
    "print(\"ANALYSE DES COUTS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Parametres de calcul\n",
    "scenarios = [\n",
    "    {\"name\": \"Prototype (10 min/jour)\", \"daily_minutes\": 10},\n",
    "    {\"name\": \"Application (1h/jour)\", \"daily_minutes\": 60},\n",
    "    {\"name\": \"Production (8h/jour)\", \"daily_minutes\": 480},\n",
    "]\n",
    "\n",
    "# Tarifs\n",
    "costs = {\n",
    "    \"Pipeline STT+LLM+TTS\": {\n",
    "        \"per_minute\": 0.006 + 0.002 + 0.001,  # Whisper + GPT-4o-mini + TTS\n",
    "        \"latency_s\": 5.0\n",
    "    },\n",
    "    \"Realtime API\": {\n",
    "        \"per_minute\": 0.06 + 0.24,  # audio in + audio out (approximation 50/50)\n",
    "        \"latency_s\": 0.5\n",
    "    },\n",
    "    \"Local (faster-whisper + Kokoro)\": {\n",
    "        \"per_minute\": 0.001,  # electricite GPU uniquement\n",
    "        \"latency_s\": 3.0\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"{'Scenario':<30} \", end=\"\")\n",
    "for approach in costs:\n",
    "    print(f\"{approach:<30} \", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    monthly_minutes = scenario[\"daily_minutes\"] * 30\n",
    "    print(f\"{scenario['name']:<30} \", end=\"\")\n",
    "    for approach, pricing in costs.items():\n",
    "        monthly_cost = monthly_minutes * pricing[\"per_minute\"]\n",
    "        print(f\"${monthly_cost:>8.2f}/mois ({pricing['latency_s']:.1f}s) \", end=\"\")\n",
    "    print()\n",
    "\n",
    "# Graphique comparatif\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Cout mensuel par scenario\n",
    "    x_labels = [s[\"name\"] for s in scenarios]\n",
    "    x_pos = np.arange(len(x_labels))\n",
    "    width = 0.25\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "    for idx, (approach, pricing) in enumerate(costs.items()):\n",
    "        monthly_costs = [s[\"daily_minutes\"] * 30 * pricing[\"per_minute\"] for s in scenarios]\n",
    "        bars = ax1.bar(x_pos + idx * width, monthly_costs, width,\n",
    "                      label=approach, color=colors[idx], alpha=0.8)\n",
    "        for bar, cost in zip(bars, monthly_costs):\n",
    "            if cost > 0:\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                        f'${cost:.0f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    ax1.set_xlabel('Scenario')\n",
    "    ax1.set_ylabel('Cout mensuel (USD)')\n",
    "    ax1.set_title('Cout mensuel par approche')\n",
    "    ax1.set_xticks(x_pos + width)\n",
    "    ax1.set_xticklabels(x_labels, fontsize=8)\n",
    "    ax1.legend(fontsize=8)\n",
    "\n",
    "    # Cout vs Latence (scatter)\n",
    "    for idx, (approach, pricing) in enumerate(costs.items()):\n",
    "        ax2.scatter(pricing[\"latency_s\"], pricing[\"per_minute\"] * 60,\n",
    "                   s=200, color=colors[idx], label=approach, alpha=0.8, zorder=5)\n",
    "        ax2.annotate(approach, (pricing[\"latency_s\"], pricing[\"per_minute\"] * 60),\n",
    "                    textcoords=\"offset points\", xytext=(10, 10), fontsize=8)\n",
    "\n",
    "    ax2.set_xlabel('Latence (s)')\n",
    "    ax2.set_ylabel('Cout par heure (USD)')\n",
    "    ax2.set_title('Compromis cout / latence')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(OUTPUT_DIR / \"cost_analysis.png\"), dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nGraphique sauvegarde : cost_analysis.png\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"matplotlib non disponible\")\n",
    "\n",
    "# Resume\n",
    "print(f\"\\nConclusion :\")\n",
    "print(f\"  Realtime API : ideal pour les interactions temps reel (assistants, chatbots vocaux)\")\n",
    "print(f\"  Pipeline : ideal pour le traitement batch (transcription, narration)\")\n",
    "print(f\"  Local : ideal pour les gros volumes et la confidentialite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Analyse des couts\n",
    "\n",
    "| Approche | Forces | Faiblesses |\n",
    "|----------|--------|------------|\n",
    "| Pipeline STT+LLM+TTS | Economique, flexible | Latence elevee |\n",
    "| Realtime API | Ultra-rapide, naturel | Couteux a grande echelle |\n",
    "| Local | Gratuit, confidentiel | GPU requis, moins de qualite |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le Realtime API coute 6-15x plus cher mais est 4-26x plus rapide\n",
    "2. Pour du prototypage (< 10 min/jour), la difference de cout est negligeable\n",
    "3. En production (> 1h/jour), le cout du Realtime API devient significatif\n",
    "4. La solution locale est la plus economique mais necessite un investissement GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif - Simulation de conversation\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"MODE INTERACTIF - CONVERSATION REALTIME\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nNote : dans un notebook, le microphone n'est pas disponible.\")\n",
    "    print(\"La conversation est simulee par texte.\")\n",
    "    print(\"Entrez un message pour l'assistant (ou vide pour quitter) :\")\n",
    "\n",
    "    try:\n",
    "        user_msg = input(\"\\nVotre message : \")\n",
    "\n",
    "        if user_msg.strip() and openai_available:\n",
    "            print(f\"\\nEnvoi au Realtime API...\")\n",
    "            print(f\"(En pratique, ceci serait envoye en audio via WebSocket)\")\n",
    "\n",
    "            # Simuler avec l'API Chat standard\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": session_config[\"session\"][\"instructions\"]},\n",
    "                    {\"role\": \"user\", \"content\": user_msg}\n",
    "                ],\n",
    "                max_tokens=200\n",
    "            )\n",
    "            answer = response.choices[0].message.content\n",
    "            print(f\"Assistant : {answer}\")\n",
    "\n",
    "            # Synthetiser la reponse en audio\n",
    "            tts_resp = client.audio.speech.create(\n",
    "                model=\"tts-1\", voice=voice,\n",
    "                input=answer, response_format=\"wav\"\n",
    "            )\n",
    "            print(f\"\\nReponse audio :\")\n",
    "            display(Audio(data=tts_resp.content, autoplay=False))\n",
    "\n",
    "            if save_results:\n",
    "                out_path = OUTPUT_DIR / \"interactive_response.wav\"\n",
    "                with open(out_path, 'wb') as f:\n",
    "                    f.write(tts_resp.content)\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type} - {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-practices",
   "metadata": {},
   "source": [
    "## Bonnes pratiques et limites\n",
    "\n",
    "### Quand utiliser le Realtime API\n",
    "\n",
    "| Cas d'usage | Recommande | Raison |\n",
    "|-------------|-----------|--------|\n",
    "| Assistant vocal interactif | Oui | Latence < 1s, natural |\n",
    "| Service client telephonique | Oui | Conversation fluide |\n",
    "| Transcription batch | Non | Trop couteux, utiliser Whisper |\n",
    "| Generation de contenu audio | Non | Pipeline plus economique |\n",
    "| Tutoriel educatif interactif | Oui | Feedback instantane |\n",
    "\n",
    "### Limites actuelles\n",
    "\n",
    "| Limite | Description | Contournement |\n",
    "|--------|-------------|---------------|\n",
    "| Pas de micro dans notebook | Jupyter ne gere pas le micro en streaming | Simuler par texte, utiliser un client natif |\n",
    "| Cout eleve | ~$0.30/min vs $0.01/min pipeline | Limiter la duree, utiliser batch quand possible |\n",
    "| Modele unique | Seulement gpt-4o-realtime-preview | Pas de choix de modele STT/TTS |\n",
    "| Langues | Optimise pour l'anglais | Tester sur la langue cible |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"STATISTIQUES DE SESSION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Modele : {realtime_model}\")\n",
    "print(f\"Voix : {voice}\")\n",
    "print(f\"Function calling : {enable_function_calling}\")\n",
    "print(f\"API disponible : {openai_available}\")\n",
    "print(f\"WebSocket disponible : {websockets_available}\")\n",
    "\n",
    "# Resume de la conversation\n",
    "if conversation_results:\n",
    "    print(f\"\\nConversation :\")\n",
    "    total_latency = sum(r.get('duration_s', 0) for r in conversation_results)\n",
    "    total_audio = sum(r.get('audio_duration_s', 0) for r in conversation_results)\n",
    "    print(f\"  Messages : {len(conversation_results)}\")\n",
    "    print(f\"  Latence totale : {total_latency:.2f}s\")\n",
    "    print(f\"  Audio genere : {total_audio:.1f}s\")\n",
    "\n",
    "    for i, r in enumerate(conversation_results):\n",
    "        print(f\"\\n  [{i+1}] User: {r['input'][:60]}...\")\n",
    "        print(f\"      Assistant: {r['transcript'][:60]}...\")\n",
    "        print(f\"      Latence: {r.get('duration_s', 0):.2f}s, Chunks: {r.get('audio_chunks', 0)}\")\n",
    "\n",
    "if conversation_log:\n",
    "    print(f\"\\nHistorique ({len(conversation_log)} messages) :\")\n",
    "    for msg in conversation_log:\n",
    "        role = msg['role'].upper()\n",
    "        content = msg['content'][:80]\n",
    "        print(f\"  [{role}] {content}\")\n",
    "\n",
    "if save_results:\n",
    "    saved = list(OUTPUT_DIR.glob('*'))\n",
    "    total_size = sum(f.stat().st_size for f in saved if f.is_file()) / (1024*1024)\n",
    "    print(f\"\\nFichiers sauvegardes : {len(saved)} ({total_size:.1f} MB) dans {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nPROCHAINES ETAPES\")\n",
    "print(f\"1. Creer du contenu educatif audio automatise (04-1)\")\n",
    "print(f\"2. Combiner audio et generation d'images (04-2)\")\n",
    "print(f\"3. Construire un assistant vocal complet (client natif)\")\n",
    "\n",
    "print(f\"\\nNotebook Realtime Voice API termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}