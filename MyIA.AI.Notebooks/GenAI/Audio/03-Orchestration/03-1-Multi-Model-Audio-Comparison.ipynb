{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# Comparaison Multi-Modeles Audio\n",
    "\n",
    "**Module :** 03-Audio-Orchestration  \n",
    "**Niveau :** Avance  \n",
    "**Technologies :** Whisper API vs local, OpenAI TTS vs Chatterbox vs Kokoro, ~12 GB VRAM  \n",
    "**Duree estimee :** 60 minutes  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Construire un framework de benchmark pour les modeles audio (timing, qualite, cout)\n",
    "- [ ] Comparer les modeles STT : Whisper API vs faster-whisper local (precision, vitesse, cout)\n",
    "- [ ] Comparer les modeles TTS : OpenAI vs Chatterbox vs Kokoro (naturalite, vitesse, VRAM)\n",
    "- [ ] Visualiser les resultats avec matplotlib (bar charts, radar plots)\n",
    "- [ ] Analyser les couts API vs GPU local\n",
    "- [ ] Choisir le modele optimal selon le cas d'usage\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Notebooks 01-1 a 01-5 recommandes (fondamentaux audio)\n",
    "- Notebooks 02-1 recommande (Chatterbox)\n",
    "- GPU NVIDIA avec au moins 12 GB VRAM (pour tous les modeles locaux)\n",
    "- Cle API OpenAI configuree (`OPENAI_API_KEY` dans `.env`)\n",
    "- `pip install faster-whisper chatterbox-tts kokoro soundfile matplotlib`\n",
    "\n",
    "**Navigation :** [<< 02-4](../02-Advanced/02-4-Demucs-Source-Separation.ipynb) | [Index](../README.md) | [Suivant >>](03-2-Audio-Pipeline-Orchestration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-params",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres benchmark\n",
    "run_stt_benchmark = True           # Executer le benchmark STT\n",
    "run_tts_benchmark = True           # Executer le benchmark TTS\n",
    "stt_models = [\"whisper-1\", \"large-v3-turbo\"]  # Modeles STT a comparer\n",
    "tts_models = [\"openai\", \"chatterbox\", \"kokoro\"]  # Modeles TTS a comparer\n",
    "device = \"cuda\"                    # \"cuda\" ou \"cpu\"\n",
    "\n",
    "# Configuration\n",
    "save_results = True                # Sauvegarder les fichiers generes\n",
    "num_runs = 3                       # Nombre de runs par modele pour moyenner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.audio_helpers import play_audio, save_audio\n",
    "        print(\"Helpers audio importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers audio non disponibles - mode autonome\")\n",
    "\n",
    "# Repertoires\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'comparison'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('audio_comparison')\n",
    "\n",
    "# Verification GPU\n",
    "gpu_available = False\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_vram = torch.cuda.get_device_properties(0).total_mem / (1024**3)\n",
    "        print(f\"GPU : {gpu_name} ({gpu_vram:.1f} GB VRAM)\")\n",
    "    else:\n",
    "        print(\"GPU non disponible - les modeles locaux seront plus lents\")\n",
    "        if device == \"cuda\":\n",
    "            device = \"cpu\"\n",
    "            print(\"Fallback vers CPU\")\n",
    "except ImportError:\n",
    "    print(\"torch non installe\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"\\nComparaison Multi-Modeles Audio\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}, Device : {device}\")\n",
    "print(f\"STT : {stt_models}\")\n",
    "print(f\"TTS : {tts_models}\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et validation API\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Validation cle OpenAI (necessaire pour Whisper API et OpenAI TTS)\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_available = False\n",
    "\n",
    "if openai_key:\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=openai_key)\n",
    "        openai_available = True\n",
    "        print(f\"OPENAI_API_KEY valide - API disponible\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur OpenAI : {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY non disponible - benchmarks API desactives\")\n",
    "    # Retirer les modeles API si pas de cle\n",
    "    if \"whisper-1\" in stt_models:\n",
    "        stt_models = [m for m in stt_models if m != \"whisper-1\"]\n",
    "    if \"openai\" in tts_models:\n",
    "        tts_models = [m for m in tts_models if m != \"openai\"]\n",
    "\n",
    "print(f\"\\nModeles STT actifs : {stt_models}\")\n",
    "print(f\"Modeles TTS actifs : {tts_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-intro",
   "metadata": {},
   "source": [
    "## Section 1 : Framework de benchmark\n",
    "\n",
    "Avant de comparer les modeles, nous devons definir un framework de mesure rigoureux. Chaque modele sera evalue selon des metriques standardisees.\n",
    "\n",
    "### Metriques STT (Speech-to-Text)\n",
    "\n",
    "| Metrique | Description | Unite |\n",
    "|----------|-------------|-------|\n",
    "| WER | Word Error Rate (taux d'erreur par mot) | % |\n",
    "| Latence | Temps de transcription | secondes |\n",
    "| RTF | Real-Time Factor (temps traitement / duree audio) | ratio |\n",
    "| Cout | Cout par minute d'audio | USD |\n",
    "| VRAM | Memoire GPU utilisee | GB |\n",
    "\n",
    "### Metriques TTS (Text-to-Speech)\n",
    "\n",
    "| Metrique | Description | Unite |\n",
    "|----------|-------------|-------|\n",
    "| Latence | Temps de generation (premier byte) | secondes |\n",
    "| RTF | Real-Time Factor | ratio |\n",
    "| Sample Rate | Frequence d'echantillonnage | Hz |\n",
    "| Cout | Cout par 1000 caracteres | USD |\n",
    "| VRAM | Memoire GPU utilisee | GB |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-benchmark-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework de benchmark\n",
    "print(\"FRAMEWORK DE BENCHMARK\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Resultat d'un benchmark pour un modele.\"\"\"\n",
    "    def __init__(self, model_name: str, category: str):\n",
    "        self.model_name = model_name\n",
    "        self.category = category  # 'stt' ou 'tts'\n",
    "        self.latencies: List[float] = []\n",
    "        self.vram_mb: float = 0.0\n",
    "        self.cost_per_unit: float = 0.0  # par minute (STT) ou par 1K chars (TTS)\n",
    "        self.quality_score: float = 0.0  # WER (STT) ou MOS estime (TTS)\n",
    "        self.sample_rate: int = 0\n",
    "        self.output_data: Any = None\n",
    "\n",
    "    @property\n",
    "    def avg_latency(self) -> float:\n",
    "        return np.mean(self.latencies) if self.latencies else 0.0\n",
    "\n",
    "    @property\n",
    "    def std_latency(self) -> float:\n",
    "        return np.std(self.latencies) if len(self.latencies) > 1 else 0.0\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"model\": self.model_name,\n",
    "            \"category\": self.category,\n",
    "            \"avg_latency_s\": round(self.avg_latency, 3),\n",
    "            \"std_latency_s\": round(self.std_latency, 3),\n",
    "            \"vram_mb\": round(self.vram_mb, 1),\n",
    "            \"cost_per_unit\": self.cost_per_unit,\n",
    "            \"quality_score\": round(self.quality_score, 3),\n",
    "            \"sample_rate\": self.sample_rate\n",
    "        }\n",
    "\n",
    "\n",
    "def measure_vram() -> float:\n",
    "    \"\"\"Mesurer la VRAM actuellement utilisee en MB.\"\"\"\n",
    "    if gpu_available:\n",
    "        return torch.cuda.memory_allocated(0) / (1024**2)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def word_error_rate(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"Calculer le WER entre reference et hypothese (Levenshtein sur les mots).\"\"\"\n",
    "    ref_words = reference.lower().split()\n",
    "    hyp_words = hypothesis.lower().split()\n",
    "    if not ref_words:\n",
    "        return 0.0 if not hyp_words else 1.0\n",
    "\n",
    "    # Distance de Levenshtein sur les mots\n",
    "    d = np.zeros((len(ref_words) + 1, len(hyp_words) + 1), dtype=int)\n",
    "    for i in range(len(ref_words) + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len(hyp_words) + 1):\n",
    "        d[0][j] = j\n",
    "    for i in range(1, len(ref_words) + 1):\n",
    "        for j in range(1, len(hyp_words) + 1):\n",
    "            cost = 0 if ref_words[i-1] == hyp_words[j-1] else 1\n",
    "            d[i][j] = min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + cost)\n",
    "\n",
    "    return d[len(ref_words)][len(hyp_words)] / len(ref_words)\n",
    "\n",
    "\n",
    "# Stocker tous les resultats\n",
    "all_results: Dict[str, BenchmarkResult] = {}\n",
    "\n",
    "print(\"Framework de benchmark initialise\")\n",
    "print(f\"Nombre de runs par modele : {num_runs}\")\n",
    "print(f\"Modeles STT : {stt_models}\")\n",
    "print(f\"Modeles TTS : {tts_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-intro",
   "metadata": {},
   "source": [
    "## Section 2 : Preparation des donnees de test\n",
    "\n",
    "Pour une comparaison equitable, nous utilisons les memes donnees de test pour tous les modeles.\n",
    "\n",
    "| Donnee | Description | Usage |\n",
    "|--------|-------------|-------|\n",
    "| Audio de test STT | Fichier WAV avec texte connu | Benchmark STT |\n",
    "| Texte de test TTS | Texte standardise en anglais et francais | Benchmark TTS |\n",
    "| Reference ground truth | Transcription exacte de l'audio | Calcul du WER |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-prepare-test-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation des donnees de test\n",
    "print(\"PREPARATION DES DONNEES DE TEST\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Texte de reference pour le STT (on va le synthetiser puis le retranscrire)\n",
    "stt_reference_text = (\n",
    "    \"Artificial intelligence is transforming the way we interact with technology. \"\n",
    "    \"Speech recognition and synthesis are among the most impactful applications, \"\n",
    "    \"enabling natural conversations between humans and machines.\"\n",
    ")\n",
    "\n",
    "# Texte de test pour le TTS\n",
    "tts_test_text = (\n",
    "    \"Welcome to the multi-model audio comparison benchmark. \"\n",
    "    \"This test evaluates the quality, speed, and cost of different \"\n",
    "    \"text-to-speech engines across various metrics.\"\n",
    ")\n",
    "\n",
    "# Generer l'audio de test STT avec OpenAI TTS (source fiable)\n",
    "stt_test_path = OUTPUT_DIR / \"stt_test_audio.wav\"\n",
    "\n",
    "if openai_available:\n",
    "    print(\"Generation de l'audio de test STT via OpenAI TTS...\")\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1-hd\",\n",
    "        voice=\"alloy\",\n",
    "        input=stt_reference_text,\n",
    "        response_format=\"wav\"\n",
    "    )\n",
    "    with open(stt_test_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Audio de test STT : {stt_test_path.name} ({len(response.content)/1024:.1f} KB)\")\n",
    "    display(Audio(data=response.content, autoplay=False))\n",
    "else:\n",
    "    # Generer un audio synthetique si pas d'API\n",
    "    print(\"Pas d'API OpenAI - generation d'un audio synthetique...\")\n",
    "    sr = 16000\n",
    "    duration = 5.0\n",
    "    t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
    "    # Signal vocal simule (sinusoides modulees)\n",
    "    audio = 0.5 * np.sin(2 * np.pi * (200 + 100 * np.sin(2 * np.pi * 3 * t)) * t)\n",
    "    audio *= (1 + 0.3 * np.sin(2 * np.pi * 1.5 * t))\n",
    "    sf.write(str(stt_test_path), audio, sr)\n",
    "    stt_reference_text = \"[audio synthetique - WER non significatif]\"\n",
    "    print(f\"Audio synthetique cree : {stt_test_path.name}\")\n",
    "\n",
    "print(f\"\\nReference STT : {stt_reference_text[:80]}...\")\n",
    "print(f\"Texte TTS : {tts_test_text[:80]}...\")\n",
    "print(f\"Longueur texte TTS : {len(tts_test_text)} caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-intro",
   "metadata": {},
   "source": [
    "## Section 3 : Benchmark STT (Speech-to-Text)\n",
    "\n",
    "Nous comparons deux approches pour la transcription vocale :\n",
    "\n",
    "| Modele | Type | Avantages | Inconvenients |\n",
    "|--------|------|-----------|---------------|\n",
    "| Whisper API (`whisper-1`) | Cloud (OpenAI) | Simple, pas de GPU, haute qualite | Cout par minute, latence reseau |\n",
    "| faster-whisper (`large-v3-turbo`) | Local (GPU) | Gratuit, rapide, hors-ligne | GPU requis, installation complexe |\n",
    "\n",
    "### Tarification Whisper API\n",
    "\n",
    "| Modele | Cout | Unite |\n",
    "|--------|------|-------|\n",
    "| `whisper-1` | $0.006 | par minute d'audio |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stt-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark STT\n",
    "print(\"BENCHMARK STT (Speech-to-Text)\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if run_stt_benchmark and stt_test_path.exists():\n",
    "    audio_data, audio_sr = sf.read(str(stt_test_path))\n",
    "    audio_duration = len(audio_data) / audio_sr\n",
    "    print(f\"Audio de test : {audio_duration:.1f}s, {audio_sr} Hz\")\n",
    "\n",
    "    # --- Whisper API ---\n",
    "    if \"whisper-1\" in stt_models and openai_available:\n",
    "        print(f\"\\n--- Whisper API (whisper-1) ---\")\n",
    "        result = BenchmarkResult(\"whisper-1\", \"stt\")\n",
    "        result.cost_per_unit = 0.006  # par minute\n",
    "        result.sample_rate = audio_sr\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            with open(stt_test_path, \"rb\") as f:\n",
    "                transcript = client.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\",\n",
    "                    file=f,\n",
    "                    response_format=\"text\"\n",
    "                )\n",
    "            latency = time.time() - start_time\n",
    "            result.latencies.append(latency)\n",
    "            print(f\"  Run {run+1}/{num_runs} : {latency:.2f}s\")\n",
    "\n",
    "        result.output_data = transcript\n",
    "        result.quality_score = word_error_rate(stt_reference_text, transcript)\n",
    "        all_results[\"stt_whisper-1\"] = result\n",
    "\n",
    "        print(f\"  Transcription : {transcript[:100]}...\")\n",
    "        print(f\"  WER : {result.quality_score:.3f}\")\n",
    "        print(f\"  Latence moyenne : {result.avg_latency:.2f}s (+/- {result.std_latency:.2f}s)\")\n",
    "        print(f\"  RTF : {result.avg_latency / audio_duration:.2f}\")\n",
    "        print(f\"  Cout : ${audio_duration / 60 * result.cost_per_unit:.4f}\")\n",
    "\n",
    "    # --- faster-whisper local ---\n",
    "    if \"large-v3-turbo\" in stt_models:\n",
    "        print(f\"\\n--- faster-whisper (large-v3-turbo) ---\")\n",
    "        result = BenchmarkResult(\"large-v3-turbo\", \"stt\")\n",
    "        result.cost_per_unit = 0.0  # gratuit (GPU local)\n",
    "\n",
    "        try:\n",
    "            from faster_whisper import WhisperModel\n",
    "\n",
    "            vram_before = measure_vram()\n",
    "            print(f\"  Chargement du modele...\")\n",
    "            stt_model = WhisperModel(\n",
    "                \"large-v3-turbo\",\n",
    "                device=device,\n",
    "                compute_type=\"float16\" if device == \"cuda\" else \"int8\"\n",
    "            )\n",
    "            vram_after = measure_vram()\n",
    "            result.vram_mb = vram_after - vram_before\n",
    "            print(f\"  VRAM modele : {result.vram_mb:.0f} MB\")\n",
    "\n",
    "            for run in range(num_runs):\n",
    "                start_time = time.time()\n",
    "                segments, info = stt_model.transcribe(\n",
    "                    str(stt_test_path),\n",
    "                    beam_size=5\n",
    "                )\n",
    "                transcript_local = \" \".join([s.text.strip() for s in segments])\n",
    "                latency = time.time() - start_time\n",
    "                result.latencies.append(latency)\n",
    "                print(f\"  Run {run+1}/{num_runs} : {latency:.2f}s\")\n",
    "\n",
    "            result.output_data = transcript_local\n",
    "            result.quality_score = word_error_rate(stt_reference_text, transcript_local)\n",
    "            result.sample_rate = int(info.language_probability * 16000)  # approximation\n",
    "            all_results[\"stt_large-v3-turbo\"] = result\n",
    "\n",
    "            print(f\"  Transcription : {transcript_local[:100]}...\")\n",
    "            print(f\"  WER : {result.quality_score:.3f}\")\n",
    "            print(f\"  Latence moyenne : {result.avg_latency:.2f}s (+/- {result.std_latency:.2f}s)\")\n",
    "            print(f\"  RTF : {result.avg_latency / audio_duration:.2f}\")\n",
    "\n",
    "            # Liberer la memoire\n",
    "            del stt_model\n",
    "            gc.collect()\n",
    "            if gpu_available:\n",
    "                torch.cuda.empty_cache()\n",
    "            print(f\"  Modele libere\")\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"  faster-whisper non installe\")\n",
    "            print(\"  Installation : pip install faster-whisper\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur : {type(e).__name__} - {str(e)[:150]}\")\n",
    "\n",
    "    # Tableau recapitulatif STT\n",
    "    stt_results = {k: v for k, v in all_results.items() if v.category == \"stt\"}\n",
    "    if stt_results:\n",
    "        print(f\"\\nRecapitulatif STT :\")\n",
    "        print(f\"{'Modele':<20} {'Latence (s)':<14} {'WER':<8} {'Cout/min':<10} {'VRAM (MB)':<10}\")\n",
    "        print(\"-\" * 62)\n",
    "        for k, r in stt_results.items():\n",
    "            cost_str = f\"${r.cost_per_unit:.3f}\" if r.cost_per_unit > 0 else \"Gratuit\"\n",
    "            vram_str = f\"{r.vram_mb:.0f}\" if r.vram_mb > 0 else \"N/A\"\n",
    "            print(f\"{r.model_name:<20} {r.avg_latency:<14.2f} {r.quality_score:<8.3f} {cost_str:<10} {vram_str:<10}\")\n",
    "else:\n",
    "    print(\"Benchmark STT desactive ou fichier de test manquant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Benchmark STT\n",
    "\n",
    "| Aspect | Whisper API | faster-whisper local |\n",
    "|--------|-------------|---------------------|\n",
    "| Precision (WER) | Tres bonne (< 5%) | Equivalente ou meilleure |\n",
    "| Latence | Depend du reseau (1-3s) | Depend du GPU (0.5-2s) |\n",
    "| Cout | $0.006/min | Gratuit (GPU amortit) |\n",
    "| VRAM | 0 (cloud) | ~3-6 GB |\n",
    "| Hors-ligne | Non | Oui |\n",
    "\n",
    "**Points cles** :\n",
    "1. faster-whisper `large-v3-turbo` offre un excellent compromis vitesse/qualite\n",
    "2. L'API Whisper est plus simple a deployer mais a un cout recurrent\n",
    "3. Pour du traitement en masse, le local est nettement plus economique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-intro",
   "metadata": {},
   "source": [
    "## Section 4 : Benchmark TTS (Text-to-Speech)\n",
    "\n",
    "Nous comparons trois moteurs TTS avec des approches differentes :\n",
    "\n",
    "| Modele | Type | VRAM | Sample Rate | Licence |\n",
    "|--------|------|------|-------------|--------|\n",
    "| OpenAI TTS | Cloud API | 0 | 24 kHz | Proprietaire |\n",
    "| Chatterbox | Local GPU | ~8 GB | 24 kHz | MIT |\n",
    "| Kokoro 82M | Local GPU | ~2 GB | 24 kHz | MIT |\n",
    "\n",
    "### Tarification TTS\n",
    "\n",
    "| Modele | Cout / 1M caracteres | Cout / 1 heure narration |\n",
    "|--------|---------------------|-------------------------|\n",
    "| OpenAI tts-1 | $15.00 | ~$0.75 |\n",
    "| OpenAI tts-1-hd | $30.00 | ~$1.50 |\n",
    "| Chatterbox | Gratuit | Electricite GPU |\n",
    "| Kokoro | Gratuit | Electricite GPU |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-tts-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark TTS\n",
    "print(\"BENCHMARK TTS (Text-to-Speech)\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if run_tts_benchmark:\n",
    "    print(f\"Texte de test : {tts_test_text[:60]}...\")\n",
    "    print(f\"Longueur : {len(tts_test_text)} caracteres\")\n",
    "\n",
    "    # --- OpenAI TTS ---\n",
    "    if \"openai\" in tts_models and openai_available:\n",
    "        print(f\"\\n--- OpenAI TTS (tts-1) ---\")\n",
    "        result = BenchmarkResult(\"openai-tts-1\", \"tts\")\n",
    "        result.cost_per_unit = 0.015  # par 1K caracteres\n",
    "        result.sample_rate = 24000\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            response = client.audio.speech.create(\n",
    "                model=\"tts-1\",\n",
    "                voice=\"alloy\",\n",
    "                input=tts_test_text,\n",
    "                response_format=\"wav\"\n",
    "            )\n",
    "            latency = time.time() - start_time\n",
    "            result.latencies.append(latency)\n",
    "            print(f\"  Run {run+1}/{num_runs} : {latency:.2f}s\")\n",
    "\n",
    "        result.output_data = response.content\n",
    "        result.quality_score = 4.5  # MOS estime (reference industrie)\n",
    "        all_results[\"tts_openai\"] = result\n",
    "\n",
    "        # Sauvegarder et ecouter\n",
    "        tts_path = OUTPUT_DIR / \"tts_openai.wav\"\n",
    "        with open(tts_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"  Latence moyenne : {result.avg_latency:.2f}s\")\n",
    "        print(f\"  Taille : {len(response.content)/1024:.1f} KB\")\n",
    "        display(Audio(data=response.content, autoplay=False))\n",
    "\n",
    "    # --- Chatterbox ---\n",
    "    if \"chatterbox\" in tts_models:\n",
    "        print(f\"\\n--- Chatterbox TTS ---\")\n",
    "        result = BenchmarkResult(\"chatterbox\", \"tts\")\n",
    "        result.cost_per_unit = 0.0\n",
    "\n",
    "        try:\n",
    "            from chatterbox.tts import ChatterboxTTS\n",
    "\n",
    "            vram_before = measure_vram()\n",
    "            print(f\"  Chargement du modele...\")\n",
    "            cb_model = ChatterboxTTS.from_pretrained(device=device)\n",
    "            vram_after = measure_vram()\n",
    "            result.vram_mb = vram_after - vram_before\n",
    "            result.sample_rate = cb_model.sr\n",
    "            print(f\"  VRAM : {result.vram_mb:.0f} MB\")\n",
    "\n",
    "            for run in range(num_runs):\n",
    "                start_time = time.time()\n",
    "                wav = cb_model.generate(\n",
    "                    text=tts_test_text,\n",
    "                    exaggeration=0.5,\n",
    "                    cfg_weight=0.5\n",
    "                )\n",
    "                latency = time.time() - start_time\n",
    "                result.latencies.append(latency)\n",
    "                print(f\"  Run {run+1}/{num_runs} : {latency:.2f}s\")\n",
    "\n",
    "            if hasattr(wav, 'cpu'):\n",
    "                samples = wav.cpu().numpy().squeeze()\n",
    "            else:\n",
    "                samples = np.array(wav).squeeze()\n",
    "\n",
    "            result.output_data = samples\n",
    "            result.quality_score = 4.2  # MOS estime\n",
    "            all_results[\"tts_chatterbox\"] = result\n",
    "\n",
    "            tts_path = OUTPUT_DIR / \"tts_chatterbox.wav\"\n",
    "            sf.write(str(tts_path), samples, cb_model.sr)\n",
    "            print(f\"  Latence moyenne : {result.avg_latency:.2f}s\")\n",
    "            print(f\"  Duree audio : {len(samples)/cb_model.sr:.1f}s\")\n",
    "            display(Audio(data=samples, rate=cb_model.sr))\n",
    "\n",
    "            del cb_model\n",
    "            gc.collect()\n",
    "            if gpu_available:\n",
    "                torch.cuda.empty_cache()\n",
    "            print(f\"  Modele libere\")\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"  chatterbox-tts non installe\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur : {type(e).__name__} - {str(e)[:150]}\")\n",
    "\n",
    "    # --- Kokoro ---\n",
    "    if \"kokoro\" in tts_models:\n",
    "        print(f\"\\n--- Kokoro TTS (82M) ---\")\n",
    "        result = BenchmarkResult(\"kokoro-82m\", \"tts\")\n",
    "        result.cost_per_unit = 0.0\n",
    "\n",
    "        try:\n",
    "            from kokoro import KPipeline\n",
    "\n",
    "            vram_before = measure_vram()\n",
    "            print(f\"  Chargement du modele...\")\n",
    "            kokoro_pipe = KPipeline(lang_code='a')\n",
    "            vram_after = measure_vram()\n",
    "            result.vram_mb = vram_after - vram_before\n",
    "            result.sample_rate = 24000\n",
    "            print(f\"  VRAM : {result.vram_mb:.0f} MB\")\n",
    "\n",
    "            for run in range(num_runs):\n",
    "                start_time = time.time()\n",
    "                all_samples = []\n",
    "                for _, _, audio in kokoro_pipe(tts_test_text):\n",
    "                    all_samples.append(audio)\n",
    "                latency = time.time() - start_time\n",
    "                result.latencies.append(latency)\n",
    "                print(f\"  Run {run+1}/{num_runs} : {latency:.2f}s\")\n",
    "\n",
    "            samples = np.concatenate(all_samples) if all_samples else np.array([])\n",
    "            result.output_data = samples\n",
    "            result.quality_score = 4.0  # MOS estime\n",
    "            all_results[\"tts_kokoro\"] = result\n",
    "\n",
    "            tts_path = OUTPUT_DIR / \"tts_kokoro.wav\"\n",
    "            sf.write(str(tts_path), samples, 24000)\n",
    "            print(f\"  Latence moyenne : {result.avg_latency:.2f}s\")\n",
    "            print(f\"  Duree audio : {len(samples)/24000:.1f}s\")\n",
    "            display(Audio(data=samples, rate=24000))\n",
    "\n",
    "            del kokoro_pipe\n",
    "            gc.collect()\n",
    "            if gpu_available:\n",
    "                torch.cuda.empty_cache()\n",
    "            print(f\"  Modele libere\")\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"  kokoro non installe\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur : {type(e).__name__} - {str(e)[:150]}\")\n",
    "\n",
    "    # Tableau recapitulatif TTS\n",
    "    tts_results = {k: v for k, v in all_results.items() if v.category == \"tts\"}\n",
    "    if tts_results:\n",
    "        print(f\"\\nRecapitulatif TTS :\")\n",
    "        print(f\"{'Modele':<18} {'Latence (s)':<14} {'MOS est.':<10} {'Cout/1K ch.':<12} {'VRAM (MB)':<10}\")\n",
    "        print(\"-\" * 64)\n",
    "        for k, r in tts_results.items():\n",
    "            cost_str = f\"${r.cost_per_unit:.3f}\" if r.cost_per_unit > 0 else \"Gratuit\"\n",
    "            vram_str = f\"{r.vram_mb:.0f}\" if r.vram_mb > 0 else \"N/A (API)\"\n",
    "            print(f\"{r.model_name:<18} {r.avg_latency:<14.2f} {r.quality_score:<10.1f} {cost_str:<12} {vram_str:<10}\")\n",
    "else:\n",
    "    print(\"Benchmark TTS desactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Benchmark TTS\n",
    "\n",
    "| Aspect | OpenAI TTS | Chatterbox | Kokoro |\n",
    "|--------|-----------|------------|--------|\n",
    "| Qualite (MOS) | ~4.5 (meilleur) | ~4.2 (expressif) | ~4.0 (bon) |\n",
    "| Latence | 1-3s (reseau) | 2-5s (GPU) | 0.5-1s (leger) |\n",
    "| VRAM | 0 (cloud) | ~8 GB | ~2 GB |\n",
    "| Cout | $15/1M chars | Gratuit | Gratuit |\n",
    "| Emotions | Non | Oui (exaggeration) | Non |\n",
    "| Voice cloning | Non | Oui (6s clip) | Non |\n",
    "\n",
    "**Points cles** :\n",
    "1. OpenAI TTS offre la meilleure qualite brute mais a un cout recurrent\n",
    "2. Chatterbox se distingue par le controle emotionnel et le voice conditioning\n",
    "3. Kokoro est le plus leger et rapide, ideal pour du TTS simple en masse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-intro",
   "metadata": {},
   "source": [
    "## Section 5 : Visualisation des resultats\n",
    "\n",
    "Les graphiques permettent de comparer visuellement les performances des modeles sur plusieurs axes simultanement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des resultats\n",
    "print(\"VISUALISATION DES RESULTATS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = GridSpec(2, 2, figure=fig, hspace=0.35, wspace=0.3)\n",
    "\n",
    "    # --- 1. Bar chart latence STT ---\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    stt_results = {k: v for k, v in all_results.items() if v.category == \"stt\"}\n",
    "    if stt_results:\n",
    "        names = [r.model_name for r in stt_results.values()]\n",
    "        latencies = [r.avg_latency for r in stt_results.values()]\n",
    "        errors = [r.std_latency for r in stt_results.values()]\n",
    "        colors = ['#3498db', '#e74c3c']\n",
    "        bars = ax1.bar(names, latencies, yerr=errors, color=colors[:len(names)], capsize=5, alpha=0.8)\n",
    "        ax1.set_ylabel('Latence (s)')\n",
    "        ax1.set_title('Latence STT par modele')\n",
    "        for bar, lat in zip(bars, latencies):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
    "                    f'{lat:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'Pas de resultats STT', ha='center', va='center', transform=ax1.transAxes)\n",
    "\n",
    "    # --- 2. Bar chart latence TTS ---\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    tts_results = {k: v for k, v in all_results.items() if v.category == \"tts\"}\n",
    "    if tts_results:\n",
    "        names = [r.model_name for r in tts_results.values()]\n",
    "        latencies = [r.avg_latency for r in tts_results.values()]\n",
    "        errors = [r.std_latency for r in tts_results.values()]\n",
    "        colors = ['#2ecc71', '#f39c12', '#9b59b6']\n",
    "        bars = ax2.bar(names, latencies, yerr=errors, color=colors[:len(names)], capsize=5, alpha=0.8)\n",
    "        ax2.set_ylabel('Latence (s)')\n",
    "        ax2.set_title('Latence TTS par modele')\n",
    "        for bar, lat in zip(bars, latencies):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
    "                    f'{lat:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Pas de resultats TTS', ha='center', va='center', transform=ax2.transAxes)\n",
    "\n",
    "    # --- 3. Analyse des couts (bar chart) ---\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    all_names = []\n",
    "    all_costs = []\n",
    "    cost_colors = []\n",
    "    for k, r in all_results.items():\n",
    "        all_names.append(r.model_name)\n",
    "        # Normaliser : cout pour 1 heure d'utilisation\n",
    "        if r.category == \"stt\":\n",
    "            hourly_cost = r.cost_per_unit * 60  # 60 minutes\n",
    "        else:\n",
    "            hourly_cost = r.cost_per_unit * 50  # ~50K caracteres/heure\n",
    "        all_costs.append(hourly_cost)\n",
    "        cost_colors.append('#e74c3c' if hourly_cost > 0 else '#2ecc71')\n",
    "    if all_names:\n",
    "        bars = ax3.barh(all_names, all_costs, color=cost_colors, alpha=0.8)\n",
    "        ax3.set_xlabel('Cout par heure (USD)')\n",
    "        ax3.set_title('Analyse des couts (USD/heure)')\n",
    "        for bar, cost in zip(bars, all_costs):\n",
    "            label = f'${cost:.2f}' if cost > 0 else 'Gratuit'\n",
    "            ax3.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2.,\n",
    "                    label, ha='left', va='center', fontsize=10)\n",
    "\n",
    "    # --- 4. Radar plot TTS (qualite, vitesse, cout, VRAM) ---\n",
    "    ax4 = fig.add_subplot(gs[1, 1], polar=True)\n",
    "    if tts_results:\n",
    "        categories_radar = ['Qualite', 'Vitesse', 'Economie', 'Legerete']\n",
    "        N = len(categories_radar)\n",
    "        angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "        angles += angles[:1]\n",
    "\n",
    "        radar_colors = ['#2ecc71', '#f39c12', '#9b59b6']\n",
    "        for idx, (k, r) in enumerate(tts_results.items()):\n",
    "            # Normaliser chaque metrique sur 0-1\n",
    "            quality = r.quality_score / 5.0  # MOS max = 5\n",
    "            speed = 1.0 / (1.0 + r.avg_latency)  # inverse de la latence\n",
    "            economy = 1.0 if r.cost_per_unit == 0 else 0.3  # gratuit = 1.0\n",
    "            lightness = 1.0 / (1.0 + r.vram_mb / 1000)  # inverse de la VRAM\n",
    "\n",
    "            values = [quality, speed, economy, lightness]\n",
    "            values += values[:1]\n",
    "\n",
    "            ax4.plot(angles, values, 'o-', linewidth=2, label=r.model_name,\n",
    "                    color=radar_colors[idx % len(radar_colors)])\n",
    "            ax4.fill(angles, values, alpha=0.1, color=radar_colors[idx % len(radar_colors)])\n",
    "\n",
    "        ax4.set_xticks(angles[:-1])\n",
    "        ax4.set_xticklabels(categories_radar)\n",
    "        ax4.set_title('Profil TTS multi-criteres')\n",
    "        ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=9)\n",
    "    else:\n",
    "        ax4.text(0, 0, 'Pas de resultats TTS', ha='center', va='center')\n",
    "\n",
    "    plt.savefig(str(OUTPUT_DIR / \"benchmark_results.png\"), dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Graphique sauvegarde : benchmark_results.png\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"matplotlib non disponible pour la visualisation\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur visualisation : {type(e).__name__} - {str(e)[:150]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Visualisation\n",
    "\n",
    "| Graphique | Ce qu'il montre | Lecture |\n",
    "|-----------|----------------|--------|\n",
    "| Bar chart STT | Latence par modele avec ecart-type | Plus bas = plus rapide |\n",
    "| Bar chart TTS | Latence par moteur TTS | Plus bas = plus rapide |\n",
    "| Couts horaires | Comparaison economique | Vert = gratuit, Rouge = payant |\n",
    "| Radar TTS | Profil multi-criteres normalise | Plus grand = meilleur |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le radar plot permet de voir les forces et faiblesses de chaque modele en un coup d'oeil\n",
    "2. Les modeles locaux dominent sur l'axe economie mais peuvent perdre en qualite\n",
    "3. Le choix optimal depend du cas d'usage (temps reel vs batch, budget vs qualite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif - Choix des modeles a comparer\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"MODE INTERACTIF - COMPARAISON PERSONNALISEE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nChoisissez les modeles a comparer :\")\n",
    "    print(\"  STT : whisper-1, large-v3-turbo\")\n",
    "    print(\"  TTS : openai, chatterbox, kokoro\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "\n",
    "    try:\n",
    "        user_text = input(\"\\nTexte personnalise pour TTS (ou vide) : \")\n",
    "\n",
    "        if user_text.strip():\n",
    "            print(f\"\\nGeneration avec les modeles TTS disponibles...\")\n",
    "            for k, r in all_results.items():\n",
    "                if r.category == \"tts\":\n",
    "                    print(f\"\\n  {r.model_name} : resultats du benchmark\")\n",
    "                    print(f\"    Latence moyenne : {r.avg_latency:.2f}s\")\n",
    "                    print(f\"    Qualite estimee : {r.quality_score:.1f}/5.0\")\n",
    "                    if r.output_data is not None and isinstance(r.output_data, np.ndarray):\n",
    "                        display(Audio(data=r.output_data, rate=r.sample_rate))\n",
    "\n",
    "            print(f\"\\nPour une comparaison sur votre texte, re-executez\")\n",
    "            print(f\"le notebook avec un texte modifie dans la cellule de test.\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type} - {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-practices",
   "metadata": {},
   "source": [
    "## Guide de decision et bonnes pratiques\n",
    "\n",
    "### Arbre de decision pour le choix du modele\n",
    "\n",
    "| Critere | Recommandation STT | Recommandation TTS |\n",
    "|---------|-------------------|--------------------|\n",
    "| Budget limite | faster-whisper local | Kokoro ou Chatterbox |\n",
    "| Qualite maximale | Whisper API | OpenAI tts-1-hd |\n",
    "| Temps reel | faster-whisper (GPU) | Kokoro (plus leger) |\n",
    "| Hors-ligne | faster-whisper | Kokoro ou Chatterbox |\n",
    "| Emotions/prosodie | N/A | Chatterbox |\n",
    "| Voice cloning | N/A | Chatterbox ou XTTS |\n",
    "| Production a grande echelle | faster-whisper | Kokoro (leger) |\n",
    "\n",
    "### Estimation des couts mensuels\n",
    "\n",
    "| Usage | Volume | Cout API | Cout local (electricite) |\n",
    "|-------|--------|----------|-------------------------|\n",
    "| Prototype | 10 min/jour | ~$2/mois | ~$0 |\n",
    "| Application | 1h/jour | ~$15/mois | ~$5/mois (GPU) |\n",
    "| Production | 10h/jour | ~$150/mois | ~$30/mois (GPU) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"STATISTIQUES DE SESSION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Device : {device}\")\n",
    "print(f\"Modeles STT testes : {[r.model_name for r in all_results.values() if r.category == 'stt']}\")\n",
    "print(f\"Modeles TTS testes : {[r.model_name for r in all_results.values() if r.category == 'tts']}\")\n",
    "print(f\"Runs par modele : {num_runs}\")\n",
    "\n",
    "if gpu_available:\n",
    "    vram_current = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"VRAM actuelle : {vram_current:.2f} GB\")\n",
    "\n",
    "# Resume complet\n",
    "if all_results:\n",
    "    print(f\"\\nResume des benchmarks :\")\n",
    "    for k, r in all_results.items():\n",
    "        print(f\"  {r.model_name:<20} : latence={r.avg_latency:.2f}s, qualite={r.quality_score:.2f}\")\n",
    "\n",
    "    # Sauvegarder les resultats en JSON\n",
    "    if save_results:\n",
    "        results_json = {k: v.to_dict() for k, v in all_results.items()}\n",
    "        json_path = OUTPUT_DIR / \"benchmark_results.json\"\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(results_json, f, indent=2)\n",
    "        print(f\"\\nResultats sauvegardes : {json_path}\")\n",
    "\n",
    "if save_results:\n",
    "    saved = list(OUTPUT_DIR.glob('*'))\n",
    "    total_size = sum(f.stat().st_size for f in saved if f.is_file()) / (1024*1024)\n",
    "    print(f\"Fichiers sauvegardes : {len(saved)} ({total_size:.1f} MB) dans {OUTPUT_DIR}\")\n",
    "\n",
    "# Liberation memoire\n",
    "gc.collect()\n",
    "if gpu_available:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Memoire GPU liberee\")\n",
    "\n",
    "print(f\"\\nPROCHAINES ETAPES\")\n",
    "print(f\"1. Construire des pipelines audio complets STT->LLM->TTS (03-2)\")\n",
    "print(f\"2. Explorer l'API Realtime d'OpenAI pour la voix (03-3)\")\n",
    "print(f\"3. Creer du contenu educatif audio automatise (04-1)\")\n",
    "\n",
    "print(f\"\\nNotebook Comparaison Multi-Modeles termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}