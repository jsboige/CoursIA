{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-header",
      "metadata": {},
      "source": [
        "# Chatterbox TTS - Synthese Vocale Expressive\n",
        "\n",
        "**Module :** 02-Audio-Advanced  \n",
        "**Niveau :** Intermediaire  \n",
        "**Technologies :** Chatterbox Turbo (ResembleAI, MIT license), ~8 GB VRAM  \n",
        "**Duree estimee :** 45 minutes  \n",
        "\n",
        "## Objectifs d'Apprentissage\n",
        "\n",
        "- [ ] Installer et charger le modele Chatterbox TTS\n",
        "- [ ] Generer de la parole expressive avec controle d'emotion\n",
        "- [ ] Maitriser les parametres d'exageration et de guidance (cfg_weight)\n",
        "- [ ] Utiliser le voice conditioning a partir d'un clip audio de reference (6s)\n",
        "- [ ] Controler la prosodie et l'intonation via les parametres du modele\n",
        "- [ ] Comparer la qualite avec OpenAI TTS et Kokoro\n",
        "- [ ] Evaluer les cas d'usage : narration, assistants, e-learning\n",
        "\n",
        "## Prerequis\n",
        "\n",
        "- GPU NVIDIA avec au moins 8 GB VRAM\n",
        "- `pip install chatterbox-tts`\n",
        "- Notebook 01-5 recommande (pour la comparaison avec Kokoro)\n",
        "\n",
        "**Navigation :** [<< 01-5](../01-Foundation/01-5-Kokoro-TTS-Local.ipynb) | [Index](../README.md) | [Suivant >>](02-2-XTTS-Voice-Cloning.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-params",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
        "\n",
        "# Configuration notebook\n",
        "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
        "skip_widgets = False               # True pour mode batch MCP\n",
        "debug_level = \"INFO\"\n",
        "\n",
        "# Parametres Chatterbox\n",
        "model_name = \"chatterbox\"           # Modele Chatterbox TTS\n",
        "device = \"cuda\"                     # \"cuda\" ou \"cpu\"\n",
        "exaggeration = 0.5                  # Intensite de l'emotion (0.0 a 1.0)\n",
        "cfg_weight = 0.5                    # Guidance weight (0.0 a 1.0)\n",
        "\n",
        "# Configuration\n",
        "generate_audio = True              # Generer les fichiers audio\n",
        "save_results = True                # Sauvegarder les fichiers generes\n",
        "compare_emotions = True            # Comparer les differentes emotions\n",
        "test_voice_conditioning = True     # Tester le voice conditioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environnement et imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio, display, HTML\n",
        "\n",
        "# Import helpers GenAI\n",
        "GENAI_ROOT = Path.cwd()\n",
        "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
        "    GENAI_ROOT = GENAI_ROOT.parent\n",
        "\n",
        "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
        "if HELPERS_PATH.exists():\n",
        "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
        "    try:\n",
        "        from helpers.audio_helpers import play_audio, save_audio\n",
        "        print(\"Helpers audio importes\")\n",
        "    except ImportError:\n",
        "        print(\"Helpers audio non disponibles - mode autonome\")\n",
        "\n",
        "# Repertoires\n",
        "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'chatterbox'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Configuration logging\n",
        "logging.basicConfig(level=getattr(logging, debug_level))\n",
        "logger = logging.getLogger('chatterbox_tts')\n",
        "\n",
        "# Verification GPU\n",
        "gpu_available = False\n",
        "try:\n",
        "    import torch\n",
        "    gpu_available = torch.cuda.is_available()\n",
        "    if gpu_available:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_vram = torch.cuda.get_device_properties(0).total_mem / (1024**3)\n",
        "        print(f\"GPU : {gpu_name} ({gpu_vram:.1f} GB VRAM)\")\n",
        "    else:\n",
        "        print(\"GPU non disponible - Chatterbox necessite un GPU pour des performances correctes\")\n",
        "        if device == \"cuda\":\n",
        "            device = \"cpu\"\n",
        "            print(\"Fallback vers CPU\")\n",
        "except ImportError:\n",
        "    print(\"torch non installe\")\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"\\nChatterbox TTS - Synthese Vocale Expressive\")\n",
        "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Mode : {notebook_mode}, Device : {device}\")\n",
        "print(f\"Exaggeration : {exaggeration}, CFG Weight : {cfg_weight}\")\n",
        "print(f\"Sortie : {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-env",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chargement .env (pour comparaison eventuelle avec OpenAI)\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "current_path = Path.cwd()\n",
        "found_env = False\n",
        "for _ in range(4):\n",
        "    env_path = current_path / '.env'\n",
        "    if env_path.exists():\n",
        "        load_dotenv(env_path)\n",
        "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
        "        found_env = True\n",
        "        break\n",
        "    current_path = current_path.parent\n",
        "\n",
        "if not found_env:\n",
        "    print(\"Aucun fichier .env trouve - pas de comparaison cloud possible\")\n",
        "\n",
        "openai_key = os.getenv('OPENAI_API_KEY')\n",
        "if openai_key:\n",
        "    print(f\"OPENAI_API_KEY disponible (pour comparaison)\")\n",
        "else:\n",
        "    print(f\"OPENAI_API_KEY non disponible - comparaison avec OpenAI desactivee\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-section1-intro",
      "metadata": {},
      "source": [
        "## Section 1 : Presentation de Chatterbox\n",
        "\n",
        "Chatterbox est un modele TTS developpe par ResembleAI, sous licence MIT. Sa particularite est le controle fin de l'expressivite vocale : emotion, prosodie, intonation.\n",
        "\n",
        "### Caracteristiques\n",
        "\n",
        "| Aspect | Chatterbox Turbo | Kokoro 82M | OpenAI TTS |\n",
        "|--------|-----------------|-----------|------------|\n",
        "| Editeur | ResembleAI | Hexgrad | OpenAI |\n",
        "| Licence | MIT | MIT | Proprietaire |\n",
        "| Parametres | ~300M | 82M | Non publie |\n",
        "| VRAM | ~8 GB | ~2 GB | N/A (API) |\n",
        "| Emotions | Oui (controle fin) | Non | Non |\n",
        "| Voice conditioning | Oui (6s clip) | Non | Non |\n",
        "| Qualite (MOS) | ~4.2 | ~4.0 | ~4.5 |\n",
        "| Cout | Gratuit | Gratuit | $15-30/1M chars |\n",
        "\n",
        "### Parametres cles\n",
        "\n",
        "| Parametre | Plage | Description |\n",
        "|-----------|-------|-------------|\n",
        "| `exaggeration` | 0.0 - 1.0 | Intensite de l'expressivite (0 = neutre, 1 = tres expressif) |\n",
        "| `cfg_weight` | 0.0 - 1.0 | Guidance : fidelite au conditionnement (0 = libre, 1 = strict) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-load-model",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chargement du modele Chatterbox\n",
        "print(\"CHARGEMENT DU MODELE CHATTERBOX\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "chatterbox_loaded = False\n",
        "\n",
        "try:\n",
        "    from chatterbox.tts import ChatterboxTTS\n",
        "\n",
        "    print(f\"Chargement Chatterbox sur {device}...\")\n",
        "    start_time = time.time()\n",
        "    model = ChatterboxTTS.from_pretrained(device=device)\n",
        "    load_time = time.time() - start_time\n",
        "    chatterbox_loaded = True\n",
        "\n",
        "    print(f\"Modele charge en {load_time:.1f}s\")\n",
        "    print(f\"Device : {device}\")\n",
        "    print(f\"Sample rate : {model.sr} Hz\")\n",
        "\n",
        "    if gpu_available:\n",
        "        vram_used = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "        print(f\"VRAM utilisee : {vram_used:.2f} GB\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"chatterbox-tts non installe\")\n",
        "    print(\"Installation : pip install chatterbox-tts\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement : {type(e).__name__} - {str(e)[:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-section2-intro",
      "metadata": {},
      "source": [
        "## Section 2 : Premiere generation\n",
        "\n",
        "La generation avec Chatterbox utilise la methode `model.generate()`. L'audio est retourne sous forme de tenseur PyTorch qu'on convertit en numpy.\n",
        "\n",
        "| Parametre | Type | Description |\n",
        "|-----------|------|-------------|\n",
        "| `text` | str | Texte a synthetiser |\n",
        "| `audio_prompt_path` | str/None | Chemin vers un clip audio de reference (optionnel) |\n",
        "| `exaggeration` | float | Controle d'expressivite (0.0-1.0) |\n",
        "| `cfg_weight` | float | Guidance weight (0.0-1.0) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-first-gen",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Premiere generation TTS avec Chatterbox\n",
        "print(\"PREMIERE GENERATION CHATTERBOX\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "sample_text = (\n",
        "    \"Welcome to this tutorial on expressive speech synthesis. \"\n",
        "    \"Chatterbox allows you to control the emotion and style of generated speech \"\n",
        "    \"with remarkable precision.\"\n",
        ")\n",
        "\n",
        "print(f\"Texte : {sample_text}\")\n",
        "print(f\"Exaggeration : {exaggeration}\")\n",
        "print(f\"CFG Weight : {cfg_weight}\")\n",
        "\n",
        "if chatterbox_loaded and generate_audio:\n",
        "    start_time = time.time()\n",
        "\n",
        "    wav = model.generate(\n",
        "        text=sample_text,\n",
        "        exaggeration=exaggeration,\n",
        "        cfg_weight=cfg_weight\n",
        "    )\n",
        "\n",
        "    gen_time = time.time() - start_time\n",
        "\n",
        "    # Conversion tenseur -> numpy\n",
        "    if hasattr(wav, 'cpu'):\n",
        "        samples = wav.cpu().numpy().squeeze()\n",
        "    else:\n",
        "        samples = np.array(wav).squeeze()\n",
        "\n",
        "    sample_rate = model.sr\n",
        "    duration = len(samples) / sample_rate\n",
        "\n",
        "    print(f\"\\nGeneration reussie\")\n",
        "    print(f\"  Duree audio : {duration:.1f}s\")\n",
        "    print(f\"  Sample rate : {sample_rate} Hz\")\n",
        "    print(f\"  Temps de generation : {gen_time:.2f}s\")\n",
        "    print(f\"  Ratio temps reel : {duration / gen_time:.1f}x\")\n",
        "\n",
        "    # Ecoute\n",
        "    print(f\"\\nEcoute :\")\n",
        "    display(Audio(data=samples, rate=sample_rate))\n",
        "\n",
        "    # Sauvegarde\n",
        "    if save_results:\n",
        "        filepath = OUTPUT_DIR / f\"chatterbox_basic.wav\"\n",
        "        sf.write(str(filepath), samples, sample_rate)\n",
        "        print(f\"Fichier sauvegarde : {filepath.name}\")\n",
        "else:\n",
        "    print(\"Modele non charge ou generation desactivee\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-section2-interpretation",
      "metadata": {},
      "source": [
        "### Interpretation : Premiere generation\n",
        "\n",
        "| Aspect | Valeur typique | Signification |\n",
        "|--------|---------------|---------------|\n",
        "| Ratio temps reel | 3-10x (GPU) | Chatterbox est plus lent que Kokoro mais reste temps reel |\n",
        "| Sample rate | 24000 Hz | Standard pour la parole haute qualite |\n",
        "| VRAM | ~8 GB | Necessaire pour le modele complet |\n",
        "\n",
        "> **Note technique** : Chatterbox genere nativement en anglais. Le support d'autres langues est experimental et peut produire des resultats variables."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-section3-intro",
      "metadata": {},
      "source": [
        "## Section 3 : Controle des emotions\n",
        "\n",
        "L'atout principal de Chatterbox est le controle fin de l'expressivite via le parametre `exaggeration` :\n",
        "\n",
        "| Exaggeration | Resultat | Cas d'usage |\n",
        "|-------------|----------|-------------|\n",
        "| 0.0 | Voix neutre, monotone | Lecture factuelle, annonces |\n",
        "| 0.25 | Legerement expressif | Narration documentaire |\n",
        "| 0.5 | Expressivite moderee | E-learning, podcasts |\n",
        "| 0.75 | Tres expressif | Histoires, contenus engageants |\n",
        "| 1.0 | Exagere | Personnages, animations |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-emotions",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test des differents niveaux d'expressivite\n",
        "print(\"CONTROLE DES EMOTIONS\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "emotion_text = (\n",
        "    \"I am absolutely thrilled to share this incredible news with you today. \"\n",
        "    \"This is going to change everything.\"\n",
        ")\n",
        "\n",
        "exaggeration_levels = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
        "emotion_results = {}\n",
        "\n",
        "if chatterbox_loaded and generate_audio and compare_emotions:\n",
        "    for exag in exaggeration_levels:\n",
        "        print(f\"\\nExaggeration = {exag}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        wav = model.generate(\n",
        "            text=emotion_text,\n",
        "            exaggeration=exag,\n",
        "            cfg_weight=cfg_weight\n",
        "        )\n",
        "\n",
        "        gen_time = time.time() - start_time\n",
        "\n",
        "        if hasattr(wav, 'cpu'):\n",
        "            samples = wav.cpu().numpy().squeeze()\n",
        "        else:\n",
        "            samples = np.array(wav).squeeze()\n",
        "\n",
        "        sample_rate = model.sr\n",
        "        duration = len(samples) / sample_rate\n",
        "\n",
        "        emotion_results[exag] = {\n",
        "            \"duration\": duration,\n",
        "            \"gen_time\": gen_time,\n",
        "            \"samples\": samples,\n",
        "            \"sample_rate\": sample_rate\n",
        "        }\n",
        "\n",
        "        print(f\"  Duree : {duration:.1f}s | Temps : {gen_time:.2f}s\")\n",
        "        display(Audio(data=samples, rate=sample_rate))\n",
        "\n",
        "        if save_results:\n",
        "            filepath = OUTPUT_DIR / f\"emotion_exag_{exag:.2f}.wav\"\n",
        "            sf.write(str(filepath), samples, sample_rate)\n",
        "\n",
        "    # Tableau recapitulatif\n",
        "    print(f\"\\nRecapitulatif des generations :\")\n",
        "    print(f\"{'Exaggeration':<15} {'Duree (s)':<12} {'Temps gen (s)':<15}\")\n",
        "    print(\"-\" * 42)\n",
        "    for exag, data in emotion_results.items():\n",
        "        print(f\"{exag:<15.2f} {data['duration']:<12.1f} {data['gen_time']:<15.2f}\")\n",
        "else:\n",
        "    print(\"Test des emotions desactive ou modele non charge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-section3-interpretation",
      "metadata": {},
      "source": [
        "### Interpretation : Controle des emotions\n",
        "\n",
        "| Exaggeration | Observation | Recommandation |\n",
        "|-------------|-------------|----------------|\n",
        "| 0.0 | Voix plate, robotique | Eviter sauf pour du contenu tres factuel |\n",
        "| 0.25-0.5 | Naturel, professionnel | Ideal pour la majorite des cas d'usage |\n",
        "| 0.75 | Tres expressif, engageant | Bon pour le storytelling |\n",
        "| 1.0 | Parfois exagere | A utiliser avec precaution |\n",
        "\n",
        "**Points cles** :\n",
        "1. L'exaggeration n'affecte pas significativement le temps de generation\n",
        "2. La duree audio peut varier selon l'expressivite (pauses, debit)\n",
        "3. La valeur 0.5 est un bon point de depart pour la plupart des usages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-section4-intro",
      "metadata": {},
      "source": [
        "## Section 4 : Voice conditioning\n",
        "\n",
        "Chatterbox peut reproduire le timbre d'une voix de reference a partir d'un clip audio de ~6 secondes. C'est la fonctionnalite de **voice conditioning** (ou voice prompting).\n",
        "\n",
        "### Principe\n",
        "\n",
        "| Etape | Description |\n",
        "|-------|-------------|\n",
        "| 1. Clip de reference | Audio WAV de ~6s avec la voix cible |\n",
        "| 2. Extraction | Le modele extrait les caracteristiques vocales |\n",
        "| 3. Generation | Le texte est synthetise avec le timbre de reference |\n",
        "\n",
        "> **Note ethique** : Le clonage vocal souleve des questions importantes. Utilisez uniquement des voix pour lesquelles vous avez le consentement explicite du locuteur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-voice-conditioning",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Voice conditioning avec clip de reference\n",
        "print(\"VOICE CONDITIONING\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "conditioning_text = (\n",
        "    \"This speech is generated using voice conditioning. \"\n",
        "    \"The model captures the timbre and characteristics of the reference speaker.\"\n",
        ")\n",
        "\n",
        "if chatterbox_loaded and generate_audio and test_voice_conditioning:\n",
        "    # Creer un clip de reference synthetique pour la demonstration\n",
        "    # En production, on utiliserait un vrai enregistrement vocal\n",
        "    print(\"Creation d'un clip de reference synthetique...\")\n",
        "\n",
        "    ref_wav = model.generate(\n",
        "        text=\"Hello, this is my reference voice clip for conditioning.\",\n",
        "        exaggeration=0.3,\n",
        "        cfg_weight=0.5\n",
        "    )\n",
        "\n",
        "    if hasattr(ref_wav, 'cpu'):\n",
        "        ref_samples = ref_wav.cpu().numpy().squeeze()\n",
        "    else:\n",
        "        ref_samples = np.array(ref_wav).squeeze()\n",
        "\n",
        "    # Sauvegarder le clip de reference\n",
        "    ref_path = OUTPUT_DIR / \"reference_clip.wav\"\n",
        "    sf.write(str(ref_path), ref_samples, model.sr)\n",
        "    print(f\"Clip de reference : {ref_path.name} ({len(ref_samples)/model.sr:.1f}s)\")\n",
        "    display(Audio(data=ref_samples, rate=model.sr))\n",
        "\n",
        "    # Generation avec voice conditioning\n",
        "    print(f\"\\nGeneration avec voice conditioning...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    wav_conditioned = model.generate(\n",
        "        text=conditioning_text,\n",
        "        audio_prompt_path=str(ref_path),\n",
        "        exaggeration=exaggeration,\n",
        "        cfg_weight=cfg_weight\n",
        "    )\n",
        "\n",
        "    gen_time = time.time() - start_time\n",
        "\n",
        "    if hasattr(wav_conditioned, 'cpu'):\n",
        "        cond_samples = wav_conditioned.cpu().numpy().squeeze()\n",
        "    else:\n",
        "        cond_samples = np.array(wav_conditioned).squeeze()\n",
        "\n",
        "    duration = len(cond_samples) / model.sr\n",
        "\n",
        "    print(f\"  Duree : {duration:.1f}s | Temps : {gen_time:.2f}s\")\n",
        "    display(Audio(data=cond_samples, rate=model.sr))\n",
        "\n",
        "    # Generation sans conditioning pour comparaison\n",
        "    print(f\"\\nGeneration SANS conditioning (meme texte) :\")\n",
        "    wav_uncond = model.generate(\n",
        "        text=conditioning_text,\n",
        "        exaggeration=exaggeration,\n",
        "        cfg_weight=cfg_weight\n",
        "    )\n",
        "\n",
        "    if hasattr(wav_uncond, 'cpu'):\n",
        "        uncond_samples = wav_uncond.cpu().numpy().squeeze()\n",
        "    else:\n",
        "        uncond_samples = np.array(wav_uncond).squeeze()\n",
        "\n",
        "    print(f\"  Duree : {len(uncond_samples)/model.sr:.1f}s\")\n",
        "    display(Audio(data=uncond_samples, rate=model.sr))\n",
        "\n",
        "    if save_results:\n",
        "        sf.write(str(OUTPUT_DIR / \"conditioned.wav\"), cond_samples, model.sr)\n",
        "        sf.write(str(OUTPUT_DIR / \"unconditioned.wav\"), uncond_samples, model.sr)\n",
        "        print(f\"Fichiers sauvegardes\")\n",
        "else:\n",
        "    print(\"Voice conditioning desactive ou modele non charge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-section4-interpretation",
      "metadata": {},
      "source": [
        "### Interpretation : Voice conditioning\n",
        "\n",
        "| Aspect | Avec conditioning | Sans conditioning |\n",
        "|--------|------------------|-------------------|\n",
        "| Timbre | Proche de la reference | Voix par defaut du modele |\n",
        "| Latence | Legerement plus elevee | Standard |\n",
        "| Qualite | Tres bonne si clip de qualite | Constante |\n",
        "\n",
        "**Points cles** :\n",
        "1. Le clip de reference doit etre de bonne qualite (pas de bruit de fond)\n",
        "2. Une duree de ~6 secondes est optimale (trop court = mauvaise capture, trop long = inutile)\n",
        "3. Le voice conditioning fonctionne mieux avec un cfg_weight plus eleve (0.5-0.8)\n",
        "\n",
        "> **Note ethique** : Ne clonez jamais une voix sans le consentement explicite de la personne concernee."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-interactive",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mode interactif - Generation personnalisee\n",
        "if notebook_mode == \"interactive\" and not skip_widgets:\n",
        "    print(\"MODE INTERACTIF - GENERATION PERSONNALISEE\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\nEntrez un texte a synthetiser avec Chatterbox :\")\n",
        "    print(\"(Laissez vide pour passer a la suite)\")\n",
        "\n",
        "    try:\n",
        "        user_text = input(\"\\nVotre texte : \")\n",
        "\n",
        "        if user_text.strip() and chatterbox_loaded:\n",
        "            user_exag = input(f\"Exaggeration [{exaggeration}] (0.0-1.0) : \").strip()\n",
        "            user_exag = float(user_exag) if user_exag else exaggeration\n",
        "\n",
        "            user_cfg = input(f\"CFG Weight [{cfg_weight}] (0.0-1.0) : \").strip()\n",
        "            user_cfg = float(user_cfg) if user_cfg else cfg_weight\n",
        "\n",
        "            print(f\"\\nGeneration en cours (exag={user_exag}, cfg={user_cfg})...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            wav = model.generate(\n",
        "                text=user_text,\n",
        "                exaggeration=user_exag,\n",
        "                cfg_weight=user_cfg\n",
        "            )\n",
        "\n",
        "            gen_time = time.time() - start_time\n",
        "\n",
        "            if hasattr(wav, 'cpu'):\n",
        "                samples = wav.cpu().numpy().squeeze()\n",
        "            else:\n",
        "                samples = np.array(wav).squeeze()\n",
        "\n",
        "            print(f\"Duree : {len(samples)/model.sr:.1f}s | Temps : {gen_time:.2f}s\")\n",
        "            display(Audio(data=samples, rate=model.sr))\n",
        "\n",
        "            if save_results:\n",
        "                ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "                filepath = OUTPUT_DIR / f\"custom_{ts}.wav\"\n",
        "                sf.write(str(filepath), samples, model.sr)\n",
        "                print(f\"Sauvegarde : {filepath.name}\")\n",
        "        else:\n",
        "            print(\"Mode interactif ignore\")\n",
        "\n",
        "    except (KeyboardInterrupt, EOFError):\n",
        "        print(\"Mode interactif interrompu\")\n",
        "    except Exception as e:\n",
        "        error_type = type(e).__name__\n",
        "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
        "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
        "        else:\n",
        "            print(f\"Erreur : {error_type} - {str(e)[:100]}\")\n",
        "else:\n",
        "    print(\"Mode batch - Interface interactive desactivee\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-practices",
      "metadata": {},
      "source": [
        "## Bonnes pratiques et guide de decision\n",
        "\n",
        "### Quand utiliser Chatterbox\n",
        "\n",
        "| Scenario | Recommandation | Raison |\n",
        "|----------|---------------|--------|\n",
        "| Narration expressive | Chatterbox | Controle fin des emotions |\n",
        "| Voice conditioning | Chatterbox | Fonctionnalite unique |\n",
        "| TTS basique rapide | Kokoro | Plus leger, plus rapide |\n",
        "| Qualite maximale | OpenAI TTS-HD | Meilleure naturalite |\n",
        "| Multilangue | XTTS v2 | 17 langues supportees |\n",
        "\n",
        "### Optimisation des parametres\n",
        "\n",
        "| Usage | Exaggeration | CFG Weight |\n",
        "|-------|-------------|------------|\n",
        "| Narration documentaire | 0.2-0.3 | 0.5 |\n",
        "| E-learning | 0.4-0.5 | 0.5 |\n",
        "| Storytelling | 0.6-0.8 | 0.4 |\n",
        "| Personnages animes | 0.8-1.0 | 0.3 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-stats",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistiques de session et prochaines etapes\n",
        "print(\"STATISTIQUES DE SESSION\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Modele : {model_name}\")\n",
        "print(f\"Device : {device}\")\n",
        "print(f\"Exaggeration : {exaggeration}\")\n",
        "print(f\"CFG Weight : {cfg_weight}\")\n",
        "print(f\"Modele charge : {'Oui' if chatterbox_loaded else 'Non'}\")\n",
        "\n",
        "if gpu_available:\n",
        "    vram_current = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "    print(f\"VRAM utilisee : {vram_current:.2f} GB\")\n",
        "\n",
        "if save_results:\n",
        "    saved = list(OUTPUT_DIR.glob('*'))\n",
        "    total_size = sum(f.stat().st_size for f in saved) / (1024*1024)\n",
        "    print(f\"Fichiers sauvegardes : {len(saved)} ({total_size:.1f} MB) dans {OUTPUT_DIR}\")\n",
        "\n",
        "# Liberation memoire\n",
        "if chatterbox_loaded:\n",
        "    print(f\"\\nLiberation du modele...\")\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if gpu_available:\n",
        "        torch.cuda.empty_cache()\n",
        "    print(f\"Memoire liberee\")\n",
        "\n",
        "print(f\"\\nPROCHAINES ETAPES\")\n",
        "print(f\"1. Decouvrir le voice cloning avec XTTS v2 (02-2)\")\n",
        "print(f\"2. Explorer la generation musicale avec MusicGen (02-3)\")\n",
        "print(f\"3. Tester la separation de sources avec Demucs (02-4)\")\n",
        "print(f\"4. Comparer tous les modeles audio (03-1)\")\n",
        "\n",
        "print(f\"\\nNotebook Chatterbox TTS termine - {datetime.now().strftime('%H:%M:%S')}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}