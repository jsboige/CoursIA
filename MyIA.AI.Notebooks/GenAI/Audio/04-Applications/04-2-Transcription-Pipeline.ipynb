{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# Pipeline de Transcription et Sous-titrage\n",
    "\n",
    "**Module :** 04-Audio-Applications  \n",
    "**Niveau :** Applications  \n",
    "**Technologies :** Whisper (local + API), pyannote (diarisation), pysrt, GPT (LLM)  \n",
    "**VRAM estimee :** ~12 GB  \n",
    "**Duree estimee :** 55 minutes  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Transcrire un fichier audio avec horodatage (timestamps)\n",
    "- [ ] Mettre en place un pipeline de transcription batch (plusieurs fichiers)\n",
    "- [ ] Comprendre la diarisation de locuteurs (identification des intervenants)\n",
    "- [ ] Generer des sous-titres aux formats SRT et VTT\n",
    "- [ ] Resumer automatiquement des reunions (transcription -> LLM -> resume)\n",
    "- [ ] Evaluer la qualite de transcription et appliquer du post-traitement\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Notebooks Foundation (01-2 Whisper STT, 01-4 Whisper Local) completes\n",
    "- Cle API OpenAI configuree (`OPENAI_API_KEY` dans `.env`)\n",
    "- faster-whisper installe (transcription locale)\n",
    "- Comprehension du STT et des formats de sous-titres\n",
    "\n",
    "**Navigation :** [Index](../README.md) | [<< Precedent](04-1-Educational-Audio-Content.ipynb) | [Suivant >>](04-3-Music-Composition-Workflow.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-params",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": "# Parametres Papermill - JAMAIS modifier ce commentaire\n\n# Configuration notebook\nnotebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\nskip_widgets = False               # True pour mode batch MCP\ndebug_level = \"INFO\"\n\n# Parametres transcription\nwhisper_model = \"large-v3-turbo\"   # Modele Whisper local\nwhisper_device = \"cuda\"            # \"cuda\" ou \"cpu\"\nwhisper_api_model = \"whisper-1\"    # Modele API OpenAI\ndefault_language = \"fr\"            # Langue par defaut\nllm_model = \"gpt-4o-mini\"         # Modele pour le resume\n\n# Configuration pipeline\nuse_local_whisper = False          # Desactive Whisper local pour validation (problÃ¨mes CUDA/CUBLAS)\ngenerate_audio = True              # Generer des fichiers audio de test\nsave_output_files = True           # Sauvegarder les transcriptions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from io import BytesIO\n",
    "import logging\n",
    "\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "# Resolution GENAI_ROOT\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.audio_helpers import (\n",
    "            transcribe_openai, transcribe_local,\n",
    "            synthesize_openai, get_audio_info\n",
    "        )\n",
    "        print(\"Helpers audio importes\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Helpers audio non disponibles - mode autonome : {e}\")\n",
    "\n",
    "# Repertoires\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'transcription'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('transcription_pipeline')\n",
    "\n",
    "print(f\"Pipeline de Transcription et Sous-titrage\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Whisper : {'local (' + whisper_model + ')' if use_local_whisper else 'API (' + whisper_api_model + ')'}\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-env",
   "metadata": {},
   "outputs": [],
   "source": "# Chargement de la configuration et validation\nfrom dotenv import load_dotenv\n\ncurrent_path = Path.cwd()\nfound_env = False\nfor _ in range(4):\n    env_path = current_path / '.env'\n    if env_path.exists():\n        load_dotenv(env_path)\n        print(f\"Fichier .env charge depuis : {env_path}\")\n        found_env = True\n        break\n    current_path = current_path.parent\n\nif not found_env:\n    print(\"Aucun fichier .env trouve dans l'arborescence\")\n\nopenai_key = os.getenv('OPENAI_API_KEY')\n\nif not openai_key:\n    if notebook_mode == \"batch\" and not generate_audio:\n        openai_key = \"dummy_key_for_validation\"\n        print(\"Mode batch sans generation : cle API ignoree\")\n    else:\n        raise ValueError(\"OPENAI_API_KEY manquante dans .env\")\n\nfrom openai import OpenAI\nclient = OpenAI(api_key=openai_key)\n\n# Validation Whisper local avec detection automatique du device et compute_type\nwhisper_local_available = False\nactual_whisper_device = \"cpu\"  # Par defaut CPU pour eviter les erreurs CUDA\nactual_compute_type = \"int8\"   # CPU utilise int8 ou float32\n\nif use_local_whisper:\n    try:\n        from faster_whisper import WhisperModel\n        \n        # Tester si CUDA est disponible\n        if whisper_device == \"cuda\":\n            try:\n                test_model = WhisperModel(\"tiny\", device=\"cuda\", compute_type=\"float16\")\n                del test_model\n                actual_whisper_device = \"cuda\"\n                actual_compute_type = \"float16\"\n                print(\"CUDA disponible - faster-whisper utilisera GPU\")\n            except Exception as cuda_err:\n                if \"cublas\" in str(cuda_err).lower() or \"cuda\" in str(cuda_err).lower():\n                    print(f\"Erreur CUDA detectee : basculement vers CPU\")\n                    actual_whisper_device = \"cpu\"\n                    actual_compute_type = \"int8\"\n                else:\n                    raise\n        else:\n            actual_whisper_device = whisper_device\n            actual_compute_type = \"float16\" if actual_whisper_device == \"cuda\" else \"int8\"\n        \n        whisper_local_available = True\n        print(f\"faster-whisper disponible (modele : {whisper_model}, device : {actual_whisper_device}, compute : {actual_compute_type})\")\n    except ImportError:\n        print(\"faster-whisper non installe - basculement vers l'API\")\n        use_local_whisper = False\n    except Exception as e:\n        print(f\"Erreur initialise Whisper : {e}\")\n        print(f\"Basculement vers l'API OpenAI\")\n        use_local_whisper = False\n\n# Generer des fichiers audio de test via TTS\ntest_audio_files = {}\nif generate_audio and openai_key != \"dummy_key_for_validation\":\n    print(\"\\nGeneration de fichiers audio de test...\")\n    test_texts = {\n        \"presentation\": (\n            \"Bonjour a tous. Bienvenue dans cette presentation sur l'intelligence artificielle. \"\n            \"Aujourd'hui nous allons aborder trois sujets principaux. \"\n            \"Premierement, les fondamentaux du machine learning. \"\n            \"Deuxiemement, les reseaux de neurones. \"\n            \"Et troisiemement, les applications pratiques.\"\n        ),\n        \"reunion\": (\n            \"Merci d'etre presents pour cette reunion d'equipe. \"\n            \"Le premier point a l'ordre du jour concerne l'avancement du projet. \"\n            \"Nous avons termine la phase de conception et nous passons maintenant au developpement. \"\n            \"Le prochain jalon est prevu pour la fin du mois.\"\n        )\n    }\n\n    for name, text in test_texts.items():\n        filepath = OUTPUT_DIR / f\"test_{name}.mp3\"\n        response = client.audio.speech.create(\n            model=\"tts-1\", voice=\"nova\", input=text, response_format=\"mp3\"\n        )\n        with open(filepath, 'wb') as f:\n            f.write(response.content)\n        test_audio_files[name] = filepath\n        print(f\"  {name} : {filepath.name} ({len(response.content)/1024:.1f} KB)\")\n\nprint(f\"\\nConfiguration prete\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-intro",
   "metadata": {},
   "source": [
    "## Section 1 : Transcription avec horodatage\n",
    "\n",
    "Une transcription de qualite necessite plus que le texte brut : les timestamps permettent de situer chaque segment dans le temps.\n",
    "\n",
    "| Methode | Latence | Qualite | Cout | Timestamps |\n",
    "|---------|---------|---------|------|------------|\n",
    "| Whisper API | ~10s/min | Bonne | $0.006/min | Segment-level |\n",
    "| Whisper local (large-v3-turbo) | ~3s/min (GPU) | Excellente | Gratuit | Word-level |\n",
    "| Whisper local (tiny) | ~0.5s/min (GPU) | Correcte | Gratuit | Segment-level |\n",
    "\n",
    "Whisper local offre des timestamps au niveau du mot, essentiels pour les sous-titres precis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-timestamped-transcription",
   "metadata": {},
   "outputs": [],
   "source": "# Transcription avec horodatage\nprint(\"TRANSCRIPTION AVEC HORODATAGE\")\nprint(\"=\" * 50)\n\ntranscription_results = {}\n\nif generate_audio and test_audio_files:\n    test_file = test_audio_files.get(\"presentation\")\n\n    if test_file and test_file.exists():\n        print(f\"Fichier : {test_file.name}\")\n\n        if use_local_whisper and whisper_local_available:\n            # Transcription locale avec timestamps\n            print(f\"\\nMethode : Whisper local ({whisper_model})\")\n            start_time = time.time()\n\n            try:\n                model = WhisperModel(whisper_model, device=actual_whisper_device, compute_type=actual_compute_type)\n                segments, info = model.transcribe(\n                    str(test_file),\n                    language=default_language,\n                    word_timestamps=True\n                )\n                segments_list = list(segments)\n                transcription_time = time.time() - start_time\n\n                print(f\"Langue detectee : {info.language} (prob: {info.language_probability:.2f})\")\n                print(f\"Duree audio : {info.duration:.1f}s\")\n                print(f\"Temps de transcription : {transcription_time:.1f}s\")\n                print(f\"Ratio temps reel : {info.duration/transcription_time:.1f}x\")\n\n                print(f\"\\nSegments avec timestamps :\")\n                full_text = \"\"\n                for seg in segments_list:\n                    print(f\"  [{seg.start:6.1f}s - {seg.end:6.1f}s] {seg.text.strip()}\")\n                    full_text += seg.text.strip() + \" \"\n\n                transcription_results[\"local\"] = {\n                    \"text\": full_text.strip(),\n                    \"segments\": segments_list,\n                    \"time\": transcription_time,\n                    \"duration\": info.duration\n                }\n            except Exception as e:\n                print(f\"Erreur transcription locale : {e}\")\n                print(f\"Basculement vers l'API OpenAI\")\n                use_local_whisper = False\n\n        if not use_local_whisper or not whisper_local_available or \"local\" not in transcription_results:\n            # Transcription via API\n            print(f\"\\nMethode : API OpenAI ({whisper_api_model})\")\n            start_time = time.time()\n\n            with open(test_file, 'rb') as f:\n                transcript = client.audio.transcriptions.create(\n                    model=whisper_api_model,\n                    file=f,\n                    language=default_language,\n                    response_format=\"verbose_json\",\n                    timestamp_granularities=[\"segment\"]\n                )\n            transcription_time = time.time() - start_time\n\n            print(f\"Temps de transcription : {transcription_time:.1f}s\")\n            print(f\"Texte : {transcript.text}\")\n\n            if hasattr(transcript, 'segments') and transcript.segments:\n                print(f\"\\nSegments avec timestamps :\")\n                for seg in transcript.segments:\n                    # TranscriptionSegment est un objet avec des attributs, pas subscriptable\n                    start = getattr(seg, 'start', seg.get('start') if hasattr(seg, 'get') else 0)\n                    end = getattr(seg, 'end', seg.get('end') if hasattr(seg, 'get') else 0)\n                    text = getattr(seg, 'text', seg.get('text') if hasattr(seg, 'get') else '')\n                    print(f\"  [{start:6.1f}s - {end:6.1f}s] {text.strip()}\")\n\n            transcription_results[\"api\"] = {\n                \"text\": transcript.text,\n                \"time\": transcription_time\n            }\n\n        # Ecouter le fichier original\n        print(f\"\\nEcoute du fichier original :\")\n        display(Audio(filename=str(test_file)))\nelse:\n    print(\"Transcription desactivee (pas de fichiers de test)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Transcription avec horodatage\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Ratio temps reel | >10x typique (GPU) | Transcription bien plus rapide que la duree audio |\n",
    "| Precision timestamps | ~0.1s segments | Suffisant pour le sous-titrage standard |\n",
    "| Detection langue | >95% probabilite | Whisper detecte fiablement le francais |\n",
    "\n",
    "**Points cles** :\n",
    "1. Les word-level timestamps ne sont disponibles qu'en mode local\n",
    "2. L'API est plus simple mais offre moins de controle\n",
    "3. Le ratio temps reel depend fortement du GPU et de la taille du modele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-intro",
   "metadata": {},
   "source": [
    "## Section 2 : Transcription batch\n",
    "\n",
    "En production, il est courant de transcrire de nombreux fichiers. Un pipeline batch doit gerer :\n",
    "\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| Decouverte | Lister les fichiers audio dans un repertoire |\n",
    "| Filtrage | Verifier les formats supportes |\n",
    "| Progression | Afficher l'avancement et les estimations de temps |\n",
    "| Erreurs | Gerer les echecs sans bloquer le pipeline |\n",
    "| Sortie | Sauvegarder les resultats de maniere structuree |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-batch-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de transcription batch\n",
    "print(\"TRANSCRIPTION BATCH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "SUPPORTED_FORMATS = {'.mp3', '.wav', '.flac', '.ogg', '.m4a', '.webm'}\n",
    "\n",
    "def batch_transcribe(input_dir: Path, output_dir: Path,\n",
    "                     language: str = \"fr\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Transcrit tous les fichiers audio d'un repertoire.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Decouverte des fichiers\n",
    "    audio_files = [\n",
    "        f for f in sorted(input_dir.iterdir())\n",
    "        if f.suffix.lower() in SUPPORTED_FORMATS\n",
    "    ]\n",
    "\n",
    "    if not audio_files:\n",
    "        print(\"Aucun fichier audio trouve\")\n",
    "        return results\n",
    "\n",
    "    print(f\"Fichiers trouves : {len(audio_files)}\")\n",
    "    total_start = time.time()\n",
    "\n",
    "    for i, audio_file in enumerate(audio_files):\n",
    "        print(f\"\\n[{i+1}/{len(audio_files)}] {audio_file.name}\")\n",
    "        file_start = time.time()\n",
    "\n",
    "        try:\n",
    "            if use_local_whisper and whisper_local_available:\n",
    "                model = WhisperModel(whisper_model, device=actual_whisper_device, compute_type=actual_compute_type)\n",
    "                segments, info = model.transcribe(str(audio_file), language=language)\n",
    "                segments_list = list(segments)\n",
    "                text = \" \".join([s.text.strip() for s in segments_list])\n",
    "                duration = info.duration\n",
    "            else:\n",
    "                with open(audio_file, 'rb') as f:\n",
    "                    transcript = client.audio.transcriptions.create(\n",
    "                        model=whisper_api_model, file=f, language=language\n",
    "                    )\n",
    "                text = transcript.text\n",
    "                duration = 0  # API ne retourne pas la duree en mode simple\n",
    "\n",
    "            file_time = time.time() - file_start\n",
    "\n",
    "            result = {\n",
    "                \"file\": audio_file.name,\n",
    "                \"text\": text,\n",
    "                \"duration\": duration,\n",
    "                \"transcription_time\": file_time,\n",
    "                \"word_count\": len(text.split()),\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # Sauvegarder la transcription\n",
    "            if save_output_files:\n",
    "                txt_path = output_dir / f\"{audio_file.stem}.txt\"\n",
    "                txt_path.write_text(text, encoding='utf-8')\n",
    "\n",
    "            print(f\"  Duree : {duration:.1f}s | Temps : {file_time:.1f}s | Mots : {len(text.split())}\")\n",
    "            print(f\"  Texte : {text[:80]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"file\": audio_file.name,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)[:100]\n",
    "            })\n",
    "            print(f\"  ERREUR : {str(e)[:80]}\")\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    success = sum(1 for r in results if r['status'] == 'success')\n",
    "    print(f\"\\nBatch termine : {success}/{len(audio_files)} reussis en {total_time:.1f}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Executer le batch\n",
    "if generate_audio and test_audio_files:\n",
    "    batch_results = batch_transcribe(OUTPUT_DIR, OUTPUT_DIR, language=default_language)\n",
    "\n",
    "    # Recapitulatif\n",
    "    if batch_results:\n",
    "        print(f\"\\nRecapitulatif batch :\")\n",
    "        print(f\"{'Fichier':<30} {'Statut':<10} {'Mots':<8} {'Temps (s)':<10}\")\n",
    "        print(\"-\" * 58)\n",
    "        for r in batch_results:\n",
    "            if r['status'] == 'success':\n",
    "                print(f\"{r['file']:<30} {r['status']:<10} {r['word_count']:<8} {r['transcription_time']:<10.1f}\")\n",
    "            else:\n",
    "                print(f\"{r['file']:<30} {r['status']:<10} {'N/A':<8} {'N/A':<10}\")\n",
    "else:\n",
    "    batch_results = []\n",
    "    print(\"Batch desactivee (pas de fichiers de test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Transcription batch\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Debit batch | Variable selon GPU | L'initialisation du modele est le goulot d'etranglement |\n",
    "| Gestion erreurs | Try/except par fichier | Un echec ne bloque pas le pipeline |\n",
    "| Sauvegarde | .txt par fichier | Format simple et universel |\n",
    "\n",
    "> **Note technique** : En production, il est preferable de charger le modele Whisper une seule fois et de transcrire tous les fichiers avec la meme instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-intro",
   "metadata": {},
   "source": [
    "## Section 3 : Diarisation de locuteurs\n",
    "\n",
    "La diarisation identifie \"qui parle quand\" dans un enregistrement. C'est essentiel pour les reunions et interviews.\n",
    "\n",
    "| Approche | Complexite | Precision | Prerequis |\n",
    "|----------|-----------|-----------|----------|\n",
    "| pyannote.audio | Elevee | Excellente | GPU, HuggingFace token |\n",
    "| Energie simple | Faible | Basique | Aucun |\n",
    "| Whisper segments | Moyenne | Correcte | Silences entre locuteurs |\n",
    "\n",
    "Nous implementons ici une approche simplifiee basee sur l'analyse d'energie pour illustrer le concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-diarization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diarisation simplifiee basee sur l'energie\n",
    "print(\"DIARISATION DE LOCUTEURS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def simple_energy_diarization(audio_path: str, window_ms: int = 500,\n",
    "                               silence_threshold: float = 0.02) -> List[Dict]:\n",
    "    \"\"\"Diarisation simplifiee basee sur l'energie du signal.\n",
    "\n",
    "    Detecte les segments de parole et les silences pour\n",
    "    estimer les tours de parole.\n",
    "    \"\"\"\n",
    "    from pydub import AudioSegment\n",
    "\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    samples = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
    "    samples = samples / np.max(np.abs(samples))  # Normalisation\n",
    "\n",
    "    sr = audio.frame_rate\n",
    "    window_samples = int(sr * window_ms / 1000)\n",
    "\n",
    "    # Calculer l'energie par fenetre\n",
    "    segments = []\n",
    "    is_speaking = False\n",
    "    speaker_id = 0\n",
    "    segment_start = 0\n",
    "\n",
    "    for i in range(0, len(samples) - window_samples, window_samples):\n",
    "        window = samples[i:i + window_samples]\n",
    "        energy = np.sqrt(np.mean(window ** 2))  # RMS energy\n",
    "\n",
    "        if energy > silence_threshold:\n",
    "            if not is_speaking:\n",
    "                segment_start = i / sr\n",
    "                is_speaking = True\n",
    "        else:\n",
    "            if is_speaking:\n",
    "                segment_end = i / sr\n",
    "                # Nouveau segment de parole\n",
    "                if segment_end - segment_start > 0.3:  # Min 300ms\n",
    "                    segments.append({\n",
    "                        \"speaker\": f\"Speaker_{speaker_id % 2}\",\n",
    "                        \"start\": segment_start,\n",
    "                        \"end\": segment_end,\n",
    "                        \"duration\": segment_end - segment_start\n",
    "                    })\n",
    "                    speaker_id += 1\n",
    "                is_speaking = False\n",
    "\n",
    "    return segments\n",
    "\n",
    "if generate_audio and test_audio_files:\n",
    "    test_file = test_audio_files.get(\"reunion\")\n",
    "\n",
    "    if test_file and test_file.exists():\n",
    "        print(f\"Fichier : {test_file.name}\")\n",
    "        diarization_segments = simple_energy_diarization(str(test_file))\n",
    "\n",
    "        print(f\"\\nSegments detectes : {len(diarization_segments)}\")\n",
    "        print(f\"{'Locuteur':<15} {'Debut (s)':<12} {'Fin (s)':<12} {'Duree (s)':<10}\")\n",
    "        print(\"-\" * 49)\n",
    "        for seg in diarization_segments:\n",
    "            print(f\"{seg['speaker']:<15} {seg['start']:<12.1f} {seg['end']:<12.1f} {seg['duration']:<10.1f}\")\n",
    "\n",
    "        # Statistiques par locuteur\n",
    "        speakers = set(s['speaker'] for s in diarization_segments)\n",
    "        print(f\"\\nStatistiques par locuteur :\")\n",
    "        for speaker in sorted(speakers):\n",
    "            speaker_segs = [s for s in diarization_segments if s['speaker'] == speaker]\n",
    "            total_dur = sum(s['duration'] for s in speaker_segs)\n",
    "            print(f\"  {speaker} : {len(speaker_segs)} segments, {total_dur:.1f}s total\")\n",
    "\n",
    "        print(f\"\\n(Diarisation simplifiee - en production, utiliser pyannote.audio)\")\n",
    "    else:\n",
    "        print(\"Fichier de test non disponible\")\n",
    "        diarization_segments = []\n",
    "else:\n",
    "    diarization_segments = []\n",
    "    print(\"Diarisation desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-intro",
   "metadata": {},
   "source": [
    "## Section 4 : Generation de sous-titres SRT/VTT\n",
    "\n",
    "Les formats de sous-titres standard permettent l'affichage synchronise avec la video :\n",
    "\n",
    "| Format | Extension | Utilisation | Specifites |\n",
    "|--------|-----------|-------------|------------|\n",
    "| SRT (SubRip) | .srt | Universel | Simple, largement supporte |\n",
    "| VTT (WebVTT) | .vtt | Web/HTML5 | Style CSS, regions, alignement |\n",
    "\n",
    "### Format SRT\n",
    "```\n",
    "1\n",
    "00:00:01,000 --> 00:00:04,500\n",
    "Bienvenue dans cette presentation\n",
    "```\n",
    "\n",
    "### Format VTT\n",
    "```\n",
    "WEBVTT\n",
    "\n",
    "00:00:01.000 --> 00:00:04.500\n",
    "Bienvenue dans cette presentation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-subtitle-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation de sous-titres SRT et VTT\n",
    "print(\"GENERATION DE SOUS-TITRES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def format_timestamp_srt(seconds: float) -> str:\n",
    "    \"\"\"Formate un timestamp en format SRT (HH:MM:SS,mmm).\"\"\"\n",
    "    td = timedelta(seconds=seconds)\n",
    "    hours = int(td.total_seconds() // 3600)\n",
    "    minutes = int((td.total_seconds() % 3600) // 60)\n",
    "    secs = int(td.total_seconds() % 60)\n",
    "    ms = int((seconds % 1) * 1000)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d},{ms:03d}\"\n",
    "\n",
    "def format_timestamp_vtt(seconds: float) -> str:\n",
    "    \"\"\"Formate un timestamp en format VTT (HH:MM:SS.mmm).\"\"\"\n",
    "    td = timedelta(seconds=seconds)\n",
    "    hours = int(td.total_seconds() // 3600)\n",
    "    minutes = int((td.total_seconds() % 3600) // 60)\n",
    "    secs = int(td.total_seconds() % 60)\n",
    "    ms = int((seconds % 1) * 1000)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}.{ms:03d}\"\n",
    "\n",
    "def transcription_to_srt(segments: List[Dict], max_chars: int = 60) -> str:\n",
    "    \"\"\"Convertit des segments transcrits en format SRT.\"\"\"\n",
    "    srt_lines = []\n",
    "    for i, seg in enumerate(segments, 1):\n",
    "        start_ts = format_timestamp_srt(seg['start'])\n",
    "        end_ts = format_timestamp_srt(seg['end'])\n",
    "        text = seg.get('text', '').strip()\n",
    "        # Couper les lignes longues\n",
    "        if len(text) > max_chars:\n",
    "            mid = len(text) // 2\n",
    "            space_pos = text.rfind(' ', 0, mid)\n",
    "            if space_pos > 0:\n",
    "                text = text[:space_pos] + '\\n' + text[space_pos+1:]\n",
    "        srt_lines.append(f\"{i}\\n{start_ts} --> {end_ts}\\n{text}\\n\")\n",
    "    return '\\n'.join(srt_lines)\n",
    "\n",
    "def transcription_to_vtt(segments: List[Dict]) -> str:\n",
    "    \"\"\"Convertit des segments transcrits en format WebVTT.\"\"\"\n",
    "    vtt_lines = [\"WEBVTT\\n\"]\n",
    "    for seg in segments:\n",
    "        start_ts = format_timestamp_vtt(seg['start'])\n",
    "        end_ts = format_timestamp_vtt(seg['end'])\n",
    "        text = seg.get('text', '').strip()\n",
    "        vtt_lines.append(f\"{start_ts} --> {end_ts}\\n{text}\\n\")\n",
    "    return '\\n'.join(vtt_lines)\n",
    "\n",
    "# Generer les sous-titres a partir de la transcription\n",
    "if generate_audio and test_audio_files:\n",
    "    test_file = test_audio_files.get(\"presentation\")\n",
    "\n",
    "    if test_file and test_file.exists():\n",
    "        # Transcrire avec timestamps\n",
    "        print(f\"Transcription de {test_file.name} pour sous-titres...\")\n",
    "\n",
    "        if use_local_whisper and whisper_local_available:\n",
    "            model = WhisperModel(whisper_model, device=actual_whisper_device, compute_type=actual_compute_type)\n",
    "            segments, info = model.transcribe(str(test_file), language=default_language)\n",
    "            sub_segments = [{\"start\": s.start, \"end\": s.end, \"text\": s.text.strip()} for s in segments]\n",
    "        else:\n",
    "            with open(test_file, 'rb') as f:\n",
    "                transcript = client.audio.transcriptions.create(\n",
    "                    model=whisper_api_model, file=f, language=default_language,\n",
    "                    response_format=\"verbose_json\", timestamp_granularities=[\"segment\"]\n",
    "                )\n",
    "            sub_segments = [{\"start\": getattr(s, 'start', 0), \"end\": getattr(s, 'end', 0), \"text\": getattr(s, 'text', '').strip()}\n",
    "                           for s in (transcript.segments or [])]\n",
    "\n",
    "        if sub_segments:\n",
    "            # Generer SRT\n",
    "            srt_content = transcription_to_srt(sub_segments)\n",
    "            print(f\"\\n--- Format SRT ---\")\n",
    "            print(srt_content[:500])\n",
    "\n",
    "            # Generer VTT\n",
    "            vtt_content = transcription_to_vtt(sub_segments)\n",
    "            print(f\"--- Format VTT ---\")\n",
    "            print(vtt_content[:500])\n",
    "\n",
    "            # Sauvegarder\n",
    "            if save_output_files:\n",
    "                srt_path = OUTPUT_DIR / \"presentation.srt\"\n",
    "                srt_path.write_text(srt_content, encoding='utf-8')\n",
    "                vtt_path = OUTPUT_DIR / \"presentation.vtt\"\n",
    "                vtt_path.write_text(vtt_content, encoding='utf-8')\n",
    "                print(f\"\\nSauvegarde : {srt_path.name}, {vtt_path.name}\")\n",
    "        else:\n",
    "            print(\"Aucun segment avec timestamps disponible\")\n",
    "else:\n",
    "    print(\"Generation de sous-titres desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Generation de sous-titres\n",
    "\n",
    "| Aspect | SRT | VTT |\n",
    "|--------|-----|-----|\n",
    "| Compatibilite | VLC, MPC, YouTube | Navigateurs web, HTML5 |\n",
    "| Style | Aucun | CSS (couleur, position) |\n",
    "| Separateur ms | Virgule (,) | Point (.) |\n",
    "| Index | Obligatoire (1, 2, 3...) | Optionnel |\n",
    "\n",
    "> **Note technique** : Pour des sous-titres de qualite broadcast, limiter chaque ligne a 42 caracteres et chaque bloc a 2 lignes maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-intro",
   "metadata": {},
   "source": [
    "## Section 5 : Resume automatique de reunions\n",
    "\n",
    "Le pipeline complet pour resumer une reunion :\n",
    "\n",
    "```\n",
    "Audio reunion -> Whisper (transcription) -> GPT (resume) -> Document structure\n",
    "```\n",
    "\n",
    "Le LLM recoit la transcription brute et produit un resume structure avec points cles, decisions et actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-meeting-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume automatique de reunions\n",
    "print(\"RESUME AUTOMATIQUE DE REUNION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def summarize_meeting(transcript_text: str, llm_client, model: str) -> str:\n",
    "    \"\"\"Resume une transcription de reunion via LLM.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Analyse cette transcription de reunion et produis un resume structure :\n",
    "\n",
    "1. RESUME (2-3 phrases)\n",
    "2. POINTS CLES (liste a puces)\n",
    "3. DECISIONS PRISES (liste numerotee)\n",
    "4. ACTIONS A SUIVRE (tableau : Action | Responsable | Echeance)\n",
    "\n",
    "Transcription :\n",
    "{transcript_text}\n",
    "\"\"\"\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "if generate_audio and test_audio_files:\n",
    "    test_file = test_audio_files.get(\"reunion\")\n",
    "\n",
    "    if test_file and test_file.exists():\n",
    "        # Transcrire la reunion\n",
    "        print(f\"Etape 1 : Transcription de {test_file.name}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        if use_local_whisper and whisper_local_available:\n",
    "            model = WhisperModel(whisper_model, device=actual_whisper_device, compute_type=actual_compute_type)\n",
    "            segments, info = model.transcribe(str(test_file), language=default_language)\n",
    "            meeting_text = \" \".join([s.text.strip() for s in segments])\n",
    "        else:\n",
    "            with open(test_file, 'rb') as f:\n",
    "                transcript = client.audio.transcriptions.create(\n",
    "                    model=whisper_api_model, file=f, language=default_language\n",
    "                )\n",
    "            meeting_text = transcript.text\n",
    "\n",
    "        transcription_time = time.time() - start_time\n",
    "        print(f\"Transcription : {len(meeting_text)} chars en {transcription_time:.1f}s\")\n",
    "        print(f\"Texte : {meeting_text[:150]}...\")\n",
    "\n",
    "        # Resumer via LLM\n",
    "        print(f\"\\nEtape 2 : Resume via {llm_model}\")\n",
    "        start_time = time.time()\n",
    "        summary = summarize_meeting(meeting_text, client, llm_model)\n",
    "        summary_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Resume genere en {summary_time:.1f}s\")\n",
    "        print(f\"\\n--- Resume de la reunion ---\")\n",
    "        print(summary)\n",
    "        print(f\"--- Fin du resume ---\")\n",
    "\n",
    "        # Sauvegarder\n",
    "        if save_output_files:\n",
    "            summary_path = OUTPUT_DIR / \"reunion_resume.md\"\n",
    "            summary_path.write_text(\n",
    "                f\"# Resume de reunion\\n\\n\"\n",
    "                f\"Date : {datetime.now().strftime('%Y-%m-%d')}\\n\\n\"\n",
    "                f\"{summary}\\n\\n\"\n",
    "                f\"---\\n\\n\"\n",
    "                f\"## Transcription complete\\n\\n{meeting_text}\",\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "            print(f\"\\nSauvegarde : {summary_path.name}\")\n",
    "else:\n",
    "    print(\"Resume desactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Resume de reunion\n",
    "\n",
    "| Etape | Temps typique | Cout (API) |\n",
    "|-------|--------------|------------|\n",
    "| Transcription (1h audio) | 30-60s (GPU local) | $0.36 (API) |\n",
    "| Resume LLM | 3-5s | ~$0.01 (GPT-4o-mini) |\n",
    "| Total pipeline | < 2 min | < $0.40 |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le pipeline complet est realiste pour un usage quotidien\n",
    "2. Le cout reste tres faible meme pour de longues reunions\n",
    "3. La qualite du resume depend directement de la qualite de la transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif - Transcrire votre propre fichier audio\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"MODE INTERACTIF - TRANSCRIPTION PERSONNALISEE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nEntrez le chemin d'un fichier audio a transcrire :\")\n",
    "    print(\"(Laissez vide pour passer)\")\n",
    "\n",
    "    try:\n",
    "        user_path = input(\"\\nChemin du fichier : \")\n",
    "\n",
    "        if user_path.strip():\n",
    "            audio_path = Path(user_path.strip())\n",
    "            if audio_path.exists():\n",
    "                print(f\"\\nTranscription de {audio_path.name}...\")\n",
    "\n",
    "                if use_local_whisper and whisper_local_available:\n",
    "                    model = WhisperModel(whisper_model, device=actual_whisper_device, compute_type=actual_compute_type)\n",
    "                    segments, info = model.transcribe(str(audio_path), language=default_language)\n",
    "                    text = \" \".join([s.text.strip() for s in segments])\n",
    "                else:\n",
    "                    with open(audio_path, 'rb') as f:\n",
    "                        transcript = client.audio.transcriptions.create(\n",
    "                            model=whisper_api_model, file=f, language=default_language\n",
    "                        )\n",
    "                    text = transcript.text\n",
    "\n",
    "                print(f\"\\nTranscription :\\n{text}\")\n",
    "                print(f\"\\nMots : {len(text.split())}\")\n",
    "            else:\n",
    "                print(f\"Fichier non trouve : {audio_path}\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type} - {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session\n",
    "print(\"STATISTIQUES DE SESSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Whisper : {'local (' + whisper_model + ')' if use_local_whisper else 'API'}\")\n",
    "print(f\"LLM resume : {llm_model}\")\n",
    "\n",
    "if transcription_results:\n",
    "    for method, data in transcription_results.items():\n",
    "        print(f\"Transcription ({method}) : {data['time']:.1f}s\")\n",
    "\n",
    "if batch_results:\n",
    "    success = sum(1 for r in batch_results if r['status'] == 'success')\n",
    "    print(f\"Batch : {success}/{len(batch_results)} fichiers transcrits\")\n",
    "\n",
    "if save_output_files:\n",
    "    saved_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"Fichiers sauvegardes : {len(saved_files)} dans {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nPROCHAINES ETAPES\")\n",
    "print(f\"1. Explorer la composition musicale (04-3-Music-Composition-Workflow)\")\n",
    "print(f\"2. Synchroniser audio et video (04-4-Audio-Video-Sync)\")\n",
    "\n",
    "print(f\"\\nNotebook termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}