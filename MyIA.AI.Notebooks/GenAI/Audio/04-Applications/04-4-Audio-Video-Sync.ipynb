{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# Synchronisation Audio-Video (Passerelle)\n",
    "\n",
    "**Module :** 04-Audio-Applications  \n",
    "**Niveau :** Applications  \n",
    "**Technologies :** OpenAI TTS, moviepy, pydub, ffmpeg-python  \n",
    "**VRAM estimee :** ~10 GB  \n",
    "**Duree estimee :** 55 minutes  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Extraire la piste audio d'une video\n",
    "- [ ] Generer une narration TTS synchronisee avec les segments video\n",
    "- [ ] Aligner l'audio avec la timeline video\n",
    "- [ ] Superposer musique de fond et narration\n",
    "- [ ] Assembler video + narration + musique + sous-titres\n",
    "- [ ] Comprendre le lien entre les series Audio et Video\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Notebooks Foundation (01-1 TTS, 01-3 Audio Operations) completes\n",
    "- Notebook 04-2 (Pipeline de Transcription) pour les sous-titres\n",
    "- Cle API OpenAI configuree (`OPENAI_API_KEY` dans `.env`)\n",
    "- moviepy, pydub, ffmpeg installes\n",
    "\n",
    "> **Note** : Ce notebook est la **passerelle** entre la serie Audio et la serie Video. Il utilise des techniques audio pour enrichir du contenu video. La serie Video (`Video/01-Foundation/`) approfondit la generation et l'edition video.\n",
    "\n",
    "**Navigation :** [Index](../README.md) | [<< Precedent](04-3-Music-Composition-Workflow.ipynb) | [Serie Video >>](../../Video/01-Foundation/01-1-Video-Operations-Basics.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-params",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres TTS narration\n",
    "tts_model = \"tts-1\"               # \"tts-1\" ou \"tts-1-hd\"\n",
    "narrator_voice = \"nova\"            # Voix de narration\n",
    "\n",
    "# Parametres video\n",
    "video_resolution = (640, 480)      # Resolution de la video de test\n",
    "video_fps = 24                     # Images par seconde\n",
    "video_duration = 15                # Duree de la video de test (secondes)\n",
    "\n",
    "# Configuration pipeline\n",
    "generate_audio = True\n",
    "save_output_files = True\n",
    "background_music_volume = -20      # Volume musique de fond en dB (relatif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from io import BytesIO\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display, HTML, Video as IPVideo\n",
    "\n",
    "# Resolution GENAI_ROOT\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.audio_helpers import (\n",
    "            synthesize_openai, play_audio_bytes,\n",
    "            estimate_audio_duration, get_audio_info\n",
    "        )\n",
    "        print(\"Helpers audio importes\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Helpers audio non disponibles : {e}\")\n",
    "\n",
    "# Repertoires de sortie\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'av_sync'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('av_sync')\n",
    "\n",
    "print(f\"Synchronisation Audio-Video\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}, TTS : {tts_model}\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la configuration et validation\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not openai_key:\n",
    "    if notebook_mode == \"batch\" and not generate_audio:\n",
    "        openai_key = \"dummy_key_for_validation\"\n",
    "        print(\"Mode batch sans generation : cle API ignoree\")\n",
    "    else:\n",
    "        raise ValueError(\"OPENAI_API_KEY manquante dans .env\")\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_key)\n",
    "\n",
    "# Verification des dependances video\n",
    "moviepy_available = False\n",
    "try:\n",
    "    from moviepy.editor import (\n",
    "        VideoFileClip, AudioFileClip, CompositeAudioClip,\n",
    "        TextClip, CompositeVideoClip, ColorClip, concatenate_videoclips\n",
    "    )\n",
    "    moviepy_available = True\n",
    "    print(\"moviepy disponible\")\n",
    "except ImportError:\n",
    "    print(\"moviepy non installe - pip install moviepy\")\n",
    "\n",
    "from pydub import AudioSegment\n",
    "print(f\"pydub disponible\")\n",
    "\n",
    "print(f\"\\nConfiguration prete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-intro",
   "metadata": {},
   "source": [
    "## Section 1 : Creation d'une video de test et extraction audio\n",
    "\n",
    "Avant de travailler sur la synchronisation, nous creons une video de test simple et montrons comment extraire sa piste audio.\n",
    "\n",
    "| Operation | Outil | Entree | Sortie |\n",
    "|-----------|-------|--------|--------|\n",
    "| Creation video | moviepy | Parametres | Fichier .mp4 |\n",
    "| Extraction audio | moviepy / ffmpeg | Video .mp4 | Audio .wav/.mp3 |\n",
    "| Analyse audio | pydub | Audio | Metadonnees |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-video-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation d'une video de test et extraction audio\n",
    "print(\"CREATION VIDEO DE TEST ET EXTRACTION AUDIO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_video_path = OUTPUT_DIR / \"test_video.mp4\"\n",
    "extracted_audio_path = OUTPUT_DIR / \"extracted_audio.wav\"\n",
    "\n",
    "if generate_audio and moviepy_available:\n",
    "    # Creer une video de test avec des scenes colorees\n",
    "    print(\"Creation de la video de test...\")\n",
    "\n",
    "    # Definir les segments de la video\n",
    "    video_segments = [\n",
    "        {\"label\": \"Introduction\", \"color\": (30, 60, 120), \"duration\": 5},\n",
    "        {\"label\": \"Concepts cles\", \"color\": (60, 120, 30), \"duration\": 5},\n",
    "        {\"label\": \"Conclusion\", \"color\": (120, 30, 60), \"duration\": 5}\n",
    "    ]\n",
    "\n",
    "    clips = []\n",
    "    for seg in video_segments:\n",
    "        # Clip de couleur unie\n",
    "        color_clip = ColorClip(\n",
    "            size=video_resolution,\n",
    "            color=seg['color'],\n",
    "            duration=seg['duration']\n",
    "        )\n",
    "\n",
    "        # Ajouter un titre\n",
    "        try:\n",
    "            txt_clip = TextClip(\n",
    "                seg['label'],\n",
    "                fontsize=40,\n",
    "                color='white',\n",
    "                font='Arial'\n",
    "            ).set_position('center').set_duration(seg['duration'])\n",
    "            clip = CompositeVideoClip([color_clip, txt_clip])\n",
    "        except Exception:\n",
    "            # Si TextClip echoue (ImageMagick non disponible), utiliser le clip brut\n",
    "            clip = color_clip\n",
    "\n",
    "        clips.append(clip)\n",
    "\n",
    "    # Concatener les clips\n",
    "    final_video = concatenate_videoclips(clips)\n",
    "    final_video = final_video.set_fps(video_fps)\n",
    "\n",
    "    # Sauvegarder la video\n",
    "    final_video.write_videofile(\n",
    "        str(test_video_path),\n",
    "        fps=video_fps,\n",
    "        codec='libx264',\n",
    "        audio=False,\n",
    "        logger=None\n",
    "    )\n",
    "    final_video.close()\n",
    "\n",
    "    print(f\"Video creee : {test_video_path.name}\")\n",
    "    print(f\"Resolution : {video_resolution[0]}x{video_resolution[1]}\")\n",
    "    print(f\"Duree : {video_duration}s | FPS : {video_fps}\")\n",
    "    print(f\"Segments : {len(video_segments)}\")\n",
    "\n",
    "    for i, seg in enumerate(video_segments):\n",
    "        start = sum(s['duration'] for s in video_segments[:i])\n",
    "        end = start + seg['duration']\n",
    "        print(f\"  [{start:2.0f}s - {end:2.0f}s] {seg['label']}\")\n",
    "\n",
    "    # Demonstration de l'extraction audio\n",
    "    print(f\"\\n(La video de test n'a pas de piste audio - nous en ajouterons une)\")\n",
    "\n",
    "elif not moviepy_available:\n",
    "    print(\"moviepy non disponible - pip install moviepy\")\n",
    "else:\n",
    "    print(\"Generation desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Video de test\n",
    "\n",
    "| Segment | Debut | Fin | Contenu |\n",
    "|---------|-------|-----|---------|\n",
    "| Introduction | 0s | 5s | Fond bleu |\n",
    "| Concepts cles | 5s | 10s | Fond vert |\n",
    "| Conclusion | 10s | 15s | Fond rouge |\n",
    "\n",
    "**Points cles** :\n",
    "1. moviepy permet de creer et manipuler des videos en Python\n",
    "2. L'extraction audio utilise la meme API (`video.audio`)\n",
    "3. La video de test servira de base pour la synchronisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-intro",
   "metadata": {},
   "source": [
    "## Section 2 : Narration TTS synchronisee\n",
    "\n",
    "Pour chaque segment video, nous generons une narration TTS adaptee. Le texte est calibre pour correspondre a la duree du segment.\n",
    "\n",
    "| Segment | Duree cible | Texte adapte | Vitesse TTS |\n",
    "|---------|------------|--------------|-------------|\n",
    "| Introduction | 5s | Court, accueillant | 1.0 |\n",
    "| Concepts cles | 5s | Dense, informatif | 0.95 |\n",
    "| Conclusion | 5s | Recapitulatif | 1.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-tts-narration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation de narration TTS synchronisee aux segments video\n",
    "print(\"NARRATION TTS SYNCHRONISEE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Scripts de narration pour chaque segment video\n",
    "narration_scripts = [\n",
    "    {\n",
    "        \"segment\": \"Introduction\",\n",
    "        \"start\": 0, \"end\": 5,\n",
    "        \"text\": \"Bienvenue dans cette presentation sur l'intelligence artificielle generative. Nous allons decouvrir ensemble les concepts fondamentaux.\",\n",
    "        \"speed\": 1.0\n",
    "    },\n",
    "    {\n",
    "        \"segment\": \"Concepts cles\",\n",
    "        \"start\": 5, \"end\": 10,\n",
    "        \"text\": \"Les modeles generatifs apprennent a creer du contenu nouveau. Ils analysent des exemples existants pour en deduire des patterns.\",\n",
    "        \"speed\": 0.95\n",
    "    },\n",
    "    {\n",
    "        \"segment\": \"Conclusion\",\n",
    "        \"start\": 10, \"end\": 15,\n",
    "        \"text\": \"En resume, l'IA generative ouvre de nouvelles possibilites creatives. Explorez les notebooks suivants pour aller plus loin.\",\n",
    "        \"speed\": 1.0\n",
    "    }\n",
    "]\n",
    "\n",
    "narration_audio_segments = []\n",
    "\n",
    "if generate_audio and openai_key != \"dummy_key_for_validation\":\n",
    "    print(f\"Generation de la narration pour {len(narration_scripts)} segments :\")\n",
    "\n",
    "    for script in narration_scripts:\n",
    "        print(f\"\\n--- {script['segment']} [{script['start']}s - {script['end']}s] ---\")\n",
    "        print(f\"Texte : {script['text'][:70]}...\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = client.audio.speech.create(\n",
    "            model=tts_model,\n",
    "            voice=narrator_voice,\n",
    "            input=script['text'],\n",
    "            response_format=\"mp3\",\n",
    "            speed=script['speed']\n",
    "        )\n",
    "        audio_bytes = response.content\n",
    "        gen_time = time.time() - start_time\n",
    "\n",
    "        # Charger avec pydub pour connaitre la duree\n",
    "        audio_seg = AudioSegment.from_mp3(BytesIO(audio_bytes))\n",
    "        audio_duration = len(audio_seg) / 1000  # ms -> s\n",
    "\n",
    "        segment_duration = script['end'] - script['start']\n",
    "\n",
    "        narration_audio_segments.append({\n",
    "            \"segment\": script['segment'],\n",
    "            \"start\": script['start'],\n",
    "            \"end\": script['end'],\n",
    "            \"audio_bytes\": audio_bytes,\n",
    "            \"audio_segment\": audio_seg,\n",
    "            \"audio_duration\": audio_duration,\n",
    "            \"target_duration\": segment_duration\n",
    "        })\n",
    "\n",
    "        fit_status = \"OK\" if audio_duration <= segment_duration else \"DEPASSE\"\n",
    "        print(f\"  Duree audio : {audio_duration:.1f}s / cible {segment_duration}s [{fit_status}]\")\n",
    "        print(f\"  Taille : {len(audio_bytes)/1024:.1f} KB | Generation : {gen_time:.1f}s\")\n",
    "        display(Audio(data=audio_bytes, autoplay=False))\n",
    "\n",
    "    # Recapitulatif\n",
    "    print(f\"\\nRecapitulatif synchronisation :\")\n",
    "    print(f\"{'Segment':<18} {'Audio (s)':<12} {'Cible (s)':<12} {'Statut':<10}\")\n",
    "    print(\"-\" * 52)\n",
    "    for seg in narration_audio_segments:\n",
    "        status = \"OK\" if seg['audio_duration'] <= seg['target_duration'] else \"A ajuster\"\n",
    "        print(f\"{seg['segment']:<18} {seg['audio_duration']:<12.1f} {seg['target_duration']:<12.0f} {status:<10}\")\n",
    "else:\n",
    "    print(\"Generation desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Narration synchronisee\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Synchronisation | Audio <= duree segment | La narration doit tenir dans le segment |\n",
    "| Depassement | Possible | Ajuster vitesse ou couper le texte |\n",
    "| Silence residuel | Normal | Comble par la musique de fond |\n",
    "\n",
    "> **Note technique** : Si la narration depasse la duree du segment, deux solutions : augmenter la vitesse TTS (max 4.0x) ou raccourcir le texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-intro",
   "metadata": {},
   "source": [
    "## Section 3 : Alignement audio sur la timeline video\n",
    "\n",
    "L'alignement consiste a placer chaque segment audio au bon moment sur la timeline de la video.\n",
    "\n",
    "```\n",
    "Video:  [====Introduction====][==Concepts cles==][====Conclusion====]\n",
    "Audio:  [narration_1][silence][narration_2][ sil ][narration_3][ sil]\n",
    "        0s           5s       5s          10s    10s          15s\n",
    "```\n",
    "\n",
    "Chaque narration est placee avec un padding de silence pour remplir exactement la duree du segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignement audio sur la timeline video\n",
    "print(\"ALIGNEMENT AUDIO - TIMELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def align_audio_to_timeline(segments: List[Dict],\n",
    "                             total_duration_ms: int) -> AudioSegment:\n",
    "    \"\"\"Aligne les segments audio sur une timeline video.\n",
    "\n",
    "    Chaque segment est place a son timestamp de debut,\n",
    "    avec du silence pour remplir les espaces.\n",
    "    \"\"\"\n",
    "    timeline = AudioSegment.silent(duration=total_duration_ms)\n",
    "\n",
    "    for seg in segments:\n",
    "        start_ms = int(seg['start'] * 1000)\n",
    "        audio = seg['audio_segment']\n",
    "\n",
    "        # Tronquer si l'audio depasse la fin du segment\n",
    "        target_ms = int(seg['target_duration'] * 1000)\n",
    "        if len(audio) > target_ms:\n",
    "            # Ajouter un fade out avant de tronquer\n",
    "            audio = audio[:target_ms].fade_out(200)\n",
    "\n",
    "        # Superposer sur la timeline\n",
    "        timeline = timeline.overlay(audio, position=start_ms)\n",
    "\n",
    "    return timeline\n",
    "\n",
    "narration_timeline = None\n",
    "\n",
    "if generate_audio and narration_audio_segments:\n",
    "    total_duration_ms = video_duration * 1000\n",
    "\n",
    "    print(f\"Duree totale video : {video_duration}s ({total_duration_ms}ms)\")\n",
    "    print(f\"Segments a aligner : {len(narration_audio_segments)}\")\n",
    "\n",
    "    narration_timeline = align_audio_to_timeline(\n",
    "        narration_audio_segments,\n",
    "        total_duration_ms\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTimeline narration :\")\n",
    "    print(f\"  Duree : {len(narration_timeline)/1000:.1f}s\")\n",
    "    print(f\"  Canaux : {narration_timeline.channels}\")\n",
    "    print(f\"  Sample rate : {narration_timeline.frame_rate}Hz\")\n",
    "    print(f\"  Volume : {narration_timeline.dBFS:.1f} dBFS\")\n",
    "\n",
    "    # Sauvegarder la timeline narration\n",
    "    if save_output_files:\n",
    "        narration_path = OUTPUT_DIR / \"narration_timeline.wav\"\n",
    "        narration_timeline.export(str(narration_path), format=\"wav\")\n",
    "        print(f\"  Sauvegarde : {narration_path.name}\")\n",
    "\n",
    "    # Ecouter\n",
    "    narration_bytes = BytesIO()\n",
    "    narration_timeline.export(narration_bytes, format=\"mp3\")\n",
    "    print(f\"\\nEcoute de la timeline narration :\")\n",
    "    display(Audio(data=narration_bytes.getvalue(), autoplay=False))\n",
    "else:\n",
    "    print(\"Alignement desactive (pas de segments narration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-intro",
   "metadata": {},
   "source": [
    "## Section 4 : Musique de fond sous la narration\n",
    "\n",
    "La superposition de musique de fond et de narration necessite un controle precis des niveaux :\n",
    "\n",
    "| Piste | Volume relatif | Raison |\n",
    "|-------|---------------|--------|\n",
    "| Narration | 0 dB (reference) | Voix au premier plan |\n",
    "| Musique de fond | -15 a -25 dB | Ambiance sans couvrir la voix |\n",
    "| Effets sonores | -5 a -10 dB | Ponctuels, perceptibles |\n",
    "\n",
    "La technique du \"ducking\" baisse automatiquement la musique quand la voix est presente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-background-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Superposition musique de fond + narration\n",
    "print(\"SUPERPOSITION MUSIQUE DE FOND\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def generate_ambient_music(duration_ms: int, sr: int = 44100) -> AudioSegment:\n",
    "    \"\"\"Genere une musique d'ambiance simple (sinusoides harmoniques).\"\"\"\n",
    "    t = np.linspace(0, duration_ms / 1000, int(sr * duration_ms / 1000))\n",
    "\n",
    "    # Accords doux (Do majeur : C4, E4, G4)\n",
    "    freqs = [261.63, 329.63, 392.00]  # C4, E4, G4\n",
    "    audio = np.zeros_like(t)\n",
    "    for f in freqs:\n",
    "        audio += 0.15 * np.sin(2 * np.pi * f * t)\n",
    "\n",
    "    # Modulation lente pour un effet d'ambiance\n",
    "    modulation = 0.5 + 0.5 * np.sin(2 * np.pi * 0.1 * t)  # 0.1 Hz\n",
    "    audio *= modulation\n",
    "\n",
    "    # Normaliser en int16\n",
    "    audio = (audio / np.max(np.abs(audio)) * 16000).astype(np.int16)\n",
    "\n",
    "    return AudioSegment(\n",
    "        data=audio.tobytes(),\n",
    "        sample_width=2,\n",
    "        frame_rate=sr,\n",
    "        channels=1\n",
    "    )\n",
    "\n",
    "mixed_audio = None\n",
    "\n",
    "if generate_audio and narration_timeline:\n",
    "    total_duration_ms = len(narration_timeline)\n",
    "\n",
    "    # Generer la musique de fond\n",
    "    print(\"Generation de la musique d'ambiance...\")\n",
    "    background_music = generate_ambient_music(total_duration_ms)\n",
    "\n",
    "    # Ajuster le volume de la musique\n",
    "    background_music = background_music + background_music_volume  # dB\n",
    "\n",
    "    # Fade in/out sur la musique\n",
    "    background_music = background_music.fade_in(1000).fade_out(2000)\n",
    "\n",
    "    print(f\"Musique de fond :\")\n",
    "    print(f\"  Duree : {len(background_music)/1000:.1f}s\")\n",
    "    print(f\"  Volume : {background_music.dBFS:.1f} dBFS (reduit de {background_music_volume}dB)\")\n",
    "\n",
    "    # Superposer narration + musique\n",
    "    print(f\"\\nSuperposition narration + musique...\")\n",
    "\n",
    "    # S'assurer que les deux ont la meme duree\n",
    "    if len(background_music) > len(narration_timeline):\n",
    "        background_music = background_music[:len(narration_timeline)]\n",
    "    elif len(background_music) < len(narration_timeline):\n",
    "        background_music = background_music + AudioSegment.silent(\n",
    "            duration=len(narration_timeline) - len(background_music)\n",
    "        )\n",
    "\n",
    "    mixed_audio = narration_timeline.overlay(background_music)\n",
    "\n",
    "    print(f\"Mix final :\")\n",
    "    print(f\"  Duree : {len(mixed_audio)/1000:.1f}s\")\n",
    "    print(f\"  Volume : {mixed_audio.dBFS:.1f} dBFS\")\n",
    "\n",
    "    # Sauvegarder et ecouter\n",
    "    if save_output_files:\n",
    "        mix_path = OUTPUT_DIR / \"narration_plus_music.wav\"\n",
    "        mixed_audio.export(str(mix_path), format=\"wav\")\n",
    "        print(f\"  Sauvegarde : {mix_path.name}\")\n",
    "\n",
    "    mix_bytes = BytesIO()\n",
    "    mixed_audio.export(mix_bytes, format=\"mp3\")\n",
    "    print(f\"\\nEcoute du mix (narration + musique de fond) :\")\n",
    "    display(Audio(data=mix_bytes.getvalue(), autoplay=False))\n",
    "else:\n",
    "    print(\"Mix desactive (pas de narration timeline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Mix audio\n",
    "\n",
    "| Piste | Volume (dBFS) | Role |\n",
    "|-------|--------------|------|\n",
    "| Narration | ~-15 a -20 | Premier plan |\n",
    "| Musique de fond | ~-35 a -45 | Ambiance discrete |\n",
    "| Mix final | ~-15 a -20 | Equilibre narration/musique |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le volume de la musique (-20dB) la rend perceptible sans couvrir la voix\n",
    "2. Les fades evitent les transitions brutales\n",
    "3. En production, utiliser du ducking dynamique pour un resultat plus naturel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-intro",
   "metadata": {},
   "source": [
    "## Section 5 : Assemblage video + audio final\n",
    "\n",
    "L'assemblage final combine la video, la narration et la musique de fond en un seul fichier.\n",
    "\n",
    "```\n",
    "Pipeline d'assemblage :\n",
    "    Video (.mp4, sans audio)\n",
    "    +  Narration TTS (timeline alignee)\n",
    "    +  Musique de fond (volume reduit)\n",
    "    =  Video finale (.mp4, avec audio mixe)\n",
    "```\n",
    "\n",
    "moviepy gere la combinaison des flux video et audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-final-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemblage video + audio final\n",
    "print(\"ASSEMBLAGE VIDEO + AUDIO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "final_video_path = OUTPUT_DIR / \"video_finale_narree.mp4\"\n",
    "\n",
    "if generate_audio and moviepy_available and mixed_audio and test_video_path.exists():\n",
    "    # Sauvegarder l'audio mixe en WAV pour moviepy\n",
    "    mixed_audio_path = OUTPUT_DIR / \"mixed_audio_temp.wav\"\n",
    "    mixed_audio.export(str(mixed_audio_path), format=\"wav\")\n",
    "\n",
    "    print(\"Assemblage en cours...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Charger la video et l'audio\n",
    "    video_clip = VideoFileClip(str(test_video_path))\n",
    "    audio_clip = AudioFileClip(str(mixed_audio_path))\n",
    "\n",
    "    # Ajuster la duree de l'audio a celle de la video\n",
    "    if audio_clip.duration > video_clip.duration:\n",
    "        audio_clip = audio_clip.subclip(0, video_clip.duration)\n",
    "\n",
    "    # Combiner video + audio\n",
    "    video_with_audio = video_clip.set_audio(audio_clip)\n",
    "\n",
    "    # Exporter\n",
    "    video_with_audio.write_videofile(\n",
    "        str(final_video_path),\n",
    "        fps=video_fps,\n",
    "        codec='libx264',\n",
    "        audio_codec='aac',\n",
    "        logger=None\n",
    "    )\n",
    "\n",
    "    assembly_time = time.time() - start_time\n",
    "\n",
    "    # Fermer les clips\n",
    "    video_clip.close()\n",
    "    audio_clip.close()\n",
    "    video_with_audio.close()\n",
    "\n",
    "    # Nettoyage fichier temporaire\n",
    "    if mixed_audio_path.exists():\n",
    "        mixed_audio_path.unlink()\n",
    "\n",
    "    print(f\"Assemblage termine en {assembly_time:.1f}s\")\n",
    "    print(f\"\\nVideo finale :\")\n",
    "    print(f\"  Fichier : {final_video_path.name}\")\n",
    "    print(f\"  Taille : {final_video_path.stat().st_size/1024:.1f} KB\")\n",
    "    print(f\"  Resolution : {video_resolution[0]}x{video_resolution[1]}\")\n",
    "    print(f\"  FPS : {video_fps}\")\n",
    "    print(f\"  Duree : {video_duration}s\")\n",
    "    print(f\"  Audio : narration TTS + musique de fond\")\n",
    "\n",
    "    # Afficher la video dans le notebook (si possible)\n",
    "    try:\n",
    "        display(IPVideo(str(final_video_path), embed=True, width=480))\n",
    "    except Exception as e:\n",
    "        print(f\"Affichage video non disponible : {str(e)[:80]}\")\n",
    "        print(f\"Ouvrez le fichier : {final_video_path}\")\n",
    "else:\n",
    "    if not moviepy_available:\n",
    "        print(\"moviepy requis pour l'assemblage\")\n",
    "    elif not mixed_audio:\n",
    "        print(\"Pas d'audio mixe disponible\")\n",
    "    elif not test_video_path.exists():\n",
    "        print(\"Video de test non disponible\")\n",
    "    else:\n",
    "        print(\"Assemblage desactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Assemblage final\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Codec video | H.264 (libx264) | Standard universel |\n",
    "| Codec audio | AAC | Compression efficace |\n",
    "| Temps d'assemblage | Variable | Depend de la duree et resolution |\n",
    "\n",
    "**Points cles** :\n",
    "1. moviepy simplifie considerablement le pipeline d'assemblage\n",
    "2. Les codecs H.264/AAC sont compatibles avec tous les lecteurs\n",
    "3. Pour de la production, utiliser `codec='libx264'` avec `-preset slow` pour une meilleure qualite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-bridge-intro",
   "metadata": {},
   "source": [
    "## Passerelle vers la serie Video\n",
    "\n",
    "Ce notebook constitue le point de jonction entre les series Audio et Video du cours GenAI. Voici les liens entre les deux series :\n",
    "\n",
    "| Serie Audio (cette serie) | Serie Video | Lien |\n",
    "|--------------------------|-------------|------|\n",
    "| TTS (narration) | Narration sur video | Audio enrichit la video |\n",
    "| Whisper (sous-titres) | Sous-titrage video | Transcription -> SRT |\n",
    "| MusicGen (musique) | Bande sonore video | Musique de fond |\n",
    "| Demucs (separation) | Post-production | Remix audio de video |\n",
    "\n",
    "### Pour continuer\n",
    "\n",
    "- **Serie Video Foundation** : `Video/01-Foundation/01-1-Video-Operations-Basics.ipynb` - Operations de base sur la video\n",
    "- **Serie Video Advanced** : Generation de video avec HunyuanVideo, LTX-Video, Wan\n",
    "- **Serie Video Orchestration** : Pipelines video complets avec ComfyUI\n",
    "\n",
    "Les techniques audio apprises dans cette serie s'appliquent directement aux workflows video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif - Personnaliser le pipeline\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"MODE INTERACTIF - PIPELINE PERSONNALISE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nEntrez un texte de narration pour l'ajouter a la video :\")\n",
    "    print(\"(Laissez vide pour passer)\")\n",
    "\n",
    "    try:\n",
    "        user_text = input(\"\\nVotre narration : \")\n",
    "\n",
    "        if user_text.strip():\n",
    "            print(f\"\\nGeneration TTS...\")\n",
    "            response = client.audio.speech.create(\n",
    "                model=tts_model,\n",
    "                voice=narrator_voice,\n",
    "                input=user_text,\n",
    "                response_format=\"mp3\"\n",
    "            )\n",
    "            print(f\"Narration generee ({len(response.content)/1024:.1f} KB) :\")\n",
    "            display(Audio(data=response.content, autoplay=False))\n",
    "\n",
    "            if save_output_files:\n",
    "                ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filepath = OUTPUT_DIR / f\"custom_narration_{ts}.mp3\"\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Sauvegarde : {filepath.name}\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type} - {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session\n",
    "print(\"STATISTIQUES DE SESSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"TTS : {tts_model}, Voix : {narrator_voice}\")\n",
    "\n",
    "if narration_audio_segments:\n",
    "    total_narration = sum(s['audio_duration'] for s in narration_audio_segments)\n",
    "    print(f\"Segments narration : {len(narration_audio_segments)} ({total_narration:.1f}s)\")\n",
    "\n",
    "if narration_timeline:\n",
    "    print(f\"Timeline narration : {len(narration_timeline)/1000:.1f}s\")\n",
    "\n",
    "if mixed_audio:\n",
    "    print(f\"Mix final : {len(mixed_audio)/1000:.1f}s\")\n",
    "\n",
    "if final_video_path.exists():\n",
    "    print(f\"Video finale : {final_video_path.name} ({final_video_path.stat().st_size/1024:.1f} KB)\")\n",
    "\n",
    "if save_output_files:\n",
    "    saved_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"Fichiers sauvegardes : {len(saved_files)} dans {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nSERIE AUDIO TERMINEE\")\n",
    "print(f\"Felicitations ! Vous avez parcouru l'ensemble de la serie Audio :\")\n",
    "print(f\"  01-Foundation : TTS, STT, operations audio, Whisper local, Kokoro\")\n",
    "print(f\"  02-Advanced   : Chatterbox, XTTS, MusicGen, Demucs\")\n",
    "print(f\"  03-Orchestration : Multi-modeles, pipelines, voix temps reel\")\n",
    "print(f\"  04-Applications  : Contenu educatif, transcription, composition, A/V sync\")\n",
    "print(f\"\\nPROCHAINE SERIE\")\n",
    "print(f\"  -> Video/01-Foundation/01-1-Video-Operations-Basics.ipynb\")\n",
    "\n",
    "print(f\"\\nNotebook termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}