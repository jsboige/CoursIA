{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# Creation de Contenu Audio Educatif\n",
    "\n",
    "**Module :** 04-Audio-Applications  \n",
    "**Niveau :** Applications  \n",
    "**Technologies :** OpenAI TTS, Kokoro TTS, Whisper, GPT (LLM), pydub  \n",
    "**VRAM estimee :** ~10 GB  \n",
    "**Duree estimee :** 50 minutes  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Generer des scripts de narration a partir de texte de cours via LLM\n",
    "- [ ] Produire une narration multi-voix (voix differentes par section ou personnage)\n",
    "- [ ] Creer du contenu audio multilingue (francais, anglais)\n",
    "- [ ] Appliquer des parametres d'accessibilite (vitesse reduite, enonciation claire)\n",
    "- [ ] Assembler un cours narrate complet a partir de sections individuelles\n",
    "- [ ] Verifier la qualite par round-trip STT (transcription de validation)\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Notebooks Foundation (01-1 a 01-5) completes\n",
    "- Cle API OpenAI configuree (`OPENAI_API_KEY` dans `.env`)\n",
    "- pydub installe (manipulation audio)\n",
    "- Comprehension de base du TTS et du STT\n",
    "\n",
    "**Navigation :** [Index](../README.md) | [<< Precedent](../03-Orchestration/03-3-Realtime-Voice-API.ipynb) | [Suivant >>](04-2-Transcription-Pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-params",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres contenu educatif\n",
    "tts_model = \"tts-1\"               # \"tts-1\" ou \"tts-1-hd\"\n",
    "narrator_voice = \"nova\"            # Voix principale de narration\n",
    "secondary_voice = \"onyx\"           # Voix secondaire (exemples, citations)\n",
    "llm_model = \"gpt-4o-mini\"         # Modele LLM pour generation de scripts\n",
    "target_languages = [\"fr\", \"en\"]    # Langues cibles\n",
    "accessibility_speed = 0.85         # Vitesse reduite pour accessibilite\n",
    "\n",
    "# Configuration sauvegarde\n",
    "generate_audio = True\n",
    "save_audio_files = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from io import BytesIO\n",
    "import logging\n",
    "\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "# Resolution GENAI_ROOT\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.audio_helpers import (\n",
    "            play_audio_bytes, synthesize_openai, transcribe_openai,\n",
    "            get_audio_info, estimate_audio_duration\n",
    "        )\n",
    "        print(\"Helpers audio importes\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Helpers audio non disponibles - mode autonome : {e}\")\n",
    "\n",
    "# Repertoire de sortie\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'educational'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('educational_audio')\n",
    "\n",
    "print(f\"Creation de Contenu Audio Educatif\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}, TTS : {tts_model}\")\n",
    "print(f\"Voix narrateur : {narrator_voice}, Voix secondaire : {secondary_voice}\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la configuration et validation API\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve dans l'arborescence\")\n",
    "\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not openai_key:\n",
    "    if notebook_mode == \"batch\" and not generate_audio:\n",
    "        print(\"Mode batch sans generation : cle API ignoree\")\n",
    "        openai_key = \"dummy_key_for_validation\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"OPENAI_API_KEY manquante dans .env\\n\"\n",
    "            \"Obtenez votre cle sur : https://platform.openai.com/api-keys\"\n",
    "        )\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_key)\n",
    "\n",
    "if openai_key != \"dummy_key_for_validation\":\n",
    "    try:\n",
    "        models = client.models.list()\n",
    "        tts_available = any('tts' in m.id for m in models)\n",
    "        print(f\"Connexion API reussie (TTS disponible : {tts_available})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur connexion : {str(e)[:100]}\")\n",
    "\n",
    "print(f\"\\nConfiguration :\")\n",
    "print(f\"  TTS : {tts_model} | LLM : {llm_model}\")\n",
    "print(f\"  Langues : {target_languages}\")\n",
    "print(f\"  Accessibilite : vitesse {accessibility_speed}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-intro",
   "metadata": {},
   "source": [
    "## Section 1 : Generation de scripts de narration via LLM\n",
    "\n",
    "La premiere etape d'un contenu audio educatif consiste a transformer un texte brut de cours en un script de narration naturel. Un LLM adapte le contenu technique en discours oral fluide :\n",
    "\n",
    "| Etape | Description | Outil |\n",
    "|-------|-------------|-------|\n",
    "| Texte source | Contenu pedagogique brut | -- |\n",
    "| Reformulation | Adaptation pour la narration orale | GPT-4o-mini |\n",
    "| Structuration | Decoupage en sections narrees | GPT-4o-mini |\n",
    "| Attribution voix | Assignation des voix par section | Configuration |\n",
    "\n",
    "Le LLM recoit le texte de cours et genere un script avec des indications de voix, pauses et intonation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-script-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation de scripts de narration a partir de texte de cours\n",
    "print(\"GENERATION DE SCRIPTS DE NARRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Texte de cours source (exemple : introduction a l'IA)\n",
    "course_text = \"\"\"\n",
    "L'intelligence artificielle (IA) est un domaine de l'informatique qui vise\n",
    "a creer des systemes capables de realiser des taches necessitant normalement\n",
    "l'intelligence humaine. Ces taches incluent la reconnaissance vocale,\n",
    "la prise de decision, la traduction et la generation de contenu.\n",
    "\n",
    "Le machine learning (apprentissage automatique) est une sous-branche de l'IA\n",
    "qui permet aux machines d'apprendre a partir de donnees sans etre explicitement\n",
    "programmees. Les reseaux de neurones profonds (deep learning) sont une technique\n",
    "de machine learning inspiree du cerveau humain.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Prompt pour le LLM : transformer en script de narration\n",
    "narration_prompt = f\"\"\"\n",
    "Transforme ce texte de cours en un script de narration audio educatif.\n",
    "Le script doit :\n",
    "- Etre naturel a l'oral (phrases courtes, transitions fluides)\n",
    "- Inclure des marqueurs [PAUSE] pour les pauses naturelles\n",
    "- Indiquer les changements de voix avec [NARRATEUR] et [EXPERT]\n",
    "- Ajouter une introduction accueillante et une conclusion\n",
    "\n",
    "Texte source :\n",
    "{course_text}\n",
    "\n",
    "Genere uniquement le script, sans commentaires.\n",
    "\"\"\"\n",
    "\n",
    "narration_script = \"\"\n",
    "if generate_audio:\n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[{\"role\": \"user\", \"content\": narration_prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    narration_script = response.choices[0].message.content\n",
    "    gen_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Script genere en {gen_time:.1f}s\")\n",
    "    print(f\"Tokens utilises : {response.usage.total_tokens}\")\n",
    "    print(f\"\\n--- Script de narration ---\")\n",
    "    print(narration_script)\n",
    "    print(f\"--- Fin du script ---\")\n",
    "    print(f\"\\nLongueur : {len(narration_script)} caracteres\")\n",
    "    print(f\"Duree estimee : {estimate_audio_duration(narration_script):.0f}s\")\n",
    "else:\n",
    "    print(\"Generation desactivee (generate_audio=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Generation de scripts\n",
    "\n",
    "| Aspect | Observation | Signification |\n",
    "|--------|-------------|---------------|\n",
    "| Temps LLM | Typiquement < 5s | GPT-4o-mini est rapide pour cette tache |\n",
    "| Marqueurs | [PAUSE], [NARRATEUR], [EXPERT] | Permettent le controle fin de la narration |\n",
    "| Longueur | Script ~2x plus long que le texte source | L'oral necessite plus de mots que l'ecrit |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le LLM adapte naturellement le registre ecrit vers l'oral\n",
    "2. Les marqueurs structurent le script pour le pipeline TTS\n",
    "3. La temperature 0.7 donne un bon equilibre naturel/fidele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-intro",
   "metadata": {},
   "source": [
    "## Section 2 : Narration multi-voix\n",
    "\n",
    "Un contenu educatif beneficie de plusieurs voix pour distinguer les roles :\n",
    "\n",
    "| Role | Voix suggeree | Utilisation |\n",
    "|------|---------------|-------------|\n",
    "| Narrateur principal | `nova` | Explications, transitions |\n",
    "| Expert / Citations | `onyx` | Definitions, concepts cles |\n",
    "| Exemples | `echo` | Illustrations pratiques |\n",
    "\n",
    "Nous allons parser le script genere, identifier les segments par voix, et generer chaque segment avec la voix appropriee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-multi-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narration multi-voix : parser et generer par segments\n",
    "print(\"NARRATION MULTI-VOIX\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import re\n",
    "\n",
    "# Mapping des roles vers les voix OpenAI\n",
    "voice_mapping = {\n",
    "    \"NARRATEUR\": narrator_voice,\n",
    "    \"EXPERT\": secondary_voice,\n",
    "    \"DEFAULT\": narrator_voice\n",
    "}\n",
    "\n",
    "def parse_narration_script(script: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Parse un script avec marqueurs [ROLE] en segments.\"\"\"\n",
    "    segments = []\n",
    "    current_role = \"DEFAULT\"\n",
    "    current_text = []\n",
    "\n",
    "    for line in script.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Detection marqueur de role\n",
    "        role_match = re.match(r'\\[(NARRATEUR|EXPERT)\\]', line)\n",
    "        if role_match:\n",
    "            # Sauvegarder le segment precedent\n",
    "            if current_text:\n",
    "                text = ' '.join(current_text).strip()\n",
    "                text = re.sub(r'\\[PAUSE\\]', '...', text)\n",
    "                if text:\n",
    "                    segments.append({\"role\": current_role, \"text\": text})\n",
    "            current_role = role_match.group(1)\n",
    "            # Texte apres le marqueur\n",
    "            remaining = line[role_match.end():].strip()\n",
    "            current_text = [remaining] if remaining else []\n",
    "        else:\n",
    "            current_text.append(line)\n",
    "\n",
    "    # Dernier segment\n",
    "    if current_text:\n",
    "        text = ' '.join(current_text).strip()\n",
    "        text = re.sub(r'\\[PAUSE\\]', '...', text)\n",
    "        if text:\n",
    "            segments.append({\"role\": current_role, \"text\": text})\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Parser le script\n",
    "if narration_script:\n",
    "    segments = parse_narration_script(narration_script)\n",
    "else:\n",
    "    # Script de demonstration si la generation a ete sautee\n",
    "    segments = [\n",
    "        {\"role\": \"NARRATEUR\", \"text\": \"Bienvenue dans ce cours sur l'intelligence artificielle.\"},\n",
    "        {\"role\": \"EXPERT\", \"text\": \"L'intelligence artificielle est un domaine qui vise a creer des systemes intelligents.\"},\n",
    "        {\"role\": \"NARRATEUR\", \"text\": \"Voyons maintenant le machine learning, une sous-branche fondamentale.\"},\n",
    "        {\"role\": \"EXPERT\", \"text\": \"Le machine learning permet aux machines d'apprendre a partir de donnees.\"}\n",
    "    ]\n",
    "\n",
    "print(f\"Segments identifies : {len(segments)}\")\n",
    "print(f\"\\nDetail des segments :\")\n",
    "for i, seg in enumerate(segments):\n",
    "    voice = voice_mapping.get(seg['role'], narrator_voice)\n",
    "    print(f\"  [{i+1}] {seg['role']:12s} -> voix '{voice}' | {seg['text'][:60]}...\")\n",
    "\n",
    "# Generer chaque segment avec la voix appropriee\n",
    "audio_segments = []\n",
    "\n",
    "if generate_audio:\n",
    "    print(f\"\\nGeneration audio par segment :\")\n",
    "    total_start = time.time()\n",
    "\n",
    "    for i, seg in enumerate(segments):\n",
    "        voice = voice_mapping.get(seg['role'], narrator_voice)\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = client.audio.speech.create(\n",
    "            model=tts_model,\n",
    "            voice=voice,\n",
    "            input=seg['text'],\n",
    "            response_format=\"mp3\",\n",
    "            speed=1.0\n",
    "        )\n",
    "\n",
    "        audio_data = response.content\n",
    "        gen_time = time.time() - start_time\n",
    "\n",
    "        audio_segments.append({\n",
    "            \"role\": seg['role'],\n",
    "            \"voice\": voice,\n",
    "            \"audio\": audio_data,\n",
    "            \"size_kb\": len(audio_data) / 1024,\n",
    "            \"time\": gen_time\n",
    "        })\n",
    "\n",
    "        print(f\"  Segment {i+1}/{len(segments)} : {voice} | {gen_time:.1f}s | {len(audio_data)/1024:.1f} KB\")\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nTotal : {len(audio_segments)} segments en {total_time:.1f}s\")\n",
    "    print(f\"Taille totale : {sum(s['size_kb'] for s in audio_segments):.1f} KB\")\n",
    "\n",
    "    # Ecouter le premier segment\n",
    "    if audio_segments:\n",
    "        print(f\"\\nEcoute du premier segment ({audio_segments[0]['role']}, voix {audio_segments[0]['voice']}) :\")\n",
    "        display(Audio(data=audio_segments[0]['audio'], autoplay=False))\n",
    "else:\n",
    "    print(\"Generation desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Narration multi-voix\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Segments | Variable (4-10 typique) | Chaque changement de role cree un segment |\n",
    "| Temps par segment | ~1-3s | L'API est rapide meme pour du texte long |\n",
    "| Differenciation | Voix distinctes par role | Ameliore la comprehension et l'engagement |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le parsing des marqueurs [ROLE] permet une attribution automatique des voix\n",
    "2. Les pauses [PAUSE] sont converties en ellipses pour un rythme naturel\n",
    "3. La multi-voix rend le contenu plus dynamique qu'un narrateur unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-intro",
   "metadata": {},
   "source": [
    "## Section 3 : Contenu multilingue\n",
    "\n",
    "L'API OpenAI TTS gere nativement de nombreuses langues. Pour creer du contenu multilingue, deux approches :\n",
    "\n",
    "| Approche | Description | Avantage |\n",
    "|----------|-------------|----------|\n",
    "| Traduction LLM + TTS | Traduire via GPT, puis synthetiser | Controle total du texte |\n",
    "| TTS direct multilingue | Fournir le texte dans la langue cible | Plus simple, qualite variable |\n",
    "\n",
    "Nous utilisons la premiere approche pour garantir la qualite pedagogique du contenu traduit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-multilingual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contenu multilingue : traduction + narration\n",
    "print(\"CONTENU MULTILINGUE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Texte source en francais\n",
    "source_text_fr = (\n",
    "    \"L'apprentissage automatique permet aux ordinateurs d'apprendre \"\n",
    "    \"a partir d'exemples. Au lieu de programmer chaque regle manuellement, \"\n",
    "    \"on fournit des donnees et l'algorithme decouvre les patterns par lui-meme.\"\n",
    ")\n",
    "\n",
    "lang_configs = {\n",
    "    \"fr\": {\"name\": \"Francais\", \"voice\": \"nova\", \"text\": source_text_fr},\n",
    "    \"en\": {\"name\": \"English\", \"voice\": \"alloy\", \"text\": None}  # A traduire\n",
    "}\n",
    "\n",
    "multilingual_results = {}\n",
    "\n",
    "if generate_audio:\n",
    "    # Etape 1 : Traduction vers l'anglais via LLM\n",
    "    print(\"Etape 1 : Traduction via LLM\")\n",
    "    translation_response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Traduis ce texte educatif en anglais, en gardant le ton pedagogique :\\n\\n{source_text_fr}\"\n",
    "        }],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    lang_configs[\"en\"][\"text\"] = translation_response.choices[0].message.content\n",
    "\n",
    "    for lang_code, config in lang_configs.items():\n",
    "        print(f\"\\n--- {config['name']} ({lang_code}) ---\")\n",
    "        print(f\"Texte : {config['text'][:80]}...\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = client.audio.speech.create(\n",
    "            model=tts_model,\n",
    "            voice=config['voice'],\n",
    "            input=config['text'],\n",
    "            response_format=\"mp3\"\n",
    "        )\n",
    "        audio_data = response.content\n",
    "        gen_time = time.time() - start_time\n",
    "\n",
    "        multilingual_results[lang_code] = {\n",
    "            \"audio\": audio_data,\n",
    "            \"text\": config['text'],\n",
    "            \"voice\": config['voice'],\n",
    "            \"size_kb\": len(audio_data) / 1024,\n",
    "            \"time\": gen_time\n",
    "        }\n",
    "\n",
    "        print(f\"Voix : {config['voice']} | {gen_time:.1f}s | {len(audio_data)/1024:.1f} KB\")\n",
    "        display(Audio(data=audio_data, autoplay=False))\n",
    "\n",
    "        if save_audio_files:\n",
    "            filepath = OUTPUT_DIR / f\"multilingual_{lang_code}.mp3\"\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(audio_data)\n",
    "            print(f\"Sauvegarde : {filepath.name}\")\n",
    "\n",
    "    # Comparaison\n",
    "    print(f\"\\nComparaison multilingue :\")\n",
    "    print(f\"{'Langue':<12} {'Voix':<10} {'Taille (KB)':<12} {'Temps (s)':<10}\")\n",
    "    print(\"-\" * 44)\n",
    "    for lang, data in multilingual_results.items():\n",
    "        print(f\"{lang:<12} {data['voice']:<10} {data['size_kb']:<12.1f} {data['time']:<10.1f}\")\n",
    "else:\n",
    "    print(\"Generation desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Contenu multilingue\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Qualite FR | Excellente | Voix OpenAI optimisees pour le francais |\n",
    "| Qualite EN | Excellente | Langue native du modele |\n",
    "| Traduction LLM | Fidele au contenu | GPT-4o-mini conserve le ton pedagogique |\n",
    "\n",
    "> **Note technique** : Pour des langues moins bien supportees, tester la qualite avec un locuteur natif avant production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-intro",
   "metadata": {},
   "source": [
    "## Section 4 : Accessibilite audio\n",
    "\n",
    "Un contenu educatif accessible doit prendre en compte differents profils d'apprenants :\n",
    "\n",
    "| Parametre | Valeur standard | Valeur accessible | Impact |\n",
    "|-----------|----------------|-------------------|--------|\n",
    "| Vitesse | 1.0x | 0.85x | Meilleure comprehension |\n",
    "| Pauses | Courtes | Allongees | Temps de reflexion |\n",
    "| Voix | Au choix | Claire, posee | Intelligibilite |\n",
    "| Repetitions | Aucune | Concepts cles repetes | Memorisation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-accessibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessibilite : vitesse reduite et enonciation claire\n",
    "print(\"ACCESSIBILITE AUDIO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "accessibility_text = (\n",
    "    \"Un reseau de neurones est compose de couches de neurones artificiels. \"\n",
    "    \"Chaque neurone recoit des entrees, les multiplie par des poids, \"\n",
    "    \"et applique une fonction d'activation. \"\n",
    "    \"Retenez bien : entrees, poids, activation. \"\n",
    "    \"C'est le fonctionnement fondamental de tout reseau de neurones.\"\n",
    ")\n",
    "\n",
    "speed_configs = [\n",
    "    {\"label\": \"Standard (1.0x)\", \"speed\": 1.0},\n",
    "    {\"label\": f\"Accessible ({accessibility_speed}x)\", \"speed\": accessibility_speed},\n",
    "    {\"label\": \"Lent (0.7x)\", \"speed\": 0.7}\n",
    "]\n",
    "\n",
    "accessibility_results = []\n",
    "\n",
    "if generate_audio:\n",
    "    for config in speed_configs:\n",
    "        print(f\"\\n--- {config['label']} ---\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = client.audio.speech.create(\n",
    "            model=tts_model,\n",
    "            voice=narrator_voice,\n",
    "            input=accessibility_text,\n",
    "            response_format=\"mp3\",\n",
    "            speed=config['speed']\n",
    "        )\n",
    "\n",
    "        audio_data = response.content\n",
    "        gen_time = time.time() - start_time\n",
    "\n",
    "        accessibility_results.append({\n",
    "            \"label\": config['label'],\n",
    "            \"speed\": config['speed'],\n",
    "            \"audio\": audio_data,\n",
    "            \"size_kb\": len(audio_data) / 1024,\n",
    "            \"time\": gen_time\n",
    "        })\n",
    "\n",
    "        print(f\"Taille : {len(audio_data)/1024:.1f} KB | Temps : {gen_time:.1f}s\")\n",
    "        display(Audio(data=audio_data, autoplay=False))\n",
    "\n",
    "    # Comparaison\n",
    "    print(f\"\\nComparaison des vitesses :\")\n",
    "    print(f\"{'Mode':<25} {'Vitesse':<10} {'Taille (KB)':<12}\")\n",
    "    print(\"-\" * 47)\n",
    "    for r in accessibility_results:\n",
    "        print(f\"{r['label']:<25} {r['speed']:<10} {r['size_kb']:<12.1f}\")\n",
    "\n",
    "    print(f\"\\nLa version accessible est ~{accessibility_results[1]['size_kb']/accessibility_results[0]['size_kb']*100:.0f}% de la taille standard\")\n",
    "else:\n",
    "    print(\"Generation desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-intro",
   "metadata": {},
   "source": [
    "## Section 5 : Assemblage d'un cours narrate complet\n",
    "\n",
    "L'assemblage final combine tous les segments audio en un fichier unique avec pydub. Le processus :\n",
    "\n",
    "1. Charger chaque segment audio genere\n",
    "2. Ajouter des silences entre les sections (transition naturelle)\n",
    "3. Concatener dans l'ordre du script\n",
    "4. Normaliser le volume global\n",
    "5. Exporter en format final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemblage du cours narrate complet\n",
    "print(\"ASSEMBLAGE DU COURS NARRATE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "\n",
    "if generate_audio and audio_segments:\n",
    "    # Silence entre les segments (ms)\n",
    "    inter_segment_silence = 800   # Entre segments du meme role\n",
    "    inter_section_silence = 1500  # Entre sections (changement de role)\n",
    "\n",
    "    # Construire le fichier final\n",
    "    final_audio = AudioSegment.silent(duration=500)  # Silence initial\n",
    "    prev_role = None\n",
    "\n",
    "    for i, seg in enumerate(audio_segments):\n",
    "        # Charger le segment\n",
    "        segment_audio = AudioSegment.from_mp3(BytesIO(seg['audio']))\n",
    "\n",
    "        # Ajouter silence adapte\n",
    "        if prev_role is not None:\n",
    "            silence_ms = inter_section_silence if seg['role'] != prev_role else inter_segment_silence\n",
    "            final_audio += AudioSegment.silent(duration=silence_ms)\n",
    "\n",
    "        final_audio += segment_audio\n",
    "        prev_role = seg['role']\n",
    "        print(f\"  Segment {i+1} : {seg['role']:12s} ({len(segment_audio)/1000:.1f}s)\")\n",
    "\n",
    "    # Silence final\n",
    "    final_audio += AudioSegment.silent(duration=500)\n",
    "\n",
    "    # Normalisation du volume\n",
    "    target_dBFS = -20.0\n",
    "    change_in_dBFS = target_dBFS - final_audio.dBFS\n",
    "    final_audio = final_audio.apply_gain(change_in_dBFS)\n",
    "\n",
    "    print(f\"\\nCours assemble :\")\n",
    "    print(f\"  Duree totale : {len(final_audio)/1000:.1f}s\")\n",
    "    print(f\"  Volume normalise : {final_audio.dBFS:.1f} dBFS\")\n",
    "    print(f\"  Segments : {len(audio_segments)}\")\n",
    "\n",
    "    # Sauvegarde\n",
    "    if save_audio_files:\n",
    "        output_path = OUTPUT_DIR / \"cours_complet_assemble.mp3\"\n",
    "        final_audio.export(str(output_path), format=\"mp3\", bitrate=\"192k\")\n",
    "        print(f\"  Sauvegarde : {output_path.name} ({output_path.stat().st_size/1024:.1f} KB)\")\n",
    "\n",
    "    # Ecouter le resultat\n",
    "    final_bytes = BytesIO()\n",
    "    final_audio.export(final_bytes, format=\"mp3\")\n",
    "    print(f\"\\nEcoute du cours complet :\")\n",
    "    display(Audio(data=final_bytes.getvalue(), autoplay=False))\n",
    "else:\n",
    "    print(\"Assemblage ignore (pas de segments audio disponibles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : Assemblage\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Silences inter-segments | 800ms | Pause naturelle entre phrases |\n",
    "| Silences inter-sections | 1500ms | Marque un changement de locuteur/sujet |\n",
    "| Normalisation | -20 dBFS | Volume confortable pour l'ecoute |\n",
    "| Format final | MP3 192kbps | Bon compromis qualite/taille |\n",
    "\n",
    "> **Note technique** : pydub utilise ffmpeg en arriere-plan. Assurez-vous que ffmpeg est installe dans votre environnement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section6-intro",
   "metadata": {},
   "source": [
    "## Section 6 : Verification qualite par round-trip STT\n",
    "\n",
    "Pour valider que la narration est fidele au texte source, nous effectuons un round-trip :\n",
    "\n",
    "```\n",
    "Texte source -> TTS -> Audio -> STT -> Texte transcrit -> Comparaison\n",
    "```\n",
    "\n",
    "La similarite entre le texte source et le texte retranscrit mesure la fidelite du pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-roundtrip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification qualite par round-trip STT\n",
    "print(\"VERIFICATION QUALITE - ROUND-TRIP STT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "roundtrip_text = (\n",
    "    \"Le deep learning utilise des reseaux de neurones profonds \"\n",
    "    \"pour apprendre des representations hierarchiques des donnees.\"\n",
    ")\n",
    "\n",
    "if generate_audio:\n",
    "    # Etape 1 : TTS\n",
    "    print(\"Etape 1 : Generation TTS\")\n",
    "    tts_response = client.audio.speech.create(\n",
    "        model=tts_model,\n",
    "        voice=narrator_voice,\n",
    "        input=roundtrip_text,\n",
    "        response_format=\"mp3\"\n",
    "    )\n",
    "    tts_audio = tts_response.content\n",
    "    print(f\"  Audio genere : {len(tts_audio)/1024:.1f} KB\")\n",
    "\n",
    "    # Sauvegarder temporairement pour Whisper\n",
    "    temp_path = OUTPUT_DIR / \"roundtrip_temp.mp3\"\n",
    "    with open(temp_path, 'wb') as f:\n",
    "        f.write(tts_audio)\n",
    "\n",
    "    # Etape 2 : STT\n",
    "    print(\"Etape 2 : Transcription STT\")\n",
    "    with open(temp_path, 'rb') as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            language=\"fr\"\n",
    "        )\n",
    "    transcribed_text = transcript.text\n",
    "    print(f\"  Texte transcrit : {transcribed_text}\")\n",
    "\n",
    "    # Etape 3 : Comparaison\n",
    "    print(\"\\nEtape 3 : Comparaison\")\n",
    "    similarity = SequenceMatcher(None,\n",
    "                                 roundtrip_text.lower(),\n",
    "                                 transcribed_text.lower()).ratio()\n",
    "\n",
    "    print(f\"  Texte original  : {roundtrip_text}\")\n",
    "    print(f\"  Texte transcrit : {transcribed_text}\")\n",
    "    print(f\"  Similarite      : {similarity*100:.1f}%\")\n",
    "\n",
    "    if similarity >= 0.9:\n",
    "        print(f\"  Verdict : EXCELLENT - Narration fidele\")\n",
    "    elif similarity >= 0.75:\n",
    "        print(f\"  Verdict : BON - Quelques differences mineures\")\n",
    "    else:\n",
    "        print(f\"  Verdict : A VERIFIER - Differences significatives\")\n",
    "\n",
    "    # Nettoyage\n",
    "    if temp_path.exists():\n",
    "        temp_path.unlink()\n",
    "else:\n",
    "    print(\"Verification desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif - Creer votre propre contenu educatif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"MODE INTERACTIF - CONTENU EDUCATIF PERSONNALISE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nEntrez un paragraphe de cours a transformer en narration audio :\")\n",
    "    print(\"(Laissez vide pour passer)\")\n",
    "\n",
    "    try:\n",
    "        user_course = input(\"\\nVotre texte de cours : \")\n",
    "\n",
    "        if user_course.strip():\n",
    "            # Generer script de narration\n",
    "            script_resp = client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Transforme en narration orale educative :\\n\\n{user_course}\"}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            user_script = script_resp.choices[0].message.content\n",
    "            print(f\"\\nScript genere :\\n{user_script}\")\n",
    "\n",
    "            # Narrer\n",
    "            tts_resp = client.audio.speech.create(\n",
    "                model=tts_model,\n",
    "                voice=narrator_voice,\n",
    "                input=user_script,\n",
    "                response_format=\"mp3\"\n",
    "            )\n",
    "            print(f\"\\nNarration generee ({len(tts_resp.content)/1024:.1f} KB) :\")\n",
    "            display(Audio(data=tts_resp.content, autoplay=False))\n",
    "\n",
    "            if save_audio_files:\n",
    "                ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filepath = OUTPUT_DIR / f\"custom_course_{ts}.mp3\"\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(tts_resp.content)\n",
    "                print(f\"Sauvegarde : {filepath.name}\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type} - {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et recapitulatif\n",
    "print(\"STATISTIQUES DE SESSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Modele TTS : {tts_model} | Modele LLM : {llm_model}\")\n",
    "print(f\"Voix narrateur : {narrator_voice} | Voix secondaire : {secondary_voice}\")\n",
    "\n",
    "if audio_segments:\n",
    "    total_segments = len(audio_segments)\n",
    "    total_size_kb = sum(s['size_kb'] for s in audio_segments)\n",
    "    print(f\"Segments multi-voix generes : {total_segments}\")\n",
    "    print(f\"Taille totale segments : {total_size_kb:.1f} KB\")\n",
    "\n",
    "if multilingual_results:\n",
    "    print(f\"Langues generees : {', '.join(multilingual_results.keys())}\")\n",
    "\n",
    "if save_audio_files:\n",
    "    saved_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"Fichiers sauvegardes : {len(saved_files)} dans {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nPROCHAINES ETAPES\")\n",
    "print(f\"1. Creer un pipeline de transcription (04-2-Transcription-Pipeline)\")\n",
    "print(f\"2. Explorer la composition musicale (04-3-Music-Composition-Workflow)\")\n",
    "print(f\"3. Synchroniser audio et video (04-4-Audio-Video-Sync)\")\n",
    "\n",
    "print(f\"\\nNotebook termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}