{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {
    "papermill": {
     "duration": 0.00412,
     "end_time": "2026-02-18T09:52:33.461125",
     "exception": false,
     "start_time": "2026-02-18T09:52:33.457005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Workflow de Composition Musicale\n",
    "\n",
    "**Module :** 04-Audio-Applications  \n",
    "**Niveau :** Applications  \n",
    "**Technologies :** MusicGen, Demucs, pydub, scipy, numpy  \n",
    "**VRAM estimee :** ~14 GB  \n",
    "**Duree estimee :** 60 minutes  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Generer de la musique a partir de descriptions textuelles avec MusicGen\n",
    "- [ ] Separer les stems (voix, batterie, basse, autres) avec Demucs\n",
    "- [ ] Remixer les stems (ajuster volumes, echanger des elements)\n",
    "- [ ] Appliquer des effets audio (reverb, EQ, compression)\n",
    "- [ ] Creer des compositions multi-sections (intro, couplet, refrain)\n",
    "- [ ] Exporter une production finale avec metadonnees\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Notebooks Foundation (01-3 Basic Audio Operations) et Advanced (02-3 MusicGen, 02-4 Demucs) completes\n",
    "- GPU avec au moins 14 GB VRAM (RTX 3090 recommande)\n",
    "- audiocraft, demucs, pydub installes\n",
    "\n",
    "**Navigation :** [Index](../README.md) | [<< Precedent](04-2-Transcription-Pipeline.ipynb) | [Suivant >>](04-4-Audio-Video-Sync.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-params",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:52:33.470545Z",
     "iopub.status.busy": "2026-02-18T09:52:33.470234Z",
     "iopub.status.idle": "2026-02-18T09:52:33.476515Z",
     "shell.execute_reply": "2026-02-18T09:52:33.475354Z"
    },
    "papermill": {
     "duration": 0.013004,
     "end_time": "2026-02-18T09:52:33.478104",
     "exception": false,
     "start_time": "2026-02-18T09:52:33.465100",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres MusicGen\n",
    "musicgen_model = \"facebook/musicgen-small\"  # small, medium, large\n",
    "generation_duration = 8            # Duree de generation en secondes\n",
    "musicgen_device = \"cuda\"           # \"cuda\" ou \"cpu\"\n",
    "\n",
    "# Parametres Demucs\n",
    "demucs_model = \"htdemucs\"          # htdemucs, htdemucs_ft, mdx_extra\n",
    "\n",
    "# Configuration pipeline\n",
    "generate_audio = True\n",
    "save_audio_files = True\n",
    "apply_effects = True               # Appliquer les effets audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:52:33.489561Z",
     "iopub.status.busy": "2026-02-18T09:52:33.488714Z",
     "iopub.status.idle": "2026-02-18T09:52:33.617141Z",
     "shell.execute_reply": "2026-02-18T09:52:33.616398Z"
    },
    "papermill": {
     "duration": 0.136442,
     "end_time": "2026-02-18T09:52:33.619107",
     "exception": false,
     "start_time": "2026-02-18T09:52:33.482665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers audio importes\n",
      "Workflow de Composition Musicale\n",
      "Date : 2026-02-18 10:52:33\n",
      "Mode : interactive\n",
      "MusicGen : facebook/musicgen-small | Demucs : htdemucs\n",
      "Sortie : D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\outputs\\audio\\composition\n"
     ]
    }
   ],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import struct\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from io import BytesIO\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "# Resolution GENAI_ROOT\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.audio_helpers import (\n",
    "            load_audio, save_audio, play_audio,\n",
    "            plot_waveform, plot_spectrogram\n",
    "        )\n",
    "        print(\"Helpers audio importes\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Helpers audio non disponibles - mode autonome : {e}\")\n",
    "\n",
    "# Repertoire de sortie\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'audio' / 'composition'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STEMS_DIR = OUTPUT_DIR / 'stems'\n",
    "STEMS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('music_composition')\n",
    "\n",
    "print(f\"Workflow de Composition Musicale\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"MusicGen : {musicgen_model} | Demucs : {demucs_model}\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-env",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:52:33.630118Z",
     "iopub.status.busy": "2026-02-18T09:52:33.629733Z",
     "iopub.status.idle": "2026-02-18T09:52:35.849151Z",
     "shell.execute_reply": "2026-02-18T09:52:35.847791Z"
    },
    "papermill": {
     "duration": 2.22605,
     "end_time": "2026-02-18T09:52:35.850474",
     "exception": false,
     "start_time": "2026-02-18T09:52:33.624424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier .env charge depuis : D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU non disponible - basculement CPU (lent)\n",
      "audiocraft non installe - pip install audiocraft\n",
      "demucs non installe - pip install demucs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsboi\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pydub et scipy disponibles\n"
     ]
    }
   ],
   "source": [
    "# Chargement configuration et validation GPU\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification GPU\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_mem / (1024**3)\n",
    "    print(f\"GPU : {gpu_name} ({gpu_mem:.1f} GB)\")\n",
    "else:\n",
    "    print(\"GPU non disponible - basculement CPU (lent)\")\n",
    "    musicgen_device = \"cpu\"\n",
    "\n",
    "# Verification des dependances\n",
    "musicgen_available = False\n",
    "demucs_available = False\n",
    "\n",
    "try:\n",
    "    from audiocraft.models import MusicGen\n",
    "    musicgen_available = True\n",
    "    print(f\"audiocraft (MusicGen) disponible\")\n",
    "except ImportError:\n",
    "    print(\"audiocraft non installe - pip install audiocraft\")\n",
    "\n",
    "try:\n",
    "    import demucs\n",
    "    demucs_available = True\n",
    "    print(f\"demucs disponible\")\n",
    "except ImportError:\n",
    "    print(\"demucs non installe - pip install demucs\")\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from scipy import signal\n",
    "print(f\"pydub et scipy disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-intro",
   "metadata": {
    "papermill": {
     "duration": 0.002286,
     "end_time": "2026-02-18T09:52:35.855305",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.853019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 1 : Generation musicale avec MusicGen\n",
    "\n",
    "MusicGen genere de la musique a partir de descriptions textuelles. Le modele produit des fichiers audio de qualite studio.\n",
    "\n",
    "| Modele | Parametres | VRAM | Qualite | Duree max |\n",
    "|--------|-----------|------|---------|----------|\n",
    "| musicgen-small | 300M | ~4 GB | Correcte | 30s |\n",
    "| musicgen-medium | 1.5B | ~8 GB | Bonne | 30s |\n",
    "| musicgen-large | 3.3B | ~14 GB | Excellente | 30s |\n",
    "\n",
    "Le prompt textuel guide le style, l'instrumentation et l'ambiance de la musique generee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-musicgen-generation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:52:35.862068Z",
     "iopub.status.busy": "2026-02-18T09:52:35.861647Z",
     "iopub.status.idle": "2026-02-18T09:52:35.870462Z",
     "shell.execute_reply": "2026-02-18T09:52:35.869688Z"
    },
    "papermill": {
     "duration": 0.014047,
     "end_time": "2026-02-18T09:52:35.871744",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.857697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION MUSICALE - MUSICGEN\n",
      "==================================================\n",
      "MusicGen non disponible - installation requise : pip install audiocraft\n"
     ]
    }
   ],
   "source": [
    "# Generation musicale avec MusicGen\n",
    "print(\"GENERATION MUSICALE - MUSICGEN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "generated_tracks = {}\n",
    "\n",
    "# Descriptions pour differentes sections d'une composition\n",
    "track_descriptions = {\n",
    "    \"intro\": \"soft ambient piano with gentle strings, calm and peaceful, cinematic intro\",\n",
    "    \"verse\": \"upbeat electronic music with synth pads and light drums, energetic but not aggressive\",\n",
    "    \"chorus\": \"powerful orchestral music with drums and brass, epic and uplifting, full arrangement\"\n",
    "}\n",
    "\n",
    "if generate_audio and musicgen_available:\n",
    "    print(f\"Chargement du modele {musicgen_model}...\")\n",
    "    start_time = time.time()\n",
    "    mg_model = MusicGen.get_pretrained(musicgen_model)\n",
    "    mg_model.set_generation_params(duration=generation_duration)\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"Modele charge en {load_time:.1f}s\")\n",
    "\n",
    "    for section_name, description in track_descriptions.items():\n",
    "        print(f\"\\n--- Generation : {section_name} ---\")\n",
    "        print(f\"Prompt : {description}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        wav = mg_model.generate([description])\n",
    "        gen_time = time.time() - start_time\n",
    "\n",
    "        # Extraire le numpy array\n",
    "        audio_data = wav[0].cpu().numpy().squeeze()\n",
    "        sample_rate = mg_model.sample_rate\n",
    "\n",
    "        generated_tracks[section_name] = {\n",
    "            \"audio\": audio_data,\n",
    "            \"sr\": sample_rate,\n",
    "            \"duration\": len(audio_data) / sample_rate,\n",
    "            \"time\": gen_time,\n",
    "            \"description\": description\n",
    "        }\n",
    "\n",
    "        print(f\"Genere en {gen_time:.1f}s | Duree : {len(audio_data)/sample_rate:.1f}s | SR : {sample_rate}Hz\")\n",
    "        display(Audio(data=audio_data, rate=sample_rate, autoplay=False))\n",
    "\n",
    "        # Sauvegarder\n",
    "        if save_audio_files:\n",
    "            import soundfile as sf\n",
    "            filepath = OUTPUT_DIR / f\"musicgen_{section_name}.wav\"\n",
    "            sf.write(str(filepath), audio_data, sample_rate)\n",
    "            print(f\"Sauvegarde : {filepath.name}\")\n",
    "\n",
    "    # Recapitulatif\n",
    "    print(f\"\\nRecapitulatif des generations :\")\n",
    "    print(f\"{'Section':<12} {'Duree (s)':<12} {'Temps gen (s)':<15}\")\n",
    "    print(\"-\" * 39)\n",
    "    for name, data in generated_tracks.items():\n",
    "        print(f\"{name:<12} {data['duration']:<12.1f} {data['time']:<15.1f}\")\n",
    "\n",
    "    # Liberer la memoire GPU\n",
    "    del mg_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"\\nMemoire GPU liberee\")\n",
    "else:\n",
    "    if not musicgen_available:\n",
    "        print(\"MusicGen non disponible - installation requise : pip install audiocraft\")\n",
    "    else:\n",
    "        print(\"Generation desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-interpretation",
   "metadata": {
    "papermill": {
     "duration": 0.001969,
     "end_time": "2026-02-18T09:52:35.877573",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.875604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Generation MusicGen\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Temps de generation | ~2-5s par section (small) | Rapide meme pour de la musique complexe |\n",
    "| Qualite | 32kHz mono | Suffisante pour prototypage, pas pour production finale |\n",
    "| Controle | Via le prompt textuel | Le style depend fortement de la description |\n",
    "\n",
    "**Points cles** :\n",
    "1. Des prompts detailles (instrumentation, ambiance, tempo) ameliorent la qualite\n",
    "2. La generation est non-deterministe : chaque execution produit un resultat different\n",
    "3. Le modele `large` offre une qualite significativement superieure au `small`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-intro",
   "metadata": {
    "papermill": {
     "duration": 0.00241,
     "end_time": "2026-02-18T09:52:35.882059",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.879649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 2 : Separation de stems avec Demucs\n",
    "\n",
    "Demucs separe un mix musical en 4 stems :\n",
    "\n",
    "| Stem | Description | Utilisation remix |\n",
    "|------|-------------|-------------------|\n",
    "| `drums` | Batterie, percussions | Ajuster le rythme |\n",
    "| `bass` | Ligne de basse | Modifier les fondations |\n",
    "| `other` | Melodie, harmonie (synths, guitares) | Elements melodiques |\n",
    "| `vocals` | Voix (si presentes) | Isoler ou supprimer la voix |\n",
    "\n",
    "La separation permet de remixer chaque element independamment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-demucs-separation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:52:35.888836Z",
     "iopub.status.busy": "2026-02-18T09:52:35.888326Z",
     "iopub.status.idle": "2026-02-18T09:52:35.898062Z",
     "shell.execute_reply": "2026-02-18T09:52:35.897045Z"
    },
    "papermill": {
     "duration": 0.015075,
     "end_time": "2026-02-18T09:52:35.899515",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.884440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEPARATION DE STEMS - DEMUCS\n",
      "==================================================\n",
      "Demucs non disponible - pip install demucs\n"
     ]
    }
   ],
   "source": [
    "# Separation de stems avec Demucs\n",
    "print(\"SEPARATION DE STEMS - DEMUCS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "separated_stems = {}\n",
    "\n",
    "if generate_audio and demucs_available and generated_tracks:\n",
    "    # Utiliser le track \"chorus\" (le plus riche)\n",
    "    target_track = \"chorus\"\n",
    "    if target_track not in generated_tracks:\n",
    "        target_track = list(generated_tracks.keys())[0]\n",
    "\n",
    "    track_data = generated_tracks[target_track]\n",
    "    print(f\"Separation du track : {target_track}\")\n",
    "    print(f\"Duree : {track_data['duration']:.1f}s | SR : {track_data['sr']}Hz\")\n",
    "\n",
    "    # Sauvegarder le fichier pour Demucs\n",
    "    import soundfile as sf\n",
    "    input_path = OUTPUT_DIR / f\"demucs_input_{target_track}.wav\"\n",
    "    sf.write(str(input_path), track_data['audio'], track_data['sr'])\n",
    "\n",
    "    # Executer Demucs\n",
    "    print(f\"\\nExecution de Demucs ({demucs_model})...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        [\"python\", \"-m\", \"demucs\", \"--name\", demucs_model,\n",
    "         \"--out\", str(STEMS_DIR), str(input_path)],\n",
    "        capture_output=True, text=True, timeout=300\n",
    "    )\n",
    "\n",
    "    sep_time = time.time() - start_time\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Separation terminee en {sep_time:.1f}s\")\n",
    "\n",
    "        # Charger les stems\n",
    "        stems_path = STEMS_DIR / demucs_model / input_path.stem\n",
    "        stem_names = [\"drums\", \"bass\", \"other\", \"vocals\"]\n",
    "\n",
    "        print(f\"\\nStems separes :\")\n",
    "        for stem_name in stem_names:\n",
    "            stem_file = stems_path / f\"{stem_name}.wav\"\n",
    "            if stem_file.exists():\n",
    "                stem_audio, stem_sr = sf.read(str(stem_file))\n",
    "                # Si stereo, prendre la moyenne pour la lecture\n",
    "                if len(stem_audio.shape) > 1:\n",
    "                    stem_mono = stem_audio.mean(axis=1)\n",
    "                else:\n",
    "                    stem_mono = stem_audio\n",
    "\n",
    "                energy = np.sqrt(np.mean(stem_mono ** 2))\n",
    "                separated_stems[stem_name] = {\n",
    "                    \"audio\": stem_audio,\n",
    "                    \"mono\": stem_mono,\n",
    "                    \"sr\": stem_sr,\n",
    "                    \"energy\": energy,\n",
    "                    \"path\": stem_file\n",
    "                }\n",
    "\n",
    "                print(f\"  {stem_name:8s} | Energie RMS : {energy:.4f} | Shape : {stem_audio.shape}\")\n",
    "                display(Audio(data=stem_mono, rate=stem_sr, autoplay=False))\n",
    "            else:\n",
    "                print(f\"  {stem_name:8s} | Fichier non trouve\")\n",
    "    else:\n",
    "        print(f\"Erreur Demucs : {result.stderr[:200]}\")\n",
    "\n",
    "elif not demucs_available:\n",
    "    print(\"Demucs non disponible - pip install demucs\")\n",
    "elif not generated_tracks:\n",
    "    print(\"Pas de tracks generees - executez la Section 1 d'abord\")\n",
    "else:\n",
    "    print(\"Separation desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-interpretation",
   "metadata": {
    "papermill": {
     "duration": 0.005941,
     "end_time": "2026-02-18T09:52:35.910140",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.904199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Separation de stems\n",
    "\n",
    "| Stem | Energie RMS typique | Observation |\n",
    "|------|-------------------|-------------|\n",
    "| drums | Elevee | Batterie bien isolee si presente |\n",
    "| bass | Moyenne | Basses frequences extraites |\n",
    "| other | Variable | Contient melodie et harmonie |\n",
    "| vocals | Faible (musique instrumentale) | Vide si pas de voix dans l'original |\n",
    "\n",
    "> **Note technique** : Pour de la musique generee par MusicGen (instrumentale), le stem `vocals` sera quasi-vide. C'est normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-intro",
   "metadata": {
    "papermill": {
     "duration": 0.004544,
     "end_time": "2026-02-18T09:52:35.920237",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.915693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 3 : Remixage des stems\n",
    "\n",
    "Le remixage consiste a recombiner les stems avec des niveaux differents pour creer une nouvelle version.\n",
    "\n",
    "| Operation | Description | Parametre |\n",
    "|-----------|-------------|----------|\n",
    "| Volume | Ajuster le gain de chaque stem | dB |\n",
    "| Mute | Supprimer un stem | On/Off |\n",
    "| Solo | Isoler un stem | On/Off |\n",
    "| Pan | Position stereo | Gauche/Centre/Droite |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-remix",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:52:35.930348Z",
     "iopub.status.busy": "2026-02-18T09:52:35.929734Z",
     "iopub.status.idle": "2026-02-18T09:52:35.939743Z",
     "shell.execute_reply": "2026-02-18T09:52:35.938077Z"
    },
    "papermill": {
     "duration": 0.017716,
     "end_time": "2026-02-18T09:52:35.941669",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.923953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMIXAGE DES STEMS\n",
      "==================================================\n",
      "Pas de stems disponibles - executez la Section 2 d'abord\n"
     ]
    }
   ],
   "source": [
    "# Remixage des stems\n",
    "print(\"REMIXAGE DES STEMS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def remix_stems(stems: Dict, mix_config: Dict[str, float],\n",
    "                sample_rate: int) -> np.ndarray:\n",
    "    \"\"\"Remixe les stems avec les gains specifies.\n",
    "\n",
    "    Args:\n",
    "        stems: Dict {name: {\"mono\": np.array, ...}}\n",
    "        mix_config: Dict {name: gain_linear} (0.0 = mute, 1.0 = normal)\n",
    "        sample_rate: Taux d'echantillonnage\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Mix final\n",
    "    \"\"\"\n",
    "    # Determiner la longueur maximale\n",
    "    max_len = max(len(s['mono']) for s in stems.values())\n",
    "\n",
    "    mix = np.zeros(max_len, dtype=np.float32)\n",
    "\n",
    "    for stem_name, stem_data in stems.items():\n",
    "        gain = mix_config.get(stem_name, 1.0)\n",
    "        mono = stem_data['mono']\n",
    "\n",
    "        # Pad si necessaire\n",
    "        if len(mono) < max_len:\n",
    "            mono = np.pad(mono, (0, max_len - len(mono)))\n",
    "\n",
    "        mix += mono * gain\n",
    "\n",
    "    # Normalisation pour eviter le clipping\n",
    "    peak = np.max(np.abs(mix))\n",
    "    if peak > 0:\n",
    "        mix = mix / peak * 0.95\n",
    "\n",
    "    return mix\n",
    "\n",
    "if separated_stems:\n",
    "    sr = list(separated_stems.values())[0]['sr']\n",
    "\n",
    "    # Differentes configurations de mix\n",
    "    mix_configs = {\n",
    "        \"Original (equilibre)\": {\"drums\": 1.0, \"bass\": 1.0, \"other\": 1.0, \"vocals\": 1.0},\n",
    "        \"Instrumental (sans voix)\": {\"drums\": 1.0, \"bass\": 1.0, \"other\": 1.0, \"vocals\": 0.0},\n",
    "        \"Rythme only\": {\"drums\": 1.2, \"bass\": 0.8, \"other\": 0.0, \"vocals\": 0.0},\n",
    "        \"Melodique (sans batterie)\": {\"drums\": 0.0, \"bass\": 0.5, \"other\": 1.2, \"vocals\": 0.8},\n",
    "    }\n",
    "\n",
    "    remix_results = {}\n",
    "    for mix_name, config in mix_configs.items():\n",
    "        print(f\"\\n--- {mix_name} ---\")\n",
    "        config_str = \" | \".join([f\"{k}: {v:.1f}\" for k, v in config.items()])\n",
    "        print(f\"  Config : {config_str}\")\n",
    "\n",
    "        mix = remix_stems(separated_stems, config, sr)\n",
    "        remix_results[mix_name] = mix\n",
    "\n",
    "        display(Audio(data=mix, rate=sr, autoplay=False))\n",
    "\n",
    "        if save_audio_files:\n",
    "            import soundfile as sf\n",
    "            safe_name = mix_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "            filepath = OUTPUT_DIR / f\"remix_{safe_name}.wav\"\n",
    "            sf.write(str(filepath), mix, sr)\n",
    "            print(f\"  Sauvegarde : {filepath.name}\")\n",
    "\n",
    "    print(f\"\\n{len(remix_results)} versions de remix creees\")\n",
    "else:\n",
    "    print(\"Pas de stems disponibles - executez la Section 2 d'abord\")\n",
    "    remix_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-interpretation",
   "metadata": {
    "papermill": {
     "duration": 0.002533,
     "end_time": "2026-02-18T09:52:35.947039",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.944506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Remixage\n",
    "\n",
    "| Version | Drums | Bass | Other | Vocals | Caractere |\n",
    "|---------|-------|------|-------|--------|----------|\n",
    "| Original | 1.0 | 1.0 | 1.0 | 1.0 | Fidele au mix original |\n",
    "| Instrumental | 1.0 | 1.0 | 1.0 | 0.0 | Karaoke / fond musical |\n",
    "| Rythme only | 1.2 | 0.8 | 0.0 | 0.0 | Base rythmique |\n",
    "| Melodique | 0.0 | 0.5 | 1.2 | 0.8 | Ambiance melodique |\n",
    "\n",
    "**Points cles** :\n",
    "1. La normalisation previent le clipping lors de l'amplification\n",
    "2. Combiner separation et remixage ouvre des possibilites creatives infinies\n",
    "3. Les gains >1.0 amplifient, <1.0 attenuent, 0.0 coupe completement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-intro",
   "metadata": {
    "papermill": {
     "duration": 0.002102,
     "end_time": "2026-02-18T09:52:35.951546",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.949444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 4 : Effets audio\n",
    "\n",
    "Les effets audio transforment le caractere sonore d'un signal :\n",
    "\n",
    "| Effet | Description | Parametre principal |\n",
    "|-------|-------------|-------------------|\n",
    "| Reverb | Simulation d'espace (salle, cathedral) | Decay time |\n",
    "| EQ | Egalisation des frequences | Bandes de frequences |\n",
    "| Compression | Reduction de la dynamique | Ratio, threshold |\n",
    "| Fade | Fondu d'entree/sortie | Duree (ms) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-audio-effects",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:52:35.957998Z",
     "iopub.status.busy": "2026-02-18T09:52:35.957432Z",
     "iopub.status.idle": "2026-02-18T09:52:35.967776Z",
     "shell.execute_reply": "2026-02-18T09:52:35.967111Z"
    },
    "papermill": {
     "duration": 0.015457,
     "end_time": "2026-02-18T09:52:35.969089",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.953632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFETS AUDIO\n",
      "==================================================\n",
      "Effets desactives ou pas de tracks disponibles\n"
     ]
    }
   ],
   "source": [
    "# Application d'effets audio\n",
    "print(\"EFFETS AUDIO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def apply_reverb(audio: np.ndarray, sr: int, decay: float = 0.3,\n",
    "                 delay_ms: float = 40) -> np.ndarray:\n",
    "    \"\"\"Applique une reverb simple (delay + feedback).\"\"\"\n",
    "    delay_samples = int(sr * delay_ms / 1000)\n",
    "    output = np.copy(audio).astype(np.float64)\n",
    "\n",
    "    # Plusieurs taps de delay pour simuler les reflexions\n",
    "    for tap_mult in [1.0, 1.7, 2.3, 3.1]:\n",
    "        tap_delay = int(delay_samples * tap_mult)\n",
    "        tap_gain = decay ** tap_mult\n",
    "        if tap_delay < len(output):\n",
    "            output[tap_delay:] += audio[:len(audio) - tap_delay] * tap_gain\n",
    "\n",
    "    # Normalisation\n",
    "    peak = np.max(np.abs(output))\n",
    "    if peak > 0:\n",
    "        output = output / peak * 0.95\n",
    "\n",
    "    return output.astype(np.float32)\n",
    "\n",
    "def apply_eq(audio: np.ndarray, sr: int,\n",
    "             bass_gain: float = 1.0, mid_gain: float = 1.0,\n",
    "             treble_gain: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"EQ 3 bandes simple (basses, mediums, aigus).\"\"\"\n",
    "    from scipy.signal import butter, sosfilt\n",
    "\n",
    "    # Filtres passe-bande\n",
    "    sos_low = butter(4, 300, btype='low', fs=sr, output='sos')\n",
    "    sos_mid = butter(4, [300, 4000], btype='band', fs=sr, output='sos')\n",
    "    sos_high = butter(4, 4000, btype='high', fs=sr, output='sos')\n",
    "\n",
    "    bass = sosfilt(sos_low, audio) * bass_gain\n",
    "    mid = sosfilt(sos_mid, audio) * mid_gain\n",
    "    treble = sosfilt(sos_high, audio) * treble_gain\n",
    "\n",
    "    result = (bass + mid + treble).astype(np.float32)\n",
    "\n",
    "    peak = np.max(np.abs(result))\n",
    "    if peak > 0:\n",
    "        result = result / peak * 0.95\n",
    "\n",
    "    return result\n",
    "\n",
    "if generate_audio and apply_effects and generated_tracks:\n",
    "    # Utiliser le track \"verse\" pour les effets\n",
    "    target = \"verse\" if \"verse\" in generated_tracks else list(generated_tracks.keys())[0]\n",
    "    source_audio = generated_tracks[target]['audio']\n",
    "    sr = generated_tracks[target]['sr']\n",
    "\n",
    "    print(f\"Application des effets sur : {target}\")\n",
    "\n",
    "    effects_results = {}\n",
    "\n",
    "    # Reverb\n",
    "    print(f\"\\n--- Reverb (decay=0.4, delay=50ms) ---\")\n",
    "    reverbed = apply_reverb(source_audio, sr, decay=0.4, delay_ms=50)\n",
    "    effects_results[\"reverb\"] = reverbed\n",
    "    display(Audio(data=reverbed, rate=sr, autoplay=False))\n",
    "\n",
    "    # EQ : boost des basses\n",
    "    print(f\"\\n--- EQ (bass boost) ---\")\n",
    "    eq_bass = apply_eq(source_audio, sr, bass_gain=1.5, mid_gain=1.0, treble_gain=0.8)\n",
    "    effects_results[\"eq_bass_boost\"] = eq_bass\n",
    "    display(Audio(data=eq_bass, rate=sr, autoplay=False))\n",
    "\n",
    "    # EQ : voix claire (mid boost)\n",
    "    print(f\"\\n--- EQ (mid boost - clarte) ---\")\n",
    "    eq_mid = apply_eq(source_audio, sr, bass_gain=0.8, mid_gain=1.3, treble_gain=1.1)\n",
    "    effects_results[\"eq_clarity\"] = eq_mid\n",
    "    display(Audio(data=eq_mid, rate=sr, autoplay=False))\n",
    "\n",
    "    # Sauvegarder\n",
    "    if save_audio_files:\n",
    "        import soundfile as sf_lib\n",
    "        for name, audio in effects_results.items():\n",
    "            filepath = OUTPUT_DIR / f\"effect_{name}.wav\"\n",
    "            sf_lib.write(str(filepath), audio, sr)\n",
    "            print(f\"Sauvegarde : {filepath.name}\")\n",
    "\n",
    "    print(f\"\\n{len(effects_results)} effets appliques\")\n",
    "else:\n",
    "    print(\"Effets desactives ou pas de tracks disponibles\")\n",
    "    effects_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-intro",
   "metadata": {
    "papermill": {
     "duration": 0.002751,
     "end_time": "2026-02-18T09:52:35.974855",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.972104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 5 : Composition multi-sections\n",
    "\n",
    "Une composition complete suit une structure musicale standard :\n",
    "\n",
    "| Section | Duree typique | Caractere | Transition |\n",
    "|---------|--------------|-----------|------------|\n",
    "| Intro | 4-8s | Doux, progressif | Fade in |\n",
    "| Couplet (Verse) | 8-16s | Energie moyenne | Crossfade |\n",
    "| Refrain (Chorus) | 8-16s | Pleine energie | Crossfade |\n",
    "| Outro | 4-8s | Decrescendo | Fade out |\n",
    "\n",
    "Nous assemblons les sections generees en appliquant des transitions fluides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-multi-section",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:52:35.981373Z",
     "iopub.status.busy": "2026-02-18T09:52:35.980903Z",
     "iopub.status.idle": "2026-02-18T09:52:35.990636Z",
     "shell.execute_reply": "2026-02-18T09:52:35.989529Z"
    },
    "papermill": {
     "duration": 0.015173,
     "end_time": "2026-02-18T09:52:35.992385",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.977212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPOSITION MULTI-SECTIONS\n",
      "==================================================\n",
      "Composition desactivee (pas de tracks disponibles)\n"
     ]
    }
   ],
   "source": [
    "# Composition multi-sections avec transitions\n",
    "print(\"COMPOSITION MULTI-SECTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if generate_audio and generated_tracks:\n",
    "    # Structure de la composition\n",
    "    composition_structure = [\"intro\", \"verse\", \"chorus\", \"verse\", \"chorus\"]\n",
    "\n",
    "    # Determiner le sample rate\n",
    "    sr = list(generated_tracks.values())[0]['sr']\n",
    "\n",
    "    # Parametres de transition\n",
    "    fade_duration_ms = 500  # Duree du crossfade en ms\n",
    "    fade_samples = int(sr * fade_duration_ms / 1000)\n",
    "\n",
    "    print(f\"Structure : {' -> '.join(composition_structure)}\")\n",
    "    print(f\"Crossfade : {fade_duration_ms}ms ({fade_samples} samples)\")\n",
    "\n",
    "    # Construire la composition\n",
    "    composition = np.array([], dtype=np.float32)\n",
    "\n",
    "    for i, section_name in enumerate(composition_structure):\n",
    "        if section_name not in generated_tracks:\n",
    "            print(f\"  Section '{section_name}' non disponible, saut\")\n",
    "            continue\n",
    "\n",
    "        section_audio = generated_tracks[section_name]['audio'].copy()\n",
    "\n",
    "        # Fade in pour la premiere section\n",
    "        if i == 0:\n",
    "            fade_in = np.linspace(0, 1, min(fade_samples, len(section_audio)))\n",
    "            section_audio[:len(fade_in)] *= fade_in\n",
    "\n",
    "        # Fade out pour la derniere section\n",
    "        if i == len(composition_structure) - 1:\n",
    "            fade_out = np.linspace(1, 0, min(fade_samples, len(section_audio)))\n",
    "            section_audio[-len(fade_out):] *= fade_out\n",
    "\n",
    "        # Crossfade avec la section precedente\n",
    "        if i > 0 and len(composition) >= fade_samples:\n",
    "            # Zone de crossfade\n",
    "            xfade_len = min(fade_samples, len(section_audio))\n",
    "            fade_out_curve = np.linspace(1, 0, xfade_len)\n",
    "            fade_in_curve = np.linspace(0, 1, xfade_len)\n",
    "\n",
    "            # Appliquer le crossfade\n",
    "            composition[-xfade_len:] *= fade_out_curve\n",
    "            section_audio[:xfade_len] *= fade_in_curve\n",
    "            composition[-xfade_len:] += section_audio[:xfade_len]\n",
    "\n",
    "            # Ajouter le reste de la section\n",
    "            composition = np.concatenate([composition, section_audio[xfade_len:]])\n",
    "        else:\n",
    "            composition = np.concatenate([composition, section_audio])\n",
    "\n",
    "        print(f\"  [{i+1}] {section_name:8s} | {len(section_audio)/sr:.1f}s | Total : {len(composition)/sr:.1f}s\")\n",
    "\n",
    "    # Normalisation finale\n",
    "    peak = np.max(np.abs(composition))\n",
    "    if peak > 0:\n",
    "        composition = composition / peak * 0.95\n",
    "\n",
    "    print(f\"\\nComposition finale :\")\n",
    "    print(f\"  Duree : {len(composition)/sr:.1f}s\")\n",
    "    print(f\"  Sections : {len(composition_structure)}\")\n",
    "    print(f\"  Sample rate : {sr}Hz\")\n",
    "\n",
    "    display(Audio(data=composition, rate=sr, autoplay=False))\n",
    "\n",
    "    # Sauvegarder\n",
    "    if save_audio_files:\n",
    "        import soundfile as sf\n",
    "        filepath = OUTPUT_DIR / \"composition_finale.wav\"\n",
    "        sf.write(str(filepath), composition, sr)\n",
    "        print(f\"\\nSauvegarde : {filepath.name} ({filepath.stat().st_size/1024:.1f} KB)\")\n",
    "else:\n",
    "    composition = np.array([])\n",
    "    print(\"Composition desactivee (pas de tracks disponibles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section5-interpretation",
   "metadata": {
    "papermill": {
     "duration": 0.004235,
     "end_time": "2026-02-18T09:52:36.001364",
     "exception": false,
     "start_time": "2026-02-18T09:52:35.997129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Composition multi-sections\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Crossfade | 500ms | Transition fluide entre sections |\n",
    "| Fade in/out | 500ms | Entree/sortie progressive |\n",
    "| Normalisation | -0.5 dBFS (0.95) | Marge de securite anti-clipping |\n",
    "\n",
    "> **Note technique** : Pour une production plus avancee, utiliser des crossfades non-lineaires (courbes exponentielles) et des transitions musicalement coherentes (sur les temps forts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section6-intro",
   "metadata": {
    "papermill": {
     "duration": 0.004079,
     "end_time": "2026-02-18T09:52:36.010002",
     "exception": false,
     "start_time": "2026-02-18T09:52:36.005923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 6 : Export final avec metadonnees\n",
    "\n",
    "L'export final ajoute des metadonnees ID3 au fichier audio pour une utilisation professionnelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-export",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:52:36.020854Z",
     "iopub.status.busy": "2026-02-18T09:52:36.020294Z",
     "iopub.status.idle": "2026-02-18T09:52:36.026783Z",
     "shell.execute_reply": "2026-02-18T09:52:36.026110Z"
    },
    "papermill": {
     "duration": 0.013504,
     "end_time": "2026-02-18T09:52:36.028255",
     "exception": false,
     "start_time": "2026-02-18T09:52:36.014751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPORT FINAL\n",
      "==================================================\n",
      "Export desactive (pas de composition disponible)\n"
     ]
    }
   ],
   "source": [
    "# Export final avec metadonnees\n",
    "print(\"EXPORT FINAL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if generate_audio and len(composition) > 0:\n",
    "    sr = list(generated_tracks.values())[0]['sr']\n",
    "\n",
    "    # Convertir numpy en AudioSegment pour l'export avec metadonnees\n",
    "    # Normaliser en int16\n",
    "    audio_int16 = (composition * 32767).astype(np.int16)\n",
    "    audio_segment = AudioSegment(\n",
    "        data=audio_int16.tobytes(),\n",
    "        sample_width=2,\n",
    "        frame_rate=sr,\n",
    "        channels=1\n",
    "    )\n",
    "\n",
    "    # Export MP3 avec metadonnees\n",
    "    export_path = OUTPUT_DIR / \"production_finale.mp3\"\n",
    "    metadata = {\n",
    "        \"title\": \"Composition IA\",\n",
    "        \"artist\": \"MusicGen + Demucs Pipeline\",\n",
    "        \"album\": \"Cours Audio GenAI\",\n",
    "        \"date\": datetime.now().strftime(\"%Y\"),\n",
    "        \"genre\": \"Electronic\",\n",
    "        \"comment\": f\"Genere avec MusicGen ({musicgen_model}) et remixe avec Demucs ({demucs_model})\"\n",
    "    }\n",
    "\n",
    "    tags = {f\"-metadata {k}\": v for k, v in metadata.items()}\n",
    "    audio_segment.export(\n",
    "        str(export_path),\n",
    "        format=\"mp3\",\n",
    "        bitrate=\"256k\",\n",
    "        tags=metadata\n",
    "    )\n",
    "\n",
    "    print(f\"Fichier exporte : {export_path.name}\")\n",
    "    print(f\"Taille : {export_path.stat().st_size/1024:.1f} KB\")\n",
    "    print(f\"Bitrate : 256 kbps\")\n",
    "    print(f\"\\nMetadonnees :\")\n",
    "    for key, value in metadata.items():\n",
    "        print(f\"  {key:10s} : {value}\")\n",
    "\n",
    "    print(f\"\\nEcoute de la production finale :\")\n",
    "    display(Audio(filename=str(export_path)))\n",
    "else:\n",
    "    print(\"Export desactive (pas de composition disponible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-interactive",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:52:36.039309Z",
     "iopub.status.busy": "2026-02-18T09:52:36.038721Z",
     "iopub.status.idle": "2026-02-18T09:52:36.046857Z",
     "shell.execute_reply": "2026-02-18T09:52:36.046264Z"
    },
    "papermill": {
     "duration": 0.015271,
     "end_time": "2026-02-18T09:52:36.048320",
     "exception": false,
     "start_time": "2026-02-18T09:52:36.033049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE INTERACTIF - COMPOSITION PERSONNALISEE\n",
      "==================================================\n",
      "\n",
      "Decrivez le style de musique que vous souhaitez generer :\n",
      "(Laissez vide pour passer)\n",
      "Mode interactif non disponible (execution automatisee)\n"
     ]
    }
   ],
   "source": [
    "# Mode interactif - Composition personnalisee\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"MODE INTERACTIF - COMPOSITION PERSONNALISEE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nDecrivez le style de musique que vous souhaitez generer :\")\n",
    "    print(\"(Laissez vide pour passer)\")\n",
    "\n",
    "    try:\n",
    "        user_desc = input(\"\\nDescription musicale : \")\n",
    "\n",
    "        if user_desc.strip() and musicgen_available:\n",
    "            print(f\"\\nGeneration en cours...\")\n",
    "            mg_model = MusicGen.get_pretrained(musicgen_model)\n",
    "            mg_model.set_generation_params(duration=generation_duration)\n",
    "\n",
    "            wav = mg_model.generate([user_desc])\n",
    "            audio = wav[0].cpu().numpy().squeeze()\n",
    "            sr = mg_model.sample_rate\n",
    "\n",
    "            print(f\"Musique generee ({len(audio)/sr:.1f}s) :\")\n",
    "            display(Audio(data=audio, rate=sr, autoplay=False))\n",
    "\n",
    "            if save_audio_files:\n",
    "                import soundfile as sf\n",
    "                ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filepath = OUTPUT_DIR / f\"custom_{ts}.wav\"\n",
    "                sf.write(str(filepath), audio, sr)\n",
    "                print(f\"Sauvegarde : {filepath.name}\")\n",
    "\n",
    "            del mg_model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type} - {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-stats",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:52:36.056024Z",
     "iopub.status.busy": "2026-02-18T09:52:36.055826Z",
     "iopub.status.idle": "2026-02-18T09:52:36.061126Z",
     "shell.execute_reply": "2026-02-18T09:52:36.060355Z"
    },
    "papermill": {
     "duration": 0.009601,
     "end_time": "2026-02-18T09:52:36.062009",
     "exception": false,
     "start_time": "2026-02-18T09:52:36.052408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTIQUES DE SESSION\n",
      "==================================================\n",
      "Date : 2026-02-18 10:52:36\n",
      "Mode : interactive\n",
      "MusicGen : facebook/musicgen-small | Demucs : htdemucs\n",
      "Fichiers sauvegardes : 1 (0.0 KB)\n",
      "\n",
      "PROCHAINES ETAPES\n",
      "1. Synchroniser audio et video (04-4-Audio-Video-Sync)\n",
      "2. Explorer la serie Video (Video/01-Foundation)\n",
      "\n",
      "Notebook termine - 10:52:36\n"
     ]
    }
   ],
   "source": [
    "# Statistiques de session\n",
    "print(\"STATISTIQUES DE SESSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"MusicGen : {musicgen_model} | Demucs : {demucs_model}\")\n",
    "\n",
    "if generated_tracks:\n",
    "    total_gen_time = sum(t['time'] for t in generated_tracks.values())\n",
    "    total_duration = sum(t['duration'] for t in generated_tracks.values())\n",
    "    print(f\"Tracks generes : {len(generated_tracks)} ({total_duration:.1f}s total)\")\n",
    "    print(f\"Temps de generation total : {total_gen_time:.1f}s\")\n",
    "\n",
    "if separated_stems:\n",
    "    print(f\"Stems separes : {len(separated_stems)}\")\n",
    "\n",
    "if remix_results:\n",
    "    print(f\"Versions remixees : {len(remix_results)}\")\n",
    "\n",
    "if len(composition) > 0:\n",
    "    sr = list(generated_tracks.values())[0]['sr']\n",
    "    print(f\"Composition finale : {len(composition)/sr:.1f}s\")\n",
    "\n",
    "if save_audio_files:\n",
    "    saved_files = list(OUTPUT_DIR.glob('*'))\n",
    "    total_size = sum(f.stat().st_size for f in saved_files if f.is_file()) / 1024\n",
    "    print(f\"Fichiers sauvegardes : {len(saved_files)} ({total_size:.1f} KB)\")\n",
    "\n",
    "print(f\"\\nPROCHAINES ETAPES\")\n",
    "print(f\"1. Synchroniser audio et video (04-4-Audio-Video-Sync)\")\n",
    "print(f\"2. Explorer la serie Video (Video/01-Foundation)\")\n",
    "\n",
    "print(f\"\\nNotebook termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.473691,
   "end_time": "2026-02-18T09:52:36.628025",
   "environment_variables": {},
   "exception": null,
   "input_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Audio\\04-Applications\\04-3-Music-Composition-Workflow.ipynb",
   "output_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Audio\\04-Applications\\04-3-Music-Composition-Workflow.ipynb",
   "parameters": {},
   "start_time": "2026-02-18T09:52:32.154334",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}