{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Qwen Image Edit 2509 - √âdition Avanc√©e d'Images\n",
    "\n",
    "**Module :** 02-Images-Advanced  \n",
    "**Niveau :** Interm√©diaire/Avanc√©  \n",
    "**Dur√©e estim√©e :** 45 minutes  \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ce notebook explore les capacit√©s avanc√©es de **Qwen-Image-Edit 2509** (version Septembre 2025), un mod√®le d'√©dition d'images de pointe int√©gr√© via ComfyUI. Par rapport au notebook d'introduction (01-5), nous abordons ici :\n",
    "\n",
    "- **√âdition pr√©cise de texte** dans les images\n",
    "- **Inpainting avanc√©** avec masques personnalis√©s\n",
    "- **Workflows multi-√©tapes** pour des transformations complexes\n",
    "- **Batch processing** pour l'efficacit√©\n",
    "- **Analyse comparative** des param√®tres\n",
    "\n",
    "### Architecture Qwen-Image-Edit 2509\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              Qwen-Image-Edit 2509 Pipeline              ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Image Input ‚Üí Qwen2.5-VL Encoder ‚Üí Diffusion Model    ‚îÇ\n",
    "‚îÇ       ‚Üì              ‚Üì                    ‚Üì             ‚îÇ\n",
    "‚îÇ  [Tokenizer]    [16-ch VAE]        [UNet 1024¬≤]        ‚îÇ\n",
    "‚îÇ       ‚Üì              ‚Üì                    ‚Üì             ‚îÇ\n",
    "‚îÇ   Text Prompt ‚Üí Cross-Attention ‚Üí Latent Space         ‚îÇ\n",
    "‚îÇ                                        ‚Üì                ‚îÇ\n",
    "‚îÇ                               VAE Decode ‚Üí Output      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Pr√©requis\n",
    "\n",
    "- Module 00-GenAI-Environment compl√©t√©\n",
    "- Service `comfyui-qwen` actif (`docker compose up -d`)\n",
    "- Notebook 01-5-Qwen-Image-Edit termin√© (concepts de base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. CONFIGURATION ET IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import base64\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chargement variables d'environnement\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "load_dotenv(\"../00-GenAI-Environment/.env\")\n",
    "\n",
    "# Configuration ComfyUI\n",
    "COMFYUI_URL = os.getenv(\"COMFYUI_API_URL\", \"http://localhost:8188\")\n",
    "COMFYUI_TOKEN = os.getenv(\"COMFYUI_AUTH_TOKEN\")\n",
    "CLIENT_ID = str(uuid.uuid4())\n",
    "\n",
    "# Validation\n",
    "if not COMFYUI_TOKEN:\n",
    "    raise ValueError(\"‚ùå COMFYUI_AUTH_TOKEN manquant dans .env\")\n",
    "\n",
    "print(\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\")\n",
    "print(\"‚ïë   Qwen-Image-Edit 2509 - √âdition Avanc√©e          ‚ïë\")\n",
    "print(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\")\n",
    "print(f\"\\nüìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üîó API URL: {COMFYUI_URL}\")\n",
    "print(f\"üîë Token: {'‚úÖ Configur√©' if COMFYUI_TOKEN else '‚ùå Manquant'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. CLIENT COMFYUI AVANC√â\n",
    "# =============================================================================\n",
    "\n",
    "class QwenImageEditClient:\n",
    "    \"\"\"\n",
    "    Client avanc√© pour Qwen-Image-Edit via ComfyUI.\n",
    "    Supporte l'authentification, le batch processing et les workflows multi-√©tapes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = COMFYUI_URL, auth_token: str = COMFYUI_TOKEN):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.client_id = str(uuid.uuid4())\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        if auth_token:\n",
    "            self.session.headers.update({\"Authorization\": f\"Bearer {auth_token}\"})\n",
    "    \n",
    "    def check_health(self) -> Dict[str, Any]:\n",
    "        \"\"\"V√©rifie la sant√© du service ComfyUI.\"\"\"\n",
    "        try:\n",
    "            resp = self.session.get(f\"{self.base_url}/system_stats\", timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                stats = resp.json()\n",
    "                return {\n",
    "                    \"status\": \"healthy\",\n",
    "                    \"vram_free\": stats.get(\"devices\", [{}])[0].get(\"vram_free\", 0) / 1e9,\n",
    "                    \"vram_total\": stats.get(\"devices\", [{}])[0].get(\"vram_total\", 0) / 1e9\n",
    "                }\n",
    "            return {\"status\": \"error\", \"code\": resp.status_code}\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"unreachable\", \"error\": str(e)}\n",
    "    \n",
    "    def upload_image(self, image: Image.Image, name: str = \"input.png\") -> str:\n",
    "        \"\"\"Upload une image vers ComfyUI.\"\"\"\n",
    "        buffer = BytesIO()\n",
    "        image.save(buffer, format=\"PNG\")\n",
    "        buffer.seek(0)\n",
    "        \n",
    "        files = {\"image\": (name, buffer, \"image/png\")}\n",
    "        data = {\"overwrite\": \"true\"}\n",
    "        \n",
    "        resp = self.session.post(f\"{self.base_url}/upload/image\", files=files, data=data)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.json().get(\"name\", name)\n",
    "        raise Exception(f\"Upload failed: {resp.text}\")\n",
    "    \n",
    "    def upload_mask(self, mask: Image.Image, name: str = \"mask.png\") -> str:\n",
    "        \"\"\"Upload un masque pour l'inpainting.\"\"\"\n",
    "        # Convertir en grayscale si n√©cessaire\n",
    "        if mask.mode != 'L':\n",
    "            mask = mask.convert('L')\n",
    "        return self.upload_image(mask, name)\n",
    "    \n",
    "    def queue_workflow(self, workflow: Dict) -> str:\n",
    "        \"\"\"Soumet un workflow et retourne le prompt_id.\"\"\"\n",
    "        payload = {\"prompt\": workflow, \"client_id\": self.client_id}\n",
    "        resp = self.session.post(f\"{self.base_url}/prompt\", json=payload)\n",
    "        \n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Queue failed: {resp.text}\")\n",
    "        return resp.json()[\"prompt_id\"]\n",
    "    \n",
    "    def wait_for_completion(self, prompt_id: str, timeout: int = 120) -> Dict:\n",
    "        \"\"\"Attend la fin d'un workflow avec timeout.\"\"\"\n",
    "        start = time.time()\n",
    "        while time.time() - start < timeout:\n",
    "            resp = self.session.get(f\"{self.base_url}/history/{prompt_id}\")\n",
    "            if resp.status_code == 200:\n",
    "                history = resp.json()\n",
    "                if prompt_id in history:\n",
    "                    return history[prompt_id]\n",
    "            time.sleep(1)\n",
    "        raise TimeoutError(f\"Workflow {prompt_id} timeout after {timeout}s\")\n",
    "    \n",
    "    def get_image(self, filename: str, subfolder: str = \"\", img_type: str = \"output\") -> Image.Image:\n",
    "        \"\"\"R√©cup√®re une image g√©n√©r√©e.\"\"\"\n",
    "        params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": img_type}\n",
    "        resp = self.session.get(f\"{self.base_url}/view\", params=params)\n",
    "        if resp.status_code == 200:\n",
    "            return Image.open(BytesIO(resp.content))\n",
    "        raise Exception(f\"Failed to get image: {resp.text}\")\n",
    "    \n",
    "    def execute_and_get_images(self, workflow: Dict, output_node: str = \"9\", \n",
    "                                timeout: int = 120, verbose: bool = True) -> List[Image.Image]:\n",
    "        \"\"\"Ex√©cute un workflow et retourne les images g√©n√©r√©es.\"\"\"\n",
    "        if verbose:\n",
    "            print(\"üöÄ Soumission du workflow...\")\n",
    "        \n",
    "        prompt_id = self.queue_workflow(workflow)\n",
    "        if verbose:\n",
    "            print(f\"üìã ID: {prompt_id}\")\n",
    "            print(\"‚è≥ G√©n√©ration en cours...\", end=\"\", flush=True)\n",
    "        \n",
    "        result = self.wait_for_completion(prompt_id, timeout)\n",
    "        if verbose:\n",
    "            print(\" ‚úÖ\")\n",
    "        \n",
    "        # Extraire les images\n",
    "        images = []\n",
    "        if \"outputs\" in result and output_node in result[\"outputs\"]:\n",
    "            for img_data in result[\"outputs\"][output_node].get(\"images\", []):\n",
    "                img = self.get_image(\n",
    "                    img_data[\"filename\"],\n",
    "                    img_data.get(\"subfolder\", \"\"),\n",
    "                    img_data.get(\"type\", \"output\")\n",
    "                )\n",
    "                images.append(img)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üñºÔ∏è {len(images)} image(s) r√©cup√©r√©e(s)\")\n",
    "        return images\n",
    "\n",
    "# Instanciation du client\n",
    "client = QwenImageEditClient()\n",
    "\n",
    "# Test de connexion\n",
    "health = client.check_health()\n",
    "print(f\"\\nüè• √âtat du service: {health['status']}\")\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"   VRAM: {health['vram_free']:.1f} / {health['vram_total']:.1f} GB libre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. WORKFLOWS AVANC√âS QWEN-IMAGE-EDIT 2509\n",
    "# =============================================================================\n",
    "\n",
    "def create_text2img_workflow(prompt: str, negative_prompt: str = \"\",\n",
    "                              width: int = 1024, height: int = 1024,\n",
    "                              steps: int = 25, cfg: float = 7.0,\n",
    "                              seed: int = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Workflow Text-to-Image avec Qwen-Image-Edit 2509.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Description de l'image √† g√©n√©rer\n",
    "        negative_prompt: √âl√©ments √† √©viter\n",
    "        width/height: Dimensions (multiples de 8)\n",
    "        steps: Nombre d'√©tapes de diffusion (20-50)\n",
    "        cfg: Guidance scale (5-15)\n",
    "        seed: Graine pour reproductibilit√©\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(0, 2**32)\n",
    "    \n",
    "    if not negative_prompt:\n",
    "        negative_prompt = \"blurry, low quality, watermark, text overlay, deformed, ugly\"\n",
    "    \n",
    "    return {\n",
    "        \"4\": {\n",
    "            \"class_type\": \"CheckpointLoaderSimple\",\n",
    "            \"inputs\": {\"ckpt_name\": \"qwen_image_edit_2509.safetensors\"}\n",
    "        },\n",
    "        \"5\": {\n",
    "            \"class_type\": \"EmptyLatentImage\",\n",
    "            \"inputs\": {\"width\": width, \"height\": height, \"batch_size\": 1}\n",
    "        },\n",
    "        \"6\": {\n",
    "            \"class_type\": \"CLIPTextEncode\",\n",
    "            \"inputs\": {\"text\": prompt, \"clip\": [\"4\", 1]}\n",
    "        },\n",
    "        \"7\": {\n",
    "            \"class_type\": \"CLIPTextEncode\",\n",
    "            \"inputs\": {\"text\": negative_prompt, \"clip\": [\"4\", 1]}\n",
    "        },\n",
    "        \"3\": {\n",
    "            \"class_type\": \"KSampler\",\n",
    "            \"inputs\": {\n",
    "                \"seed\": seed,\n",
    "                \"steps\": steps,\n",
    "                \"cfg\": cfg,\n",
    "                \"sampler_name\": \"euler_ancestral\",\n",
    "                \"scheduler\": \"normal\",\n",
    "                \"denoise\": 1.0,\n",
    "                \"model\": [\"4\", 0],\n",
    "                \"positive\": [\"6\", 0],\n",
    "                \"negative\": [\"7\", 0],\n",
    "                \"latent_image\": [\"5\", 0]\n",
    "            }\n",
    "        },\n",
    "        \"8\": {\n",
    "            \"class_type\": \"VAEDecode\",\n",
    "            \"inputs\": {\"samples\": [\"3\", 0], \"vae\": [\"4\", 2]}\n",
    "        },\n",
    "        \"9\": {\n",
    "            \"class_type\": \"SaveImage\",\n",
    "            \"inputs\": {\"filename_prefix\": \"Qwen2509_t2i\", \"images\": [\"8\", 0]}\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def create_img2img_workflow(image_name: str, prompt: str, \n",
    "                            negative_prompt: str = \"\",\n",
    "                            denoise: float = 0.7, steps: int = 25,\n",
    "                            cfg: float = 7.0, seed: int = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Workflow Image-to-Image pour l'√©dition.\n",
    "    \n",
    "    Args:\n",
    "        image_name: Nom du fichier image upload√©\n",
    "        prompt: Instructions d'√©dition\n",
    "        denoise: Force de l'√©dition (0.0=rien, 1.0=r√©g√©n√©ration compl√®te)\n",
    "                 0.3-0.5: Ajustements subtils\n",
    "                 0.5-0.7: Modifications mod√©r√©es\n",
    "                 0.7-0.9: Transformations significatives\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(0, 2**32)\n",
    "    \n",
    "    if not negative_prompt:\n",
    "        negative_prompt = \"blurry, low quality, artifacts, distorted\"\n",
    "    \n",
    "    return {\n",
    "        \"1\": {\n",
    "            \"class_type\": \"LoadImage\",\n",
    "            \"inputs\": {\"image\": image_name}\n",
    "        },\n",
    "        \"4\": {\n",
    "            \"class_type\": \"CheckpointLoaderSimple\",\n",
    "            \"inputs\": {\"ckpt_name\": \"qwen_image_edit_2509.safetensors\"}\n",
    "        },\n",
    "        \"5\": {\n",
    "            \"class_type\": \"VAEEncode\",\n",
    "            \"inputs\": {\"pixels\": [\"1\", 0], \"vae\": [\"4\", 2]}\n",
    "        },\n",
    "        \"6\": {\n",
    "            \"class_type\": \"CLIPTextEncode\",\n",
    "            \"inputs\": {\"text\": prompt, \"clip\": [\"4\", 1]}\n",
    "        },\n",
    "        \"7\": {\n",
    "            \"class_type\": \"CLIPTextEncode\",\n",
    "            \"inputs\": {\"text\": negative_prompt, \"clip\": [\"4\", 1]}\n",
    "        },\n",
    "        \"3\": {\n",
    "            \"class_type\": \"KSampler\",\n",
    "            \"inputs\": {\n",
    "                \"seed\": seed,\n",
    "                \"steps\": steps,\n",
    "                \"cfg\": cfg,\n",
    "                \"sampler_name\": \"euler_ancestral\",\n",
    "                \"scheduler\": \"normal\",\n",
    "                \"denoise\": denoise,\n",
    "                \"model\": [\"4\", 0],\n",
    "                \"positive\": [\"6\", 0],\n",
    "                \"negative\": [\"7\", 0],\n",
    "                \"latent_image\": [\"5\", 0]\n",
    "            }\n",
    "        },\n",
    "        \"8\": {\n",
    "            \"class_type\": \"VAEDecode\",\n",
    "            \"inputs\": {\"samples\": [\"3\", 0], \"vae\": [\"4\", 2]}\n",
    "        },\n",
    "        \"9\": {\n",
    "            \"class_type\": \"SaveImage\",\n",
    "            \"inputs\": {\"filename_prefix\": \"Qwen2509_i2i\", \"images\": [\"8\", 0]}\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def create_inpaint_workflow(image_name: str, mask_name: str, prompt: str,\n",
    "                            negative_prompt: str = \"\",\n",
    "                            denoise: float = 0.9, steps: int = 30,\n",
    "                            cfg: float = 7.5, seed: int = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Workflow Inpainting avec masque personnalis√©.\n",
    "    \n",
    "    Le masque doit √™tre en niveaux de gris:\n",
    "    - Blanc (255): Zone √† r√©g√©n√©rer\n",
    "    - Noir (0): Zone √† pr√©server\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(0, 2**32)\n",
    "    \n",
    "    if not negative_prompt:\n",
    "        negative_prompt = \"blurry, low quality, artifacts, seams, visible mask edges\"\n",
    "    \n",
    "    return {\n",
    "        \"1\": {\n",
    "            \"class_type\": \"LoadImage\",\n",
    "            \"inputs\": {\"image\": image_name}\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"class_type\": \"LoadImage\",\n",
    "            \"inputs\": {\"image\": mask_name}\n",
    "        },\n",
    "        \"4\": {\n",
    "            \"class_type\": \"CheckpointLoaderSimple\",\n",
    "            \"inputs\": {\"ckpt_name\": \"qwen_image_edit_2509.safetensors\"}\n",
    "        },\n",
    "        \"10\": {\n",
    "            \"class_type\": \"ImageToMask\",\n",
    "            \"inputs\": {\"image\": [\"2\", 0], \"channel\": \"red\"}\n",
    "        },\n",
    "        \"5\": {\n",
    "            \"class_type\": \"VAEEncodeForInpaint\",\n",
    "            \"inputs\": {\n",
    "                \"pixels\": [\"1\", 0],\n",
    "                \"vae\": [\"4\", 2],\n",
    "                \"mask\": [\"10\", 0],\n",
    "                \"grow_mask_by\": 6\n",
    "            }\n",
    "        },\n",
    "        \"6\": {\n",
    "            \"class_type\": \"CLIPTextEncode\",\n",
    "            \"inputs\": {\"text\": prompt, \"clip\": [\"4\", 1]}\n",
    "        },\n",
    "        \"7\": {\n",
    "            \"class_type\": \"CLIPTextEncode\",\n",
    "            \"inputs\": {\"text\": negative_prompt, \"clip\": [\"4\", 1]}\n",
    "        },\n",
    "        \"3\": {\n",
    "            \"class_type\": \"KSampler\",\n",
    "            \"inputs\": {\n",
    "                \"seed\": seed,\n",
    "                \"steps\": steps,\n",
    "                \"cfg\": cfg,\n",
    "                \"sampler_name\": \"euler_ancestral\",\n",
    "                \"scheduler\": \"normal\",\n",
    "                \"denoise\": denoise,\n",
    "                \"model\": [\"4\", 0],\n",
    "                \"positive\": [\"6\", 0],\n",
    "                \"negative\": [\"7\", 0],\n",
    "                \"latent_image\": [\"5\", 0]\n",
    "            }\n",
    "        },\n",
    "        \"8\": {\n",
    "            \"class_type\": \"VAEDecode\",\n",
    "            \"inputs\": {\"samples\": [\"3\", 0], \"vae\": [\"4\", 2]}\n",
    "        },\n",
    "        \"9\": {\n",
    "            \"class_type\": \"SaveImage\",\n",
    "            \"inputs\": {\"filename_prefix\": \"Qwen2509_inpaint\", \"images\": [\"8\", 0]}\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Workflows Qwen-Image-Edit 2509 d√©finis\")\n",
    "print(\"   - create_text2img_workflow()\")\n",
    "print(\"   - create_img2img_workflow()\")\n",
    "print(\"   - create_inpaint_workflow()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 4. G√©n√©ration Text-to-Image\n",
    "\n",
    "Commen√ßons par g√©n√©rer une image de base que nous √©diterons ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. TEXT-TO-IMAGE: Cr√©ation d'une image de base\n",
    "# =============================================================================\n",
    "\n",
    "# Prompt cr√©atif pour une sc√®ne √©ditable\n",
    "prompt_base = \"\"\"\n",
    "A cozy coffee shop interior, wooden tables, warm lighting,\n",
    "large window with rain outside, vintage aesthetic,\n",
    "empty cup on table, potted plant, high quality photography\n",
    "\"\"\".strip()\n",
    "\n",
    "# Cr√©er le workflow\n",
    "workflow_t2i = create_text2img_workflow(\n",
    "    prompt=prompt_base,\n",
    "    width=1024,\n",
    "    height=768,\n",
    "    steps=30,\n",
    "    cfg=7.5,\n",
    "    seed=42  # Fix√© pour reproductibilit√©\n",
    ")\n",
    "\n",
    "# Ex√©cuter\n",
    "print(\"\\nüì∏ G√©n√©ration de l'image de base...\")\n",
    "print(f\"Prompt: {prompt_base[:80]}...\")\n",
    "\n",
    "images_t2i = client.execute_and_get_images(workflow_t2i, output_node=\"9\")\n",
    "\n",
    "if images_t2i:\n",
    "    base_image = images_t2i[0]\n",
    "    \n",
    "    # Affichage\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.imshow(base_image)\n",
    "    plt.title(\"Image de Base - Coffee Shop\", fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìê Dimensions: {base_image.size}\")\n",
    "else:\n",
    "    print(\"‚ùå Erreur de g√©n√©ration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 5. √âdition Image-to-Image\n",
    "\n",
    "Explorons diff√©rents niveaux de `denoise` pour comprendre son impact sur l'√©dition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. IMAGE-TO-IMAGE: Analyse comparative du param√®tre denoise\n",
    "# =============================================================================\n",
    "\n",
    "# Upload de l'image de base\n",
    "if 'base_image' in dir():\n",
    "    uploaded_name = client.upload_image(base_image, \"base_coffee_shop.png\")\n",
    "    print(f\"‚úÖ Image upload√©e: {uploaded_name}\")\n",
    "    \n",
    "    # Test avec diff√©rents niveaux de denoise\n",
    "    denoise_levels = [0.3, 0.5, 0.7, 0.9]\n",
    "    edit_prompt = \"Same scene but with snow falling outside the window, winter atmosphere\"\n",
    "    \n",
    "    results = []\n",
    "    seed_fixed = 12345  # M√™me seed pour comparer\n",
    "    \n",
    "    print(f\"\\nüé® √âdition avec prompt: '{edit_prompt}'\")\n",
    "    print(\"\\nComparaison des niveaux de denoise:\")\n",
    "    \n",
    "    for denoise in denoise_levels:\n",
    "        print(f\"\\n--- Denoise = {denoise} ---\")\n",
    "        \n",
    "        workflow = create_img2img_workflow(\n",
    "            image_name=uploaded_name,\n",
    "            prompt=edit_prompt,\n",
    "            denoise=denoise,\n",
    "            steps=25,\n",
    "            cfg=7.0,\n",
    "            seed=seed_fixed\n",
    "        )\n",
    "        \n",
    "        images = client.execute_and_get_images(workflow, verbose=False)\n",
    "        if images:\n",
    "            results.append((denoise, images[0]))\n",
    "            print(f\"   ‚úÖ G√©n√©r√©\")\n",
    "    \n",
    "    # Affichage comparatif\n",
    "    if results:\n",
    "        fig, axes = plt.subplots(1, len(results) + 1, figsize=(20, 5))\n",
    "        \n",
    "        # Image originale\n",
    "        axes[0].imshow(base_image)\n",
    "        axes[0].set_title(\"Original\", fontsize=12)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # R√©sultats\n",
    "        for i, (denoise, img) in enumerate(results):\n",
    "            axes[i+1].imshow(img)\n",
    "            axes[i+1].set_title(f\"Denoise = {denoise}\", fontsize=12)\n",
    "            axes[i+1].axis('off')\n",
    "        \n",
    "        plt.suptitle(\"Impact du param√®tre Denoise sur l'√©dition\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nüìä Observations:\")\n",
    "        print(\"   0.3: Changements subtils, structure tr√®s pr√©serv√©e\")\n",
    "        print(\"   0.5: Modifications visibles, bonne balance\")\n",
    "        print(\"   0.7: Transformations significatives\")\n",
    "        print(\"   0.9: Quasi-r√©g√©n√©ration, peu de l'original conserv√©\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ex√©cutez d'abord la cellule Text-to-Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 6. Inpainting Avanc√© avec Masque Personnalis√©\n",
    "\n",
    "L'inpainting permet de modifier uniquement certaines zones de l'image. Nous allons cr√©er un masque programmatique pour remplacer un √©l√©ment sp√©cifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. INPAINTING: √âdition localis√©e avec masque\n",
    "# =============================================================================\n",
    "\n",
    "def create_rectangular_mask(width: int, height: int, \n",
    "                            x1: int, y1: int, x2: int, y2: int,\n",
    "                            feather: int = 10) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Cr√©e un masque rectangulaire avec bords adoucis.\n",
    "    \n",
    "    Args:\n",
    "        width, height: Dimensions du masque\n",
    "        x1, y1, x2, y2: Coordonn√©es du rectangle (zone √† modifier)\n",
    "        feather: Adoucissement des bords en pixels\n",
    "    \n",
    "    Returns:\n",
    "        Image grayscale (blanc = zone √† modifier)\n",
    "    \"\"\"\n",
    "    mask = Image.new('L', (width, height), 0)  # Noir = pr√©server\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    draw.rectangle([x1, y1, x2, y2], fill=255)  # Blanc = modifier\n",
    "    \n",
    "    # Adoucissement optionnel (blur simple)\n",
    "    if feather > 0:\n",
    "        from PIL import ImageFilter\n",
    "        mask = mask.filter(ImageFilter.GaussianBlur(radius=feather))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_circular_mask(width: int, height: int,\n",
    "                         cx: int, cy: int, radius: int,\n",
    "                         feather: int = 15) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Cr√©e un masque circulaire pour l'inpainting.\n",
    "    \"\"\"\n",
    "    mask = Image.new('L', (width, height), 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    draw.ellipse([cx-radius, cy-radius, cx+radius, cy+radius], fill=255)\n",
    "    \n",
    "    if feather > 0:\n",
    "        from PIL import ImageFilter\n",
    "        mask = mask.filter(ImageFilter.GaussianBlur(radius=feather))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "# Exemple: Remplacer la tasse sur la table\n",
    "if 'base_image' in dir():\n",
    "    w, h = base_image.size\n",
    "    \n",
    "    # Cr√©er un masque pour le centre-bas de l'image (o√π la table/tasse serait)\n",
    "    mask = create_rectangular_mask(\n",
    "        w, h,\n",
    "        x1=int(w*0.35), y1=int(h*0.55),\n",
    "        x2=int(w*0.65), y2=int(h*0.85),\n",
    "        feather=20\n",
    "    )\n",
    "    \n",
    "    # Upload du masque\n",
    "    mask_name = client.upload_mask(mask, \"edit_mask.png\")\n",
    "    print(f\"‚úÖ Masque upload√©: {mask_name}\")\n",
    "    \n",
    "    # Visualisation du masque\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(base_image)\n",
    "    axes[0].set_title(\"Image Originale\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(mask, cmap='gray')\n",
    "    axes[1].set_title(\"Masque (blanc = zone √† modifier)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = base_image.copy().convert('RGBA')\n",
    "    mask_rgba = Image.new('RGBA', overlay.size, (255, 0, 0, 0))\n",
    "    mask_draw = ImageDraw.Draw(mask_rgba)\n",
    "    # Convertir le masque en overlay rouge semi-transparent\n",
    "    mask_array = np.array(mask)\n",
    "    red_overlay = np.zeros((h, w, 4), dtype=np.uint8)\n",
    "    red_overlay[:,:,0] = 255  # Rouge\n",
    "    red_overlay[:,:,3] = (mask_array * 0.5).astype(np.uint8)  # Alpha\n",
    "    overlay_img = Image.alpha_composite(overlay, Image.fromarray(red_overlay))\n",
    "    \n",
    "    axes[2].imshow(overlay_img)\n",
    "    axes[2].set_title(\"Zone d'√©dition (rouge)\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Image de base non disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6b. EX√âCUTION DE L'INPAINTING\n",
    "# =============================================================================\n",
    "\n",
    "if 'uploaded_name' in dir() and 'mask_name' in dir():\n",
    "    # Prompt pour la zone masqu√©e\n",
    "    inpaint_prompt = \"A beautiful laptop with glowing screen, modern design, on wooden table\"\n",
    "    \n",
    "    print(f\"\\nüé® Inpainting: '{inpaint_prompt}'\")\n",
    "    \n",
    "    workflow_inpaint = create_inpaint_workflow(\n",
    "        image_name=uploaded_name,\n",
    "        mask_name=mask_name,\n",
    "        prompt=inpaint_prompt,\n",
    "        denoise=0.95,  # Haut pour remplacer compl√®tement\n",
    "        steps=35,\n",
    "        cfg=8.0,\n",
    "        seed=99999\n",
    "    )\n",
    "    \n",
    "    images_inpaint = client.execute_and_get_images(workflow_inpaint)\n",
    "    \n",
    "    if images_inpaint:\n",
    "        inpainted_image = images_inpaint[0]\n",
    "        \n",
    "        # Comparaison avant/apr√®s\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "        \n",
    "        axes[0].imshow(base_image)\n",
    "        axes[0].set_title(\"Avant Inpainting\", fontsize=14)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(inpainted_image)\n",
    "        axes[1].set_title(\"Apr√®s Inpainting\", fontsize=14)\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.suptitle(f\"Inpainting: '{inpaint_prompt}'\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pr√©requis manquants (image ou masque)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 7. Batch Processing: G√©n√©ration Multiple\n",
    "\n",
    "Pour l'efficacit√©, g√©n√©rons plusieurs variations en parall√®le avec diff√©rents prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. BATCH PROCESSING: Variations multiples\n",
    "# =============================================================================\n",
    "\n",
    "def batch_generate(prompts: List[str], base_seed: int = 1000, **kwargs) -> List[Tuple[str, Image.Image]]:\n",
    "    \"\"\"\n",
    "    G√©n√®re plusieurs images √† partir d'une liste de prompts.\n",
    "    \n",
    "    Args:\n",
    "        prompts: Liste de descriptions\n",
    "        base_seed: Seed de d√©part (incr√©ment√© pour chaque image)\n",
    "        **kwargs: Arguments pass√©s √† create_text2img_workflow\n",
    "    \n",
    "    Returns:\n",
    "        Liste de tuples (prompt, image)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\n[{i+1}/{len(prompts)}] G√©n√©ration...\")\n",
    "        print(f\"   Prompt: {prompt[:60]}...\")\n",
    "        \n",
    "        workflow = create_text2img_workflow(\n",
    "            prompt=prompt,\n",
    "            seed=base_seed + i,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        images = client.execute_and_get_images(workflow, verbose=False)\n",
    "        if images:\n",
    "            results.append((prompt, images[0]))\n",
    "            print(f\"   ‚úÖ Succ√®s\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå √âchec\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# G√©n√©ration de variations th√©matiques\n",
    "variation_prompts = [\n",
    "    \"A futuristic cityscape at sunset, flying cars, neon lights, cyberpunk style\",\n",
    "    \"An ancient Japanese temple in autumn, red maple leaves, misty mountains\",\n",
    "    \"An underwater coral reef, tropical fish, sunlight rays, crystal clear water\",\n",
    "    \"A cozy library interior, tall bookshelves, reading nook, warm lamp light\"\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Batch Generation - 4 Th√®mes\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "batch_results = batch_generate(\n",
    "    variation_prompts,\n",
    "    base_seed=2024,\n",
    "    width=768,\n",
    "    height=768,\n",
    "    steps=25,\n",
    "    cfg=7.5\n",
    ")\n",
    "\n",
    "# Affichage grille\n",
    "if batch_results:\n",
    "    n = len(batch_results)\n",
    "    cols = 2\n",
    "    rows = (n + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (prompt, img) in enumerate(batch_results):\n",
    "        axes[i].imshow(img)\n",
    "        # Titre court\n",
    "        short_title = prompt.split(',')[0][:40]\n",
    "        axes[i].set_title(short_title, fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Masquer les axes vides\n",
    "    for i in range(len(batch_results), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Batch Generation - Variations Th√©matiques\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ {len(batch_results)} images g√©n√©r√©es avec succ√®s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 8. Analyse Comparative: CFG Scale\n",
    "\n",
    "Le param√®tre `cfg` (Classifier-Free Guidance) contr√¥le l'adh√©rence au prompt. Explorons son impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. ANALYSE CFG: Impact du Guidance Scale\n",
    "# =============================================================================\n",
    "\n",
    "test_prompt = \"A majestic dragon breathing fire, fantasy art, highly detailed scales, dramatic lighting\"\n",
    "cfg_values = [3.0, 7.0, 12.0, 18.0]\n",
    "fixed_seed = 7777\n",
    "\n",
    "print(f\"\\nüìä Analyse CFG Scale\")\n",
    "print(f\"Prompt: '{test_prompt[:50]}...'\")\n",
    "print(f\"Valeurs test√©es: {cfg_values}\")\n",
    "\n",
    "cfg_results = []\n",
    "\n",
    "for cfg in cfg_values:\n",
    "    print(f\"\\n--- CFG = {cfg} ---\")\n",
    "    \n",
    "    workflow = create_text2img_workflow(\n",
    "        prompt=test_prompt,\n",
    "        width=768,\n",
    "        height=768,\n",
    "        steps=25,\n",
    "        cfg=cfg,\n",
    "        seed=fixed_seed\n",
    "    )\n",
    "    \n",
    "    images = client.execute_and_get_images(workflow, verbose=False)\n",
    "    if images:\n",
    "        cfg_results.append((cfg, images[0]))\n",
    "        print(f\"   ‚úÖ G√©n√©r√©\")\n",
    "\n",
    "# Affichage comparatif\n",
    "if cfg_results:\n",
    "    fig, axes = plt.subplots(1, len(cfg_results), figsize=(16, 5))\n",
    "    \n",
    "    for i, (cfg, img) in enumerate(cfg_results):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"CFG = {cfg}\", fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Impact du Classifier-Free Guidance (CFG)\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìà Observations CFG:\")\n",
    "    print(\"   3.0: Tr√®s cr√©atif mais peut s'√©loigner du prompt\")\n",
    "    print(\"   7.0: Balance optimale (recommand√©)\")\n",
    "    print(\"  12.0: Adh√©rence forte, d√©tails plus nets\")\n",
    "    print(\"  18.0: Tr√®s contraint, risque de saturation/artefacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 9. Exercices Pratiques\n",
    "\n",
    "### Exercice 1: √âdition de Style\n",
    "Prenez l'image de base du coffee shop et appliquez diff√©rents styles artistiques (impressionniste, anime, r√©aliste) en utilisant img2img avec un denoise de 0.6.\n",
    "\n",
    "### Exercice 2: Inpainting Cr√©atif\n",
    "Cr√©ez un masque circulaire au centre de l'image et remplacez cette zone par un personnage de votre choix.\n",
    "\n",
    "### Exercice 3: Exploration des Schedulers\n",
    "Modifiez le workflow pour tester diff√©rents schedulers (`normal`, `karras`, `exponential`) et comparez les r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9. ESPACE D'EXERCICES\n",
    "# =============================================================================\n",
    "\n",
    "# Exercice 1: Style Transfer\n",
    "# D√©commentez et compl√©tez:\n",
    "\n",
    "# style_prompts = [\n",
    "#     \"Same scene, impressionist painting style, visible brushstrokes\",\n",
    "#     \"Same scene, anime style, vibrant colors, Studio Ghibli\",\n",
    "#     \"Same scene, photorealistic, DSLR quality, 8k resolution\"\n",
    "# ]\n",
    "# \n",
    "# for style in style_prompts:\n",
    "#     workflow = create_img2img_workflow(\n",
    "#         image_name=uploaded_name,\n",
    "#         prompt=style,\n",
    "#         denoise=0.6\n",
    "#     )\n",
    "#     # ... g√©n√©rer et afficher\n",
    "\n",
    "print(\"üìù Espace d'exercices - D√©commentez le code ci-dessus pour commencer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 10. R√©capitulatif et Points Cl√©s\n",
    "\n",
    "### Param√®tres Essentiels Qwen-Image-Edit 2509\n",
    "\n",
    "| Param√®tre | Plage | Recommand√© | Impact |\n",
    "|-----------|-------|------------|--------|\n",
    "| `steps` | 15-50 | 25-30 | Qualit√© vs. Vitesse |\n",
    "| `cfg` | 1-20 | 7-8 | Adh√©rence au prompt |\n",
    "| `denoise` | 0-1 | 0.5-0.7 | Force de l'√©dition |\n",
    "| `seed` | 0-2¬≥¬≤ | Al√©atoire | Reproductibilit√© |\n",
    "\n",
    "### Bonnes Pratiques\n",
    "\n",
    "1. **Toujours tester avec seed fixe** pour comparer les param√®tres\n",
    "2. **Utiliser des masques avec feathering** pour des transitions naturelles\n",
    "3. **Commencer avec cfg=7** puis ajuster si n√©cessaire\n",
    "4. **Pour l'inpainting, denoise ‚â• 0.8** pour un remplacement complet\n",
    "\n",
    "### Ressources\n",
    "\n",
    "- [Documentation ComfyUI](https://docs.comfy.org/)\n",
    "- [Qwen-VL Papers](https://arxiv.org/abs/2308.12966)\n",
    "- Notebook suivant: **02-2-FLUX-1-Advanced-Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIN DU NOTEBOOK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"   ‚úÖ Notebook Qwen-Image-Edit 2509 Compl√©t√©\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÖ Termin√©: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nüìö Concepts couverts:\")\n",
    "print(\"   ‚Ä¢ Text-to-Image avec Qwen 2509\")\n",
    "print(\"   ‚Ä¢ Image-to-Image et analyse du denoise\")\n",
    "print(\"   ‚Ä¢ Inpainting avec masques personnalis√©s\")\n",
    "print(\"   ‚Ä¢ Batch processing\")\n",
    "print(\"   ‚Ä¢ Analyse comparative CFG\")\n",
    "print(\"\\n‚û°Ô∏è  Prochain notebook: 02-2-FLUX-1-Advanced-Generation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
