{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Sora API - Generation Video Cloud\n",
    "\n",
    "**Module :** 04-Applications  \n",
    "**Niveau :** Applications  \n",
    "**Technologies :** OpenAI API (Sora), httpx, requests  \n",
    "**Duree estimee :** 40 minutes  \n",
    "**VRAM :** 0 (API cloud uniquement)  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Comprendre l'architecture et les capacites de Sora (OpenAI)\n",
    "- [ ] Generer une video a partir de texte via l'API Sora\n",
    "- [ ] Generer une video a partir d'une image via l'API Sora\n",
    "- [ ] Explorer l'edition et l'extension de videos via l'API\n",
    "- [ ] Analyser les couts et les limites de l'API\n",
    "- [ ] Comparer Sora avec les modeles locaux (HunyuanVideo, LTX, Wan, SVD)\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Notebooks 01-1 a 03-3 (fondations, techniques avancees, orchestration)\n",
    "- Cle API OpenAI avec acces Sora (OPENAI_API_KEY)\n",
    "- Packages : `openai`, `httpx`, `Pillow`, `matplotlib`\n",
    "\n",
    "**Note** : L'API Sora peut ne pas etre publiquement disponible au moment de l'execution.\n",
    "Le code est structure avec try/except et reponses de demonstration pour permettre\n",
    "l'apprentissage meme sans acces a l'API.\n",
    "\n",
    "**Navigation** : [<< 04-2 Workflows Creatifs](04-2-Creative-Video-Workflows.ipynb) | [04-4 Pipeline Production >>](04-4-Production-Video-Pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres Sora API\n",
    "sora_model = \"sora\"                # Modele Sora a utiliser\n",
    "video_resolution = \"1080p\"         # Resolution souhaitee\n",
    "video_duration_target = 5          # Duree cible en secondes\n",
    "video_fps_target = 24              # FPS cible\n",
    "\n",
    "# Options\n",
    "enable_text_to_video = True        # Generation texte -> video\n",
    "enable_image_to_video = True       # Generation image -> video\n",
    "enable_video_edit = True           # Edition video\n",
    "use_mock_responses = True          # Utiliser des reponses simulees si API indisponible\n",
    "save_results = True                # Sauvegarder les resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Resolution GENAI_ROOT\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.video_helpers import get_video_info, extract_frames, display_frame_grid\n",
    "        print(\"Helpers video importes\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Helpers video non disponibles ({e}) - mode autonome\")\n",
    "\n",
    "# Repertoire de sortie\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'video'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('sora_api')\n",
    "\n",
    "print(f\"Sora API - Generation Video Cloud\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Resolution cible : {video_resolution}, {video_duration_target}s @ {video_fps_target}fps\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et verification de l'API\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification des dependances et de la cle API\n",
    "print(\"\\n--- VERIFICATION API ET DEPENDANCES ---\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "dependencies = {}\n",
    "api_available = False\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    dependencies['openai'] = True\n",
    "    print(f\"openai : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['openai'] = False\n",
    "    print(f\"openai : NON INSTALLE (pip install openai)\")\n",
    "\n",
    "try:\n",
    "    import httpx\n",
    "    dependencies['httpx'] = True\n",
    "    print(f\"httpx : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['httpx'] = False\n",
    "    print(f\"httpx : NON INSTALLE (pip install httpx)\")\n",
    "\n",
    "try:\n",
    "    import imageio\n",
    "    dependencies['imageio'] = True\n",
    "    print(f\"imageio : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['imageio'] = False\n",
    "    print(f\"imageio : NON INSTALLE\")\n",
    "\n",
    "# Verifier la cle API\n",
    "openai_key = os.environ.get('OPENAI_API_KEY', '')\n",
    "openai_base = os.environ.get('OPENAI_BASE_URL', '')\n",
    "\n",
    "if openai_key:\n",
    "    print(f\"\\nOPENAI_API_KEY : configuree ({openai_key[:12]}...)\")\n",
    "    if openai_base:\n",
    "        print(f\"OPENAI_BASE_URL : {openai_base}\")\n",
    "    \n",
    "    # Tester la connexion API\n",
    "    if dependencies.get('openai', False):\n",
    "        try:\n",
    "            client_kwargs = {'api_key': openai_key}\n",
    "            if openai_base:\n",
    "                client_kwargs['base_url'] = openai_base\n",
    "            client = OpenAI(**client_kwargs)\n",
    "            # Test leger (lister les modeles)\n",
    "            models = client.models.list()\n",
    "            sora_models = [m.id for m in models if 'sora' in m.id.lower()]\n",
    "            if sora_models:\n",
    "                print(f\"Modeles Sora disponibles : {', '.join(sora_models)}\")\n",
    "                api_available = True\n",
    "            else:\n",
    "                print(f\"Aucun modele Sora detecte dans la liste des modeles\")\n",
    "                print(f\"Mode demonstration active (reponses simulees)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Test API : {type(e).__name__} - {str(e)[:80]}\")\n",
    "            print(f\"Mode demonstration active\")\n",
    "else:\n",
    "    print(f\"\\nOPENAI_API_KEY : non configuree\")\n",
    "    print(f\"Mode demonstration active (reponses simulees)\")\n",
    "\n",
    "print(f\"\\nAPI Sora disponible : {api_available}\")\n",
    "print(f\"Mode mock : {use_mock_responses or not api_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Section 1 : Introduction a Sora et son architecture\n",
    "\n",
    "Sora est le modele de generation video d'OpenAI, annonce en fevrier 2024.\n",
    "Il utilise une architecture de type Diffusion Transformer (DiT) operant\n",
    "dans un espace latent spatio-temporel compresse.\n",
    "\n",
    "### Caracteristiques techniques\n",
    "\n",
    "| Aspect | Detail |\n",
    "|--------|--------|\n",
    "| Architecture | Diffusion Transformer (DiT) |\n",
    "| Espace latent | Compression spatio-temporelle 3D |\n",
    "| Entree | Texte, image, video |\n",
    "| Sortie | Video jusqu'a 1 minute |\n",
    "| Resolutions | 480p, 720p, 1080p |\n",
    "| Coherence temporelle | Attention sur toute la sequence |\n",
    "\n",
    "### Difference avec les modeles locaux\n",
    "\n",
    "Contrairement aux modeles locaux (HunyuanVideo, LTX-Video, Wan, SVD) qui necessitent\n",
    "un GPU puissant (16-48 GB VRAM), Sora fonctionne entierement en cloud.\n",
    "L'acces se fait via l'API OpenAI, avec facturation a l'utilisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper : generation de reponses simulees pour demonstration\n",
    "print(\"\\n--- PREPARATION DES OUTILS ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "\n",
    "def generate_mock_video_frames(prompt: str, n_frames: int = 48,\n",
    "                                width: int = 640, height: int = 360) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Genere des frames simulees illustrant le concept du prompt.\n",
    "    Utilise en mode demonstration quand l'API Sora n'est pas disponible.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    rng = np.random.RandomState(hash(prompt) % (2**31))\n",
    "    \n",
    "    # Couleur de base derivee du prompt\n",
    "    base_hue = hash(prompt) % 360\n",
    "    \n",
    "    for i in range(n_frames):\n",
    "        t = i / max(n_frames - 1, 1)\n",
    "        \n",
    "        # Generer un paysage abstrait anime\n",
    "        img = Image.new('RGB', (width, height))\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Ciel gradient\n",
    "        for y in range(height // 2):\n",
    "            ratio = y / (height // 2)\n",
    "            r = int(30 + 100 * ratio + 20 * np.sin(2 * np.pi * t))\n",
    "            g = int(50 + 80 * ratio)\n",
    "            b = int(120 + 80 * (1 - ratio))\n",
    "            draw.line([(0, y), (width, y)], fill=(min(255, r), min(255, g), min(255, b)))\n",
    "        \n",
    "        # Sol\n",
    "        for y in range(height // 2, height):\n",
    "            ratio = (y - height // 2) / (height // 2)\n",
    "            r = int(60 + 40 * ratio)\n",
    "            g = int(100 + 50 * (1 - ratio) + 20 * np.sin(2 * np.pi * t + 1))\n",
    "            b = int(40 + 30 * ratio)\n",
    "            draw.line([(0, y), (width, y)], fill=(min(255, r), min(255, g), min(255, b)))\n",
    "        \n",
    "        # Objet en mouvement\n",
    "        cx = int(width * 0.2 + width * 0.6 * t)\n",
    "        cy = int(height * 0.35 + 20 * np.sin(4 * np.pi * t))\n",
    "        draw.ellipse([cx - 25, cy - 25, cx + 25, cy + 25], fill=(255, 220, 100))\n",
    "        \n",
    "        # Filigrane mode demo\n",
    "        try:\n",
    "            font_small = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "        except (OSError, IOError):\n",
    "            font_small = ImageFont.load_default()\n",
    "        draw.text((10, height - 20), \"[DEMO] Sora API - Simulation\",\n",
    "                  fill=(180, 180, 180), font=font_small)\n",
    "        draw.text((10, 10), f\"Prompt: {prompt[:50]}...\",\n",
    "                  fill=(200, 200, 220), font=font_small)\n",
    "        \n",
    "        frames.append(np.array(img))\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "class SoraAPIWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper pour l'API Sora avec fallback sur des reponses simulees.\n",
    "    Permet d'explorer l'API meme sans acces reel.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_available: bool, client=None, use_mock: bool = True):\n",
    "        self.api_available = api_available\n",
    "        self.client = client\n",
    "        self.use_mock = use_mock or not api_available\n",
    "        self.call_log = []\n",
    "    \n",
    "    def text_to_video(self, prompt: str, duration: int = 5,\n",
    "                      resolution: str = \"720p\") -> dict:\n",
    "        \"\"\"Generation texte -> video.\"\"\"\n",
    "        call_info = {\n",
    "            \"method\": \"text_to_video\",\n",
    "            \"prompt\": prompt,\n",
    "            \"duration\": duration,\n",
    "            \"resolution\": resolution,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        if self.api_available and not self.use_mock:\n",
    "            try:\n",
    "                # Appel API reel (structure hypothetique basee sur la doc OpenAI)\n",
    "                response = self.client.videos.generate(\n",
    "                    model=sora_model,\n",
    "                    prompt=prompt,\n",
    "                    duration=duration,\n",
    "                    resolution=resolution,\n",
    "                )\n",
    "                call_info[\"status\"] = \"success\"\n",
    "                call_info[\"response\"] = response\n",
    "                self.call_log.append(call_info)\n",
    "                return {\"status\": \"success\", \"data\": response}\n",
    "            except Exception as e:\n",
    "                call_info[\"status\"] = \"error\"\n",
    "                call_info[\"error\"] = str(e)\n",
    "        \n",
    "        # Mode demonstration\n",
    "        res_map = {\"480p\": (854, 480), \"720p\": (1280, 720), \"1080p\": (1920, 1080)}\n",
    "        w, h = res_map.get(resolution, (640, 360))\n",
    "        # Reduire pour la demo\n",
    "        w, h = w // 2, h // 2\n",
    "        n_frames = duration * video_fps_target\n",
    "        \n",
    "        frames = generate_mock_video_frames(prompt, n_frames, w, h)\n",
    "        call_info[\"status\"] = \"mock\"\n",
    "        call_info[\"frames_generated\"] = len(frames)\n",
    "        self.call_log.append(call_info)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"mock\",\n",
    "            \"frames\": frames,\n",
    "            \"metadata\": {\n",
    "                \"model\": sora_model,\n",
    "                \"prompt\": prompt,\n",
    "                \"duration\": duration,\n",
    "                \"resolution\": f\"{w}x{h} (demo)\",\n",
    "                \"fps\": video_fps_target,\n",
    "                \"n_frames\": len(frames),\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def image_to_video(self, image: Image.Image, prompt: str,\n",
    "                       duration: int = 5) -> dict:\n",
    "        \"\"\"Generation image -> video (animation d'une image).\"\"\"\n",
    "        call_info = {\n",
    "            \"method\": \"image_to_video\",\n",
    "            \"prompt\": prompt,\n",
    "            \"duration\": duration,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        # Mode demonstration : animer l'image source\n",
    "        w, h = image.size\n",
    "        n_frames = duration * video_fps_target\n",
    "        base = np.array(image)\n",
    "        frames = []\n",
    "        \n",
    "        for i in range(n_frames):\n",
    "            t = i / max(n_frames - 1, 1)\n",
    "            # Effet Ken Burns : zoom progressif + translation\n",
    "            zoom = 1.0 + 0.15 * t\n",
    "            new_w = int(w * zoom)\n",
    "            new_h = int(h * zoom)\n",
    "            img_z = image.resize((new_w, new_h), Image.LANCZOS)\n",
    "            # Centrer le crop avec translation\n",
    "            offset_x = int((new_w - w) * (0.3 + 0.4 * t))\n",
    "            offset_y = int((new_h - h) * 0.5)\n",
    "            img_crop = img_z.crop((offset_x, offset_y, offset_x + w, offset_y + h))\n",
    "            frames.append(np.array(img_crop))\n",
    "        \n",
    "        call_info[\"status\"] = \"mock\"\n",
    "        self.call_log.append(call_info)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"mock\",\n",
    "            \"frames\": frames,\n",
    "            \"metadata\": {\n",
    "                \"model\": sora_model,\n",
    "                \"method\": \"image_to_video\",\n",
    "                \"duration\": duration,\n",
    "                \"resolution\": f\"{w}x{h}\",\n",
    "                \"effect\": \"Ken Burns (zoom + pan)\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialiser le wrapper\n",
    "sora = SoraAPIWrapper(\n",
    "    api_available=api_available,\n",
    "    client=client if api_available else None,\n",
    "    use_mock=use_mock_responses\n",
    ")\n",
    "\n",
    "print(f\"SoraAPIWrapper initialise\")\n",
    "print(f\"  API reelle : {api_available}\")\n",
    "print(f\"  Mode mock : {sora.use_mock}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Section 2 : Generation texte vers video\n",
    "\n",
    "La generation text-to-video est la fonctionnalite principale de Sora.\n",
    "Un prompt textuel decrit la scene souhaitee, et le modele genere la video correspondante.\n",
    "\n",
    "### Bonnes pratiques pour les prompts Sora\n",
    "\n",
    "| Aspect | Conseil |\n",
    "|--------|--------|\n",
    "| Description de scene | Detailler le sujet, le lieu, l'eclairage |\n",
    "| Mouvement camera | Preciser : \"camera slowly pans left\", \"tracking shot\" |\n",
    "| Style visuel | \"cinematic\", \"documentary\", \"aerial drone shot\" |\n",
    "| Duree | Adapter la complexite du prompt a la duree |\n",
    "| Eviter | Texte a lire, visages specifiques, physique complexe |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation texte -> video\n",
    "if enable_text_to_video:\n",
    "    print(\"\\n--- GENERATION TEXTE -> VIDEO ---\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Prompts de demonstration\n",
    "    prompts = [\n",
    "        {\n",
    "            \"name\": \"Nature\",\n",
    "            \"prompt\": \"A serene mountain lake at sunrise, mist rising from the water, \"\n",
    "                      \"pine trees reflecting in the still surface, cinematic drone shot \"\n",
    "                      \"slowly descending toward the water.\",\n",
    "            \"duration\": 5,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Urbain\",\n",
    "            \"prompt\": \"A busy Tokyo street at night, neon signs reflecting on wet \"\n",
    "                      \"pavement, people with umbrellas walking, tracking shot moving \"\n",
    "                      \"forward through the crowd.\",\n",
    "            \"duration\": 5,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    results_t2v = []\n",
    "    \n",
    "    for p in prompts:\n",
    "        print(f\"\\nGeneration : {p['name']}\")\n",
    "        print(f\"  Prompt : {p['prompt'][:80]}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = sora.text_to_video(\n",
    "            prompt=p['prompt'],\n",
    "            duration=p['duration'],\n",
    "            resolution=video_resolution\n",
    "        )\n",
    "        gen_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"  Status : {result['status']}\")\n",
    "        print(f\"  Frames : {len(result.get('frames', []))}\")\n",
    "        print(f\"  Temps : {gen_time:.2f}s\")\n",
    "        \n",
    "        results_t2v.append({\"name\": p['name'], \"result\": result, \"time\": gen_time})\n",
    "    \n",
    "    # Afficher les premiers et derniers frames\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "    for row, r in enumerate(results_t2v):\n",
    "        frames = r['result']['frames']\n",
    "        sample_idx = np.linspace(0, len(frames) - 1, 4, dtype=int)\n",
    "        for col, idx in enumerate(sample_idx):\n",
    "            axes[row, col].imshow(frames[idx])\n",
    "            axes[row, col].set_title(f\"{r['name']} t={idx/video_fps_target:.1f}s\", fontsize=9)\n",
    "            axes[row, col].axis('off')\n",
    "    plt.suptitle(\"Generation Texte -> Video (Sora API)\", fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sauvegarder\n",
    "    if dependencies.get('imageio', False) and save_results:\n",
    "        for r in results_t2v:\n",
    "            path = OUTPUT_DIR / f\"sora_t2v_{r['name'].lower()}.mp4\"\n",
    "            writer = imageio.get_writer(str(path), fps=video_fps_target, codec='libx264')\n",
    "            for f in r['result']['frames']:\n",
    "                writer.append_data(f)\n",
    "            writer.close()\n",
    "            print(f\"Sauvegarde : {path.name} ({path.stat().st_size / 1024:.0f} KB)\")\n",
    "else:\n",
    "    print(\"Generation texte -> video desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Interpretation : Generation texte -> video\n",
    "\n",
    "| Metrique | Valeur attendue (API reelle) | Valeur demo |\n",
    "|----------|------------------------------|-------------|\n",
    "| Temps de generation | 30-120s selon duree/resolution | < 1s (simulation) |\n",
    "| Coherence temporelle | Elevee (attention sur toute la sequence) | Basique (interpolation) |\n",
    "| Qualite visuelle | Photo-realiste | Schema simplifie |\n",
    "| Cout estime | ~$0.10 - $0.50 par video | Gratuit |\n",
    "\n",
    "**Points cles** :\n",
    "1. Sora maintient une coherence temporelle superieure aux modeles frame-par-frame\n",
    "2. Les prompts detailles avec indications de camera produisent de meilleurs resultats\n",
    "3. En mode demo, les frames simulees illustrent la structure de l'API sans cout\n",
    "\n",
    "## Section 3 : Generation image vers video\n",
    "\n",
    "L'image-to-video anime une image fixe en video. C'est utile pour :\n",
    "- Animer des illustrations ou photos\n",
    "- Creer des effets cinematographiques (Ken Burns, parallaxe)\n",
    "- Etendre une image en sequence temporelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation image -> video\n",
    "if enable_image_to_video:\n",
    "    print(\"\\n--- GENERATION IMAGE -> VIDEO ---\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Creer une image source de demonstration\n",
    "    source_width, source_height = 640, 360\n",
    "    source_img = Image.new('RGB', (source_width, source_height))\n",
    "    draw = ImageDraw.Draw(source_img)\n",
    "    \n",
    "    # Paysage simplifie\n",
    "    for y in range(source_height):\n",
    "        if y < source_height * 0.6:  # Ciel\n",
    "            ratio = y / (source_height * 0.6)\n",
    "            r, g, b = int(50 + 100 * ratio), int(80 + 100 * ratio), int(180 + 50 * (1 - ratio))\n",
    "        else:  # Sol\n",
    "            ratio = (y - source_height * 0.6) / (source_height * 0.4)\n",
    "            r, g, b = int(80 + 50 * ratio), int(140 - 40 * ratio), int(60 + 20 * ratio)\n",
    "        draw.line([(0, y), (source_width, y)], fill=(r, g, b))\n",
    "    \n",
    "    # Montagnes\n",
    "    peaks = [(0, 200), (100, 140), (200, 170), (320, 120), (450, 160), (550, 130), (640, 190)]\n",
    "    mountains = peaks + [(640, source_height * 0.6), (0, source_height * 0.6)]\n",
    "    draw.polygon(mountains, fill=(70, 90, 60))\n",
    "    \n",
    "    # Soleil\n",
    "    draw.ellipse([480, 40, 540, 100], fill=(255, 220, 100))\n",
    "    \n",
    "    print(f\"Image source : {source_width}x{source_height}\")\n",
    "    \n",
    "    # Animation\n",
    "    i2v_prompt = \"The landscape comes alive: clouds drift across the sky, sunlight shifts\"\n",
    "    result_i2v = sora.image_to_video(source_img, i2v_prompt, duration=video_duration_target)\n",
    "    \n",
    "    print(f\"Status : {result_i2v['status']}\")\n",
    "    print(f\"Frames generees : {len(result_i2v['frames'])}\")\n",
    "    print(f\"Effet : {result_i2v['metadata'].get('effect', 'N/A')}\")\n",
    "    \n",
    "    # Afficher source + frames animees\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(16, 3))\n",
    "    axes[0].imshow(source_img)\n",
    "    axes[0].set_title(\"Image source\", fontsize=10)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    i2v_frames = result_i2v['frames']\n",
    "    sample_idx = np.linspace(0, len(i2v_frames) - 1, 4, dtype=int)\n",
    "    for i, idx in enumerate(sample_idx):\n",
    "        axes[i + 1].imshow(i2v_frames[idx])\n",
    "        axes[i + 1].set_title(f\"t = {idx / video_fps_target:.1f}s\", fontsize=10)\n",
    "        axes[i + 1].axis('off')\n",
    "    plt.suptitle(\"Image -> Video (effet Ken Burns)\", fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if dependencies.get('imageio', False) and save_results:\n",
    "        i2v_path = OUTPUT_DIR / \"sora_i2v_landscape.mp4\"\n",
    "        writer = imageio.get_writer(str(i2v_path), fps=video_fps_target, codec='libx264')\n",
    "        for f in i2v_frames:\n",
    "            writer.append_data(f)\n",
    "        writer.close()\n",
    "        print(f\"Sauvegarde : {i2v_path.name} ({i2v_path.stat().st_size / 1024:.0f} KB)\")\n",
    "else:\n",
    "    print(\"Generation image -> video desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Section 4 : Analyse des couts et comparaison locale vs cloud\n",
    "\n",
    "Le choix entre generation video locale et cloud depend de plusieurs facteurs :\n",
    "cout, qualite, latence, et infrastructure disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse comparative : Sora vs modeles locaux\n",
    "print(\"\\n--- COMPARAISON SORA VS MODELES LOCAUX ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Tableau comparatif detaille\n",
    "comparison_data = {\n",
    "    \"Modele\": [\n",
    "        \"Sora (OpenAI)\",\n",
    "        \"HunyuanVideo\",\n",
    "        \"LTX-Video\",\n",
    "        \"Wan 2.1\",\n",
    "        \"Stable Video Diffusion\",\n",
    "        \"AnimateDiff\",\n",
    "    ],\n",
    "    \"Type\": [\"Cloud API\", \"Local\", \"Local\", \"Local\", \"Local\", \"Local\"],\n",
    "    \"VRAM\": [\"0 (cloud)\", \"~40 GB\", \"~12 GB\", \"~24 GB\", \"~16 GB\", \"~12 GB\"],\n",
    "    \"Qualite\": [\"Tres haute\", \"Haute\", \"Moyenne\", \"Haute\", \"Moyenne\", \"Moyenne\"],\n",
    "    \"Duree max\": [\"~60s\", \"~6s\", \"~5s\", \"~10s\", \"~4s\", \"~2s\"],\n",
    "    \"Resolution max\": [\"1080p\", \"720p\", \"768p\", \"720p\", \"576p\", \"512p\"],\n",
    "    \"Cout / video\": [\"~$0.10-0.50\", \"Electricite\", \"Electricite\", \"Electricite\", \"Electricite\", \"Electricite\"],\n",
    "    \"Latence\": [\"30-120s\", \"60-300s\", \"30-120s\", \"60-180s\", \"30-60s\", \"20-60s\"],\n",
    "    \"Coherence temp.\": [\"Excellente\", \"Bonne\", \"Correcte\", \"Bonne\", \"Correcte\", \"Limitee\"],\n",
    "}\n",
    "\n",
    "# Affichage tableau\n",
    "header = f\"{'Modele':<25} {'Type':<12} {'VRAM':<12} {'Qualite':<12} {'Duree':<10} {'Cout':<15}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for i in range(len(comparison_data['Modele'])):\n",
    "    row = (f\"{comparison_data['Modele'][i]:<25} \"\n",
    "           f\"{comparison_data['Type'][i]:<12} \"\n",
    "           f\"{comparison_data['VRAM'][i]:<12} \"\n",
    "           f\"{comparison_data['Qualite'][i]:<12} \"\n",
    "           f\"{comparison_data['Duree max'][i]:<10} \"\n",
    "           f\"{comparison_data['Cout / video'][i]:<15}\")\n",
    "    print(row)\n",
    "\n",
    "# Analyse cout pour volume de production\n",
    "print(\"\\n--- ANALYSE DE COUT POUR PRODUCTION ---\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "scenarios = [\n",
    "    {\"name\": \"Prototype (10 videos/mois)\", \"count\": 10, \"sora_cost\": 0.30},\n",
    "    {\"name\": \"Production (100 videos/mois)\", \"count\": 100, \"sora_cost\": 0.25},\n",
    "    {\"name\": \"Industriel (1000 videos/mois)\", \"count\": 1000, \"sora_cost\": 0.20},\n",
    "]\n",
    "\n",
    "# Cout GPU local estime : amortissement RTX 4090 (~$1600) sur 3 ans + electricite\n",
    "gpu_monthly_cost = 1600 / 36 + 50  # ~$94/mois\n",
    "local_time_per_video = 120  # secondes\n",
    "local_videos_per_hour = 3600 / local_time_per_video  # ~30\n",
    "\n",
    "print(f\"\\n{'Scenario':<35} {'Sora/mois':>12} {'Local/mois':>12} {'Recommandation':>18}\")\n",
    "print(\"-\" * 80)\n",
    "for s in scenarios:\n",
    "    sora_monthly = s['count'] * s['sora_cost']\n",
    "    local_monthly = gpu_monthly_cost  # Cout fixe\n",
    "    recommendation = \"Cloud\" if sora_monthly < local_monthly else \"Local\"\n",
    "    print(f\"  {s['name']:<33} ${sora_monthly:>10.2f} ${local_monthly:>10.2f} {recommendation:>18}\")\n",
    "\n",
    "print(f\"\\nNote : Le cout local inclut l'amortissement GPU ($1600/3 ans) + electricite (~$50/mois)\")\n",
    "print(f\"Le seuil de rentabilite local est d'environ {int(gpu_monthly_cost / 0.25)} videos/mois\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Interpretation : Comparaison cout et performance\n",
    "\n",
    "| Critere | Cloud (Sora) | Local (GPU) |\n",
    "|---------|-------------|-------------|\n",
    "| Investissement initial | Nul | Eleve ($1600+ pour GPU) |\n",
    "| Cout variable | Par video | Electricite uniquement |\n",
    "| Seuil rentabilite | < 400 videos/mois | > 400 videos/mois |\n",
    "| Maintenance | Zero | MAJ drivers, CUDA, modeles |\n",
    "| Disponibilite | 24/7, rate-limited | Depend du hardware |\n",
    "| Confidentialite | Donnees transitent par OpenAI | Traitement local |\n",
    "\n",
    "**Points cles** :\n",
    "1. Pour le prototypage et les petits volumes, le cloud est plus economique\n",
    "2. Pour la production a grande echelle, le GPU local est rentabilise en quelques mois\n",
    "3. La confidentialite des donnees peut etre un facteur decisif (medical, defense)\n",
    "4. La qualite Sora depasse actuellement les modeles locaux open-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative des couts\n",
    "volumes = np.arange(10, 1010, 10)\n",
    "sora_costs = volumes * 0.25  # $0.25 par video\n",
    "local_costs = np.full_like(volumes, gpu_monthly_cost, dtype=float)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cout mensuel\n",
    "ax1.plot(volumes, sora_costs, 'b-', linewidth=2, label='Sora (cloud)')\n",
    "ax1.plot(volumes, local_costs, 'r--', linewidth=2, label='GPU local')\n",
    "crossover = int(gpu_monthly_cost / 0.25)\n",
    "ax1.axvline(x=crossover, color='gray', linestyle=':', alpha=0.7)\n",
    "ax1.annotate(f'Seuil : {crossover} vid/mois', xy=(crossover, gpu_monthly_cost),\n",
    "             xytext=(crossover + 100, gpu_monthly_cost + 50),\n",
    "             arrowprops=dict(arrowstyle='->', color='gray'), fontsize=9, color='gray')\n",
    "ax1.set_xlabel('Videos par mois')\n",
    "ax1.set_ylabel('Cout mensuel ($)')\n",
    "ax1.set_title('Cout mensuel : Cloud vs Local')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Comparaison qualitative\n",
    "categories = ['Qualite', 'Coherence\\ntemporelle', 'Duree\\nmax', 'Latence', 'Facilite\\nsetup']\n",
    "sora_scores = [9, 9, 9, 7, 10]\n",
    "local_scores = [7, 6, 5, 5, 4]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, sora_scores, width, label='Sora', color='steelblue')\n",
    "ax2.bar(x + width/2, local_scores, width, label='Local (best)', color='coral')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(categories, fontsize=9)\n",
    "ax2.set_ylabel('Score (1-10)')\n",
    "ax2.set_title('Comparaison qualitative')\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 11)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Analyse comparative : Sora API vs Generation Video Locale',\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif - Test de prompt personnalise\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Entrez un prompt pour generer une video de demonstration.\")\n",
    "    print(\"(Laissez vide pour passer)\")\n",
    "    \n",
    "    try:\n",
    "        custom_prompt = input(\"\\nPrompt video (ou vide) : \").strip()\n",
    "        \n",
    "        if custom_prompt:\n",
    "            print(f\"Generation avec : {custom_prompt[:80]}...\")\n",
    "            result_custom = sora.text_to_video(custom_prompt, duration=3)\n",
    "            print(f\"Status : {result_custom['status']}, Frames : {len(result_custom.get('frames', []))}\")\n",
    "            \n",
    "            if result_custom.get('frames'):\n",
    "                fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "                idx_list = np.linspace(0, len(result_custom['frames']) - 1, 4, dtype=int)\n",
    "                for i, idx in enumerate(idx_list):\n",
    "                    axes[i].imshow(result_custom['frames'][idx])\n",
    "                    axes[i].set_title(f\"t={idx/video_fps_target:.1f}s\", fontsize=9)\n",
    "                    axes[i].axis('off')\n",
    "                plt.suptitle(f\"Prompt personnalise\", fontsize=11, fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type:\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"API Sora disponible : {api_available}\")\n",
    "print(f\"Mode mock : {sora.use_mock}\")\n",
    "\n",
    "print(f\"\\nAppels API effectues : {len(sora.call_log)}\")\n",
    "for i, call in enumerate(sora.call_log):\n",
    "    print(f\"  {i+1}. {call['method']} - Status: {call['status']}\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('sora_*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(f\"\\nDependances utilisees :\")\n",
    "for dep, available in dependencies.items():\n",
    "    status = \"utilisee\" if available else \"non disponible\"\n",
    "    print(f\"  {dep} : {status}\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 04-4 : Pipeline Video de Production (pipeline complet bout-en-bout)\")\n",
    "print(f\"2. Revenir aux notebooks Audio pour combiner audio + video\")\n",
    "\n",
    "print(f\"\\nNotebook 04-3 Sora API Cloud Video termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}