{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Workflows Video Creatifs\n",
    "\n",
    "**Module :** 04-Applications  \n",
    "**Niveau :** Applications  \n",
    "**Technologies :** moviepy, diffusers, Pillow, numpy, scipy  \n",
    "**Duree estimee :** 50 minutes  \n",
    "**VRAM :** ~16 GB (optionnel, certaines sections CPU uniquement)  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Appliquer un transfert de style sur des frames video (approche conceptuelle avec diffusers img2img)\n",
    "- [ ] Creer un clip musical avec decoupes synchronisees sur le rythme\n",
    "- [ ] Produire un effet stop-motion a partir d'une video (echantillonnage et traitement de frames)\n",
    "- [ ] Appliquer un color grading et des effets cinematographiques\n",
    "- [ ] Composer des grilles et collages video\n",
    "- [ ] Creer des animations de texte creatives\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Notebooks 01-1 a 03-3 (fondations, techniques avancees, orchestration)\n",
    "- Notebook 04-1 (generation de videos educatives)\n",
    "- Packages : `moviepy`, `Pillow`, `numpy`, `scipy`, `matplotlib`\n",
    "- Optionnel : `diffusers`, `torch` pour le transfert de style IA\n",
    "\n",
    "**Navigation** : [<< 04-1 Educational Video](04-1-Educational-Video-Generation.ipynb) | [04-3 Sora API >>](04-3-Sora-API-Cloud-Video.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres video\n",
    "video_width = 640                  # Largeur video de travail\n",
    "video_height = 480                 # Hauteur video de travail\n",
    "video_fps = 24                     # FPS\n",
    "video_duration = 6                 # Duree video source (secondes)\n",
    "output_format = \"mp4\"              # Format de sortie\n",
    "\n",
    "# Options creatives\n",
    "enable_style_transfer = True       # Activer le transfert de style\n",
    "enable_music_video = True          # Activer la creation music video\n",
    "enable_stop_motion = True          # Activer l'effet stop-motion\n",
    "enable_color_grading = True        # Activer le color grading\n",
    "enable_text_animation = True       # Activer les animations de texte\n",
    "save_results = True                # Sauvegarder les resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Resolution GENAI_ROOT\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.video_helpers import get_video_info, extract_frames, display_frame_grid\n",
    "        print(\"Helpers video importes\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Helpers video non disponibles ({e}) - mode autonome\")\n",
    "\n",
    "# Repertoire de sortie\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'video'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('creative_video')\n",
    "\n",
    "print(f\"Workflows Video Creatifs\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Video : {video_width}x{video_height} @ {video_fps}fps, {video_duration}s\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et verification des dependances\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification des dependances\n",
    "print(\"\\n--- VERIFICATION DES DEPENDANCES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "dependencies = {}\n",
    "\n",
    "try:\n",
    "    import moviepy.editor as mpy\n",
    "    dependencies['moviepy'] = True\n",
    "    print(f\"moviepy : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['moviepy'] = False\n",
    "    print(f\"moviepy : NON INSTALLE (pip install moviepy)\")\n",
    "\n",
    "try:\n",
    "    import imageio\n",
    "    dependencies['imageio'] = True\n",
    "    print(f\"imageio : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['imageio'] = False\n",
    "    print(f\"imageio : NON INSTALLE (pip install imageio imageio-ffmpeg)\")\n",
    "\n",
    "try:\n",
    "    from scipy import signal, ndimage\n",
    "    dependencies['scipy'] = True\n",
    "    print(f\"scipy : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['scipy'] = False\n",
    "    print(f\"scipy : NON INSTALLE (pip install scipy)\")\n",
    "\n",
    "try:\n",
    "    import diffusers\n",
    "    import torch\n",
    "    dependencies['diffusers'] = True\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print(f\"diffusers : disponible (GPU: {gpu_available})\")\n",
    "except ImportError:\n",
    "    dependencies['diffusers'] = False\n",
    "    print(f\"diffusers : NON INSTALLE (optionnel)\")\n",
    "\n",
    "available_count = sum(dependencies.values())\n",
    "total_count = len(dependencies)\n",
    "print(f\"\\nDependances disponibles : {available_count}/{total_count}\")\n",
    "\n",
    "# Generation d'une video source de travail\n",
    "print(\"\\n--- GENERATION VIDEO SOURCE ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "n_frames = video_fps * video_duration\n",
    "source_frames = []\n",
    "for i in range(n_frames):\n",
    "    t = i / n_frames\n",
    "    # Scene avec mouvement : gradient + cercle anime\n",
    "    img = Image.new('RGB', (video_width, video_height))\n",
    "    pixels = np.zeros((video_height, video_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Gradient de fond anime\n",
    "    for y in range(video_height):\n",
    "        r = int(50 + 100 * (y / video_height) + 50 * np.sin(2 * np.pi * t))\n",
    "        g = int(30 + 80 * (1 - y / video_height))\n",
    "        b = int(100 + 100 * np.cos(2 * np.pi * t))\n",
    "        pixels[y, :] = [min(255, max(0, r)), min(255, max(0, g)), min(255, max(0, b))]\n",
    "    \n",
    "    img = Image.fromarray(pixels)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Objets animes\n",
    "    cx = int(video_width * 0.5 + video_width * 0.3 * np.cos(2 * np.pi * t))\n",
    "    cy = int(video_height * 0.4 + video_height * 0.2 * np.sin(4 * np.pi * t))\n",
    "    draw.ellipse([cx - 30, cy - 30, cx + 30, cy + 30], fill=(255, 200, 50))\n",
    "    \n",
    "    # Deuxieme objet\n",
    "    cx2 = int(video_width * 0.3 + video_width * 0.15 * np.sin(3 * np.pi * t))\n",
    "    cy2 = int(video_height * 0.6)\n",
    "    draw.rectangle([cx2 - 20, cy2 - 20, cx2 + 20, cy2 + 20], fill=(100, 200, 255))\n",
    "    \n",
    "    source_frames.append(np.array(img))\n",
    "\n",
    "print(f\"Video source generee : {len(source_frames)} frames\")\n",
    "print(f\"Resolution : {video_width}x{video_height} @ {video_fps}fps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Section 1 : Transfert de style video\n",
    "\n",
    "Le transfert de style consiste a appliquer le style visuel d'une image de reference\n",
    "a chaque frame d'une video. Nous implementons ici deux approches :\n",
    "- **CPU** : Transfert de style algorithmique (filtres Pillow, manipulation de couleurs)\n",
    "- **GPU** (optionnel) : img2img avec un modele de diffusion (diffusers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfert de style video\n",
    "if enable_style_transfer:\n",
    "    print(\"\\n--- TRANSFERT DE STYLE VIDEO ---\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Approche CPU : filtres artistiques avec Pillow\n",
    "    def apply_oil_painting_effect(frame: np.ndarray, radius: int = 4) -> np.ndarray:\n",
    "        \"\"\"Simule un effet peinture a l'huile avec quantification des couleurs.\"\"\"\n",
    "        img = Image.fromarray(frame)\n",
    "        # Reduire les details avec un flou\n",
    "        img = img.filter(ImageFilter.MedianFilter(size=radius * 2 + 1))\n",
    "        # Renforcer les contours\n",
    "        enhancer = ImageEnhance.Contrast(img)\n",
    "        img = enhancer.enhance(1.5)\n",
    "        # Quantifier les couleurs pour l'effet peinture\n",
    "        img = img.quantize(colors=24, method=Image.Quantize.MEDIANCUT)\n",
    "        img = img.convert('RGB')\n",
    "        return np.array(img)\n",
    "    \n",
    "    def apply_watercolor_effect(frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Simule un effet aquarelle avec flou et saturation.\"\"\"\n",
    "        img = Image.fromarray(frame)\n",
    "        # Flou gaussien leger\n",
    "        img = img.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "        # Augmenter la saturation\n",
    "        enhancer = ImageEnhance.Color(img)\n",
    "        img = enhancer.enhance(1.8)\n",
    "        # Reduire la nettete\n",
    "        enhancer = ImageEnhance.Sharpness(img)\n",
    "        img = enhancer.enhance(0.3)\n",
    "        return np.array(img)\n",
    "    \n",
    "    def apply_sketch_effect(frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Simule un effet dessin au crayon avec detection de contours.\"\"\"\n",
    "        img = Image.fromarray(frame).convert('L')  # Niveaux de gris\n",
    "        # Detection de contours\n",
    "        edges = img.filter(ImageFilter.FIND_EDGES)\n",
    "        # Inverser pour fond blanc, traits noirs\n",
    "        edges = Image.eval(edges, lambda x: 255 - x)\n",
    "        # Renforcer le contraste\n",
    "        enhancer = ImageEnhance.Contrast(edges)\n",
    "        edges = enhancer.enhance(2.0)\n",
    "        return np.array(edges.convert('RGB'))\n",
    "    \n",
    "    # Appliquer les trois styles sur un echantillon de frames\n",
    "    sample_indices = [0, n_frames // 3, 2 * n_frames // 3]\n",
    "    styles = [\n",
    "        (\"Peinture a l'huile\", apply_oil_painting_effect),\n",
    "        (\"Aquarelle\", apply_watercolor_effect),\n",
    "        (\"Dessin\", apply_sketch_effect),\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(styles) + 1, len(sample_indices), figsize=(14, 12))\n",
    "    \n",
    "    for col, idx in enumerate(sample_indices):\n",
    "        # Original\n",
    "        axes[0, col].imshow(source_frames[idx])\n",
    "        axes[0, col].set_title(f\"Original (frame {idx})\", fontsize=9)\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # Styles\n",
    "        for row, (style_name, style_fn) in enumerate(styles):\n",
    "            styled = style_fn(source_frames[idx])\n",
    "            axes[row + 1, col].imshow(styled)\n",
    "            axes[row + 1, col].set_title(f\"{style_name}\", fontsize=9)\n",
    "            axes[row + 1, col].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Comparaison des styles artistiques\", fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Generer une video complete avec un style\n",
    "    print(f\"\\nGeneration video style 'aquarelle' ({len(source_frames)} frames)...\")\n",
    "    start_time = time.time()\n",
    "    styled_frames = [apply_watercolor_effect(f) for f in source_frames]\n",
    "    style_time = time.time() - start_time\n",
    "    print(f\"Style applique en {style_time:.2f}s ({len(source_frames) / style_time:.0f} fps)\")\n",
    "    \n",
    "    if dependencies.get('imageio', False) and save_results:\n",
    "        styled_path = OUTPUT_DIR / \"creative_watercolor.mp4\"\n",
    "        writer = imageio.get_writer(str(styled_path), fps=video_fps, codec='libx264')\n",
    "        for f in styled_frames:\n",
    "            writer.append_data(f)\n",
    "        writer.close()\n",
    "        print(f\"Video sauvegardee : {styled_path.name} ({styled_path.stat().st_size / 1024:.0f} KB)\")\n",
    "else:\n",
    "    print(\"Transfert de style desactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Interpretation : Transfert de style\n",
    "\n",
    "| Style | Technique | Temps/frame | Usage ideal |\n",
    "|-------|-----------|-------------|-------------|\n",
    "| Peinture a l'huile | Filtre median + quantification | ~50ms | Clips courts artistiques |\n",
    "| Aquarelle | Flou + saturation | ~20ms | Ambiances douces |\n",
    "| Dessin | Detection de contours | ~10ms | Storyboards, tutoriels |\n",
    "\n",
    "**Points cles** :\n",
    "1. Les filtres Pillow sont rapides mais limites en qualite artistique\n",
    "2. Pour un vrai transfert de style neuronal, utiliser diffusers img2img (GPU requis)\n",
    "3. La coherence temporelle est un defi : chaque frame est traitee independamment\n",
    "\n",
    "## Section 2 : Creation de clip musical\n",
    "\n",
    "La creation d'un clip musical synchronise implique de detecter le rythme (BPM)\n",
    "et de placer des coupes, effets et transitions sur les temps forts.\n",
    "Ici, nous simulons un signal audio rythmique et synchronisons les effets video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation de clip musical avec synchronisation\n",
    "if enable_music_video:\n",
    "    print(\"\\n--- CREATION CLIP MUSICAL ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Simuler la detection de beats (BPM = 120)\n",
    "    bpm = 120\n",
    "    beat_interval = 60.0 / bpm  # Secondes entre chaque beat\n",
    "    total_seconds = video_duration\n",
    "    \n",
    "    # Generer les positions des beats\n",
    "    beat_times = np.arange(0, total_seconds, beat_interval)\n",
    "    beat_frames = (beat_times * video_fps).astype(int)\n",
    "    beat_frames = beat_frames[beat_frames < len(source_frames)]\n",
    "    \n",
    "    print(f\"BPM simule : {bpm}\")\n",
    "    print(f\"Intervalle entre beats : {beat_interval:.3f}s\")\n",
    "    print(f\"Nombre de beats : {len(beat_frames)}\")\n",
    "    print(f\"Positions (frames) : {beat_frames.tolist()}\")\n",
    "    \n",
    "    # Effets synchronises sur les beats\n",
    "    def apply_beat_flash(frame: np.ndarray, intensity: float = 0.5) -> np.ndarray:\n",
    "        \"\"\"Flash lumineux sur un beat.\"\"\"\n",
    "        flash = np.full_like(frame, 255)\n",
    "        return np.clip(frame * (1 - intensity) + flash * intensity, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    def apply_beat_zoom(frame: np.ndarray, zoom_factor: float = 1.15) -> np.ndarray:\n",
    "        \"\"\"Effet zoom leger sur un beat.\"\"\"\n",
    "        h, w = frame.shape[:2]\n",
    "        img = Image.fromarray(frame)\n",
    "        \n",
    "        # Zoom au centre\n",
    "        new_w = int(w * zoom_factor)\n",
    "        new_h = int(h * zoom_factor)\n",
    "        img_zoomed = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "        \n",
    "        # Recadrer au centre\n",
    "        left = (new_w - w) // 2\n",
    "        top = (new_h - h) // 2\n",
    "        img_cropped = img_zoomed.crop((left, top, left + w, top + h))\n",
    "        \n",
    "        return np.array(img_cropped)\n",
    "    \n",
    "    # Appliquer les effets avec decroissance\n",
    "    music_frames = [f.copy() for f in source_frames]\n",
    "    beat_decay_frames = 3  # Nombre de frames pour l'attenuation de l'effet\n",
    "    \n",
    "    for beat_frame in beat_frames:\n",
    "        for offset in range(beat_decay_frames):\n",
    "            idx = beat_frame + offset\n",
    "            if idx >= len(music_frames):\n",
    "                break\n",
    "            decay = 1.0 - (offset / beat_decay_frames)\n",
    "            # Alterner flash et zoom\n",
    "            if (np.where(beat_frames == beat_frame)[0][0]) % 2 == 0:\n",
    "                music_frames[idx] = apply_beat_flash(music_frames[idx], 0.4 * decay)\n",
    "            else:\n",
    "                zoom = 1.0 + 0.1 * decay\n",
    "                music_frames[idx] = apply_beat_zoom(music_frames[idx], zoom)\n",
    "    \n",
    "    print(f\"Effets appliques sur {len(beat_frames)} beats\")\n",
    "    \n",
    "    # Visualiser quelques beats\n",
    "    if len(beat_frames) >= 3:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(14, 6))\n",
    "        for col in range(3):\n",
    "            b_idx = beat_frames[col]\n",
    "            axes[0, col].imshow(source_frames[b_idx])\n",
    "            axes[0, col].set_title(f\"Original (beat {col+1}, frame {b_idx})\", fontsize=9)\n",
    "            axes[0, col].axis('off')\n",
    "            axes[1, col].imshow(music_frames[b_idx])\n",
    "            axes[1, col].set_title(f\"Avec effet\", fontsize=9)\n",
    "            axes[1, col].axis('off')\n",
    "        plt.suptitle(f\"Effets synchronises sur les beats ({bpm} BPM)\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if dependencies.get('imageio', False) and save_results:\n",
    "        music_path = OUTPUT_DIR / \"creative_music_video.mp4\"\n",
    "        writer = imageio.get_writer(str(music_path), fps=video_fps, codec='libx264')\n",
    "        for f in music_frames:\n",
    "            writer.append_data(f)\n",
    "        writer.close()\n",
    "        print(f\"Video sauvegardee : {music_path.name} ({music_path.stat().st_size / 1024:.0f} KB)\")\n",
    "else:\n",
    "    print(\"Creation clip musical desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Interpretation : Clip musical\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| BPM | 120 | Rythme moyen (pop/dance) |\n",
    "| Intervalle beats | 0.5s | Un beat toutes les 12 frames a 24fps |\n",
    "| Effets | Flash + Zoom | Alternance pour varier le rythme visuel |\n",
    "| Decay | 3 frames | Attenuation progressive de l'effet |\n",
    "\n",
    "**Points cles** :\n",
    "1. En production, le BPM serait detecte par analyse audio (librosa.beat)\n",
    "2. L'attenuation progressive donne un rendu plus naturel que des effets instantanes\n",
    "3. La synchronisation audio-video est critique : 1 frame de decalage est perceptible\n",
    "\n",
    "## Section 3 : Effet stop-motion et color grading\n",
    "\n",
    "L'effet stop-motion simule une animation image par image en reduisant le taux de frames\n",
    "et en ajoutant des imperfections. Le color grading transforme l'ambiance de la video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effet stop-motion\n",
    "if enable_stop_motion:\n",
    "    print(\"\\n--- EFFET STOP-MOTION ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    def create_stop_motion(frames: List[np.ndarray], hold_count: int = 4,\n",
    "                           jitter: float = 2.0) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Cree un effet stop-motion : chaque frame est maintenue pendant N frames\n",
    "        avec un leger decalage aleatoire pour simuler l'imprecision.\n",
    "        \n",
    "        Args:\n",
    "            frames: Frames source\n",
    "            hold_count: Nombre de frames a maintenir chaque image\n",
    "            jitter: Amplitude du decalage aleatoire en pixels\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        rng = np.random.RandomState(42)\n",
    "        \n",
    "        for i in range(0, len(frames), hold_count):\n",
    "            base_frame = frames[i]\n",
    "            \n",
    "            for j in range(hold_count):\n",
    "                if i + j >= len(frames):\n",
    "                    break\n",
    "                # Ajouter un leger jitter de position\n",
    "                if jitter > 0:\n",
    "                    dx = int(rng.uniform(-jitter, jitter))\n",
    "                    dy = int(rng.uniform(-jitter, jitter))\n",
    "                    img = Image.fromarray(base_frame)\n",
    "                    shifted = Image.new('RGB', img.size, (0, 0, 0))\n",
    "                    shifted.paste(img, (dx, dy))\n",
    "                    result.append(np.array(shifted))\n",
    "                else:\n",
    "                    result.append(base_frame.copy())\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    stop_frames = create_stop_motion(source_frames, hold_count=4, jitter=2.0)\n",
    "    effective_fps = video_fps / 4\n",
    "    \n",
    "    print(f\"Frames originales : {len(source_frames)}\")\n",
    "    print(f\"Frames stop-motion : {len(stop_frames)}\")\n",
    "    print(f\"FPS effectif : {effective_fps:.0f} (maintien de 4 frames)\")\n",
    "\n",
    "# Color grading cinematographique\n",
    "if enable_color_grading:\n",
    "    print(\"\\n--- COLOR GRADING ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    def apply_color_grade(frame: np.ndarray, style: str = \"teal_orange\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applique un color grading cinematographique.\n",
    "        \n",
    "        Styles : teal_orange (blockbuster), vintage (film ancien), cold (thriller)\n",
    "        \"\"\"\n",
    "        img = frame.astype(np.float32) / 255.0\n",
    "        \n",
    "        if style == \"teal_orange\":\n",
    "            # Tons chauds dans les hautes lumieres, froids dans les ombres\n",
    "            shadows = img < 0.4\n",
    "            highlights = img > 0.6\n",
    "            result = img.copy()\n",
    "            # Ombres : teinte bleu-vert\n",
    "            result[:, :, 0] = np.where(shadows[:, :, 0], img[:, :, 0] * 0.7, img[:, :, 0])\n",
    "            result[:, :, 1] = np.where(shadows[:, :, 1], img[:, :, 1] * 0.9, img[:, :, 1])\n",
    "            result[:, :, 2] = np.where(shadows[:, :, 2], img[:, :, 2] * 1.2, img[:, :, 2])\n",
    "            # Hautes lumieres : teinte orange\n",
    "            result[:, :, 0] = np.where(highlights[:, :, 0], img[:, :, 0] * 1.2, result[:, :, 0])\n",
    "            result[:, :, 1] = np.where(highlights[:, :, 1], img[:, :, 1] * 0.95, result[:, :, 1])\n",
    "            result[:, :, 2] = np.where(highlights[:, :, 2], img[:, :, 2] * 0.8, result[:, :, 2])\n",
    "        \n",
    "        elif style == \"vintage\":\n",
    "            # Desaturation + teinte sepia\n",
    "            gray = np.mean(img, axis=2, keepdims=True)\n",
    "            result = img * 0.4 + gray * 0.6  # Desaturation partielle\n",
    "            result[:, :, 0] = result[:, :, 0] * 1.1  # Plus de rouge\n",
    "            result[:, :, 1] = result[:, :, 1] * 1.0\n",
    "            result[:, :, 2] = result[:, :, 2] * 0.85  # Moins de bleu\n",
    "        \n",
    "        elif style == \"cold\":\n",
    "            # Tons froids, contraste eleve\n",
    "            result = img.copy()\n",
    "            result[:, :, 0] = img[:, :, 0] * 0.85  # Moins de rouge\n",
    "            result[:, :, 2] = img[:, :, 2] * 1.15  # Plus de bleu\n",
    "            # Augmenter le contraste\n",
    "            result = (result - 0.5) * 1.3 + 0.5\n",
    "        \n",
    "        return (np.clip(result, 0, 1) * 255).astype(np.uint8)\n",
    "    \n",
    "    # Comparer les styles sur un echantillon\n",
    "    sample_frame = source_frames[n_frames // 2]\n",
    "    grade_styles = [\"teal_orange\", \"vintage\", \"cold\"]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    axes[0].imshow(sample_frame)\n",
    "    axes[0].set_title(\"Original\", fontsize=10)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    for i, style in enumerate(grade_styles):\n",
    "        graded = apply_color_grade(sample_frame, style)\n",
    "        axes[i + 1].imshow(graded)\n",
    "        axes[i + 1].set_title(style.replace('_', ' ').title(), fontsize=10)\n",
    "        axes[i + 1].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Color Grading - Comparaison des styles\", fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Styles disponibles : {', '.join(grade_styles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Interpretation : Stop-motion et color grading\n",
    "\n",
    "| Technique | Parametres | Effet visuel |\n",
    "|-----------|-----------|---------------|\n",
    "| Stop-motion (hold=4) | 6 fps effectif | Saccade animee, look retro |\n",
    "| Teal & Orange | Ombres bleutees, lumieres orange | Style blockbuster hollywoodien |\n",
    "| Vintage | Desaturation + sepia | Effet film ancien, nostalgie |\n",
    "| Cold | Bleus renforces, contraste eleve | Atmosphere thriller, tension |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le stop-motion est obtenu en repetant chaque frame N fois (reduction du FPS effectif)\n",
    "2. Le jitter (tremblements aleatoires) simule l'imprecision du positionnement manual\n",
    "3. Le color grading professionnel utilise des LUTs (Look-Up Tables), ici simule par calcul\n",
    "\n",
    "## Section 4 : Collages video et animations de texte\n",
    "\n",
    "Les compositions video permettent d'afficher plusieurs flux simultanement.\n",
    "Les animations de texte ajoutent un impact visuel pour les titres et generiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collage video : grille 2x2 avec differents styles\n",
    "print(\"\\n--- COLLAGE VIDEO ET ANIMATIONS ---\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def create_video_grid(frame_sets: List[List[np.ndarray]],\n",
    "                      grid_size: Tuple[int, int] = (2, 2),\n",
    "                      cell_size: Tuple[int, int] = (320, 240),\n",
    "                      border: int = 4) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Cree une grille video a partir de plusieurs sequences de frames.\n",
    "    \n",
    "    Args:\n",
    "        frame_sets: Liste de sequences de frames\n",
    "        grid_size: (colonnes, lignes)\n",
    "        cell_size: (largeur, hauteur) de chaque cellule\n",
    "        border: Epaisseur des bordures en pixels\n",
    "    \"\"\"\n",
    "    cols, rows = grid_size\n",
    "    total_w = cols * cell_size[0] + (cols + 1) * border\n",
    "    total_h = rows * cell_size[1] + (rows + 1) * border\n",
    "    \n",
    "    n_frames = min(len(fs) for fs in frame_sets)\n",
    "    result = []\n",
    "    \n",
    "    for f_idx in range(n_frames):\n",
    "        canvas = np.zeros((total_h, total_w, 3), dtype=np.uint8)\n",
    "        canvas[:, :] = [30, 30, 40]  # Fond sombre\n",
    "        \n",
    "        for s_idx, frames in enumerate(frame_sets):\n",
    "            if s_idx >= cols * rows:\n",
    "                break\n",
    "            row = s_idx // cols\n",
    "            col = s_idx % cols\n",
    "            \n",
    "            x = border + col * (cell_size[0] + border)\n",
    "            y = border + row * (cell_size[1] + border)\n",
    "            \n",
    "            # Redimensionner la frame\n",
    "            img = Image.fromarray(frames[f_idx]).resize(cell_size, Image.LANCZOS)\n",
    "            canvas[y:y + cell_size[1], x:x + cell_size[0]] = np.array(img)\n",
    "        \n",
    "        result.append(canvas)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Generer les variantes pour la grille\n",
    "print(\"Generation des variantes pour la grille 2x2...\")\n",
    "watercolor_frames = [apply_watercolor_effect(f) for f in source_frames]\n",
    "sketch_frames = [apply_sketch_effect(f) for f in source_frames]\n",
    "teal_frames = [apply_color_grade(f, 'teal_orange') for f in source_frames]\n",
    "\n",
    "grid_frames = create_video_grid(\n",
    "    [source_frames, watercolor_frames, sketch_frames, teal_frames],\n",
    "    grid_size=(2, 2), cell_size=(320, 240), border=4\n",
    ")\n",
    "\n",
    "print(f\"Grille generee : {len(grid_frames)} frames\")\n",
    "print(f\"Resolution grille : {grid_frames[0].shape[1]}x{grid_frames[0].shape[0]}\")\n",
    "\n",
    "# Apercu\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "preview_idx = [0, len(grid_frames) // 2, len(grid_frames) - 1]\n",
    "labels_grid = [\"Original\", \"Aquarelle\", \"Dessin\", \"Teal & Orange\"]\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(grid_frames[preview_idx[i]])\n",
    "    ax.set_title(f\"t = {preview_idx[i] / video_fps:.1f}s\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Collage video 2x2 : Original / Aquarelle / Dessin / Teal & Orange\",\n",
    "             fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if dependencies.get('imageio', False) and save_results:\n",
    "    grid_path = OUTPUT_DIR / \"creative_grid_collage.mp4\"\n",
    "    writer = imageio.get_writer(str(grid_path), fps=video_fps, codec='libx264')\n",
    "    for f in grid_frames:\n",
    "        writer.append_data(f)\n",
    "    writer.close()\n",
    "    print(f\"Video sauvegardee : {grid_path.name} ({grid_path.stat().st_size / 1024:.0f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animations de texte creatives\n",
    "if enable_text_animation:\n",
    "    print(\"\\n--- ANIMATIONS DE TEXTE ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    def create_text_animation(text: str, width: int, height: int,\n",
    "                              duration: float, fps: int,\n",
    "                              style: str = \"typewriter\") -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Cree une animation de texte.\n",
    "        \n",
    "        Styles : typewriter, slide_in, fade_in, scale_up\n",
    "        \"\"\"\n",
    "        n_frames = int(duration * fps)\n",
    "        frames = []\n",
    "        \n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 42)\n",
    "        except (OSError, IOError):\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        for i in range(n_frames):\n",
    "            t = i / max(n_frames - 1, 1)\n",
    "            img = Image.new('RGB', (width, height), (15, 20, 40))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            \n",
    "            if style == \"typewriter\":\n",
    "                # Reveler caractere par caractere\n",
    "                chars_visible = int(len(text) * min(t * 2, 1.0))\n",
    "                visible_text = text[:chars_visible]\n",
    "                bbox = draw.textbbox((0, 0), visible_text, font=font)\n",
    "                tw = bbox[2] - bbox[0]\n",
    "                draw.text(((width - tw) // 2, height // 2 - 20), visible_text,\n",
    "                          fill=(255, 255, 255), font=font)\n",
    "                # Curseur clignotant\n",
    "                if chars_visible < len(text) and (i // (fps // 4)) % 2 == 0:\n",
    "                    cursor_x = (width - tw) // 2 + tw\n",
    "                    draw.text((cursor_x, height // 2 - 20), \"|\",\n",
    "                              fill=(100, 180, 255), font=font)\n",
    "            \n",
    "            elif style == \"slide_in\":\n",
    "                # Glissement depuis la droite\n",
    "                bbox = draw.textbbox((0, 0), text, font=font)\n",
    "                tw = bbox[2] - bbox[0]\n",
    "                ease = 1 - (1 - t) ** 3  # Ease-out cubic\n",
    "                x = int(width + tw - ease * (width + tw - (width - tw) // 2))\n",
    "                draw.text((x, height // 2 - 20), text,\n",
    "                          fill=(255, 255, 255), font=font)\n",
    "            \n",
    "            elif style == \"fade_in\":\n",
    "                # Apparition progressive\n",
    "                bbox = draw.textbbox((0, 0), text, font=font)\n",
    "                tw = bbox[2] - bbox[0]\n",
    "                alpha = min(t * 3, 1.0)\n",
    "                color = tuple(int(255 * alpha) for _ in range(3))\n",
    "                draw.text(((width - tw) // 2, height // 2 - 20), text,\n",
    "                          fill=color, font=font)\n",
    "            \n",
    "            frames.append(np.array(img))\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    # Generer les trois styles\n",
    "    text_demo = \"Workflows Creatifs\"\n",
    "    anim_styles = [\"typewriter\", \"slide_in\", \"fade_in\"]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(anim_styles), 4, figsize=(16, 9))\n",
    "    \n",
    "    for row, style in enumerate(anim_styles):\n",
    "        anim_frames = create_text_animation(text_demo, video_width, video_height,\n",
    "                                            2.0, video_fps, style)\n",
    "        sample_idx = np.linspace(0, len(anim_frames) - 1, 4, dtype=int)\n",
    "        for col, idx in enumerate(sample_idx):\n",
    "            axes[row, col].imshow(anim_frames[idx])\n",
    "            axes[row, col].set_title(f\"{style} t={idx/video_fps:.2f}s\", fontsize=8)\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Animations de texte - 3 styles\", fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Styles d'animation disponibles : {', '.join(anim_styles)}\")\n",
    "else:\n",
    "    print(\"Animations de texte desactivees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Interpretation : Collages et animations\n",
    "\n",
    "| Technique | Complexite | Usage |\n",
    "|-----------|-----------|-------|\n",
    "| Grille video 2x2 | Composition spatiale | Comparaisons cote a cote |\n",
    "| Typewriter | Revelation progressive | Titres narratifs |\n",
    "| Slide-in | Translation avec easing | Generiques, sous-titres |\n",
    "| Fade-in | Opacite progressive | Transitions douces |\n",
    "\n",
    "**Points cles** :\n",
    "1. L'easing (ease-out cubic) rend les mouvements plus naturels que le lineaire\n",
    "2. Les grilles video sont utiles pour les comparaisons avant/apres ou multi-styles\n",
    "3. La combinaison de ces techniques produit des videos professionnelles\n",
    "\n",
    "## Recapitulatif des techniques creatives\n",
    "\n",
    "| Technique | Temps CPU | GPU requis | Difficulte |\n",
    "|-----------|-----------|-----------|------------|\n",
    "| Style transfer (filtres) | Rapide | Non | Facile |\n",
    "| Style transfer (IA) | Lent | Oui (~16 GB) | Avance |\n",
    "| Music video sync | Rapide | Non | Moyen |\n",
    "| Stop-motion | Rapide | Non | Facile |\n",
    "| Color grading | Rapide | Non | Moyen |\n",
    "| Collage video | Moyen | Non | Facile |\n",
    "| Animation texte | Rapide | Non | Moyen |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif - Choix du style\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Choisissez un style pour generer une video complete.\")\n",
    "    print(\"Styles : oil, watercolor, sketch, teal_orange, vintage, cold\")\n",
    "    print(\"(Laissez vide pour passer)\")\n",
    "    \n",
    "    try:\n",
    "        chosen_style = input(\"\\nStyle (ou vide) : \").strip()\n",
    "        \n",
    "        if chosen_style:\n",
    "            style_functions = {\n",
    "                \"oil\": apply_oil_painting_effect,\n",
    "                \"watercolor\": apply_watercolor_effect,\n",
    "                \"sketch\": apply_sketch_effect,\n",
    "            }\n",
    "            grade_functions = {\n",
    "                \"teal_orange\": lambda f: apply_color_grade(f, \"teal_orange\"),\n",
    "                \"vintage\": lambda f: apply_color_grade(f, \"vintage\"),\n",
    "                \"cold\": lambda f: apply_color_grade(f, \"cold\"),\n",
    "            }\n",
    "            all_functions = {**style_functions, **grade_functions}\n",
    "            \n",
    "            if chosen_style in all_functions:\n",
    "                print(f\"Application du style '{chosen_style}'...\")\n",
    "                custom_frames = [all_functions[chosen_style](f) for f in source_frames]\n",
    "                print(f\"Style applique sur {len(custom_frames)} frames\")\n",
    "            else:\n",
    "                print(f\"Style '{chosen_style}' non reconnu\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type:\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Video source : {video_width}x{video_height} @ {video_fps}fps, {video_duration}s\")\n",
    "print(f\"Frames source : {len(source_frames)}\")\n",
    "\n",
    "techniques_used = []\n",
    "if enable_style_transfer: techniques_used.append(\"Style transfer\")\n",
    "if enable_music_video: techniques_used.append(\"Music video\")\n",
    "if enable_stop_motion: techniques_used.append(\"Stop-motion\")\n",
    "if enable_color_grading: techniques_used.append(\"Color grading\")\n",
    "if enable_text_animation: techniques_used.append(\"Text animation\")\n",
    "print(f\"Techniques appliquees : {', '.join(techniques_used)}\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('creative_*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(f\"\\nDependances utilisees :\")\n",
    "for dep, available in dependencies.items():\n",
    "    status = \"utilisee\" if available else \"non disponible\"\n",
    "    print(f\"  {dep} : {status}\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 04-3 : Sora API - Generation Video Cloud\")\n",
    "print(f\"2. Notebook 04-4 : Pipeline Video de Production (pipeline complet)\")\n",
    "\n",
    "print(f\"\\nNotebook 04-2 Creative Video Workflows termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}