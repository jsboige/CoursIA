{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Pipeline Video de Production\n",
    "\n",
    "**Module :** 04-Applications  \n",
    "**Niveau :** Applications  \n",
    "**Technologies :** OpenAI API, diffusers, moviepy, pydub, Pillow  \n",
    "**Duree estimee :** 55 minutes  \n",
    "**VRAM :** ~18 GB (optionnel, CPU possible pour l'essentiel)  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Generer un script video a partir d'un sujet avec un LLM\n",
    "- [ ] Produire des images pour chaque scene (DALL-E ou generation locale)\n",
    "- [ ] Animer les images avec des effets cinematographiques (zoom, pan, avec moviepy)\n",
    "- [ ] Generer une narration vocale avec OpenAI TTS\n",
    "- [ ] Creer des sous-titres synchronises a partir du texte de narration\n",
    "- [ ] Assembler le pipeline complet : video + audio + sous-titres\n",
    "- [ ] Evaluer la qualite avec des metriques de production\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Ensemble des notebooks Video 01-1 a 04-3\n",
    "- Notebooks Audio (recommandes pour la partie TTS et sous-titres)\n",
    "- Packages : `moviepy`, `Pillow`, `numpy`, `matplotlib`, `openai`, `pydub`\n",
    "- Optionnel : `diffusers`, `torch` pour la generation d'images locale\n",
    "- Cle API OpenAI pour TTS et generation de contenu\n",
    "\n",
    "Ce notebook est le **point culminant** de la serie Video et fait reference\n",
    "aux techniques des series Video et Audio.\n",
    "\n",
    "**Navigation** : [<< 04-3 Sora API](04-3-Sora-API-Cloud-Video.ipynb) | [Audio 01-1 >>](../../Audio/01-Foundation/01-1-OpenAI-TTS-Intro.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres du pipeline\n",
    "video_topic = \"Intelligence Artificielle\"  # Sujet de la video\n",
    "video_width = 1280                 # Largeur video finale\n",
    "video_height = 720                 # Hauteur video finale\n",
    "video_fps = 30                     # FPS de la video finale\n",
    "scene_duration = 8                 # Duree par scene (secondes)\n",
    "output_format = \"mp4\"              # Format de sortie\n",
    "\n",
    "# Options du pipeline\n",
    "enable_llm_script = True           # Generer le script avec un LLM\n",
    "enable_image_gen = True            # Generer les images des scenes\n",
    "enable_animation = True            # Animer les images\n",
    "enable_tts = True                  # Generer la narration TTS\n",
    "enable_subtitles = True            # Generer les sous-titres\n",
    "enable_assembly = True             # Assembler le tout\n",
    "use_mock_api = True                # Utiliser des reponses simulees si API indisponible\n",
    "save_results = True                # Sauvegarder les resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import struct\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Resolution GENAI_ROOT\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.video_helpers import get_video_info, extract_frames, display_frame_grid\n",
    "        print(\"Helpers video importes\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Helpers video non disponibles ({e}) - mode autonome\")\n",
    "\n",
    "# Repertoire de sortie\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'video'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PIPELINE_DIR = OUTPUT_DIR / 'pipeline'\n",
    "PIPELINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('production_pipeline')\n",
    "\n",
    "print(f\"Pipeline Video de Production\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Sujet : {video_topic}\")\n",
    "print(f\"Video : {video_width}x{video_height} @ {video_fps}fps\")\n",
    "print(f\"Sortie : {PIPELINE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et verification des dependances\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification des dependances\n",
    "print(\"\\n--- VERIFICATION DES DEPENDANCES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "dependencies = {}\n",
    "\n",
    "try:\n",
    "    import moviepy.editor as mpy\n",
    "    dependencies['moviepy'] = True\n",
    "    print(f\"moviepy : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['moviepy'] = False\n",
    "    print(f\"moviepy : NON INSTALLE (pip install moviepy)\")\n",
    "\n",
    "try:\n",
    "    import imageio\n",
    "    dependencies['imageio'] = True\n",
    "    print(f\"imageio : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['imageio'] = False\n",
    "    print(f\"imageio : NON INSTALLE\")\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    dependencies['openai'] = True\n",
    "    print(f\"openai : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['openai'] = False\n",
    "    print(f\"openai : NON INSTALLE (pip install openai)\")\n",
    "\n",
    "try:\n",
    "    from pydub import AudioSegment\n",
    "    dependencies['pydub'] = True\n",
    "    print(f\"pydub : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['pydub'] = False\n",
    "    print(f\"pydub : NON INSTALLE (pip install pydub)\")\n",
    "\n",
    "try:\n",
    "    import diffusers\n",
    "    import torch\n",
    "    dependencies['diffusers'] = True\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print(f\"diffusers : disponible (GPU: {gpu_available})\")\n",
    "except ImportError:\n",
    "    dependencies['diffusers'] = False\n",
    "    print(f\"diffusers : NON INSTALLE (optionnel)\")\n",
    "\n",
    "# Verifier cle OpenAI\n",
    "openai_key = os.environ.get('OPENAI_API_KEY', '')\n",
    "openai_base = os.environ.get('OPENAI_BASE_URL', '')\n",
    "api_available = bool(openai_key and dependencies.get('openai', False))\n",
    "\n",
    "print(f\"\\nAPI OpenAI : {'disponible' if api_available else 'non disponible'}\")\n",
    "print(f\"Mode mock : {use_mock_api or not api_available}\")\n",
    "\n",
    "available_count = sum(dependencies.values())\n",
    "total_count = len(dependencies)\n",
    "print(f\"Dependances disponibles : {available_count}/{total_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Etape 1 : Generation du script video avec LLM\n",
    "\n",
    "Le pipeline commence par la generation d'un script structure.\n",
    "Un LLM (GPT-4 ou equivalent) produit une liste de scenes avec :\n",
    "- Description visuelle (pour la generation d'images)\n",
    "- Texte de narration (pour le TTS)\n",
    "- Instructions de camera (pour l'animation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etape 1 : Generation du script\n",
    "print(\"\\n--- ETAPE 1 : GENERATION DU SCRIPT ---\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "\n",
    "def generate_script_with_llm(topic: str) -> List[dict]:\n",
    "    \"\"\"Genere un script video a partir d'un sujet en utilisant l'API OpenAI.\"\"\"\n",
    "    if api_available and not use_mock_api:\n",
    "        try:\n",
    "            client_kwargs = {'api_key': openai_key}\n",
    "            if openai_base:\n",
    "                client_kwargs['base_url'] = openai_base\n",
    "            client = OpenAI(**client_kwargs)\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": (\n",
    "                        \"Tu es un scenariste de videos educatives. \"\n",
    "                        \"Genere un script structure en JSON avec 5 scenes. \"\n",
    "                        \"Chaque scene a : title, visual_description, narration, camera_motion. \"\n",
    "                        \"Reponds uniquement avec le JSON.\"\n",
    "                    )},\n",
    "                    {\"role\": \"user\", \"content\": f\"Cree un script video sur : {topic}\"}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=1500,\n",
    "            )\n",
    "            \n",
    "            content = response.choices[0].message.content\n",
    "            # Extraire le JSON de la reponse\n",
    "            if '```json' in content:\n",
    "                content = content.split('```json')[1].split('```')[0]\n",
    "            elif '```' in content:\n",
    "                content = content.split('```')[1].split('```')[0]\n",
    "            \n",
    "            scenes = json.loads(content)\n",
    "            if isinstance(scenes, dict) and 'scenes' in scenes:\n",
    "                scenes = scenes['scenes']\n",
    "            return scenes\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur API LLM : {type(e).__name__} - {str(e)[:80]}\")\n",
    "            print(\"Utilisation du script par defaut\")\n",
    "    \n",
    "    # Script par defaut (mode demo)\n",
    "    return [\n",
    "        {\n",
    "            \"title\": \"Introduction\",\n",
    "            \"visual_description\": \"Un reseau de neurones lumineux sur fond sombre, \"\n",
    "                                  \"avec des connexions qui s'illuminent progressivement\",\n",
    "            \"narration\": f\"Bienvenue dans cette exploration de l'{topic}. \"\n",
    "                         f\"Nous allons decouvrir comment cette technologie transforme notre monde.\",\n",
    "            \"camera_motion\": \"zoom_in\",\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Origines historiques\",\n",
    "            \"visual_description\": \"Une frise chronologique avec des portraits de pionniers \"\n",
    "                                  \"de l'IA : Turing, McCarthy, Minsky\",\n",
    "            \"narration\": f\"L'{topic} trouve ses racines dans les annees 1950, \"\n",
    "                         f\"avec les travaux fondateurs d'Alan Turing et de John McCarthy.\",\n",
    "            \"camera_motion\": \"pan_right\",\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Fonctionnement\",\n",
    "            \"visual_description\": \"Schema anime montrant des donnees qui traversent \"\n",
    "                                  \"les couches d'un reseau de neurones profond\",\n",
    "            \"narration\": \"Les systemes modernes reposent sur des reseaux de neurones \"\n",
    "                         \"profonds qui apprennent a partir de milliards d'exemples.\",\n",
    "            \"camera_motion\": \"slow_zoom\",\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Applications\",\n",
    "            \"visual_description\": \"Montage d'applications : voiture autonome, diagnostic medical, \"\n",
    "                                  \"assistant vocal, traduction automatique\",\n",
    "            \"narration\": f\"Aujourd'hui, l'{topic} est presente partout : des voitures autonomes \"\n",
    "                         f\"aux assistants vocaux, en passant par la medecine et la traduction.\",\n",
    "            \"camera_motion\": \"pan_left\",\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Perspectives\",\n",
    "            \"visual_description\": \"Vue futuriste d'une ville connectee avec des flux \"\n",
    "                                  \"de donnees visibles dans le ciel\",\n",
    "            \"narration\": \"L'avenir nous reserve des avancees encore plus spectaculaires. \"\n",
    "                         \"La question n'est plus si, mais comment nous guiderons cette revolution.\",\n",
    "            \"camera_motion\": \"zoom_out\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "# Generer le script\n",
    "script_scenes = generate_script_with_llm(video_topic)\n",
    "\n",
    "print(f\"Script genere : {len(script_scenes)} scenes\")\n",
    "print(f\"Duree totale estimee : {len(script_scenes) * scene_duration}s\")\n",
    "print(f\"\\nResume du script :\")\n",
    "for i, scene in enumerate(script_scenes):\n",
    "    print(f\"  Scene {i+1} : {scene['title']}\")\n",
    "    print(f\"    Visuel : {scene['visual_description'][:60]}...\")\n",
    "    print(f\"    Narration : {scene['narration'][:60]}...\")\n",
    "    print(f\"    Camera : {scene['camera_motion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Interpretation : Generation du script\n",
    "\n",
    "| Element du script | Role dans le pipeline |\n",
    "|-------------------|----------------------|\n",
    "| `title` | Identifiant de la scene, sous-titrage des chapitres |\n",
    "| `visual_description` | Prompt pour la generation d'images (DALL-E ou local) |\n",
    "| `narration` | Texte pour le TTS (voix off) et les sous-titres |\n",
    "| `camera_motion` | Instruction d'animation (zoom, pan, etc.) |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le script structure sert de plan directeur pour toutes les etapes suivantes\n",
    "2. La qualite du prompt LLM influence directement la coherence de la video finale\n",
    "3. Un bon script equilibre la duree des narrations avec le temps d'affichage des visuels\n",
    "\n",
    "## Etape 2 : Generation des images de chaque scene\n",
    "\n",
    "A partir des descriptions visuelles du script, nous generons une image\n",
    "representant chaque scene. En production, on utiliserait DALL-E 3 ou\n",
    "un modele local comme SDXL. Ici, nous generons des images stylisees avec Pillow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etape 2 : Generation des images\n",
    "print(\"\\n--- ETAPE 2 : GENERATION DES IMAGES ---\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "\n",
    "def generate_scene_image(scene: dict, width: int, height: int,\n",
    "                         scene_idx: int) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Genere une image representative pour une scene.\n",
    "    En mode demo, utilise Pillow pour un rendu stylise.\n",
    "    En production, utiliserait DALL-E 3 ou SDXL.\n",
    "    \"\"\"\n",
    "    img = Image.new('RGB', (width, height))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    rng = np.random.RandomState(scene_idx * 42)\n",
    "    \n",
    "    # Palette de couleurs par scene\n",
    "    palettes = [\n",
    "        [(20, 30, 80), (40, 60, 140), (100, 150, 255)],    # Bleu tech\n",
    "        [(60, 40, 20), (120, 80, 40), (200, 160, 100)],    # Sepia historique\n",
    "        [(20, 40, 30), (40, 100, 60), (80, 200, 120)],     # Vert data\n",
    "        [(50, 20, 60), (100, 40, 120), (180, 80, 220)],    # Violet innovation\n",
    "        [(20, 20, 40), (40, 60, 100), (80, 120, 200)],     # Bleu futuriste\n",
    "    ]\n",
    "    palette = palettes[scene_idx % len(palettes)]\n",
    "    \n",
    "    # Fond gradient\n",
    "    for y in range(height):\n",
    "        ratio = y / height\n",
    "        r = int(palette[0][0] * (1 - ratio) + palette[1][0] * ratio)\n",
    "        g = int(palette[0][1] * (1 - ratio) + palette[1][1] * ratio)\n",
    "        b = int(palette[0][2] * (1 - ratio) + palette[1][2] * ratio)\n",
    "        draw.line([(0, y), (width, y)], fill=(r, g, b))\n",
    "    \n",
    "    # Formes decoratives (abstraction de la description)\n",
    "    for _ in range(15):\n",
    "        cx = rng.randint(0, width)\n",
    "        cy = rng.randint(0, height)\n",
    "        radius = rng.randint(10, 60)\n",
    "        alpha = rng.uniform(0.2, 0.8)\n",
    "        color = tuple(int(c * alpha + palette[2][i] * (1 - alpha))\n",
    "                      for i, c in enumerate(palette[1]))\n",
    "        draw.ellipse([cx - radius, cy - radius, cx + radius, cy + radius],\n",
    "                     fill=color, outline=palette[2])\n",
    "    \n",
    "    # Lignes de connexion (style reseau)\n",
    "    points = [(rng.randint(0, width), rng.randint(0, height)) for _ in range(8)]\n",
    "    for i in range(len(points) - 1):\n",
    "        draw.line([points[i], points[i + 1]], fill=palette[2], width=1)\n",
    "    \n",
    "    # Titre de la scene\n",
    "    try:\n",
    "        font_title = ImageFont.truetype(\"arial.ttf\", 36)\n",
    "        font_sub = ImageFont.truetype(\"arial.ttf\", 18)\n",
    "    except (OSError, IOError):\n",
    "        font_title = ImageFont.load_default()\n",
    "        font_sub = ImageFont.load_default()\n",
    "    \n",
    "    # Bandeau titre\n",
    "    draw.rectangle([0, height - 100, width, height], fill=(0, 0, 0, 180))\n",
    "    draw.text((40, height - 90), scene['title'], fill=(255, 255, 255), font=font_title)\n",
    "    desc_short = scene['visual_description'][:70] + '...'\n",
    "    draw.text((40, height - 45), desc_short, fill=(180, 190, 210), font=font_sub)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "# Generer les images\n",
    "scene_images = []\n",
    "for i, scene in enumerate(script_scenes):\n",
    "    img = generate_scene_image(scene, video_width, video_height, i)\n",
    "    scene_images.append(img)\n",
    "    \n",
    "    if save_results:\n",
    "        img_path = PIPELINE_DIR / f\"scene_{i+1:02d}.png\"\n",
    "        img.save(str(img_path))\n",
    "    \n",
    "    print(f\"  Scene {i+1} : {scene['title']} - Image generee\")\n",
    "\n",
    "print(f\"\\n{len(scene_images)} images generees ({video_width}x{video_height})\")\n",
    "\n",
    "# Apercu en grille\n",
    "n_scenes = len(scene_images)\n",
    "cols = min(n_scenes, 5)\n",
    "fig, axes = plt.subplots(1, cols, figsize=(16, 4))\n",
    "if cols == 1:\n",
    "    axes = [axes]\n",
    "for i in range(cols):\n",
    "    axes[i].imshow(scene_images[i])\n",
    "    axes[i].set_title(f\"Scene {i+1}: {script_scenes[i]['title']}\", fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle(\"Images des scenes du pipeline\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Etape 3 : Animation des images\n",
    "\n",
    "Chaque image statique est transformee en sequence animee avec un mouvement\n",
    "de camera cinematographique : zoom avant/arriere, panoramique gauche/droite.\n",
    "Ces effets, connus sous le nom d'\"effets Ken Burns\", donnent vie aux images fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etape 3 : Animation des images avec effets camera\n",
    "if enable_animation:\n",
    "    print(\"\\n--- ETAPE 3 : ANIMATION DES IMAGES ---\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    def animate_image(image: Image.Image, motion: str, duration: float,\n",
    "                      fps: int) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Anime une image avec un mouvement de camera.\n",
    "        \n",
    "        Mouvements : zoom_in, zoom_out, pan_left, pan_right, slow_zoom\n",
    "        \"\"\"\n",
    "        w, h = image.size\n",
    "        n_frames = int(duration * fps)\n",
    "        frames = []\n",
    "        \n",
    "        # Precharger l'image en haute resolution pour le zoom\n",
    "        scale_factor = 1.3\n",
    "        img_large = image.resize((int(w * scale_factor), int(h * scale_factor)), Image.LANCZOS)\n",
    "        lw, lh = img_large.size\n",
    "        \n",
    "        for i in range(n_frames):\n",
    "            t = i / max(n_frames - 1, 1)\n",
    "            ease_t = t * t * (3 - 2 * t)  # Smoothstep easing\n",
    "            \n",
    "            if motion == \"zoom_in\":\n",
    "                zoom = 1.0 + 0.25 * ease_t\n",
    "                crop_w = int(w / zoom)\n",
    "                crop_h = int(h / zoom)\n",
    "                left = (w - crop_w) // 2\n",
    "                top = (h - crop_h) // 2\n",
    "                frame = image.crop((left, top, left + crop_w, top + crop_h))\n",
    "                frame = frame.resize((w, h), Image.LANCZOS)\n",
    "            \n",
    "            elif motion == \"zoom_out\":\n",
    "                zoom = 1.25 - 0.25 * ease_t\n",
    "                crop_w = int(lw / zoom)\n",
    "                crop_h = int(lh / zoom)\n",
    "                left = (lw - crop_w) // 2\n",
    "                top = (lh - crop_h) // 2\n",
    "                frame = img_large.crop((left, top, left + crop_w, top + crop_h))\n",
    "                frame = frame.resize((w, h), Image.LANCZOS)\n",
    "            \n",
    "            elif motion == \"pan_right\":\n",
    "                offset = int((lw - w) * ease_t)\n",
    "                frame = img_large.crop((offset, (lh - h) // 2,\n",
    "                                        offset + w, (lh - h) // 2 + h))\n",
    "            \n",
    "            elif motion == \"pan_left\":\n",
    "                offset = int((lw - w) * (1 - ease_t))\n",
    "                frame = img_large.crop((offset, (lh - h) // 2,\n",
    "                                        offset + w, (lh - h) // 2 + h))\n",
    "            \n",
    "            elif motion == \"slow_zoom\":\n",
    "                zoom = 1.0 + 0.1 * ease_t\n",
    "                crop_w = int(w / zoom)\n",
    "                crop_h = int(h / zoom)\n",
    "                left = (w - crop_w) // 2\n",
    "                top = (h - crop_h) // 2\n",
    "                frame = image.crop((left, top, left + crop_w, top + crop_h))\n",
    "                frame = frame.resize((w, h), Image.LANCZOS)\n",
    "            \n",
    "            else:\n",
    "                frame = image  # Pas de mouvement\n",
    "            \n",
    "            frames.append(np.array(frame))\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    # Animer toutes les scenes\n",
    "    animated_scenes = []\n",
    "    total_frames = 0\n",
    "    \n",
    "    for i, (image, scene) in enumerate(zip(scene_images, script_scenes)):\n",
    "        motion = scene.get('camera_motion', 'slow_zoom')\n",
    "        scene_frames = animate_image(image, motion, scene_duration, video_fps)\n",
    "        animated_scenes.append(scene_frames)\n",
    "        total_frames += len(scene_frames)\n",
    "        print(f\"  Scene {i+1} : {scene['title']} - {len(scene_frames)} frames ({motion})\")\n",
    "    \n",
    "    print(f\"\\nTotal frames animees : {total_frames}\")\n",
    "    print(f\"Duree totale : {total_frames / video_fps:.1f}s\")\n",
    "else:\n",
    "    # Pas d'animation : frames statiques\n",
    "    animated_scenes = []\n",
    "    for image in scene_images:\n",
    "        static = [np.array(image)] * (scene_duration * video_fps)\n",
    "        animated_scenes.append(static)\n",
    "    print(\"Animation desactivee - frames statiques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Etape 4 : Generation de la narration TTS\n",
    "\n",
    "La narration vocale est generee a partir du texte de chaque scene avec OpenAI TTS.\n",
    "En mode demonstration, nous generons un signal audio synthetique (ton sinusoidal)\n",
    "de la bonne duree pour simuler la narration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etape 4 : Generation de la narration TTS\n",
    "if enable_tts:\n",
    "    print(\"\\n--- ETAPE 4 : GENERATION NARRATION TTS ---\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    def generate_tts_audio(text: str, scene_idx: int,\n",
    "                           target_duration: float) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"\n",
    "        Genere l'audio TTS pour une scene.\n",
    "        Retourne (audio_samples, sample_rate).\n",
    "        \"\"\"\n",
    "        sample_rate = 22050\n",
    "        \n",
    "        if api_available and not use_mock_api:\n",
    "            try:\n",
    "                client_kwargs = {'api_key': openai_key}\n",
    "                if openai_base:\n",
    "                    client_kwargs['base_url'] = openai_base\n",
    "                client = OpenAI(**client_kwargs)\n",
    "                \n",
    "                response = client.audio.speech.create(\n",
    "                    model=\"tts-1\",\n",
    "                    voice=\"nova\",\n",
    "                    input=text,\n",
    "                    response_format=\"wav\",\n",
    "                )\n",
    "                \n",
    "                audio_path = PIPELINE_DIR / f\"narration_{scene_idx:02d}.wav\"\n",
    "                response.stream_to_file(str(audio_path))\n",
    "                \n",
    "                if dependencies.get('pydub', False):\n",
    "                    audio_seg = AudioSegment.from_wav(str(audio_path))\n",
    "                    samples = np.array(audio_seg.get_array_of_samples(), dtype=np.float32)\n",
    "                    samples = samples / max(abs(samples.max()), abs(samples.min()), 1)\n",
    "                    return samples, audio_seg.frame_rate\n",
    "            except Exception as e:\n",
    "                print(f\"    Erreur TTS API : {type(e).__name__} - {str(e)[:60]}\")\n",
    "        \n",
    "        # Mode demonstration : generer un signal audio synthetique\n",
    "        n_samples = int(target_duration * sample_rate)\n",
    "        t_arr = np.linspace(0, target_duration, n_samples)\n",
    "        \n",
    "        # Simuler la prosodie de la parole avec des modulations\n",
    "        base_freq = 150 + scene_idx * 10  # Frequence de base variable\n",
    "        # Modulation de frequence pour simuler l'intonation\n",
    "        freq_mod = base_freq + 30 * np.sin(2 * np.pi * 0.5 * t_arr)\n",
    "        # Signal avec enveloppe\n",
    "        phase = 2 * np.pi * np.cumsum(freq_mod) / sample_rate\n",
    "        signal = 0.3 * np.sin(phase)\n",
    "        # Enveloppe : fade in/out\n",
    "        envelope = np.ones(n_samples)\n",
    "        fade_len = int(0.1 * sample_rate)\n",
    "        envelope[:fade_len] = np.linspace(0, 1, fade_len)\n",
    "        envelope[-fade_len:] = np.linspace(1, 0, fade_len)\n",
    "        signal = signal * envelope\n",
    "        \n",
    "        return signal.astype(np.float32), sample_rate\n",
    "    \n",
    "    # Generer la narration pour chaque scene\n",
    "    narration_audio = []\n",
    "    narration_texts = []\n",
    "    \n",
    "    for i, scene in enumerate(script_scenes):\n",
    "        text = scene['narration']\n",
    "        narration_texts.append(text)\n",
    "        \n",
    "        audio_data, sr = generate_tts_audio(text, i, scene_duration)\n",
    "        narration_audio.append((audio_data, sr))\n",
    "        \n",
    "        duration_actual = len(audio_data) / sr\n",
    "        print(f\"  Scene {i+1} : {duration_actual:.1f}s audio \"\n",
    "              f\"({len(text)} chars, {sr}Hz)\")\n",
    "    \n",
    "    # Concatener tout l'audio\n",
    "    combined_sr = narration_audio[0][1]\n",
    "    combined_audio = np.concatenate([a[0] for a in narration_audio])\n",
    "    total_audio_duration = len(combined_audio) / combined_sr\n",
    "    \n",
    "    print(f\"\\nAudio total : {total_audio_duration:.1f}s, {combined_sr}Hz\")\n",
    "    print(f\"Echantillons : {len(combined_audio)}\")\n",
    "    \n",
    "    # Sauvegarder l'audio combine en WAV\n",
    "    if save_results:\n",
    "        combined_audio_path = PIPELINE_DIR / \"narration_combined.wav\"\n",
    "        # Ecriture WAV simple\n",
    "        audio_int16 = (combined_audio * 32767).astype(np.int16)\n",
    "        import wave\n",
    "        with wave.open(str(combined_audio_path), 'w') as wav_file:\n",
    "            wav_file.setnchannels(1)\n",
    "            wav_file.setsampwidth(2)\n",
    "            wav_file.setframerate(combined_sr)\n",
    "            wav_file.writeframes(audio_int16.tobytes())\n",
    "        print(f\"Audio sauvegarde : {combined_audio_path.name} \"\n",
    "              f\"({combined_audio_path.stat().st_size / 1024:.0f} KB)\")\n",
    "else:\n",
    "    narration_audio = []\n",
    "    narration_texts = [s['narration'] for s in script_scenes]\n",
    "    print(\"TTS desactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Etape 5 : Sous-titres et assemblage final\n",
    "\n",
    "Les sous-titres sont generes a partir du texte de narration avec un timing\n",
    "synchronise sur chaque scene. L'assemblage final combine video + audio + sous-titres\n",
    "en un seul fichier MP4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etape 5 : Sous-titres et assemblage final\n",
    "print(\"\\n--- ETAPE 5 : SOUS-TITRES ET ASSEMBLAGE ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generation des sous-titres\n",
    "if enable_subtitles:\n",
    "    print(\"Generation des sous-titres...\")\n",
    "    \n",
    "    def generate_srt(scenes: List[dict], duration_per_scene: float) -> str:\n",
    "        \"\"\"Genere un fichier SRT a partir des textes de narration.\"\"\"\n",
    "        srt_content = \"\"\n",
    "        for i, scene in enumerate(scenes):\n",
    "            start_time = i * duration_per_scene\n",
    "            end_time = (i + 1) * duration_per_scene\n",
    "            \n",
    "            # Formater en HH:MM:SS,mmm\n",
    "            def fmt_time(seconds):\n",
    "                h = int(seconds // 3600)\n",
    "                m = int((seconds % 3600) // 60)\n",
    "                s = int(seconds % 60)\n",
    "                ms = int((seconds % 1) * 1000)\n",
    "                return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n",
    "            \n",
    "            # Decouper le texte en segments de ~60 caracteres\n",
    "            text = scene['narration']\n",
    "            words = text.split()\n",
    "            lines = []\n",
    "            current_line = \"\"\n",
    "            for word in words:\n",
    "                if len(current_line) + len(word) + 1 > 60:\n",
    "                    lines.append(current_line)\n",
    "                    current_line = word\n",
    "                else:\n",
    "                    current_line = f\"{current_line} {word}\".strip()\n",
    "            if current_line:\n",
    "                lines.append(current_line)\n",
    "            \n",
    "            # Repartir le temps entre les lignes\n",
    "            time_per_line = duration_per_scene / max(len(lines), 1)\n",
    "            for j, line in enumerate(lines):\n",
    "                sub_start = start_time + j * time_per_line\n",
    "                sub_end = sub_start + time_per_line\n",
    "                sub_idx = i * 10 + j + 1  # Index unique\n",
    "                srt_content += f\"{sub_idx}\\n\"\n",
    "                srt_content += f\"{fmt_time(sub_start)} --> {fmt_time(sub_end)}\\n\"\n",
    "                srt_content += f\"{line}\\n\\n\"\n",
    "        \n",
    "        return srt_content\n",
    "    \n",
    "    srt_text = generate_srt(script_scenes, scene_duration)\n",
    "    \n",
    "    if save_results:\n",
    "        srt_path = PIPELINE_DIR / \"subtitles.srt\"\n",
    "        srt_path.write_text(srt_text, encoding='utf-8')\n",
    "        print(f\"Sous-titres sauvegardes : {srt_path.name}\")\n",
    "    \n",
    "    # Afficher un extrait\n",
    "    print(f\"\\nExtrait SRT (3 premiÃ¨res entrees) :\")\n",
    "    srt_lines = srt_text.strip().split('\\n\\n')[:3]\n",
    "    for block in srt_lines:\n",
    "        print(f\"  {block}\")\n",
    "\n",
    "# Assemblage final : ajouter les sous-titres aux frames\n",
    "print(\"\\nAssemblage video avec sous-titres incrustes...\")\n",
    "\n",
    "def burn_subtitle(frame: np.ndarray, text: str,\n",
    "                  width: int, height: int) -> np.ndarray:\n",
    "    \"\"\"Incruste un sous-titre en bas de la frame.\"\"\"\n",
    "    img = Image.fromarray(frame)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 22)\n",
    "    except (OSError, IOError):\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Fond semi-transparent pour le sous-titre\n",
    "    bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    tw = bbox[2] - bbox[0]\n",
    "    th = bbox[3] - bbox[1]\n",
    "    x = (width - tw) // 2\n",
    "    y = height - 70\n",
    "    \n",
    "    # Rectangle de fond\n",
    "    padding = 8\n",
    "    draw.rectangle(\n",
    "        [x - padding, y - padding, x + tw + padding, y + th + padding],\n",
    "        fill=(0, 0, 0)\n",
    "    )\n",
    "    draw.text((x, y), text, fill=(255, 255, 255), font=font)\n",
    "    \n",
    "    return np.array(img)\n",
    "\n",
    "\n",
    "# Assembler toutes les scenes avec transitions et sous-titres\n",
    "final_frames = []\n",
    "transition_frames_count = int(0.5 * video_fps)  # 0.5s de fondu entre scenes\n",
    "\n",
    "for scene_idx, (scene_frames, scene_data) in enumerate(zip(animated_scenes, script_scenes)):\n",
    "    text = scene_data['narration']\n",
    "    # Decouper le texte pour les sous-titres\n",
    "    words = text.split()\n",
    "    mid = len(words) // 2\n",
    "    sub_parts = [' '.join(words[:mid]), ' '.join(words[mid:])]\n",
    "    \n",
    "    for f_idx, frame in enumerate(scene_frames):\n",
    "        # Choisir la partie de sous-titre\n",
    "        part_idx = 0 if f_idx < len(scene_frames) // 2 else 1\n",
    "        if enable_subtitles and sub_parts[part_idx]:\n",
    "            # Limiter la longueur du sous-titre\n",
    "            sub_text = sub_parts[part_idx][:70]\n",
    "            frame = burn_subtitle(frame, sub_text, video_width, video_height)\n",
    "        final_frames.append(frame)\n",
    "    \n",
    "    # Transition fondu vers la scene suivante\n",
    "    if scene_idx < len(animated_scenes) - 1:\n",
    "        last_frame = scene_frames[-1]\n",
    "        next_first = animated_scenes[scene_idx + 1][0]\n",
    "        for t_idx in range(transition_frames_count):\n",
    "            alpha = t_idx / max(transition_frames_count - 1, 1)\n",
    "            blended = ((1 - alpha) * last_frame.astype(np.float32) +\n",
    "                       alpha * next_first.astype(np.float32)).astype(np.uint8)\n",
    "            final_frames.append(blended)\n",
    "\n",
    "print(f\"Frames finales : {len(final_frames)}\")\n",
    "print(f\"Duree video : {len(final_frames) / video_fps:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export de la video finale\n",
    "print(\"\\n--- EXPORT VIDEO FINALE ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "final_video_path = PIPELINE_DIR / f\"production_video.{output_format}\"\n",
    "\n",
    "if dependencies.get('imageio', False):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    writer = imageio.get_writer(\n",
    "        str(final_video_path), fps=video_fps, codec='libx264',\n",
    "        output_params=['-crf', '20', '-preset', 'medium']\n",
    "    )\n",
    "    for frame in final_frames:\n",
    "        writer.append_data(frame)\n",
    "    writer.close()\n",
    "    \n",
    "    export_time = time.time() - start_time\n",
    "    file_size_mb = final_video_path.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Video exportee : {final_video_path.name}\")\n",
    "    print(f\"  Resolution : {video_width}x{video_height}\")\n",
    "    print(f\"  FPS : {video_fps}\")\n",
    "    print(f\"  Duree : {len(final_frames) / video_fps:.1f}s\")\n",
    "    print(f\"  Taille : {file_size_mb:.2f} MB\")\n",
    "    print(f\"  Temps d'export : {export_time:.2f}s\")\n",
    "    \n",
    "    # Apercu final\n",
    "    n_preview = 6\n",
    "    preview_idx = np.linspace(0, len(final_frames) - 1, n_preview, dtype=int)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 7))\n",
    "    for ax, idx in zip(axes.flatten(), preview_idx):\n",
    "        ax.imshow(final_frames[idx])\n",
    "        ax.set_title(f\"t = {idx / video_fps:.1f}s\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(\"Pipeline Video de Production - Apercu final\", fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"imageio non disponible - export impossible\")\n",
    "\n",
    "# Metriques de qualite\n",
    "print(\"\\n--- METRIQUES DE QUALITE ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "metrics = {\n",
    "    \"Scenes\": len(script_scenes),\n",
    "    \"Duree totale\": f\"{len(final_frames) / video_fps:.1f}s\",\n",
    "    \"Resolution\": f\"{video_width}x{video_height}\",\n",
    "    \"FPS\": video_fps,\n",
    "    \"Frames totales\": len(final_frames),\n",
    "    \"Taille fichier\": f\"{file_size_mb:.2f} MB\" if dependencies.get('imageio', False) else \"N/A\",\n",
    "    \"Sous-titres\": \"Oui\" if enable_subtitles else \"Non\",\n",
    "    \"Narration audio\": \"Oui\" if enable_tts and narration_audio else \"Non\",\n",
    "    \"Animations camera\": \"Oui\" if enable_animation else \"Non\",\n",
    "    \"Transitions\": f\"{len(script_scenes) - 1} fondus\",\n",
    "}\n",
    "\n",
    "print(f\"{'Metrique':<25} {'Valeur':<30}\")\n",
    "print(\"-\" * 55)\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key:<23} {str(value):<30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Interpretation : Pipeline complet\n",
    "\n",
    "| Etape | Entree | Sortie | Outil |\n",
    "|-------|--------|--------|-------|\n",
    "| 1. Script | Sujet (texte) | Scenes structurees | LLM (GPT-4) |\n",
    "| 2. Images | Descriptions visuelles | PNG par scene | DALL-E / Pillow |\n",
    "| 3. Animation | Images + instructions camera | Sequences de frames | Pillow / moviepy |\n",
    "| 4. Narration | Texte de narration | Audio WAV | OpenAI TTS |\n",
    "| 5. Sous-titres | Texte de narration | SRT + incrustation | Python |\n",
    "| 6. Assemblage | Frames + audio + SRT | MP4 final | imageio / moviepy |\n",
    "\n",
    "**Points cles** :\n",
    "1. Chaque etape est independante et peut etre amelioree separement\n",
    "2. Le script LLM est le point de depart : un bon script produit une bonne video\n",
    "3. La synchronisation audio/video est essentielle pour un resultat professionnel\n",
    "4. En production, chaque etape utiliserait les meilleurs modeles disponibles\n",
    "\n",
    "## Recapitulatif de la serie Video\n",
    "\n",
    "Ce notebook conclut la serie Video. Voici un resume des competences acquises :\n",
    "\n",
    "| Tier | Notebooks | Competences |\n",
    "|------|-----------|-------------|\n",
    "| 01-Foundation | 01-1 a 01-5 | Operations de base, extraction, analyse, amelioration, generation |\n",
    "| 02-Advanced | 02-1 a 02-4 | Modeles avances, controle fin, evaluation qualite |\n",
    "| 03-Orchestration | 03-1 a 03-3 | Multi-modeles, workflows, ComfyUI |\n",
    "| 04-Applications | 04-1 a 04-4 | Videos educatives, workflows creatifs, API cloud, pipeline production |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif - Choix du sujet\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Entrez un sujet pour generer un nouveau pipeline video.\")\n",
    "    print(\"(Laissez vide pour passer)\")\n",
    "    \n",
    "    try:\n",
    "        custom_topic = input(\"\\nSujet (ou vide) : \").strip()\n",
    "        \n",
    "        if custom_topic:\n",
    "            print(f\"Pour generer une video sur '{custom_topic}',\")\n",
    "            print(f\"modifiez le parametre video_topic et relancez le notebook.\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type:\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Sujet : {video_topic}\")\n",
    "print(f\"Scenes : {len(script_scenes)}\")\n",
    "print(f\"Video : {video_width}x{video_height} @ {video_fps}fps\")\n",
    "print(f\"Frames finales : {len(final_frames)}\")\n",
    "print(f\"Duree : {len(final_frames) / video_fps:.1f}s\")\n",
    "\n",
    "# Etapes du pipeline\n",
    "pipeline_steps = [\n",
    "    (\"Script LLM\", enable_llm_script),\n",
    "    (\"Images\", enable_image_gen),\n",
    "    (\"Animation\", enable_animation),\n",
    "    (\"TTS\", enable_tts),\n",
    "    (\"Sous-titres\", enable_subtitles),\n",
    "    (\"Assemblage\", enable_assembly),\n",
    "]\n",
    "print(f\"\\nEtapes du pipeline :\")\n",
    "for step_name, enabled in pipeline_steps:\n",
    "    status = \"activee\" if enabled else \"desactivee\"\n",
    "    print(f\"  {step_name} : {status}\")\n",
    "\n",
    "if save_results and PIPELINE_DIR.exists():\n",
    "    generated_files = list(PIPELINE_DIR.glob('*'))\n",
    "    total_size_mb = sum(f.stat().st_size for f in generated_files) / (1024 * 1024)\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}, {total_size_mb:.1f} MB total) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(f\"\\nDependances utilisees :\")\n",
    "for dep, available in dependencies.items():\n",
    "    status = \"utilisee\" if available else \"non disponible\"\n",
    "    print(f\"  {dep} : {status}\")\n",
    "\n",
    "print(f\"\\n--- FIN DE LA SERIE VIDEO ---\")\n",
    "print(f\"Ce notebook conclut la serie Video (04-Applications).\")\n",
    "print(f\"Pour combiner audio et video, consultez la serie Audio :\")\n",
    "print(f\"  -> Audio/01-Foundation/01-1-OpenAI-TTS-Intro.ipynb\")\n",
    "\n",
    "print(f\"\\nNotebook 04-4 Production Video Pipeline termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}