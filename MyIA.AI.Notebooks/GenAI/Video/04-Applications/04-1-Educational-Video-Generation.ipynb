{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Generation de Videos Educatives\n",
    "\n",
    "**Module :** 04-Applications  \n",
    "**Niveau :** Applications  \n",
    "**Technologies :** moviepy, Pillow (text rendering), diffusers (optionnel), OpenAI API  \n",
    "**Duree estimee :** 45 minutes  \n",
    "**VRAM :** ~12 GB (optionnel, CPU possible pour l'essentiel)  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Generer des images de slides a partir de contenu textuel avec Pillow\n",
    "- [ ] Creer des transitions animees entre slides avec moviepy\n",
    "- [ ] Ajouter des overlays textuels et annotations sur la video\n",
    "- [ ] Assembler une video educative complete avec timing par section\n",
    "- [ ] (Optionnel) Ameliorer les visuels avec la generation d'images IA\n",
    "- [ ] Exporter la video finale avec marqueurs de chapitres\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Python 3.10+\n",
    "- Notebooks 01-1 a 03-3 (fondations video, techniques avancees, orchestration)\n",
    "- Packages : `moviepy`, `Pillow`, `numpy`, `matplotlib`\n",
    "- Optionnel : `diffusers`, `torch` pour la generation d'images IA\n",
    "- Optionnel : `openai` pour la generation de contenu via API\n",
    "\n",
    "**Navigation** : [<< 03-3 ComfyUI Video Workflows](../03-Orchestration/03-3-ComfyUI-Video-Workflows.ipynb) | [04-2 Workflows Creatifs >>](04-2-Creative-Video-Workflows.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres video educative\n",
    "slide_width = 1280                 # Largeur des slides en pixels\n",
    "slide_height = 720                 # Hauteur des slides en pixels\n",
    "slide_duration = 6                 # Duree par defaut d'un slide (secondes)\n",
    "transition_duration = 1.0          # Duree des transitions (secondes)\n",
    "output_fps = 30                    # FPS de la video finale\n",
    "output_format = \"mp4\"              # Format de sortie\n",
    "\n",
    "# Options de contenu\n",
    "enable_ai_visuals = False          # Utiliser la generation d'images IA\n",
    "enable_openai_content = False      # Generer le contenu avec OpenAI\n",
    "save_results = True                # Sauvegarder les resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Resolution GENAI_ROOT\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.video_helpers import get_video_info, extract_frames, display_frame_grid\n",
    "        print(\"Helpers video importes\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Helpers video non disponibles ({e}) - mode autonome\")\n",
    "\n",
    "# Repertoire de sortie\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'video'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('edu_video')\n",
    "\n",
    "print(f\"Generation de Videos Educatives\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Slides : {slide_width}x{slide_height}, {slide_duration}s/slide\")\n",
    "print(f\"Sortie : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et verification des dependances\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification des dependances\n",
    "print(\"\\n--- VERIFICATION DES DEPENDANCES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "dependencies = {}\n",
    "\n",
    "try:\n",
    "    import moviepy.editor as mpy\n",
    "    dependencies['moviepy'] = True\n",
    "    print(f\"moviepy : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['moviepy'] = False\n",
    "    print(f\"moviepy : NON INSTALLE (pip install moviepy)\")\n",
    "\n",
    "try:\n",
    "    import imageio\n",
    "    dependencies['imageio'] = True\n",
    "    print(f\"imageio : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['imageio'] = False\n",
    "    print(f\"imageio : NON INSTALLE (pip install imageio imageio-ffmpeg)\")\n",
    "\n",
    "try:\n",
    "    import diffusers\n",
    "    import torch\n",
    "    dependencies['diffusers'] = True\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print(f\"diffusers : disponible (GPU: {gpu_available})\")\n",
    "except ImportError:\n",
    "    dependencies['diffusers'] = False\n",
    "    print(f\"diffusers : NON INSTALLE (optionnel)\")\n",
    "\n",
    "try:\n",
    "    import openai\n",
    "    dependencies['openai'] = True\n",
    "    print(f\"openai : disponible\")\n",
    "except ImportError:\n",
    "    dependencies['openai'] = False\n",
    "    print(f\"openai : NON INSTALLE (optionnel)\")\n",
    "\n",
    "available_count = sum(dependencies.values())\n",
    "total_count = len(dependencies)\n",
    "print(f\"\\nDependances disponibles : {available_count}/{total_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Section 1 : Definition du contenu educatif\n",
    "\n",
    "La premiere etape de la creation d'une video educative est la definition du contenu.\n",
    "Chaque slide est defini par un titre, un corps de texte, et optionnellement des points cles.\n",
    "Cette structure permet de generer automatiquement les visuels et le timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition du contenu educatif sous forme structuree\n",
    "print(\"\\n--- DEFINITION DU CONTENU EDUCATIF ---\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Structure du cours : liste de slides\n",
    "course_content = [\n",
    "    {\n",
    "        \"type\": \"title\",\n",
    "        \"title\": \"Introduction au Machine Learning\",\n",
    "        \"subtitle\": \"Concepts fondamentaux et applications\",\n",
    "        \"duration\": 4,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"content\",\n",
    "        \"title\": \"Qu'est-ce que le Machine Learning ?\",\n",
    "        \"bullets\": [\n",
    "            \"Apprentissage a partir de donnees\",\n",
    "            \"Amelioration avec l'experience\",\n",
    "            \"Generalisation a de nouveaux cas\",\n",
    "            \"Alternative a la programmation explicite\",\n",
    "        ],\n",
    "        \"duration\": slide_duration,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"content\",\n",
    "        \"title\": \"Types d'apprentissage\",\n",
    "        \"bullets\": [\n",
    "            \"Supervise : donnees etiquetees (classification, regression)\",\n",
    "            \"Non supervise : decouverte de structures (clustering)\",\n",
    "            \"Par renforcement : apprentissage par essai-erreur\",\n",
    "        ],\n",
    "        \"duration\": slide_duration,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"content\",\n",
    "        \"title\": \"Pipeline typique ML\",\n",
    "        \"bullets\": [\n",
    "            \"1. Collecte et preparation des donnees\",\n",
    "            \"2. Selection et entrainement du modele\",\n",
    "            \"3. Evaluation des performances\",\n",
    "            \"4. Deploiement et monitoring\",\n",
    "        ],\n",
    "        \"duration\": slide_duration,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"diagram\",\n",
    "        \"title\": \"Architecture d'un reseau de neurones\",\n",
    "        \"layers\": [4, 6, 6, 3],\n",
    "        \"labels\": [\"Entree\", \"Cachee 1\", \"Cachee 2\", \"Sortie\"],\n",
    "        \"duration\": slide_duration + 2,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"content\",\n",
    "        \"title\": \"Applications concretes\",\n",
    "        \"bullets\": [\n",
    "            \"Vision par ordinateur (detection d'objets)\",\n",
    "            \"Traitement du langage naturel (traduction)\",\n",
    "            \"Systemes de recommandation\",\n",
    "            \"Vehicules autonomes\",\n",
    "        ],\n",
    "        \"duration\": slide_duration,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"title\",\n",
    "        \"title\": \"Merci !\",\n",
    "        \"subtitle\": \"Questions et ressources supplementaires\",\n",
    "        \"duration\": 4,\n",
    "    },\n",
    "]\n",
    "\n",
    "total_duration = sum(s[\"duration\"] for s in course_content)\n",
    "total_duration_with_transitions = total_duration + transition_duration * (len(course_content) - 1)\n",
    "\n",
    "print(f\"Nombre de slides : {len(course_content)}\")\n",
    "print(f\"Duree contenu : {total_duration}s\")\n",
    "print(f\"Duree avec transitions : {total_duration_with_transitions:.1f}s\")\n",
    "print(f\"\\nStructure du cours :\")\n",
    "for i, slide in enumerate(course_content):\n",
    "    print(f\"  Slide {i+1} [{slide['type']}] : {slide['title']} ({slide['duration']}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Section 2 : Generation des images de slides\n",
    "\n",
    "Chaque slide est rendu sous forme d'image PNG avec Pillow.\n",
    "Le rendu comprend un fond colore, le titre, les points cles et un pied de page.\n",
    "Pour les slides de type \"diagram\", nous generons une visualisation specifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de rendu des slides avec Pillow\n",
    "print(\"\\n--- GENERATION DES IMAGES DE SLIDES ---\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Palette de couleurs professionnelle\n",
    "COLORS = {\n",
    "    \"bg_primary\": (30, 40, 80),       # Bleu fonce\n",
    "    \"bg_secondary\": (40, 55, 110),    # Bleu moyen\n",
    "    \"text_title\": (255, 255, 255),    # Blanc\n",
    "    \"text_body\": (220, 225, 240),     # Gris clair\n",
    "    \"accent\": (100, 180, 255),        # Bleu clair\n",
    "    \"bullet\": (255, 200, 80),         # Jaune\n",
    "    \"footer\": (150, 160, 190),        # Gris\n",
    "}\n",
    "\n",
    "\n",
    "def render_title_slide(slide: dict, width: int, height: int) -> Image.Image:\n",
    "    \"\"\"Rendu d'un slide de titre (ouverture/fermeture).\"\"\"\n",
    "    img = Image.new('RGB', (width, height), COLORS['bg_primary'])\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Bande decorative\n",
    "    draw.rectangle([0, height // 2 - 60, width, height // 2 + 60],\n",
    "                   fill=COLORS['bg_secondary'])\n",
    "    \n",
    "    # Titre principal centre\n",
    "    title = slide['title']\n",
    "    try:\n",
    "        font_title = ImageFont.truetype(\"arial.ttf\", 48)\n",
    "        font_sub = ImageFont.truetype(\"arial.ttf\", 28)\n",
    "    except (OSError, IOError):\n",
    "        font_title = ImageFont.load_default()\n",
    "        font_sub = ImageFont.load_default()\n",
    "    \n",
    "    bbox = draw.textbbox((0, 0), title, font=font_title)\n",
    "    tw = bbox[2] - bbox[0]\n",
    "    draw.text(((width - tw) // 2, height // 2 - 30), title,\n",
    "              fill=COLORS['text_title'], font=font_title)\n",
    "    \n",
    "    # Sous-titre\n",
    "    if 'subtitle' in slide:\n",
    "        bbox_sub = draw.textbbox((0, 0), slide['subtitle'], font=font_sub)\n",
    "        sw = bbox_sub[2] - bbox_sub[0]\n",
    "        draw.text(((width - sw) // 2, height // 2 + 30), slide['subtitle'],\n",
    "                  fill=COLORS['accent'], font=font_sub)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def render_content_slide(slide: dict, width: int, height: int,\n",
    "                         slide_num: int) -> Image.Image:\n",
    "    \"\"\"Rendu d'un slide de contenu avec titre et points cles.\"\"\"\n",
    "    img = Image.new('RGB', (width, height), COLORS['bg_primary'])\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # En-tete\n",
    "    draw.rectangle([0, 0, width, 90], fill=COLORS['bg_secondary'])\n",
    "    \n",
    "    try:\n",
    "        font_title = ImageFont.truetype(\"arial.ttf\", 36)\n",
    "        font_body = ImageFont.truetype(\"arial.ttf\", 24)\n",
    "        font_footer = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "    except (OSError, IOError):\n",
    "        font_title = ImageFont.load_default()\n",
    "        font_body = ImageFont.load_default()\n",
    "        font_footer = ImageFont.load_default()\n",
    "    \n",
    "    # Titre\n",
    "    draw.text((40, 25), slide['title'], fill=COLORS['text_title'], font=font_title)\n",
    "    \n",
    "    # Points cles avec puces\n",
    "    y_offset = 130\n",
    "    line_spacing = 55\n",
    "    for bullet in slide.get('bullets', []):\n",
    "        # Puce decorative\n",
    "        draw.ellipse([50, y_offset + 8, 62, y_offset + 20], fill=COLORS['bullet'])\n",
    "        # Texte\n",
    "        draw.text((80, y_offset), bullet, fill=COLORS['text_body'], font=font_body)\n",
    "        y_offset += line_spacing\n",
    "    \n",
    "    # Pied de page\n",
    "    draw.line([40, height - 50, width - 40, height - 50], fill=COLORS['footer'], width=1)\n",
    "    draw.text((40, height - 40), f\"Slide {slide_num}\",\n",
    "              fill=COLORS['footer'], font=font_footer)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def render_diagram_slide(slide: dict, width: int, height: int,\n",
    "                         slide_num: int) -> Image.Image:\n",
    "    \"\"\"Rendu d'un slide avec diagramme de reseau de neurones.\"\"\"\n",
    "    # Creer le diagramme avec matplotlib\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width / 100, height / 100), dpi=100)\n",
    "    fig.patch.set_facecolor(np.array(COLORS['bg_primary']) / 255.0)\n",
    "    ax.set_facecolor(np.array(COLORS['bg_primary']) / 255.0)\n",
    "    \n",
    "    layers = slide['layers']\n",
    "    labels = slide.get('labels', [f'Couche {i}' for i in range(len(layers))])\n",
    "    n_layers = len(layers)\n",
    "    max_neurons = max(layers)\n",
    "    \n",
    "    # Dessiner les neurones et connexions\n",
    "    positions = {}\n",
    "    for l_idx, n_neurons in enumerate(layers):\n",
    "        x = l_idx / (n_layers - 1) if n_layers > 1 else 0.5\n",
    "        for n_idx in range(n_neurons):\n",
    "            y = (n_idx - n_neurons / 2 + 0.5) / max_neurons\n",
    "            positions[(l_idx, n_idx)] = (x, y)\n",
    "    \n",
    "    # Connexions\n",
    "    for l_idx in range(n_layers - 1):\n",
    "        for n1 in range(layers[l_idx]):\n",
    "            for n2 in range(layers[l_idx + 1]):\n",
    "                p1 = positions[(l_idx, n1)]\n",
    "                p2 = positions[(l_idx + 1, n2)]\n",
    "                ax.plot([p1[0], p2[0]], [p1[1], p2[1]],\n",
    "                        color=np.array(COLORS['footer']) / 255.0, alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    # Neurones\n",
    "    for (l_idx, n_idx), (x, y) in positions.items():\n",
    "        color = np.array(COLORS['accent']) / 255.0 if l_idx in [0, n_layers - 1] else np.array(COLORS['bullet']) / 255.0\n",
    "        ax.scatter(x, y, s=200, c=[color], zorder=5, edgecolors='white', linewidths=0.5)\n",
    "    \n",
    "    # Labels des couches\n",
    "    for l_idx, label in enumerate(labels):\n",
    "        x = l_idx / (n_layers - 1) if n_layers > 1 else 0.5\n",
    "        ax.text(x, -0.55, label, ha='center', va='top',\n",
    "                color=np.array(COLORS['text_body']) / 255.0, fontsize=11)\n",
    "    \n",
    "    ax.set_title(slide['title'], color='white', fontsize=16, pad=20, fontweight='bold')\n",
    "    ax.set_xlim(-0.15, 1.15)\n",
    "    ax.set_ylim(-0.7, 0.7)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Convertir figure en image PIL\n",
    "    fig.canvas.draw()\n",
    "    buf = fig.canvas.buffer_rgba()\n",
    "    img = Image.frombuffer('RGBA', fig.canvas.get_width_height(), buf, 'raw', 'RGBA', 0, 1)\n",
    "    img = img.convert('RGB').resize((width, height), Image.LANCZOS)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "# Generer tous les slides\n",
    "slide_images = []\n",
    "for i, slide in enumerate(course_content):\n",
    "    if slide['type'] == 'title':\n",
    "        img = render_title_slide(slide, slide_width, slide_height)\n",
    "    elif slide['type'] == 'diagram':\n",
    "        img = render_diagram_slide(slide, slide_width, slide_height, i + 1)\n",
    "    else:\n",
    "        img = render_content_slide(slide, slide_width, slide_height, i + 1)\n",
    "    slide_images.append(img)\n",
    "    print(f\"  Slide {i+1}/{len(course_content)} genere : {slide['title']}\")\n",
    "\n",
    "print(f\"\\n{len(slide_images)} slides generes ({slide_width}x{slide_height})\")\n",
    "\n",
    "# Apercu des slides\n",
    "n_preview = min(4, len(slide_images))\n",
    "fig, axes = plt.subplots(1, n_preview, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(slide_images[i])\n",
    "    ax.set_title(f\"Slide {i+1}\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Apercu des premiers slides\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Interpretation : Generation des slides\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Rendu Pillow | CPU uniquement | Pas besoin de GPU pour le texte et les formes |\n",
    "| Diagrammes | matplotlib -> PIL | Les graphiques scientifiques sont convertis en images |\n",
    "| Polices | TrueType avec fallback | Fonctionne meme sans polices systeme installees |\n",
    "| Palette | Tons bleu fonce | Palette professionnelle a fort contraste |\n",
    "\n",
    "**Points cles** :\n",
    "1. La separation contenu/rendu permet de modifier le style sans toucher au contenu\n",
    "2. Chaque type de slide (titre, contenu, diagramme) a son propre moteur de rendu\n",
    "3. Les diagrammes matplotlib offrent une grande flexibilite pour les schemas techniques\n",
    "\n",
    "## Section 3 : Transitions animees entre slides\n",
    "\n",
    "Les transitions entre slides rendent la video plus fluide et professionnelle.\n",
    "Nous implementons ici un fondu enchaine (cross-dissolve) entre chaque paire de slides consecutifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation de transitions animees entre slides\n",
    "print(\"\\n--- CREATION DES TRANSITIONS ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "\n",
    "def create_crossfade_frames(img_from: Image.Image, img_to: Image.Image,\n",
    "                            duration: float, fps: int) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Cree les frames d'un fondu enchaine entre deux images.\n",
    "    \n",
    "    Args:\n",
    "        img_from: Image de depart\n",
    "        img_to: Image d'arrivee\n",
    "        duration: Duree de la transition en secondes\n",
    "        fps: Images par seconde\n",
    "    \n",
    "    Returns:\n",
    "        Liste de frames numpy (H, W, 3)\n",
    "    \"\"\"\n",
    "    n_frames = int(duration * fps)\n",
    "    arr_from = np.array(img_from, dtype=np.float32)\n",
    "    arr_to = np.array(img_to, dtype=np.float32)\n",
    "    \n",
    "    frames = []\n",
    "    for i in range(n_frames):\n",
    "        alpha = i / max(n_frames - 1, 1)\n",
    "        # Interpolation lineaire\n",
    "        blended = (1 - alpha) * arr_from + alpha * arr_to\n",
    "        frames.append(blended.astype(np.uint8))\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "def create_slide_with_text_animation(slide_img: Image.Image, slide_data: dict,\n",
    "                                     duration: float, fps: int) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Cree les frames d'un slide avec apparition progressive du texte.\n",
    "    Les premiers 20% de la duree montrent un fondu d'apparition.\n",
    "    \"\"\"\n",
    "    n_frames = int(duration * fps)\n",
    "    arr = np.array(slide_img)\n",
    "    frames = []\n",
    "    \n",
    "    fade_in_frames = int(n_frames * 0.15)  # 15% pour le fade-in\n",
    "    \n",
    "    for i in range(n_frames):\n",
    "        if i < fade_in_frames:\n",
    "            # Progressivement reveler depuis un fond sombre\n",
    "            alpha = i / max(fade_in_frames - 1, 1)\n",
    "            dark = np.full_like(arr, COLORS['bg_primary'])\n",
    "            frame = ((1 - alpha) * dark + alpha * arr).astype(np.uint8)\n",
    "        else:\n",
    "            frame = arr.copy()\n",
    "        frames.append(frame)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "# Assembler toutes les frames avec transitions\n",
    "all_frames = []\n",
    "total_expected = 0\n",
    "\n",
    "for i, (slide_img, slide_data) in enumerate(zip(slide_images, course_content)):\n",
    "    # Frames du slide\n",
    "    slide_frames = create_slide_with_text_animation(\n",
    "        slide_img, slide_data, slide_data['duration'], output_fps\n",
    "    )\n",
    "    all_frames.extend(slide_frames)\n",
    "    \n",
    "    # Transition vers le slide suivant\n",
    "    if i < len(slide_images) - 1:\n",
    "        transition_frames = create_crossfade_frames(\n",
    "            slide_img, slide_images[i + 1], transition_duration, output_fps\n",
    "        )\n",
    "        all_frames.extend(transition_frames)\n",
    "    \n",
    "    print(f\"  Slide {i+1} : {len(slide_frames)} frames\" +\n",
    "          (f\" + {len(transition_frames)} transition\" if i < len(slide_images) - 1 else \"\"))\n",
    "\n",
    "print(f\"\\nTotal frames assemblees : {len(all_frames)}\")\n",
    "print(f\"Duree estimee : {len(all_frames) / output_fps:.1f}s\")\n",
    "print(f\"Memoire estimee : ~{len(all_frames) * slide_width * slide_height * 3 / 1024 / 1024:.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Interpretation : Transitions\n",
    "\n",
    "| Type de transition | Technique | Duree typique |\n",
    "|-------------------|-----------|---------------|\n",
    "| Fondu enchaine | Interpolation lineaire des pixels | 0.5 - 1.5s |\n",
    "| Fade-in depuis noir | Interpolation avec fond sombre | 0.3 - 0.8s |\n",
    "| Apparition du texte | Masquage progressif | Variable |\n",
    "\n",
    "**Points cles** :\n",
    "1. Les transitions sont generees frame par frame en memoire avant l'encodage\n",
    "2. La consommation memoire est proportionnelle au nombre total de frames\n",
    "3. Pour de longues videos, il faudrait un encodage en streaming plutot qu'en memoire\n",
    "\n",
    "## Section 4 : Overlay texte et annotations\n",
    "\n",
    "Nous ajoutons maintenant des annotations dynamiques : barre de progression,\n",
    "numero de chapitre, et indicateur de timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des overlays dynamiques sur les frames\n",
    "print(\"\\n--- AJOUT DES OVERLAYS DYNAMIQUES ---\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "\n",
    "def add_progress_bar(frame: np.ndarray, progress: float,\n",
    "                     bar_height: int = 6) -> np.ndarray:\n",
    "    \"\"\"Ajoute une barre de progression en bas de la frame.\"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "    result = frame.copy()\n",
    "    \n",
    "    # Fond de la barre\n",
    "    result[h - bar_height:h, :] = [40, 40, 60]\n",
    "    \n",
    "    # Progression\n",
    "    fill_width = int(w * progress)\n",
    "    result[h - bar_height:h, :fill_width] = COLORS['accent']\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def add_chapter_indicator(frame: np.ndarray, chapter_name: str,\n",
    "                          frame_idx: int, total_frames: int) -> np.ndarray:\n",
    "    \"\"\"Ajoute un indicateur de chapitre et timestamp.\"\"\"\n",
    "    img = Image.fromarray(frame)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    try:\n",
    "        font_small = ImageFont.truetype(\"arial.ttf\", 14)\n",
    "    except (OSError, IOError):\n",
    "        font_small = ImageFont.load_default()\n",
    "    \n",
    "    timestamp = f\"{frame_idx / output_fps:.1f}s / {total_frames / output_fps:.1f}s\"\n",
    "    \n",
    "    # Fond semi-transparent pour le texte (rectangle sombre)\n",
    "    h, w = frame.shape[:2]\n",
    "    draw.rectangle([w - 200, 10, w - 10, 55], fill=(20, 25, 50))\n",
    "    draw.text((w - 190, 15), chapter_name[:25], fill=COLORS['accent'], font=font_small)\n",
    "    draw.text((w - 190, 33), timestamp, fill=COLORS['footer'], font=font_small)\n",
    "    \n",
    "    return np.array(img)\n",
    "\n",
    "\n",
    "# Appliquer les overlays a toutes les frames\n",
    "total_frames = len(all_frames)\n",
    "\n",
    "# Calculer les limites de chaque chapitre (en frames)\n",
    "chapter_boundaries = []\n",
    "cumulative = 0\n",
    "for i, slide in enumerate(course_content):\n",
    "    start = cumulative\n",
    "    end = cumulative + int(slide['duration'] * output_fps)\n",
    "    chapter_boundaries.append((start, end, slide['title']))\n",
    "    cumulative = end + int(transition_duration * output_fps)\n",
    "\n",
    "print(f\"Application des overlays sur {total_frames} frames...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for f_idx in range(total_frames):\n",
    "    progress = f_idx / max(total_frames - 1, 1)\n",
    "    \n",
    "    # Determiner le chapitre courant\n",
    "    current_chapter = \"Transition\"\n",
    "    for (c_start, c_end, c_name) in chapter_boundaries:\n",
    "        if c_start <= f_idx < c_end:\n",
    "            current_chapter = c_name\n",
    "            break\n",
    "    \n",
    "    # Appliquer les overlays\n",
    "    all_frames[f_idx] = add_progress_bar(all_frames[f_idx], progress)\n",
    "    all_frames[f_idx] = add_chapter_indicator(\n",
    "        all_frames[f_idx], current_chapter, f_idx, total_frames\n",
    "    )\n",
    "\n",
    "overlay_time = time.time() - start_time\n",
    "print(f\"Overlays appliques en {overlay_time:.2f}s\")\n",
    "print(f\"Vitesse : {total_frames / overlay_time:.0f} frames/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Section 5 : Assemblage et export de la video finale\n",
    "\n",
    "Toutes les frames sont pretes. Nous les encodons maintenant en fichier MP4\n",
    "avec imageio et le codec H.264. Un apercu visuel confirme le resultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export de la video educative finale\n",
    "print(\"\\n--- EXPORT VIDEO EDUCATIVE ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "output_path = OUTPUT_DIR / f\"educational_video.{output_format}\"\n",
    "\n",
    "if dependencies.get('imageio', False):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    writer = imageio.get_writer(\n",
    "        str(output_path), fps=output_fps, codec='libx264',\n",
    "        output_params=['-crf', '23', '-preset', 'medium']\n",
    "    )\n",
    "    for frame in all_frames:\n",
    "        writer.append_data(frame)\n",
    "    writer.close()\n",
    "    \n",
    "    export_time = time.time() - start_time\n",
    "    file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Video exportee : {output_path.name}\")\n",
    "    print(f\"  Resolution : {slide_width}x{slide_height}\")\n",
    "    print(f\"  FPS : {output_fps}\")\n",
    "    print(f\"  Duree : {len(all_frames) / output_fps:.1f}s\")\n",
    "    print(f\"  Taille : {file_size_mb:.2f} MB\")\n",
    "    print(f\"  Temps d'export : {export_time:.2f}s\")\n",
    "    print(f\"  Vitesse encodage : {len(all_frames) / export_time:.0f} frames/s\")\n",
    "    \n",
    "    # Apercu de frames extraites de la video finale\n",
    "    preview_indices = np.linspace(0, len(all_frames) - 1, 6, dtype=int)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 7))\n",
    "    for ax, idx in zip(axes.flatten(), preview_indices):\n",
    "        ax.imshow(all_frames[idx])\n",
    "        ax.set_title(f\"t = {idx / output_fps:.1f}s\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(\"Apercu de la video educative finale\", fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"imageio non disponible - export impossible\")\n",
    "\n",
    "# Generer les marqueurs de chapitres\n",
    "print(f\"\\nMarqueurs de chapitres :\")\n",
    "print(f\"{'Chapitre':<45} {'Debut':>8} {'Fin':>8}\")\n",
    "print(\"-\" * 65)\n",
    "for (start, end, name) in chapter_boundaries:\n",
    "    t_start = start / output_fps\n",
    "    t_end = end / output_fps\n",
    "    print(f\"  {name:<43} {t_start:>6.1f}s {t_end:>6.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Interpretation : Video educative\n",
    "\n",
    "| Metrique | Valeur | Signification |\n",
    "|----------|--------|---------------|\n",
    "| Resolution | 1280x720 | HD standard, bon compromis qualite/taille |\n",
    "| Codec H.264 CRF 23 | Compression visuelle | Qualite bonne avec taille raisonnable |\n",
    "| Chapitres | Metadata temporelle | Permet la navigation dans la video |\n",
    "| Barre de progression | Overlay dynamique | Repere visuel pour le spectateur |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le parametre CRF (Constant Rate Factor) controle le compromis qualite/taille\n",
    "2. Les marqueurs de chapitres facilitent la navigation dans les longues videos\n",
    "3. L'approche frame-par-frame en memoire fonctionne pour les courtes videos (<5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif - Personnalisation du contenu\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Vous pouvez modifier le contenu du cours et regenerer la video.\")\n",
    "    print(\"(Laissez vide pour passer)\")\n",
    "    \n",
    "    try:\n",
    "        custom_title = input(\"\\nTitre du cours (ou vide) : \").strip()\n",
    "        \n",
    "        if custom_title:\n",
    "            print(f\"Regeneration avec le titre : {custom_title}\")\n",
    "            course_content[0]['title'] = custom_title\n",
    "            print(\"Relancez les cellules de generation pour appliquer le changement.\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"Mode interactif interrompu\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type:\n",
    "            print(\"Mode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"Erreur : {error_type}\")\n",
    "else:\n",
    "    print(\"Mode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Slides generes : {len(slide_images)}\")\n",
    "print(f\"Frames totales : {len(all_frames)}\")\n",
    "print(f\"Duree video : {len(all_frames) / output_fps:.1f}s\")\n",
    "print(f\"Resolution : {slide_width}x{slide_height} @ {output_fps}fps\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('educational_*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(f\"\\nDependances utilisees :\")\n",
    "for dep, available in dependencies.items():\n",
    "    status = \"utilisee\" if available else \"non disponible\"\n",
    "    print(f\"  {dep} : {status}\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 04-2 : Workflows Video Creatifs (style transfer, music video)\")\n",
    "print(f\"2. Notebook 04-3 : Sora API - Generation Video Cloud\")\n",
    "print(f\"3. Notebook 04-4 : Pipeline Video de Production (pipeline complet)\")\n",
    "\n",
    "print(f\"\\nNotebook 04-1 Educational Video Generation termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}