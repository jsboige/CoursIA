{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.002065,
     "end_time": "2026-02-19T09:29:13.567256",
     "exception": false,
     "start_time": "2026-02-19T09:29:13.565191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AnimateDiff - Introduction a la Generation Text-to-Video\n",
    "\n",
    "**Module :** 01-Video-Foundation  \n",
    "**Niveau :** Intermediaire  \n",
    "**Technologies :** AnimateDiff, diffusers, Stable Diffusion v1.5  \n",
    "**Duree estimee :** 45 minutes  \n",
    "**VRAM :** ~12 GB  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Comprendre l'architecture AnimateDiff (motion module + base model)\n",
    "- [ ] Charger un pipeline AnimateDiff avec la bibliotheque diffusers\n",
    "- [ ] Generer des videos a partir de descriptions textuelles\n",
    "- [ ] Explorer les parametres cles (num_frames, guidance_scale, num_inference_steps)\n",
    "- [ ] Sauvegarder les resultats en GIF et MP4\n",
    "- [ ] Comparer differents prompts et parametres\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- GPU avec 12+ GB VRAM (RTX 3060 12GB minimum)\n",
    "- Notebooks 01-1 a 01-4 completes\n",
    "- Packages : `diffusers>=0.30`, `transformers`, `torch`, `accelerate`, `imageio`, `imageio-ffmpeg`\n",
    "\n",
    "## Principe d'AnimateDiff\n",
    "\n",
    "AnimateDiff ajoute un **motion module** (attention temporelle) a un modele Stable Diffusion\n",
    "existant pour generer des sequences de frames coherentes temporellement.\n",
    "\n",
    "| Composant | Role |\n",
    "|-----------|------|\n",
    "| **Base model** | Stable Diffusion v1.5 (generation d'images) |\n",
    "| **Motion adapter** | Module d'attention temporelle (coherence entre frames) |\n",
    "| **Scheduler** | Controle du processus de debruitage (DDIM, Euler, etc.) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:13.571548Z",
     "iopub.status.busy": "2026-02-19T09:29:13.571347Z",
     "iopub.status.idle": "2026-02-19T09:29:13.575914Z",
     "shell.execute_reply": "2026-02-19T09:29:13.575412Z"
    },
    "papermill": {
     "duration": 0.007985,
     "end_time": "2026-02-19T09:29:13.576873",
     "exception": false,
     "start_time": "2026-02-19T09:29:13.568888",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres modele\n",
    "base_model = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"  # Modele de base SD\n",
    "motion_adapter = \"guoyww/animatediff-motion-adapter-v1-5-3\"  # Motion module\n",
    "\n",
    "# Parametres generation\n",
    "num_frames = 16                    # Nombre de frames a generer\n",
    "guidance_scale = 7.5               # CFG scale (adherence au prompt)\n",
    "num_inference_steps = 25           # Nombre d'etapes de debruitage\n",
    "height = 512                       # Hauteur video\n",
    "width = 512                        # Largeur video\n",
    "fps_output = 8                     # FPS de la video de sortie\n",
    "\n",
    "# Configuration\n",
    "run_generation = True              # Executer la generation\n",
    "save_as_gif = True                 # Sauvegarder en GIF\n",
    "save_as_mp4 = True                 # Sauvegarder en MP4\n",
    "save_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95841e9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:13.581361Z",
     "iopub.status.busy": "2026-02-19T09:29:13.581167Z",
     "iopub.status.idle": "2026-02-19T09:29:13.583989Z",
     "shell.execute_reply": "2026-02-19T09:29:13.583496Z"
    },
    "papermill": {
     "duration": 0.006171,
     "end_time": "2026-02-19T09:29:13.584879",
     "exception": false,
     "start_time": "2026-02-19T09:29:13.578708",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "notebook_mode = \"batch\"\n",
    "skip_widgets = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:13.589936Z",
     "iopub.status.busy": "2026-02-19T09:29:13.589729Z",
     "iopub.status.idle": "2026-02-19T09:29:13.954665Z",
     "shell.execute_reply": "2026-02-19T09:29:13.954051Z"
    },
    "papermill": {
     "duration": 0.368209,
     "end_time": "2026-02-19T09:29:13.955466",
     "exception": false,
     "start_time": "2026-02-19T09:29:13.587257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnimateDiff - Generation Text-to-Video\n",
      "Date : 2026-02-19 10:29:13\n",
      "Mode : batch\n",
      "Frames : 16, Steps : 25, CFG : 7.5\n"
     ]
    }
   ],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.genai_helpers import setup_genai_logging\n",
    "        print(\"Helpers GenAI importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers GenAI non disponibles - mode autonome\")\n",
    "\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'animatediff'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('animatediff')\n",
    "\n",
    "print(f\"AnimateDiff - Generation Text-to-Video\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Frames : {num_frames}, Steps : {num_inference_steps}, CFG : {guidance_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:13.960892Z",
     "iopub.status.busy": "2026-02-19T09:29:13.960561Z",
     "iopub.status.idle": "2026-02-19T09:29:24.992680Z",
     "shell.execute_reply": "2026-02-19T09:29:24.992238Z"
    },
    "papermill": {
     "duration": 11.03566,
     "end_time": "2026-02-19T09:29:24.993434",
     "exception": false,
     "start_time": "2026-02-19T09:29:13.957774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucun fichier .env trouve\n",
      "\n",
      "--- VERIFICATION GPU ---\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA non disponible.\n",
      "AnimateDiff necessite un GPU. Le notebook montrera le code sans executer.\n",
      "\n",
      "--- VERIFICATION DEPENDANCES ---\n",
      "========================================\n",
      "huggingface-hub : v1.4.1\n",
      "  Warning: huggingface-hub v1.4.1 n'est pas compatible avec diffusers\n",
      "  AnimateDiff ne sera pas disponible (conflit de version)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusers : v0.36.0\n",
      "AnimateDiff pipeline : NON DISPONIBLE (dependances)\n",
      "\n",
      "Device : cpu\n",
      "Generation activee : False\n"
     ]
    }
   ],
   "source": [
    "# Chargement .env et verification GPU\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification GPU\n",
    "print(\"\\n--- VERIFICATION GPU ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "    vram_free = (torch.cuda.get_device_properties(0).total_mem - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "    \n",
    "    print(f\"GPU : {gpu_name}\")\n",
    "    print(f\"VRAM totale : {vram_total:.1f} GB\")\n",
    "    print(f\"VRAM libre : {vram_free:.1f} GB\")\n",
    "    print(f\"CUDA : {torch.version.cuda}\")\n",
    "    \n",
    "    if vram_total < 12:\n",
    "        print(f\"\\nAttention : VRAM ({vram_total:.0f} GB) < 12 GB recommandes\")\n",
    "        print(\"Reduction de la resolution et du nombre de frames\")\n",
    "        if vram_total < 8:\n",
    "            height = 256\n",
    "            width = 256\n",
    "            num_frames = 8\n",
    "            print(f\"  Resolution reduite a {width}x{height}, {num_frames} frames\")\n",
    "else:\n",
    "    print(\"CUDA non disponible.\")\n",
    "    print(\"AnimateDiff necessite un GPU. Le notebook montrera le code sans executer.\")\n",
    "    run_generation = False\n",
    "\n",
    "# Verification des dependances\n",
    "print(\"\\n--- VERIFICATION DEPENDANCES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Verification huggingface-hub version AVANT d'importer diffusers\n",
    "animatediff_available = False\n",
    "try:\n",
    "    import huggingface_hub\n",
    "    hf_version = huggingface_hub.__version__\n",
    "    print(f\"huggingface-hub : v{hf_version}\")\n",
    "    # Verifier si la version est compatible\n",
    "    from packaging import version\n",
    "    if version.parse(hf_version) >= version.parse(\"1.0\"):\n",
    "        print(f\"  Warning: huggingface-hub v{hf_version} n'est pas compatible avec diffusers\")\n",
    "        print(f\"  AnimateDiff ne sera pas disponible (conflit de version)\")\n",
    "        animatediff_available = False\n",
    "    else:\n",
    "        animatediff_available = True\n",
    "except ImportError:\n",
    "    print(\"huggingface-hub NON INSTALLE\")\n",
    "    animatediff_available = False\n",
    "except Exception as e:\n",
    "    print(f\"Erreur verification huggingface-hub: {e}\")\n",
    "    animatediff_available = False\n",
    "\n",
    "# Verification diffusers\n",
    "try:\n",
    "    import diffusers\n",
    "    print(f\"diffusers : v{diffusers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"diffusers NON INSTALLE\")\n",
    "    animatediff_available = False\n",
    "\n",
    "# Import AnimateDiff seulement si les dependances sont OK\n",
    "if animatediff_available:\n",
    "    try:\n",
    "        from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n",
    "        from diffusers.utils import export_to_gif, export_to_video\n",
    "        print(\"AnimateDiff pipeline : disponible\")\n",
    "    except ImportError as e:\n",
    "        error_str = str(e)\n",
    "        if \"huggingface-hub\" in error_str:\n",
    "            print(f\"AnimateDiff pipeline : NON DISPONIBLE (conflit de version huggingface-hub)\")\n",
    "            print(f\"  Solution: pip install 'huggingface-hub<1.0'\")\n",
    "        else:\n",
    "            print(f\"AnimateDiff pipeline : NON DISPONIBLE - {error_str[:100]}\")\n",
    "        animatediff_available = False\n",
    "        run_generation = False\n",
    "else:\n",
    "    if not animatediff_available:\n",
    "        print(\"AnimateDiff pipeline : NON DISPONIBLE (dependances)\")\n",
    "    run_generation = False\n",
    "\n",
    "print(f\"\\nDevice : {device}\")\n",
    "print(f\"Generation activee : {run_generation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {
    "papermill": {
     "duration": 0.001749,
     "end_time": "2026-02-19T09:29:24.997097",
     "exception": false,
     "start_time": "2026-02-19T09:29:24.995348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 1 : Chargement du pipeline AnimateDiff\n",
    "\n",
    "Le pipeline AnimateDiff combine :\n",
    "1. Un **motion adapter** (module d'attention temporelle entraine separement)\n",
    "2. Un **modele Stable Diffusion** de base (genere les frames individuelles)\n",
    "3. Un **scheduler** (controle le processus de debruitage)\n",
    "\n",
    "Le motion adapter s'insere dans les couches d'attention du U-Net de Stable Diffusion\n",
    "pour ajouter une dimension temporelle au traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:25.001292Z",
     "iopub.status.busy": "2026-02-19T09:29:25.000861Z",
     "iopub.status.idle": "2026-02-19T09:29:25.005754Z",
     "shell.execute_reply": "2026-02-19T09:29:25.005265Z"
    },
    "papermill": {
     "duration": 0.007974,
     "end_time": "2026-02-19T09:29:25.006590",
     "exception": false,
     "start_time": "2026-02-19T09:29:24.998616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement pipeline desactive\n"
     ]
    }
   ],
   "source": [
    "# Chargement du pipeline AnimateDiff\n",
    "pipe = None\n",
    "\n",
    "if run_generation:\n",
    "    print(\"\\n--- CHARGEMENT DU PIPELINE ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # 1. Charger le motion adapter\n",
    "        print(f\"Chargement motion adapter : {motion_adapter}\")\n",
    "        adapter = MotionAdapter.from_pretrained(\n",
    "            motion_adapter,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        print(f\"  Motion adapter charge\")\n",
    "        \n",
    "        # 2. Charger le pipeline complet\n",
    "        print(f\"Chargement base model : {base_model}\")\n",
    "        start_load = time.time()\n",
    "        \n",
    "        pipe = AnimateDiffPipeline.from_pretrained(\n",
    "            base_model,\n",
    "            motion_adapter=adapter,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # 3. Configurer le scheduler\n",
    "        pipe.scheduler = DDIMScheduler.from_pretrained(\n",
    "            base_model,\n",
    "            subfolder=\"scheduler\",\n",
    "            clip_sample=False,\n",
    "            timestep_spacing=\"linspace\",\n",
    "            beta_schedule=\"linear\",\n",
    "            steps_offset=1\n",
    "        )\n",
    "        \n",
    "        # 4. Transferer sur GPU\n",
    "        pipe = pipe.to(device)\n",
    "        \n",
    "        # 5. Optimisations memoire\n",
    "        pipe.enable_vae_slicing()  # Reduit la VRAM pour le decodage VAE\n",
    "        try:\n",
    "            pipe.enable_model_cpu_offload()  # Offload intelligent CPU<->GPU\n",
    "        except Exception:\n",
    "            pass  # Pas critique si indisponible\n",
    "        \n",
    "        load_time = time.time() - start_load\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            vram_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            print(f\"  VRAM utilisee : {vram_used:.1f} GB\")\n",
    "        \n",
    "        print(f\"Pipeline charge en {load_time:.1f}s\")\n",
    "        print(f\"  Scheduler : DDIMScheduler\")\n",
    "        print(f\"  VAE slicing : active\")\n",
    "        print(f\"  Resolution : {width}x{height}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur chargement pipeline : {type(e).__name__}: {str(e)[:200]}\")\n",
    "        print(\"Le notebook continuera sans generation.\")\n",
    "        run_generation = False\n",
    "        pipe = None\n",
    "else:\n",
    "    print(\"Chargement pipeline desactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {
    "papermill": {
     "duration": 0.001629,
     "end_time": "2026-02-19T09:29:25.010076",
     "exception": false,
     "start_time": "2026-02-19T09:29:25.008447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 2 : Generation text-to-video\n",
    "\n",
    "Nous allons generer une premiere video a partir d'un prompt textuel. Les parametres\n",
    "principaux sont :\n",
    "\n",
    "| Parametre | Impact | Valeur typique |\n",
    "|-----------|--------|---------------|\n",
    "| `num_frames` | Duree de la video | 8-24 |\n",
    "| `guidance_scale` | Adherence au prompt (trop haut = artefacts) | 5.0-10.0 |\n",
    "| `num_inference_steps` | Qualite (plus = mieux mais plus lent) | 20-50 |\n",
    "| `height/width` | Resolution (carree recommandee) | 256-512 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:25.014188Z",
     "iopub.status.busy": "2026-02-19T09:29:25.013981Z",
     "iopub.status.idle": "2026-02-19T09:29:25.023599Z",
     "shell.execute_reply": "2026-02-19T09:29:25.022794Z"
    },
    "papermill": {
     "duration": 0.012826,
     "end_time": "2026-02-19T09:29:25.024469",
     "exception": false,
     "start_time": "2026-02-19T09:29:25.011643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GENERATION TEXT-TO-VIDEO ---\n",
      "========================================\n",
      "**MODE PEDAGOGIQUE**\n",
      "\n",
      "Sur un environnement GPU (RTX 3090, 24GB VRAM), ce code genererait :\n",
      "\n",
      "| Parametre | Valeur |\n",
      "|-----------|--------|\n",
      "| Prompt | a serene lake at sunset with mountains in the back... |\n",
      "| Frames | 16 |\n",
      "| Resolution | 512x512 |\n",
      "| Steps | 25 |\n",
      "| CFG Scale | 7.5 |\n",
      "| Temps estime | ~45s |\n",
      "| VRAM | ~10 GB |\n",
      "\n",
      "**Resultat attendu :**\n",
      "Une video de 2 secondes (16 frames @ 8 fps) montrant un lac paisible\n",
      "au coucher du soleil avec des montagnes en arriere-plan et des\n",
      "reflets dorees sur l'eau. Le mouvement sera subtil : ondulations de\n",
      "l'eau, nuages se deplacant lentement, lumiere changeante.\n",
      "\n",
      "**Code pour reproduire :**\n",
      "```python\n",
      "result = generate_video('a serene lake at sunset with mountains i...', seed=42)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Generation text-to-video\n",
    "print(\"\\n--- GENERATION TEXT-TO-VIDEO ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def generate_video(prompt: str, negative_prompt: str = \"\",\n",
    "                   seed: int = 42) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Genere une video a partir d'un prompt textuel.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Description textuelle de la video\n",
    "        negative_prompt: Elements a eviter\n",
    "        seed: Graine aleatoire pour reproductibilite\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec frames, temps de generation et metadonnees\n",
    "    \"\"\"\n",
    "    if pipe is None:\n",
    "        return {\"success\": False, \"error\": \"Pipeline non charge\"}\n",
    "    \n",
    "    try:\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            vram_before = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        output = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt or \"low quality, blurry, distorted\",\n",
    "            num_frames=num_frames,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            generator=generator\n",
    "        )\n",
    "        \n",
    "        gen_time = time.time() - start_time\n",
    "        frames = output.frames[0]  # Liste d'images PIL\n",
    "        \n",
    "        result = {\n",
    "            \"success\": True,\n",
    "            \"frames\": frames,\n",
    "            \"generation_time\": gen_time,\n",
    "            \"time_per_frame\": gen_time / num_frames,\n",
    "            \"prompt\": prompt,\n",
    "            \"seed\": seed,\n",
    "            \"params\": {\n",
    "                \"num_frames\": num_frames,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"num_inference_steps\": num_inference_steps,\n",
    "                \"height\": height,\n",
    "                \"width\": width\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            result[\"vram_peak\"] = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": f\"{type(e).__name__}: {str(e)[:200]}\"}\n",
    "\n",
    "\n",
    "# Premier test : generation simple\n",
    "prompt_1 = \"a serene lake at sunset with mountains in the background, golden light reflecting on water, cinematic\"\n",
    "\n",
    "if run_generation:\n",
    "    print(f\"Prompt : {prompt_1}\")\n",
    "    print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n",
    "    print(f\"Resolution : {width}x{height}\")\n",
    "    print(f\"\\nGeneration en cours...\")\n",
    "    \n",
    "    result_1 = generate_video(prompt_1, seed=42)\n",
    "    \n",
    "    if result_1['success']:\n",
    "        frames = result_1['frames']\n",
    "        print(f\"\\nGeneration reussie\")\n",
    "        print(f\"  Temps total : {result_1['generation_time']:.1f}s\")\n",
    "        print(f\"  Temps/frame : {result_1['time_per_frame']:.1f}s\")\n",
    "        print(f\"  Frames : {len(frames)}\")\n",
    "        if 'vram_peak' in result_1:\n",
    "            print(f\"  VRAM pic : {result_1['vram_peak']:.1f} GB\")\n",
    "        \n",
    "        # Affichage en grille\n",
    "        n_display = min(8, len(frames))\n",
    "        indices = np.linspace(0, len(frames) - 1, n_display, dtype=int)\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        axes_flat = axes.flatten()\n",
    "        for i, idx in enumerate(indices):\n",
    "            if i < len(axes_flat):\n",
    "                axes_flat[i].imshow(frames[idx])\n",
    "                axes_flat[i].set_title(f\"Frame {idx + 1}/{len(frames)}\", fontsize=9)\n",
    "                axes_flat[i].axis('off')\n",
    "        for i in range(len(indices), len(axes_flat)):\n",
    "            axes_flat[i].axis('off')\n",
    "        plt.suptitle(f\"AnimateDiff : {prompt_1[:60]}...\", fontsize=11, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Sauvegarde GIF\n",
    "        if save_as_gif:\n",
    "            gif_path = OUTPUT_DIR / \"animatediff_demo.gif\"\n",
    "            export_to_gif(frames, str(gif_path))\n",
    "            gif_size_kb = gif_path.stat().st_size / 1024\n",
    "            print(f\"  GIF sauvegarde : {gif_path.name} ({gif_size_kb:.1f} KB)\")\n",
    "        \n",
    "        # Sauvegarde MP4\n",
    "        if save_as_mp4:\n",
    "            mp4_path = OUTPUT_DIR / \"animatediff_demo.mp4\"\n",
    "            export_to_video(frames, str(mp4_path), fps=fps_output)\n",
    "            mp4_size_kb = mp4_path.stat().st_size / 1024\n",
    "            print(f\"  MP4 sauvegarde : {mp4_path.name} ({mp4_size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"Erreur : {result_1['error']}\")\n",
    "else:\n",
    "    # Mode pedagogique : afficher ce qui serait genere\n",
    "    print(\"**MODE PEDAGOGIQUE**\")\n",
    "    print(f\"\\nSur un environnement GPU (RTX 3090, 24GB VRAM), ce code genererait :\")\n",
    "    print(f\"\")\n",
    "    print(f\"| Parametre | Valeur |\")\n",
    "    print(f\"|-----------|--------|\")\n",
    "    print(f\"| Prompt | {prompt_1[:50]}... |\")\n",
    "    print(f\"| Frames | {num_frames} |\")\n",
    "    print(f\"| Resolution | {width}x{height} |\")\n",
    "    print(f\"| Steps | {num_inference_steps} |\")\n",
    "    print(f\"| CFG Scale | {guidance_scale} |\")\n",
    "    print(f\"| Temps estime | ~45s |\")\n",
    "    print(f\"| VRAM | ~10 GB |\")\n",
    "    print(f\"\")\n",
    "    print(f\"**Resultat attendu :**\")\n",
    "    print(f\"Une video de 2 secondes (16 frames @ 8 fps) montrant un lac paisible\")\n",
    "    print(f\"au coucher du soleil avec des montagnes en arriere-plan et des\")\n",
    "    print(f\"reflets dorees sur l'eau. Le mouvement sera subtil : ondulations de\")\n",
    "    print(f\"l'eau, nuages se deplacant lentement, lumiere changeante.\")\n",
    "    print(f\"\")\n",
    "    print(f\"**Code pour reproduire :**\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"result = generate_video('{prompt_1[:40]}...', seed=42)\")\n",
    "    print(f\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {
    "papermill": {
     "duration": 0.001689,
     "end_time": "2026-02-19T09:29:25.028070",
     "exception": false,
     "start_time": "2026-02-19T09:29:25.026381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 3 : Exploration des parametres\n",
    "\n",
    "Nous allons tester differentes combinaisons de parametres pour comprendre leur impact\n",
    "sur la qualite et le temps de generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:25.032178Z",
     "iopub.status.busy": "2026-02-19T09:29:25.031965Z",
     "iopub.status.idle": "2026-02-19T09:29:25.038573Z",
     "shell.execute_reply": "2026-02-19T09:29:25.037788Z"
    },
    "papermill": {
     "duration": 0.009692,
     "end_time": "2026-02-19T09:29:25.039379",
     "exception": false,
     "start_time": "2026-02-19T09:29:25.029687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison de prompts : generation desactivee\n",
      "\n",
      "Exemples de prompts efficaces pour AnimateDiff :\n",
      "  - Mouvements naturels : eau, vent, nuages\n",
      "  - Animaux en mouvement : chat marchant, oiseau volant\n",
      "  - Paysages dynamiques : coucher de soleil, aurores boreales\n",
      "  - Actions simples : fusee decollant, fleur eclosant\n"
     ]
    }
   ],
   "source": [
    "# Comparaison de prompts\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- COMPARAISON DE PROMPTS ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    prompts = [\n",
    "        {\n",
    "            \"text\": \"a cat walking gracefully on a windowsill, soft morning light, cozy atmosphere\",\n",
    "            \"label\": \"Chat\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"ocean waves crashing on a rocky shore, dramatic clouds, powerful nature\",\n",
    "            \"label\": \"Ocean\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"a rocket launching into space with fire and smoke, cinematic epic shot\",\n",
    "            \"label\": \"Fusee\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for p_idx, prompt_info in enumerate(prompts):\n",
    "        print(f\"\\nGeneration {p_idx + 1}/{len(prompts)} : {prompt_info['label']}\")\n",
    "        print(f\"  Prompt : {prompt_info['text'][:70]}...\")\n",
    "        \n",
    "        result = generate_video(prompt_info['text'], seed=42 + p_idx)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  Temps : {result['generation_time']:.1f}s\")\n",
    "            comparison_results.append({\n",
    "                \"label\": prompt_info['label'],\n",
    "                \"prompt\": prompt_info['text'],\n",
    "                \"frames\": result['frames'],\n",
    "                \"time\": result['generation_time']\n",
    "            })\n",
    "            \n",
    "            # Sauvegarder chaque video\n",
    "            if save_as_gif:\n",
    "                gif_path = OUTPUT_DIR / f\"comparison_{prompt_info['label'].lower()}.gif\"\n",
    "                export_to_gif(result['frames'], str(gif_path))\n",
    "        else:\n",
    "            print(f\"  Erreur : {result['error']}\")\n",
    "    \n",
    "    # Affichage comparatif\n",
    "    if comparison_results:\n",
    "        n_videos = len(comparison_results)\n",
    "        n_preview = 4\n",
    "        fig, axes = plt.subplots(n_videos, n_preview, figsize=(3.5 * n_preview, 3 * n_videos))\n",
    "        if n_videos == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, cr in enumerate(comparison_results):\n",
    "            frame_indices = np.linspace(0, len(cr['frames']) - 1, n_preview, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(cr['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(cr['label'], fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Comparaison de prompts - AnimateDiff\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Tableau recapitulatif\n",
    "        print(f\"\\nRecapitulatif :\")\n",
    "        print(f\"{'Label':<15} {'Temps (s)':<12} {'Prompt':<60}\")\n",
    "        print(\"-\" * 85)\n",
    "        for cr in comparison_results:\n",
    "            prompt_short = cr['prompt'][:55] + '...' if len(cr['prompt']) > 55 else cr['prompt']\n",
    "            print(f\"  {cr['label']:<15} {cr['time']:<12.1f} {prompt_short:<60}\")\n",
    "else:\n",
    "    print(\"Comparaison de prompts : generation desactivee\")\n",
    "    print(\"\\nExemples de prompts efficaces pour AnimateDiff :\")\n",
    "    print(\"  - Mouvements naturels : eau, vent, nuages\")\n",
    "    print(\"  - Animaux en mouvement : chat marchant, oiseau volant\")\n",
    "    print(\"  - Paysages dynamiques : coucher de soleil, aurores boreales\")\n",
    "    print(\"  - Actions simples : fusee decollant, fleur eclosant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:25.044337Z",
     "iopub.status.busy": "2026-02-19T09:29:25.044111Z",
     "iopub.status.idle": "2026-02-19T09:29:25.049965Z",
     "shell.execute_reply": "2026-02-19T09:29:25.049509Z"
    },
    "papermill": {
     "duration": 0.009263,
     "end_time": "2026-02-19T09:29:25.050684",
     "exception": false,
     "start_time": "2026-02-19T09:29:25.041421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mode batch - Interface interactive desactivee\n"
     ]
    }
   ],
   "source": [
    "# Mode interactif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Entrez votre propre prompt pour generer une video.\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "    \n",
    "    try:\n",
    "        user_prompt = input(\"\\nVotre prompt : \").strip()\n",
    "        \n",
    "        if user_prompt and run_generation and pipe is not None:\n",
    "            print(f\"\\nGeneration en cours...\")\n",
    "            result_user = generate_video(user_prompt, seed=123)\n",
    "            \n",
    "            if result_user['success']:\n",
    "                print(f\"Generation reussie en {result_user['generation_time']:.1f}s\")\n",
    "                \n",
    "                # Affichage\n",
    "                n_display = min(8, len(result_user['frames']))\n",
    "                fig, axes = plt.subplots(1, n_display, figsize=(2.5 * n_display, 3))\n",
    "                if n_display == 1:\n",
    "                    axes = [axes]\n",
    "                indices = np.linspace(0, len(result_user['frames']) - 1, n_display, dtype=int)\n",
    "                for ax, idx in zip(axes, indices):\n",
    "                    ax.imshow(result_user['frames'][idx])\n",
    "                    ax.set_title(f\"Frame {idx+1}\", fontsize=8)\n",
    "                    ax.axis('off')\n",
    "                plt.suptitle(f\"Votre video : {user_prompt[:50]}...\", fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                if save_as_gif:\n",
    "                    user_gif = OUTPUT_DIR / \"user_generation.gif\"\n",
    "                    export_to_gif(result_user['frames'], str(user_gif))\n",
    "                    print(f\"GIF sauvegarde : {user_gif.name}\")\n",
    "            else:\n",
    "                print(f\"Erreur : {result_user['error']}\")\n",
    "        elif user_prompt:\n",
    "            print(\"Generation non disponible (pipeline non charge)\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError) as e:\n",
    "        print(f\"\\nMode interactif interrompu ({type(e).__name__})\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"\\nMode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"\\nErreur inattendue : {error_type} - {str(e)[:100]}\")\n",
    "            print(\"Passage a la suite du notebook\")\n",
    "else:\n",
    "    print(\"\\nMode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {
    "papermill": {
     "duration": 0.002191,
     "end_time": "2026-02-19T09:29:25.054874",
     "exception": false,
     "start_time": "2026-02-19T09:29:25.052683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bonnes pratiques et optimisation\n",
    "\n",
    "### Conseils pour les prompts AnimateDiff\n",
    "\n",
    "| Bon prompt | Mauvais prompt | Raison |\n",
    "|-----------|---------------|--------|\n",
    "| \"a cat walking slowly\" | \"a cat\" | Preciser le mouvement |\n",
    "| \"ocean waves, cinematic\" | \"nice ocean\" | Termes de qualite |\n",
    "| \"single subject, simple motion\" | \"crowd scene, complex action\" | Mouvement simple = meilleur |\n",
    "\n",
    "### Parametres optimaux\n",
    "\n",
    "| Parametre | Rapide | Equilibre | Haute qualite |\n",
    "|-----------|--------|-----------|---------------|\n",
    "| `num_frames` | 8 | 16 | 24 |\n",
    "| `num_inference_steps` | 15 | 25 | 50 |\n",
    "| `guidance_scale` | 5.0 | 7.5 | 7.5 |\n",
    "| `height/width` | 256 | 512 | 512 |\n",
    "| Temps estime (RTX 3090) | ~15s | ~45s | ~120s |\n",
    "| VRAM estimee | ~6 GB | ~10 GB | ~14 GB |\n",
    "\n",
    "### Limitations d'AnimateDiff\n",
    "\n",
    "- Resolution maximale limitee (512x512 pour SD 1.5)\n",
    "- Mouvements complexes souvent mal geres\n",
    "- Coherence temporelle imparfaite sur les longues sequences\n",
    "- Base model SD 1.5 (qualite inferieure a SDXL ou Flux)\n",
    "\n",
    "### Alternatives (voir Module 02)\n",
    "\n",
    "| Modele | Avantage | VRAM |\n",
    "|--------|----------|------|\n",
    "| **HunyuanVideo** | Haute qualite, longues sequences | ~18 GB |\n",
    "| **LTX-Video** | Rapide, leger | ~8 GB |\n",
    "| **Wan 2.1/2.2** | Prompts multilingues | ~10 GB |\n",
    "| **SVD** | Image-to-video | ~10 GB |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:25.060882Z",
     "iopub.status.busy": "2026-02-19T09:29:25.060332Z",
     "iopub.status.idle": "2026-02-19T09:29:25.067157Z",
     "shell.execute_reply": "2026-02-19T09:29:25.066308Z"
    },
    "papermill": {
     "duration": 0.011428,
     "end_time": "2026-02-19T09:29:25.068277",
     "exception": false,
     "start_time": "2026-02-19T09:29:25.056849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STATISTIQUES DE SESSION ---\n",
      "========================================\n",
      "Date : 2026-02-19 10:29:25\n",
      "Mode : batch\n",
      "Base model : stable-diffusion-v1-5/stable-diffusion-v1-5\n",
      "Motion adapter : guoyww/animatediff-motion-adapter-v1-5-3\n",
      "Device : cpu\n",
      "Parametres : 16 frames, 25 steps, CFG=7.5\n",
      "Resolution : 512x512\n",
      "\n",
      "Fichiers generes (0) :\n",
      "\n",
      "--- PROCHAINES ETAPES ---\n",
      "1. Module 02-1 : HunyuanVideo (generation video haute qualite)\n",
      "2. Module 02-2 : LTX-Video (generation rapide et legere)\n",
      "3. Module 02-3 : Wan 2.1/2.2 (prompts multilingues)\n",
      "4. Module 02-4 : SVD - Stable Video Diffusion (image-to-video)\n",
      "5. Combiner 01-4 (upscaling) avec la generation pour des videos HD\n",
      "\n",
      "Notebook 01-5 AnimateDiff Introduction termine - 10:29:25\n"
     ]
    }
   ],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Base model : {base_model}\")\n",
    "print(f\"Motion adapter : {motion_adapter}\")\n",
    "print(f\"Device : {device}\")\n",
    "print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n",
    "print(f\"Resolution : {width}x{height}\")\n",
    "\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "    print(f\"VRAM pic session : {vram_peak:.1f} GB\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "# Liberation VRAM\n",
    "if pipe is not None:\n",
    "    del pipe\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nVRAM liberee\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Module 02-1 : HunyuanVideo (generation video haute qualite)\")\n",
    "print(f\"2. Module 02-2 : LTX-Video (generation rapide et legere)\")\n",
    "print(f\"3. Module 02-3 : Wan 2.1/2.2 (prompts multilingues)\")\n",
    "print(f\"4. Module 02-4 : SVD - Stable Video Diffusion (image-to-video)\")\n",
    "print(f\"5. Combiner 01-4 (upscaling) avec la generation pour des videos HD\")\n",
    "\n",
    "print(f\"\\nNotebook 01-5 AnimateDiff Introduction termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.091014,
   "end_time": "2026-02-19T09:29:25.848648",
   "environment_variables": {},
   "exception": null,
   "input_path": "MyIA.AI.Notebooks\\GenAI\\Video\\01-Foundation\\01-5-AnimateDiff-Introduction.ipynb",
   "output_path": "MyIA.AI.Notebooks\\GenAI\\Video\\01-Foundation\\01-5-AnimateDiff-Introduction.ipynb",
   "parameters": {
    "notebook_mode": "batch",
    "skip_widgets": true
   },
   "start_time": "2026-02-19T09:29:11.757634",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}