{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Qwen2.5-VL Video Analysis - Comprehension Video Locale\n",
    "\n",
    "**Module :** 01-Video-Foundation  \n",
    "**Niveau :** Intermediaire  \n",
    "**Technologies :** Qwen2.5-VL 7B, transformers, torch  \n",
    "**Duree estimee :** 50 minutes  \n",
    "**VRAM :** ~18 GB (degradation possible sur GPU plus petits)  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Charger le modele Qwen2.5-VL-7B-Instruct localement avec transformers\n",
    "- [ ] Comprendre le video understanding avec echantillonnage FPS dynamique\n",
    "- [ ] Realiser une analyse frame par frame\n",
    "- [ ] Effectuer du temporal grounding (localiser des evenements avec timestamps)\n",
    "- [ ] Extraire du texte (OCR) dans les frames video\n",
    "- [ ] Comparer les performances avec GPT-5 API (qualite, vitesse, cout)\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- GPU avec 18+ GB VRAM (RTX 3090, RTX 4090, A100)\n",
    "- Notebook 01-2 (GPT-5 Video Understanding) complete\n",
    "- Packages : `transformers>=4.45`, `torch`, `qwen-vl-utils`, `decord`, `accelerate`\n",
    "\n",
    "> **Note** : Si votre GPU a moins de 18 GB de VRAM, le notebook tentera un chargement\n",
    "> en precision reduite (bfloat16 ou int8). Les resultats peuvent varier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres modele\n",
    "model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"  # Modele Hugging Face\n",
    "device = \"cuda\"                    # \"cuda\" ou \"cpu\" (tres lent)\n",
    "torch_dtype = \"bfloat16\"           # \"bfloat16\", \"float16\", \"auto\"\n",
    "max_frames = 16                    # Nombre max de frames pour le modele\n",
    "\n",
    "# Configuration video de test\n",
    "sample_fps = 24\n",
    "sample_duration = 10\n",
    "\n",
    "# Configuration analyse\n",
    "run_analysis = True                # Executer les analyses (False pour validation)\n",
    "run_ocr = True                     # Tester l'OCR video\n",
    "run_temporal_grounding = True      # Tester le temporal grounding\n",
    "save_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.genai_helpers import setup_genai_logging\n",
    "        print(\"Helpers GenAI importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers GenAI non disponibles - mode autonome\")\n",
    "\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'video_qwen_vl'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('video_qwen_vl')\n",
    "\n",
    "print(f\"Qwen2.5-VL Video Analysis\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}, Modele : {model_name}\")\n",
    "print(f\"Max frames : {max_frames}, Device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et verification GPU\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification GPU et VRAM\n",
    "print(\"\\n--- VERIFICATION GPU ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "    vram_free = (torch.cuda.get_device_properties(0).total_mem - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "    \n",
    "    print(f\"GPU : {gpu_name}\")\n",
    "    print(f\"VRAM totale : {vram_total:.1f} GB\")\n",
    "    print(f\"VRAM libre : {vram_free:.1f} GB\")\n",
    "    print(f\"CUDA : {torch.version.cuda}\")\n",
    "    print(f\"PyTorch : {torch.__version__}\")\n",
    "    \n",
    "    if vram_total < 18:\n",
    "        print(f\"\\nAttention : VRAM ({vram_total:.0f} GB) < 18 GB recommandes\")\n",
    "        print(\"Le modele sera charge en precision reduite si possible\")\n",
    "        if vram_total < 8:\n",
    "            print(\"VRAM insuffisante. Passage en mode CPU (tres lent).\")\n",
    "            device = \"cpu\"\n",
    "elif device == \"cuda\":\n",
    "    print(\"CUDA non disponible. Passage en mode CPU (tres lent).\")\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    print(f\"Mode CPU selectionne (pas de GPU)\")\n",
    "\n",
    "print(f\"\\nDevice final : {device}\")\n",
    "print(f\"Precision : {torch_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Section 1 : Chargement du modele Qwen2.5-VL\n",
    "\n",
    "Qwen2.5-VL est un modele vision-langage capable de comprendre des videos nativement.\n",
    "Contrairement a GPT-5 qui recoit des frames statiques, Qwen-VL peut traiter une\n",
    "sequence de frames avec une notion de temporalite integree.\n",
    "\n",
    "| Modele | Parametres | VRAM | Contexte video |\n",
    "|--------|-----------|------|----------------|\n",
    "| Qwen2.5-VL-2B | 2B | ~6 GB | Basique |\n",
    "| Qwen2.5-VL-7B | 7B | ~18 GB | Avance |\n",
    "| Qwen2.5-VL-72B | 72B | ~150 GB | Etat de l'art |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modele Qwen2.5-VL\n",
    "print(\"\\n--- CHARGEMENT DU MODELE ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "model = None\n",
    "processor = None\n",
    "\n",
    "if run_analysis:\n",
    "    try:\n",
    "        from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "        \n",
    "        # Selection de la precision selon la VRAM\n",
    "        dtype_map = {\n",
    "            \"bfloat16\": torch.bfloat16,\n",
    "            \"float16\": torch.float16,\n",
    "            \"auto\": \"auto\"\n",
    "        }\n",
    "        selected_dtype = dtype_map.get(torch_dtype, torch.bfloat16)\n",
    "        \n",
    "        print(f\"Chargement de {model_name}...\")\n",
    "        print(f\"  Precision : {torch_dtype}\")\n",
    "        print(f\"  Device : {device}\")\n",
    "        start_load = time.time()\n",
    "        \n",
    "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=selected_dtype,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None,\n",
    "            attn_implementation=\"flash_attention_2\" if device == \"cuda\" else \"eager\"\n",
    "        )\n",
    "        \n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        load_time = time.time() - start_load\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            vram_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            print(f\"  VRAM utilisee : {vram_used:.1f} GB\")\n",
    "        \n",
    "        print(f\"Modele charge en {load_time:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur chargement modele : {type(e).__name__}: {str(e)[:150]}\")\n",
    "        print(\"Le notebook continuera sans executer les analyses.\")\n",
    "        run_analysis = False\n",
    "        run_ocr = False\n",
    "        run_temporal_grounding = False\n",
    "else:\n",
    "    print(\"Chargement modele desactive (run_analysis=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Section 2 : Creation de la video de test et analyse\n",
    "\n",
    "Nous reutilisons le meme type de video multi-scenes que dans le notebook 01-2\n",
    "pour pouvoir comparer les resultats entre GPT-5 et Qwen-VL. En ajoutant du texte\n",
    "lisible dans les frames, nous pourrons aussi tester les capacites OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation video de test avec texte OCR et evenements temporels\n",
    "import imageio\n",
    "\n",
    "print(\"\\n--- CREATION VIDEO DE TEST ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "scenes = [\n",
    "    {\"name\": \"Introduction\", \"bg\": (40, 40, 100), \"text\": \"Bienvenue dans CoursIA\",\n",
    "     \"event\": \"titre_apparait\"},\n",
    "    {\"name\": \"Chapitre 1\", \"bg\": (100, 60, 40), \"text\": \"Machine Learning\",\n",
    "     \"event\": \"chapitre_ml\"},\n",
    "    {\"name\": \"Demo\", \"bg\": (40, 100, 60), \"text\": \"Regression lineaire y=ax+b\",\n",
    "     \"event\": \"formule_visible\"},\n",
    "    {\"name\": \"Resultats\", \"bg\": (100, 100, 40), \"text\": \"Accuracy: 95.3%\",\n",
    "     \"event\": \"resultat_affiche\"},\n",
    "    {\"name\": \"Conclusion\", \"bg\": (80, 40, 100), \"text\": \"Merci - Questions?\",\n",
    "     \"event\": \"fin_presentation\"},\n",
    "]\n",
    "\n",
    "width, height = 640, 480\n",
    "frames_per_scene = sample_fps * sample_duration // len(scenes)\n",
    "all_frames = []\n",
    "event_timestamps = {}\n",
    "\n",
    "for scene_idx, scene in enumerate(scenes):\n",
    "    event_timestamps[scene['event']] = scene_idx * frames_per_scene / sample_fps\n",
    "    \n",
    "    for frame_i in range(frames_per_scene):\n",
    "        t = frame_i / frames_per_scene\n",
    "        global_frame = scene_idx * frames_per_scene + frame_i\n",
    "        global_t = global_frame / sample_fps\n",
    "        \n",
    "        img = Image.new('RGB', (width, height), scene['bg'])\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Barre de progression en bas\n",
    "        progress = global_frame / (frames_per_scene * len(scenes))\n",
    "        draw.rectangle([0, height-10, int(width * progress), height], fill=(200, 200, 200))\n",
    "        \n",
    "        # Texte principal (pour OCR)\n",
    "        draw.text((width // 2 - 80, height // 2 - 20), scene['text'], fill='white')\n",
    "        \n",
    "        # Numero de scene\n",
    "        draw.text((20, 20), f\"Scene {scene_idx + 1}/5 : {scene['name']}\", fill='white')\n",
    "        draw.text((20, height - 30), f\"t={global_t:.1f}s | Frame {global_frame}\", fill='white')\n",
    "        \n",
    "        # Cercle anime\n",
    "        cx = int(width * 0.8 + 50 * np.cos(2 * np.pi * t))\n",
    "        cy = int(height * 0.3 + 30 * np.sin(2 * np.pi * t))\n",
    "        draw.ellipse([cx-15, cy-15, cx+15, cy+15], fill=(255, 255, 100))\n",
    "        \n",
    "        all_frames.append(np.array(img))\n",
    "\n",
    "# Sauvegarde\n",
    "test_video_path = OUTPUT_DIR / \"test_qwen_analysis.mp4\"\n",
    "writer = imageio.get_writer(str(test_video_path), fps=sample_fps, codec='libx264')\n",
    "for frame in all_frames:\n",
    "    writer.append_data(frame)\n",
    "writer.close()\n",
    "\n",
    "print(f\"Video creee : {test_video_path.name}\")\n",
    "print(f\"  {len(scenes)} scenes, {len(all_frames)} frames, {len(all_frames)/sample_fps:.1f}s\")\n",
    "print(f\"\\nEvenements temporels (verite terrain) :\")\n",
    "for event, ts in event_timestamps.items():\n",
    "    print(f\"  {event} : t={ts:.1f}s\")\n",
    "\n",
    "# Apercu\n",
    "fig, axes = plt.subplots(1, 5, figsize=(16, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    idx = i * frames_per_scene + frames_per_scene // 2\n",
    "    ax.imshow(all_frames[idx])\n",
    "    ax.set_title(scenes[i]['name'], fontsize=9)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Scenes de la video de test\", fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse video avec Qwen2.5-VL\n",
    "print(\"\\n--- ANALYSE VIDEO QWEN2.5-VL ---\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def analyze_video_qwen(video_path: str, prompt: str,\n",
    "                       max_frames: int = 16) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyse une video avec Qwen2.5-VL.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Chemin vers le fichier video\n",
    "        prompt: Question ou instruction d'analyse\n",
    "        max_frames: Nombre max de frames a traiter\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec analyse, temps de traitement et VRAM\n",
    "    \"\"\"\n",
    "    if model is None or processor is None:\n",
    "        return {\"success\": False, \"error\": \"Modele non charge\"}\n",
    "    \n",
    "    try:\n",
    "        # Construction du message avec video\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\",\n",
    "                    \"video\": str(video_path),\n",
    "                    \"max_pixels\": 360 * 420,\n",
    "                    \"fps\": 1.0  # 1 frame par seconde pour economiser la VRAM\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        # Preparation des inputs\n",
    "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        from qwen_vl_utils import process_vision_info\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        \n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # Generation\n",
    "        if device == \"cuda\":\n",
    "            vram_before = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512\n",
    "            )\n",
    "        gen_time = time.time() - start_time\n",
    "        \n",
    "        # Decodage\n",
    "        generated_ids = output_ids[:, inputs['input_ids'].shape[1]:]\n",
    "        response_text = processor.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        result = {\n",
    "            \"success\": True,\n",
    "            \"analysis\": response_text,\n",
    "            \"generation_time\": gen_time,\n",
    "            \"output_tokens\": generated_ids.shape[1],\n",
    "        }\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            vram_after = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            result[\"vram_peak\"] = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "            result[\"vram_delta\"] = vram_after - vram_before\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": f\"{type(e).__name__}: {str(e)[:200]}\"}\n",
    "\n",
    "\n",
    "# Analyse 1 : Description globale\n",
    "if run_analysis:\n",
    "    print(\"Analyse 1 : Description globale\")\n",
    "    result_global = analyze_video_qwen(\n",
    "        test_video_path,\n",
    "        \"Decris cette video en francais. Identifie les differentes scenes, \"\n",
    "        \"les elements visuels principaux et la progression temporelle.\"\n",
    "    )\n",
    "    \n",
    "    if result_global['success']:\n",
    "        print(f\"  Temps : {result_global['generation_time']:.1f}s\")\n",
    "        print(f\"  Tokens generes : {result_global['output_tokens']}\")\n",
    "        if 'vram_peak' in result_global:\n",
    "            print(f\"  VRAM pic : {result_global['vram_peak']:.1f} GB\")\n",
    "        print(f\"\\n--- Analyse Qwen-VL ---\")\n",
    "        print(result_global['analysis'])\n",
    "    else:\n",
    "        print(f\"  Erreur : {result_global['error']}\")\n",
    "else:\n",
    "    print(\"Analyse desactivee\")\n",
    "    result_global = {\"success\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR video et temporal grounding\n",
    "results_ocr = {\"success\": False}\n",
    "results_grounding = {\"success\": False}\n",
    "\n",
    "if run_ocr and run_analysis:\n",
    "    print(\"\\n--- OCR VIDEO ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    results_ocr = analyze_video_qwen(\n",
    "        test_video_path,\n",
    "        \"Lis tout le texte visible dans cette video. Pour chaque texte trouve, \"\n",
    "        \"indique a quel moment il apparait et ce qu'il dit exactement.\"\n",
    "    )\n",
    "    \n",
    "    if results_ocr['success']:\n",
    "        print(f\"  Temps : {results_ocr['generation_time']:.1f}s\")\n",
    "        print(f\"\\n--- OCR Resultats ---\")\n",
    "        print(results_ocr['analysis'])\n",
    "    else:\n",
    "        print(f\"  Erreur OCR : {results_ocr['error']}\")\n",
    "\n",
    "if run_temporal_grounding and run_analysis:\n",
    "    print(\"\\n--- TEMPORAL GROUNDING ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    results_grounding = analyze_video_qwen(\n",
    "        test_video_path,\n",
    "        \"Dans cette video, identifie precisement les moments suivants :\\n\"\n",
    "        \"1. Quand le titre apparait pour la premiere fois\\n\"\n",
    "        \"2. Quand une formule mathematique est visible\\n\"\n",
    "        \"3. Quand un resultat de precision (accuracy) est affiche\\n\"\n",
    "        \"Donne les timestamps au format [debut - fin] en secondes.\"\n",
    "    )\n",
    "    \n",
    "    if results_grounding['success']:\n",
    "        print(f\"  Temps : {results_grounding['generation_time']:.1f}s\")\n",
    "        print(f\"\\n--- Temporal Grounding ---\")\n",
    "        print(results_grounding['analysis'])\n",
    "        \n",
    "        # Comparaison avec la verite terrain\n",
    "        print(f\"\\nVerite terrain (timestamps reels) :\")\n",
    "        for event, ts in event_timestamps.items():\n",
    "            print(f\"  {event} : t={ts:.1f}s\")\n",
    "    else:\n",
    "        print(f\"  Erreur grounding : {results_grounding['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Posez une question sur la video de test a Qwen-VL.\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "    \n",
    "    try:\n",
    "        user_question = input(\"\\nVotre question : \").strip()\n",
    "        \n",
    "        if user_question and run_analysis:\n",
    "            result_user = analyze_video_qwen(test_video_path, user_question)\n",
    "            if result_user['success']:\n",
    "                print(f\"\\nReponse Qwen-VL :\")\n",
    "                print(result_user['analysis'])\n",
    "                print(f\"\\n({result_user['generation_time']:.1f}s, {result_user['output_tokens']} tokens)\")\n",
    "            else:\n",
    "                print(f\"Erreur : {result_user['error']}\")\n",
    "        elif not user_question:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError) as e:\n",
    "        print(f\"\\nMode interactif interrompu ({type(e).__name__})\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"\\nMode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"\\nErreur inattendue : {error_type} - {str(e)[:100]}\")\n",
    "            print(\"Passage a la suite du notebook\")\n",
    "else:\n",
    "    print(\"\\nMode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Comparaison GPT-5 vs Qwen2.5-VL\n",
    "\n",
    "| Critere | GPT-5 (API) | Qwen2.5-VL (local) |\n",
    "|---------|-------------|--------------------|\n",
    "| **Acces** | API cloud (cle requise) | Local (GPU requis) |\n",
    "| **VRAM** | 0 (cloud) | ~18 GB |\n",
    "| **Latence** | 2-10s (reseau) | 5-30s (GPU) |\n",
    "| **Cout** | ~$0.01-0.05/analyse | Gratuit apres setup |\n",
    "| **Confidentialite** | Donnees envoyees au cloud | 100% local |\n",
    "| **Video native** | Non (frames statiques) | Oui (temporal attention) |\n",
    "| **OCR** | Bon | Bon |\n",
    "| **Temporal grounding** | Approximatif | Natif |\n",
    "\n",
    "**Recommandations** :\n",
    "- Utilisez **GPT-5** pour du prototypage rapide ou sans GPU\n",
    "- Utilisez **Qwen-VL** pour du traitement en volume ou des donnees sensibles\n",
    "- Pour la production, evaluez sur votre cas d'usage specifique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Modele : {model_name}\")\n",
    "print(f\"Device : {device}\")\n",
    "\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    vram_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "    print(f\"VRAM actuelle : {vram_used:.1f} GB\")\n",
    "    print(f\"VRAM pic : {vram_peak:.1f} GB\")\n",
    "\n",
    "analyses_done = sum([\n",
    "    result_global.get('success', False),\n",
    "    results_ocr.get('success', False),\n",
    "    results_grounding.get('success', False)\n",
    "])\n",
    "print(f\"Analyses reussies : {analyses_done}/3\")\n",
    "\n",
    "if save_results:\n",
    "    results_data = {\n",
    "        \"model\": model_name,\n",
    "        \"device\": device,\n",
    "        \"global_analysis\": result_global if result_global.get('success') else None,\n",
    "        \"ocr\": results_ocr if results_ocr.get('success') else None,\n",
    "        \"temporal_grounding\": results_grounding if results_grounding.get('success') else None,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    results_file = OUTPUT_DIR / f\"qwen_vl_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "    print(f\"Resultats sauvegardes : {results_file.name}\")\n",
    "\n",
    "# Liberation VRAM\n",
    "if model is not None:\n",
    "    del model\n",
    "    del processor\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"VRAM liberee\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 01-4 : Amelioration video avec Real-ESRGAN (upscaling, interpolation)\")\n",
    "print(f\"2. Notebook 01-5 : Introduction a AnimateDiff (generation text-to-video)\")\n",
    "print(f\"3. Module 02 : Modeles generatifs video avances (HunyuanVideo, LTX-Video)\")\n",
    "\n",
    "print(f\"\\nNotebook 01-3 Qwen-VL Video Analysis termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}