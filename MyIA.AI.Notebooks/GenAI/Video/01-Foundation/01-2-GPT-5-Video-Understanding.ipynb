{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# GPT-5 Video Understanding - Comprehension Video par IA\n",
    "\n",
    "**Module :** 01-Video-Foundation  \n",
    "**Niveau :** Debutant  \n",
    "**Technologies :** OpenAI GPT-5, video understanding, base64 frames  \n",
    "**Duree estimee :** 40 minutes  \n",
    "**VRAM :** 0 (API cloud uniquement)  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Echantillonner des frames d'une video pour les envoyer a GPT-5\n",
    "- [ ] Encoder des frames en base64 pour l'API multimodale\n",
    "- [ ] Obtenir une description de scene et un raisonnement temporel\n",
    "- [ ] Poser des questions sur le contenu d'une video (Video Q&A)\n",
    "- [ ] Comparer les strategies d'echantillonnage (uniforme vs keyframe)\n",
    "- [ ] Analyser du contenu educatif a partir de videos\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Cle API OpenAI ou OpenRouter configuree (`OPENAI_API_KEY` dans `.env`)\n",
    "- Notebook 01-1 (Video Operations Basics) complete\n",
    "- Packages : `openai`, `decord`, `Pillow`, `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres GPT-5\n",
    "model_name = \"gpt-5-mini\"           # \"gpt-5\", \"gpt-5-mini\"\n",
    "n_frames = 8                       # Nombre de frames a envoyer\n",
    "max_tokens = 1000                  # Tokens de reponse max\n",
    "temperature = 0.3                  # Basse pour analyse factuelle\n",
    "\n",
    "# Configuration video\n",
    "sample_fps = 24                    # FPS de la video de test\n",
    "sample_duration = 10               # Duree video de test (secondes)\n",
    "frame_strategy = \"uniform\"         # \"uniform\" ou \"keyframe\"\n",
    "\n",
    "# Configuration sauvegarde\n",
    "save_analysis = True               # Sauvegarder les analyses\n",
    "analyze_video = True               # Executer les analyses (False pour validation seule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.genai_helpers import setup_genai_logging\n",
    "        print(\"Helpers GenAI importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers GenAI non disponibles - mode autonome\")\n",
    "\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'video_gpt5'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('video_gpt5')\n",
    "\n",
    "print(f\"GPT-5 Video Understanding\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}, Modele : {model_name}\")\n",
    "print(f\"Frames a envoyer : {n_frames}, Strategie : {frame_strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et configuration API OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve dans l'arborescence\")\n",
    "\n",
    "# Configuration client OpenAI\n",
    "print(\"\\n--- CONFIGURATION API ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_base = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')\n",
    "\n",
    "if not openai_key:\n",
    "    if notebook_mode == \"batch\" and not analyze_video:\n",
    "        print(\"Mode batch sans analyse : cle API ignoree\")\n",
    "        openai_key = \"dummy_key_for_validation\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"OPENAI_API_KEY manquante dans .env\\n\"\n",
    "            \"Configurez votre cle API OpenAI ou OpenRouter.\"\n",
    "        )\n",
    "\n",
    "# Initialisation client\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_key,\n",
    "    base_url=openai_base\n",
    ")\n",
    "\n",
    "# Test de connexion\n",
    "if openai_key != \"dummy_key_for_validation\":\n",
    "    try:\n",
    "        models = client.models.list()\n",
    "        gpt5_models = [m for m in models.data if 'gpt-5' in m.id.lower()]\n",
    "        print(f\"Connexion API reussie\")\n",
    "        if gpt5_models:\n",
    "            for m in gpt5_models[:3]:\n",
    "                print(f\"  Modele disponible : {m.id}\")\n",
    "        print(f\"Modele selectionne : {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Avertissement connexion : {str(e)[:100]}\")\n",
    "        print(\"Le notebook continuera, les appels API pourraient echouer.\")\n",
    "else:\n",
    "    print(\"Test connexion ignore (dummy key)\")\n",
    "\n",
    "print(f\"\\nBase URL : {openai_base}\")\n",
    "print(f\"Max tokens : {max_tokens}\")\n",
    "print(f\"Temperature : {temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Section 1 : Creation de la video de test et extraction de frames\n",
    "\n",
    "Nous creons une video de test plus riche que dans le notebook 01-1, avec plusieurs\n",
    "\"scenes\" distinctes pour que GPT-5 puisse demontrer son raisonnement temporel.\n",
    "Chaque scene contient des formes, couleurs et textes differents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation d'une video multi-scenes pour test\n",
    "print(\"\\n--- CREATION VIDEO MULTI-SCENES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import imageio\n",
    "\n",
    "# Definition des scenes\n",
    "scenes = [\n",
    "    {\"name\": \"Scene 1 : Matin\", \"bg_color\": (255, 200, 100), \"shape\": \"circle\",\n",
    "     \"shape_color\": (255, 100, 0), \"text\": \"Lever du soleil\"},\n",
    "    {\"name\": \"Scene 2 : Midi\", \"bg_color\": (100, 180, 255), \"shape\": \"rectangle\",\n",
    "     \"shape_color\": (0, 100, 200), \"text\": \"Ciel bleu\"},\n",
    "    {\"name\": \"Scene 3 : Apres-midi\", \"bg_color\": (100, 200, 100), \"shape\": \"triangle\",\n",
    "     \"shape_color\": (0, 150, 50), \"text\": \"Nature verte\"},\n",
    "    {\"name\": \"Scene 4 : Soir\", \"bg_color\": (200, 100, 150), \"shape\": \"circle\",\n",
    "     \"shape_color\": (150, 0, 80), \"text\": \"Coucher de soleil\"},\n",
    "    {\"name\": \"Scene 5 : Nuit\", \"bg_color\": (30, 30, 80), \"shape\": \"star\",\n",
    "     \"shape_color\": (255, 255, 200), \"text\": \"Etoiles\"},\n",
    "]\n",
    "\n",
    "width, height = 640, 480\n",
    "frames_per_scene = sample_fps * sample_duration // len(scenes)\n",
    "all_frames = []\n",
    "\n",
    "for scene_idx, scene in enumerate(scenes):\n",
    "    for frame_i in range(frames_per_scene):\n",
    "        t = frame_i / frames_per_scene  # progression dans la scene\n",
    "        \n",
    "        img = Image.new('RGB', (width, height), scene['bg_color'])\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Position animee de la forme\n",
    "        cx = int(width * 0.5 + width * 0.25 * np.cos(2 * np.pi * t))\n",
    "        cy = int(height * 0.5 + height * 0.2 * np.sin(2 * np.pi * t))\n",
    "        r = 50\n",
    "        \n",
    "        if scene['shape'] == 'circle':\n",
    "            draw.ellipse([cx-r, cy-r, cx+r, cy+r], fill=scene['shape_color'])\n",
    "        elif scene['shape'] == 'rectangle':\n",
    "            draw.rectangle([cx-r, cy-r, cx+r, cy+r], fill=scene['shape_color'])\n",
    "        elif scene['shape'] == 'triangle':\n",
    "            draw.polygon([(cx, cy-r), (cx-r, cy+r), (cx+r, cy+r)],\n",
    "                         fill=scene['shape_color'])\n",
    "        elif scene['shape'] == 'star':\n",
    "            # Etoile simple a 5 branches (petits cercles)\n",
    "            for angle in range(0, 360, 72):\n",
    "                sx = cx + int(r * 0.7 * np.cos(np.radians(angle)))\n",
    "                sy = cy + int(r * 0.7 * np.sin(np.radians(angle)))\n",
    "                draw.ellipse([sx-8, sy-8, sx+8, sy+8], fill=scene['shape_color'])\n",
    "        \n",
    "        # Texte de la scene\n",
    "        draw.text((20, 20), scene['name'], fill='white')\n",
    "        draw.text((20, 40), scene['text'], fill='white')\n",
    "        global_frame = scene_idx * frames_per_scene + frame_i\n",
    "        draw.text((20, height - 25), f\"Frame {global_frame}/{frames_per_scene * len(scenes)}\",\n",
    "                  fill='white')\n",
    "        \n",
    "        all_frames.append(np.array(img))\n",
    "\n",
    "# Sauvegarder la video\n",
    "test_video_path = OUTPUT_DIR / \"test_multiscene.mp4\"\n",
    "writer = imageio.get_writer(str(test_video_path), fps=sample_fps, codec='libx264')\n",
    "for frame in all_frames:\n",
    "    writer.append_data(frame)\n",
    "writer.close()\n",
    "\n",
    "total_frames = len(all_frames)\n",
    "total_duration = total_frames / sample_fps\n",
    "file_size_kb = test_video_path.stat().st_size / 1024\n",
    "\n",
    "print(f\"Video multi-scenes creee\")\n",
    "print(f\"  Fichier : {test_video_path.name}\")\n",
    "print(f\"  Scenes : {len(scenes)}\")\n",
    "print(f\"  Frames totales : {total_frames}\")\n",
    "print(f\"  Duree : {total_duration:.1f}s\")\n",
    "print(f\"  Taille : {file_size_kb:.1f} KB\")\n",
    "\n",
    "# Apercu des scenes\n",
    "scene_preview_indices = [int(i * frames_per_scene + frames_per_scene // 2) for i in range(len(scenes))]\n",
    "fig, axes = plt.subplots(1, len(scenes), figsize=(16, 3))\n",
    "for ax, idx in zip(axes, scene_preview_indices):\n",
    "    ax.imshow(all_frames[idx])\n",
    "    ax.set_title(f\"t={idx/sample_fps:.1f}s\", fontsize=9)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Apercu des 5 scenes\", fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Section 2 : Strategies d'echantillonnage et encodage base64\n",
    "\n",
    "Pour envoyer une video a GPT-5, on ne transmet pas la video complete mais un echantillon\n",
    "de N frames encodees en base64. Le choix de la strategie d'echantillonnage impacte\n",
    "directement la qualite de la comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategies d'echantillonnage de frames\n",
    "print(\"\\n--- STRATEGIES D'ECHANTILLONNAGE ---\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def sample_frames_uniform(frames: List[np.ndarray], n: int) -> Tuple[List[np.ndarray], List[int]]:\n",
    "    \"\"\"\n",
    "    Echantillonnage uniforme : N frames regulierement espacees.\n",
    "    Avantage : couverture temporelle complete.\n",
    "    \"\"\"\n",
    "    total = len(frames)\n",
    "    indices = np.linspace(0, total - 1, n, dtype=int).tolist()\n",
    "    return [frames[i] for i in indices], indices\n",
    "\n",
    "\n",
    "def sample_frames_keyframe(frames: List[np.ndarray], n: int,\n",
    "                           threshold: float = 30.0) -> Tuple[List[np.ndarray], List[int]]:\n",
    "    \"\"\"\n",
    "    Echantillonnage par keyframes : detecte les changements de scene.\n",
    "    Avantage : capture les moments de transition.\n",
    "    \"\"\"\n",
    "    # Calculer la difference entre frames consecutives\n",
    "    diffs = []\n",
    "    for i in range(1, len(frames)):\n",
    "        diff = np.mean(np.abs(frames[i].astype(float) - frames[i-1].astype(float)))\n",
    "        diffs.append((i, diff))\n",
    "    \n",
    "    # Trier par difference decroissante (plus grands changements)\n",
    "    diffs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Prendre les N-1 plus grands changements + la premiere frame\n",
    "    keyframe_indices = [0]\n",
    "    for idx, diff_val in diffs[:n - 1]:\n",
    "        keyframe_indices.append(idx)\n",
    "    \n",
    "    keyframe_indices.sort()\n",
    "    return [frames[i] for i in keyframe_indices], keyframe_indices\n",
    "\n",
    "\n",
    "def encode_frame_base64(frame: np.ndarray, quality: int = 85) -> str:\n",
    "    \"\"\"\n",
    "    Encode une frame numpy en base64 JPEG pour l'API GPT-5.\n",
    "    \"\"\"\n",
    "    img = Image.fromarray(frame)\n",
    "    buffer = BytesIO()\n",
    "    img.save(buffer, format='JPEG', quality=quality)\n",
    "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "\n",
    "# Comparer les deux strategies\n",
    "uniform_frames, uniform_indices = sample_frames_uniform(all_frames, n_frames)\n",
    "keyframe_frames, keyframe_indices = sample_frames_keyframe(all_frames, n_frames)\n",
    "\n",
    "print(f\"Echantillonnage uniforme ({n_frames} frames) :\")\n",
    "print(f\"  Indices : {uniform_indices}\")\n",
    "print(f\"  Timestamps : {[f'{i/sample_fps:.1f}s' for i in uniform_indices]}\")\n",
    "\n",
    "print(f\"\\nEchantillonnage keyframe ({n_frames} frames) :\")\n",
    "print(f\"  Indices : {keyframe_indices}\")\n",
    "print(f\"  Timestamps : {[f'{i/sample_fps:.1f}s' for i in keyframe_indices]}\")\n",
    "\n",
    "# Visualisation comparative\n",
    "fig, axes = plt.subplots(2, n_frames, figsize=(2.5 * n_frames, 5))\n",
    "for i in range(n_frames):\n",
    "    axes[0, i].imshow(uniform_frames[i])\n",
    "    axes[0, i].set_title(f\"t={uniform_indices[i]/sample_fps:.1f}s\", fontsize=8)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(keyframe_frames[i])\n",
    "    axes[1, i].set_title(f\"t={keyframe_indices[i]/sample_fps:.1f}s\", fontsize=8)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Uniforme\", fontsize=10, fontweight='bold')\n",
    "axes[1, 0].set_ylabel(\"Keyframe\", fontsize=10, fontweight='bold')\n",
    "plt.suptitle(\"Comparaison des strategies d'echantillonnage\", fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Encodage base64 des frames selectionnees\n",
    "strategy = frame_strategy\n",
    "selected_frames = uniform_frames if strategy == \"uniform\" else keyframe_frames\n",
    "selected_indices = uniform_indices if strategy == \"uniform\" else keyframe_indices\n",
    "\n",
    "encoded_frames = [encode_frame_base64(f) for f in selected_frames]\n",
    "total_payload_kb = sum(len(e) for e in encoded_frames) / 1024\n",
    "\n",
    "print(f\"\\nFrames encodees en base64 (strategie : {strategy})\")\n",
    "print(f\"  Nombre : {len(encoded_frames)}\")\n",
    "print(f\"  Taille payload totale : {total_payload_kb:.1f} KB\")\n",
    "print(f\"  Taille moyenne par frame : {total_payload_kb / len(encoded_frames):.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Interpretation : Strategies d'echantillonnage\n",
    "\n",
    "| Strategie | Avantage | Inconvenient | Cas d'usage |\n",
    "|-----------|----------|-------------|-------------|\n",
    "| **Uniforme** | Couverture temporelle reguliere | Peut rater des transitions | Videos a rythme constant |\n",
    "| **Keyframe** | Capture les changements majeurs | Peut ignorer les passages calmes | Videos avec scenes distinctes |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le choix de la strategie depend du type de video et de la question posee\n",
    "2. Plus de frames = meilleure comprehension mais cout API plus eleve\n",
    "3. L'encodage JPEG avec quality=85 offre un bon rapport qualite/taille\n",
    "\n",
    "## Section 3 : Description de scene et raisonnement temporel\n",
    "\n",
    "Nous envoyons maintenant les frames echantillonnees a GPT-5 pour obtenir\n",
    "une description de la video et un raisonnement sur la progression temporelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse video avec GPT-5\n",
    "print(\"\\n--- ANALYSE VIDEO AVEC GPT-5 ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def analyze_video_with_gpt5(\n",
    "    encoded_frames: List[str],\n",
    "    frame_timestamps: List[float],\n",
    "    prompt: str,\n",
    "    model: str = \"gpt-5-mini\",\n",
    "    max_tokens: int = 1000\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Envoie des frames video a GPT-5 pour analyse.\n",
    "    \n",
    "    Args:\n",
    "        encoded_frames: Liste de frames encodees en base64\n",
    "        frame_timestamps: Timestamps correspondants en secondes\n",
    "        prompt: Question ou instruction d'analyse\n",
    "        model: Modele GPT-5 a utiliser\n",
    "        max_tokens: Nombre max de tokens de reponse\n",
    "    \n",
    "    Returns:\n",
    "        Dictionnaire avec analyse, metadonnees et statistiques\n",
    "    \"\"\"\n",
    "    # Construction du message multimodal\n",
    "    content = []\n",
    "    \n",
    "    # Instructions avec contexte temporel\n",
    "    context = (\n",
    "        f\"{prompt}\\n\\n\"\n",
    "        f\"La video contient {len(encoded_frames)} frames extraites aux timestamps suivants : \"\n",
    "        f\"{', '.join([f'{t:.1f}s' for t in frame_timestamps])}.\"\n",
    "    )\n",
    "    content.append({\"type\": \"text\", \"text\": context})\n",
    "    \n",
    "    # Ajout des frames\n",
    "    for i, (b64_frame, ts) in enumerate(zip(encoded_frames, frame_timestamps)):\n",
    "        content.append({\"type\": \"text\", \"text\": f\"Frame {i+1} (t={ts:.1f}s) :\"})\n",
    "        content.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{b64_frame}\",\n",
    "                \"detail\": \"low\"  # Economie de tokens pour les frames video\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": content}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"analysis\": response.choices[0].message.content,\n",
    "            \"tokens_used\": {\n",
    "                \"prompt\": response.usage.prompt_tokens,\n",
    "                \"completion\": response.usage.completion_tokens,\n",
    "                \"total\": response.usage.total_tokens\n",
    "            },\n",
    "            \"response_time\": elapsed,\n",
    "            \"model\": model,\n",
    "            \"n_frames\": len(encoded_frames)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"response_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "\n",
    "# Analyse 1 : Description de scene\n",
    "if analyze_video:\n",
    "    timestamps = [idx / sample_fps for idx in selected_indices]\n",
    "    \n",
    "    print(\"Analyse 1 : Description de scene\")\n",
    "    scene_prompt = (\n",
    "        \"Decris cette video en francais. Pour chaque frame, identifie la scene, \"\n",
    "        \"les couleurs dominantes, les formes presentes et le texte visible. \"\n",
    "        \"Fournis ensuite un resume de la progression temporelle de la video.\"\n",
    "    )\n",
    "    \n",
    "    result_scene = analyze_video_with_gpt5(\n",
    "        encoded_frames, timestamps, scene_prompt,\n",
    "        model=model_name, max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    if result_scene['success']:\n",
    "        print(f\"  Temps de reponse : {result_scene['response_time']:.2f}s\")\n",
    "        print(f\"  Tokens utilises : {result_scene['tokens_used']}\")\n",
    "        print(f\"\\n--- Analyse GPT-5 ---\")\n",
    "        print(result_scene['analysis'])\n",
    "    else:\n",
    "        print(f\"  Erreur : {result_scene['error']}\")\n",
    "    \n",
    "    # Analyse 2 : Raisonnement temporel\n",
    "    print(\"\\n\\nAnalyse 2 : Raisonnement temporel\")\n",
    "    temporal_prompt = (\n",
    "        \"En analysant la progression des frames dans le temps, reponds en francais :\\n\"\n",
    "        \"1. Combien de scenes distinctes detectes-tu ?\\n\"\n",
    "        \"2. A quels moments les transitions se produisent-elles ?\\n\"\n",
    "        \"3. Y a-t-il un theme ou une narration qui relie les scenes ?\\n\"\n",
    "        \"4. Quels elements changent et quels elements restent constants ?\"\n",
    "    )\n",
    "    \n",
    "    result_temporal = analyze_video_with_gpt5(\n",
    "        encoded_frames, timestamps, temporal_prompt,\n",
    "        model=model_name, max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    if result_temporal['success']:\n",
    "        print(f\"  Temps de reponse : {result_temporal['response_time']:.2f}s\")\n",
    "        print(f\"  Tokens utilises : {result_temporal['tokens_used']}\")\n",
    "        print(f\"\\n--- Raisonnement temporel GPT-5 ---\")\n",
    "        print(result_temporal['analysis'])\n",
    "    else:\n",
    "        print(f\"  Erreur : {result_temporal['error']}\")\n",
    "else:\n",
    "    print(\"Analyse video desactivee (analyze_video=False)\")\n",
    "    result_scene = {\"success\": False, \"error\": \"Analyse desactivee\"}\n",
    "    result_temporal = {\"success\": False, \"error\": \"Analyse desactivee\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Q&A : poser des questions sur le contenu\n",
    "if analyze_video:\n",
    "    print(\"\\n--- VIDEO Q&A ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    questions = [\n",
    "        \"Quelle est la couleur dominante de la premiere scene ?\",\n",
    "        \"Combien de formes geometriques differentes apparaissent dans la video ?\",\n",
    "        \"La video represente-t-elle un cycle (jour/nuit, saisons, etc.) ?\",\n",
    "    ]\n",
    "    \n",
    "    qa_results = []\n",
    "    for q_idx, question in enumerate(questions, 1):\n",
    "        print(f\"\\nQuestion {q_idx} : {question}\")\n",
    "        \n",
    "        qa_prompt = (\n",
    "            f\"Reponds en francais a cette question sur la video :\\n{question}\\n\\n\"\n",
    "            f\"Donne une reponse concise et precise basee sur les frames fournies.\"\n",
    "        )\n",
    "        \n",
    "        result_qa = analyze_video_with_gpt5(\n",
    "            encoded_frames, timestamps, qa_prompt,\n",
    "            model=model_name, max_tokens=300\n",
    "        )\n",
    "        \n",
    "        if result_qa['success']:\n",
    "            print(f\"  Reponse : {result_qa['analysis'][:200]}\")\n",
    "            print(f\"  ({result_qa['response_time']:.1f}s, {result_qa['tokens_used']['total']} tokens)\")\n",
    "            qa_results.append({\"question\": question, \"answer\": result_qa['analysis'],\n",
    "                              \"tokens\": result_qa['tokens_used']['total']})\n",
    "        else:\n",
    "            print(f\"  Erreur : {result_qa['error']}\")\n",
    "            qa_results.append({\"question\": question, \"error\": result_qa['error']})\n",
    "    \n",
    "    # Tableau recapitulatif Q&A\n",
    "    print(f\"\\nRecapitulatif Q&A :\")\n",
    "    print(f\"{'Question':<60} {'Tokens':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    for qa in qa_results:\n",
    "        q_short = qa['question'][:55] + '...' if len(qa['question']) > 55 else qa['question']\n",
    "        tokens = qa.get('tokens', 'N/A')\n",
    "        print(f\"  {q_short:<60} {tokens:<10}\")\n",
    "else:\n",
    "    print(\"Q&A desactive (analyze_video=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif - Question personnalisee sur la video\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Posez votre propre question sur la video de test.\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "    \n",
    "    try:\n",
    "        user_question = input(\"\\nVotre question : \").strip()\n",
    "        \n",
    "        if user_question and analyze_video:\n",
    "            user_prompt = (\n",
    "                f\"Reponds en francais a cette question sur la video :\\n{user_question}\"\n",
    "            )\n",
    "            result_user = analyze_video_with_gpt5(\n",
    "                encoded_frames, timestamps, user_prompt,\n",
    "                model=model_name, max_tokens=max_tokens\n",
    "            )\n",
    "            if result_user['success']:\n",
    "                print(f\"\\nReponse GPT-5 :\")\n",
    "                print(result_user['analysis'])\n",
    "                print(f\"\\n({result_user['response_time']:.1f}s, {result_user['tokens_used']['total']} tokens)\")\n",
    "            else:\n",
    "                print(f\"Erreur : {result_user['error']}\")\n",
    "        elif not user_question:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError) as e:\n",
    "        print(f\"\\nMode interactif interrompu ({type(e).__name__})\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"\\nMode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"\\nErreur inattendue : {error_type} - {str(e)[:100]}\")\n",
    "            print(\"Passage a la suite du notebook\")\n",
    "else:\n",
    "    print(\"\\nMode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Bonnes pratiques et optimisation\n",
    "\n",
    "| Parametre | Recommandation | Impact |\n",
    "|-----------|---------------|--------|\n",
    "| **n_frames** | 4-8 pour videos courtes, 12-16 pour longues | Plus de frames = meilleure comprehension, cout accru |\n",
    "| **detail** | `low` pour frames video, `high` pour images claires | `low` economise ~65 tokens/image |\n",
    "| **quality JPEG** | 75-85 | Bon compromis taille/qualite pour l'API |\n",
    "| **temperature** | 0.1-0.3 pour analyse factuelle | Basse temperature = reponses plus fiables |\n",
    "| **Strategie** | Uniforme par defaut, keyframe pour scenes variees | Keyframe capture mieux les transitions |\n",
    "\n",
    "**Estimation des couts** :\n",
    "- 8 frames en `detail=low` : ~700 tokens d'image\n",
    "- Prompt + reponse : ~500-1500 tokens de texte\n",
    "- Total par analyse : ~1200-2200 tokens\n",
    "\n",
    "**Limitations** :\n",
    "- GPT-5 ne recoit pas la video, mais des images statiques : pas de perception du mouvement\n",
    "- Le raisonnement temporel repose sur les indices visuels et textuels dans les frames\n",
    "- Les videos tres longues necessitent un echantillonnage plus agressif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Modele : {model_name}\")\n",
    "print(f\"Strategie : {frame_strategy}\")\n",
    "print(f\"Frames envoyees : {n_frames}\")\n",
    "print(f\"Payload total : {total_payload_kb:.1f} KB\")\n",
    "\n",
    "# Sauvegarder les resultats\n",
    "if save_analysis and analyze_video:\n",
    "    results_file = OUTPUT_DIR / f\"gpt5_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    results_data = {\n",
    "        \"model\": model_name,\n",
    "        \"n_frames\": n_frames,\n",
    "        \"strategy\": frame_strategy,\n",
    "        \"scene_analysis\": result_scene if result_scene.get('success') else None,\n",
    "        \"temporal_analysis\": result_temporal if result_temporal.get('success') else None,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    # Retirer les champs non serialisables\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "    print(f\"Resultats sauvegardes : {results_file.name}\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 01-3 : Analyse video locale avec Qwen2.5-VL (GPU, temporal grounding)\")\n",
    "print(f\"2. Comparer GPT-5 (API cloud) vs Qwen-VL (local GPU) en qualite et cout\")\n",
    "print(f\"3. Notebook 01-4 : Amelioration video avec Real-ESRGAN\")\n",
    "print(f\"4. Notebook 01-5 : Generation video avec AnimateDiff\")\n",
    "\n",
    "print(f\"\\nNotebook 01-2 GPT-5 Video Understanding termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}