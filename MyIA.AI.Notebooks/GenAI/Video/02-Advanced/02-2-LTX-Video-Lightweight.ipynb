{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# LTX-Video - Generation Video Rapide et Legere\n",
    "\n",
    "**Module :** 02-Video-Advanced  \n",
    "**Niveau :** Intermediaire  \n",
    "**Technologies :** LTX-Video (Lightricks), diffusers  \n",
    "**Duree estimee :** 45 minutes  \n",
    "**VRAM :** ~8 GB  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Comprendre l'architecture LTX-Video et son approche legere\n",
    "- [ ] Charger le pipeline LTX-Video via diffusers\n",
    "- [ ] Generer des videos text-to-video rapidement\n",
    "- [ ] Animer une image statique (image-to-video)\n",
    "- [ ] Appliquer du style transfer video-to-video\n",
    "- [ ] Comparer les compromis vitesse/qualite avec HunyuanVideo\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- GPU avec 8+ GB VRAM (RTX 3060 8GB minimum)\n",
    "- Notebook 02-1 (HunyuanVideo) complete pour comparaison\n",
    "- Packages : `diffusers>=0.32`, `transformers`, `torch`, `accelerate`, `imageio`, `imageio-ffmpeg`\n",
    "\n",
    "**Navigation :** [<< 02-1](02-1-HunyuanVideo-Generation.ipynb) | [Index](../README.md) | [Suivant >>](02-3-Wan-Video-Generation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres modele\n",
    "model_id = \"Lightricks/LTX-Video\"  # Modele LTX-Video\n",
    "device = \"cuda\"                    # Device de calcul\n",
    "\n",
    "# Parametres generation\n",
    "num_frames = 16                    # Nombre de frames a generer\n",
    "num_inference_steps = 20           # Nombre d'etapes de debruitage\n",
    "guidance_scale = 7.5               # CFG scale (adherence au prompt)\n",
    "height = 480                       # Hauteur video\n",
    "width = 704                        # Largeur video\n",
    "fps_output = 8                     # FPS de la video de sortie\n",
    "\n",
    "# Configuration\n",
    "run_generation = True              # Executer la generation\n",
    "run_img2vid = True                 # Tester image-to-video\n",
    "save_as_mp4 = True                 # Sauvegarder en MP4\n",
    "save_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.genai_helpers import setup_genai_logging\n",
    "        print(\"Helpers GenAI importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers GenAI non disponibles - mode autonome\")\n",
    "\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'ltx_video'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('ltx_video')\n",
    "\n",
    "print(f\"LTX-Video - Generation Video Rapide et Legere\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Frames : {num_frames}, Steps : {num_inference_steps}, CFG : {guidance_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et verification GPU\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification GPU\n",
    "print(\"\\n--- VERIFICATION GPU ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "    vram_free = (torch.cuda.get_device_properties(0).total_mem - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "    \n",
    "    print(f\"GPU : {gpu_name}\")\n",
    "    print(f\"VRAM totale : {vram_total:.1f} GB\")\n",
    "    print(f\"VRAM libre : {vram_free:.1f} GB\")\n",
    "    print(f\"CUDA : {torch.version.cuda}\")\n",
    "    \n",
    "    if vram_total < 8:\n",
    "        print(f\"\\nAttention : VRAM ({vram_total:.0f} GB) < 8 GB recommandes\")\n",
    "        height = 320\n",
    "        width = 480\n",
    "        num_frames = 8\n",
    "        print(f\"  Resolution reduite a {width}x{height}, {num_frames} frames\")\n",
    "else:\n",
    "    print(\"CUDA non disponible.\")\n",
    "    print(\"LTX-Video necessite un GPU. Le notebook montrera le code sans executer.\")\n",
    "    run_generation = False\n",
    "    run_img2vid = False\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Verification des dependances\n",
    "print(\"\\n--- VERIFICATION DEPENDANCES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "deps_ok = True\n",
    "\n",
    "try:\n",
    "    import diffusers\n",
    "    print(f\"diffusers : v{diffusers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"diffusers NON INSTALLE (pip install diffusers>=0.32)\")\n",
    "    deps_ok = False\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"transformers : v{transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"transformers NON INSTALLE\")\n",
    "    deps_ok = False\n",
    "\n",
    "try:\n",
    "    import imageio\n",
    "    print(f\"imageio : v{imageio.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"imageio NON INSTALLE\")\n",
    "    deps_ok = False\n",
    "\n",
    "if not deps_ok:\n",
    "    print(\"\\nDependances manquantes. Le notebook montrera le code sans executer.\")\n",
    "    run_generation = False\n",
    "    run_img2vid = False\n",
    "\n",
    "print(f\"\\nDevice : {device}\")\n",
    "print(f\"Generation activee : {run_generation}\")\n",
    "print(f\"Image-to-video activee : {run_img2vid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Section 1 : Architecture LTX-Video\n",
    "\n",
    "LTX-Video est un modele de generation video developpe par Lightricks, concu pour etre\n",
    "rapide et leger tout en maintenant une qualite acceptable. Il supporte trois modes :\n",
    "text-to-video, image-to-video et video-to-video.\n",
    "\n",
    "| Composant | Description |\n",
    "|-----------|-------------|\n",
    "| **Architecture** | DiT (Diffusion Transformer) avec compression spatio-temporelle |\n",
    "| **VAE** | Video VAE avec facteur de compression 1:192 |\n",
    "| **Text encoder** | T5-XXL pour la comprehension des prompts |\n",
    "| **Scheduler** | Flow matching pour un debruitage efficace |\n",
    "\n",
    "### Avantages de LTX-Video\n",
    "\n",
    "| Aspect | LTX-Video | HunyuanVideo (02-1) |\n",
    "|--------|-----------|---------------------|\n",
    "| VRAM requise | ~8 GB | ~18 GB |\n",
    "| Vitesse | 2-5x plus rapide | Reference |\n",
    "| Modes | Text/Image/Video | Text uniquement |\n",
    "| Qualite | Bonne | Tres bonne |\n",
    "| Resolution | Jusqu'a 720x480 | Jusqu'a 720p |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du pipeline LTX-Video\n",
    "pipe_t2v = None\n",
    "\n",
    "if run_generation:\n",
    "    print(\"\\n--- CHARGEMENT DU PIPELINE ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        from diffusers import LTXPipeline\n",
    "        from diffusers.utils import export_to_video\n",
    "        \n",
    "        start_load = time.time()\n",
    "        \n",
    "        print(f\"Chargement modele : {model_id}\")\n",
    "        \n",
    "        pipe_t2v = LTXPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        pipe_t2v = pipe_t2v.to(device)\n",
    "        \n",
    "        # Optimisations memoire\n",
    "        pipe_t2v.enable_vae_slicing()\n",
    "        try:\n",
    "            pipe_t2v.enable_model_cpu_offload()\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        load_time = time.time() - start_load\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            vram_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            print(f\"  VRAM utilisee : {vram_used:.1f} GB\")\n",
    "        \n",
    "        print(f\"Pipeline text-to-video charge en {load_time:.1f}s\")\n",
    "        print(f\"  VAE slicing : actif\")\n",
    "        print(f\"  Resolution : {width}x{height}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur chargement pipeline : {type(e).__name__}: {str(e)[:200]}\")\n",
    "        print(\"Le notebook continuera sans generation.\")\n",
    "        run_generation = False\n",
    "        pipe_t2v = None\n",
    "else:\n",
    "    print(\"Chargement pipeline desactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Section 2 : Generation text-to-video\n",
    "\n",
    "LTX-Video excelle dans la generation rapide de videos courtes a partir de prompts textuels.\n",
    "Son architecture legere permet des iterations plus rapides que HunyuanVideo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation text-to-video\n",
    "print(\"\\n--- GENERATION TEXT-TO-VIDEO ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def generate_ltx_video(prompt: str, negative_prompt: str = \"\",\n",
    "                       seed: int = 42, pipeline=None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Genere une video avec LTX-Video.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Description textuelle de la video\n",
    "        negative_prompt: Elements a eviter\n",
    "        seed: Graine aleatoire pour reproductibilite\n",
    "        pipeline: Pipeline a utiliser (defaut: pipe_t2v)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec frames, temps de generation et metadonnees\n",
    "    \"\"\"\n",
    "    active_pipe = pipeline or pipe_t2v\n",
    "    if active_pipe is None:\n",
    "        return {\"success\": False, \"error\": \"Pipeline non charge\"}\n",
    "    \n",
    "    try:\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        output = active_pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt or \"low quality, blurry, distorted, worst quality\",\n",
    "            num_frames=num_frames,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            generator=generator\n",
    "        )\n",
    "        \n",
    "        gen_time = time.time() - start_time\n",
    "        frames = output.frames[0]\n",
    "        \n",
    "        result = {\n",
    "            \"success\": True,\n",
    "            \"frames\": frames,\n",
    "            \"generation_time\": gen_time,\n",
    "            \"time_per_frame\": gen_time / num_frames,\n",
    "            \"prompt\": prompt,\n",
    "            \"seed\": seed,\n",
    "            \"params\": {\n",
    "                \"num_frames\": num_frames,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"num_inference_steps\": num_inference_steps,\n",
    "                \"height\": height,\n",
    "                \"width\": width\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            result[\"vram_peak\"] = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": f\"{type(e).__name__}: {str(e)[:200]}\"}\n",
    "\n",
    "\n",
    "# Premier test\n",
    "prompt_1 = \"a butterfly landing on a colorful flower in a garden, macro photography, soft bokeh background, natural lighting\"\n",
    "\n",
    "if run_generation:\n",
    "    print(f\"Prompt : {prompt_1}\")\n",
    "    print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n",
    "    print(f\"Resolution : {width}x{height}\")\n",
    "    print(f\"\\nGeneration en cours...\")\n",
    "    \n",
    "    result_1 = generate_ltx_video(prompt_1, seed=42)\n",
    "    \n",
    "    if result_1['success']:\n",
    "        frames = result_1['frames']\n",
    "        print(f\"\\nGeneration reussie\")\n",
    "        print(f\"  Temps total : {result_1['generation_time']:.1f}s\")\n",
    "        print(f\"  Temps/frame : {result_1['time_per_frame']:.1f}s\")\n",
    "        print(f\"  Frames : {len(frames)}\")\n",
    "        if 'vram_peak' in result_1:\n",
    "            print(f\"  VRAM pic : {result_1['vram_peak']:.1f} GB\")\n",
    "        \n",
    "        # Affichage en grille\n",
    "        n_display = min(8, len(frames))\n",
    "        indices = np.linspace(0, len(frames) - 1, n_display, dtype=int)\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        axes_flat = axes.flatten()\n",
    "        for i, idx in enumerate(indices):\n",
    "            if i < len(axes_flat):\n",
    "                axes_flat[i].imshow(frames[idx])\n",
    "                axes_flat[i].set_title(f\"Frame {idx + 1}/{len(frames)}\", fontsize=9)\n",
    "                axes_flat[i].axis('off')\n",
    "        for i in range(len(indices), len(axes_flat)):\n",
    "            axes_flat[i].axis('off')\n",
    "        plt.suptitle(f\"LTX-Video : {prompt_1[:60]}...\", fontsize=11, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        if save_as_mp4:\n",
    "            mp4_path = OUTPUT_DIR / \"ltx_demo.mp4\"\n",
    "            export_to_video(frames, str(mp4_path), fps=fps_output)\n",
    "            mp4_size_kb = mp4_path.stat().st_size / 1024\n",
    "            print(f\"  MP4 sauvegarde : {mp4_path.name} ({mp4_size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"Erreur : {result_1['error']}\")\n",
    "else:\n",
    "    print(\"Generation desactivee\")\n",
    "    print(f\"\\nExemple de code pour generer :\")\n",
    "    print(f\"  result = generate_ltx_video('{prompt_1[:50]}...', seed=42)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Interpretation : Text-to-video LTX\n",
    "\n",
    "| Aspect | Valeur typique | Comparaison HunyuanVideo |\n",
    "|--------|---------------|-------------------------|\n",
    "| Temps total | 10-30s (RTX 3090) | 2-5x plus rapide |\n",
    "| VRAM pic | 6-8 GB | 2-3x moins gourmand |\n",
    "| Qualite | Bonne | Legerement inferieure |\n",
    "| Coherence temporelle | Bonne | Comparable |\n",
    "\n",
    "**Points cles** :\n",
    "1. LTX-Video est ideal pour le prototypage rapide et les iterations\n",
    "2. La qualite est suffisante pour la plupart des cas d'usage non-production\n",
    "3. Le facteur de compression VAE (1:192) explique la vitesse elevee\n",
    "\n",
    "## Section 3 : Image-to-video (animation d'image)\n",
    "\n",
    "LTX-Video peut animer une image statique en video coherente. Cela permet de donner\n",
    "vie a des photos, illustrations ou images generees par des modeles text-to-image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image-to-video : animation d'image statique\n",
    "if run_img2vid:\n",
    "    print(\"\\n--- IMAGE-TO-VIDEO ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        from diffusers import LTXImageToVideoPipeline\n",
    "        from diffusers.utils import export_to_video\n",
    "        \n",
    "        # Creer une image de test (ou charger une existante)\n",
    "        print(\"Creation d'une image de test...\")\n",
    "        test_img = Image.new('RGB', (width, height), (100, 150, 200))\n",
    "        from PIL import ImageDraw\n",
    "        draw = ImageDraw.Draw(test_img)\n",
    "        \n",
    "        # Dessiner un paysage simple\n",
    "        # Ciel gradient\n",
    "        for y in range(height // 2):\n",
    "            r = int(100 + 50 * y / (height // 2))\n",
    "            g = int(150 + 50 * y / (height // 2))\n",
    "            b = int(220 - 20 * y / (height // 2))\n",
    "            draw.line([(0, y), (width, y)], fill=(r, g, b))\n",
    "        # Sol vert\n",
    "        draw.rectangle([0, height // 2, width, height], fill=(80, 140, 60))\n",
    "        # Soleil\n",
    "        draw.ellipse([width - 120, 30, width - 40, 110], fill=(255, 220, 100))\n",
    "        \n",
    "        test_img_path = OUTPUT_DIR / \"test_input_image.png\"\n",
    "        test_img.save(str(test_img_path))\n",
    "        print(f\"Image de test sauvegardee : {test_img_path.name}\")\n",
    "        \n",
    "        # Charger le pipeline image-to-video\n",
    "        print(f\"\\nChargement pipeline image-to-video...\")\n",
    "        pipe_i2v = LTXImageToVideoPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        pipe_i2v = pipe_i2v.to(device)\n",
    "        pipe_i2v.enable_vae_slicing()\n",
    "        \n",
    "        # Generer la video a partir de l'image\n",
    "        i2v_prompt = \"the landscape comes alive, clouds moving slowly, grass swaying in the wind, golden hour lighting\"\n",
    "        \n",
    "        print(f\"Prompt : {i2v_prompt}\")\n",
    "        print(f\"Generation image-to-video en cours...\")\n",
    "        \n",
    "        generator = torch.Generator(device=device).manual_seed(42)\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        output_i2v = pipe_i2v(\n",
    "            image=test_img,\n",
    "            prompt=i2v_prompt,\n",
    "            negative_prompt=\"low quality, blurry, distorted\",\n",
    "            num_frames=num_frames,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            generator=generator\n",
    "        )\n",
    "        i2v_time = time.time() - start_time\n",
    "        \n",
    "        i2v_frames = output_i2v.frames[0]\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            vram_i2v = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "            print(f\"  VRAM pic : {vram_i2v:.1f} GB\")\n",
    "        \n",
    "        print(f\"Generation reussie en {i2v_time:.1f}s\")\n",
    "        print(f\"  Frames : {len(i2v_frames)}\")\n",
    "        \n",
    "        # Affichage : image source + frames generees\n",
    "        fig, axes = plt.subplots(2, 5, figsize=(18, 7))\n",
    "        \n",
    "        # Premiere ligne : image source + premieres frames\n",
    "        axes[0][0].imshow(test_img)\n",
    "        axes[0][0].set_title(\"Image source\", fontsize=9, fontweight='bold')\n",
    "        axes[0][0].axis('off')\n",
    "        \n",
    "        for i in range(1, 5):\n",
    "            idx = i - 1\n",
    "            if idx < len(i2v_frames):\n",
    "                axes[0][i].imshow(i2v_frames[idx])\n",
    "                axes[0][i].set_title(f\"Frame {idx + 1}\", fontsize=9)\n",
    "            axes[0][i].axis('off')\n",
    "        \n",
    "        # Deuxieme ligne : frames suivantes\n",
    "        later_indices = np.linspace(4, len(i2v_frames) - 1, 5, dtype=int)\n",
    "        for i, idx in enumerate(later_indices):\n",
    "            axes[1][i].imshow(i2v_frames[idx])\n",
    "            axes[1][i].set_title(f\"Frame {idx + 1}\", fontsize=9)\n",
    "            axes[1][i].axis('off')\n",
    "        \n",
    "        plt.suptitle(\"Image-to-Video : Animation d'un paysage\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        if save_as_mp4:\n",
    "            mp4_path = OUTPUT_DIR / \"ltx_img2vid.mp4\"\n",
    "            export_to_video(i2v_frames, str(mp4_path), fps=fps_output)\n",
    "            print(f\"  MP4 sauvegarde : {mp4_path.name}\")\n",
    "        \n",
    "        # Liberation du pipeline i2v\n",
    "        del pipe_i2v\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"Pipeline image-to-video non disponible : {e}\")\n",
    "        print(\"Verifiez la version de diffusers (>=0.32 requise)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur image-to-video : {type(e).__name__}: {str(e)[:200]}\")\n",
    "else:\n",
    "    print(\"Image-to-video desactive\")\n",
    "    print(\"\\nPrincipe : LTXImageToVideoPipeline prend une image PIL + prompt\")\n",
    "    print(\"et genere une video qui anime l'image de maniere coherente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Interpretation : Image-to-video\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Fidelite a l'image source | Elevee | La premiere frame est proche de l'image d'entree |\n",
    "| Mouvement genere | Guide par le prompt | \"clouds moving\", \"grass swaying\" orientent le mouvement |\n",
    "| VRAM | Similaire au text-to-video | L'image d'entree ne change pas significativement la consommation |\n",
    "\n",
    "**Points cles** :\n",
    "1. L'image-to-video est utile pour animer des photos, illustrations ou images generees par DALL-E / SDXL\n",
    "2. Le prompt doit decrire le mouvement souhaite, pas juste la scene statique\n",
    "3. Les mouvements subtils (nuages, eau, vent) donnent les meilleurs resultats\n",
    "\n",
    "## Section 4 : Comparaison vitesse vs qualite\n",
    "\n",
    "Nous allons comparer differentes configurations de LTX-Video pour trouver le meilleur\n",
    "compromis entre vitesse de generation et qualite du resultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison vitesse vs qualite\n",
    "if run_generation and pipe_t2v is not None:\n",
    "    print(\"\\n--- COMPARAISON VITESSE VS QUALITE ---\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    test_prompt = \"a kitten playing with a ball of yarn, cute, soft lighting, cozy room\"\n",
    "    \n",
    "    configs = [\n",
    "        {\"steps\": 10, \"cfg\": 5.0, \"label\": \"Rapide (10 steps)\"},\n",
    "        {\"steps\": 20, \"cfg\": 7.5, \"label\": \"Equilibre (20 steps)\"},\n",
    "        {\"steps\": 40, \"cfg\": 7.5, \"label\": \"Qualite (40 steps)\"},\n",
    "    ]\n",
    "    \n",
    "    speed_results = []\n",
    "    \n",
    "    for cfg in configs:\n",
    "        print(f\"\\nTest : {cfg['label']}\")\n",
    "        \n",
    "        # Modifier temporairement\n",
    "        orig_steps = num_inference_steps\n",
    "        orig_cfg = guidance_scale\n",
    "        num_inference_steps = cfg['steps']\n",
    "        guidance_scale = cfg['cfg']\n",
    "        \n",
    "        result = generate_ltx_video(test_prompt, seed=42)\n",
    "        \n",
    "        # Restaurer\n",
    "        num_inference_steps = orig_steps\n",
    "        guidance_scale = orig_cfg\n",
    "        \n",
    "        if result['success']:\n",
    "            speed_results.append({\n",
    "                \"label\": cfg['label'],\n",
    "                \"steps\": cfg['steps'],\n",
    "                \"frames\": result['frames'],\n",
    "                \"time\": result['generation_time'],\n",
    "                \"vram_peak\": result.get('vram_peak', 0)\n",
    "            })\n",
    "            print(f\"  Temps : {result['generation_time']:.1f}s\")\n",
    "        else:\n",
    "            print(f\"  Erreur : {result['error']}\")\n",
    "    \n",
    "    # Affichage comparatif\n",
    "    if speed_results:\n",
    "        n_configs = len(speed_results)\n",
    "        n_preview = 4\n",
    "        fig, axes = plt.subplots(n_configs, n_preview, figsize=(3.5 * n_preview, 3 * n_configs))\n",
    "        if n_configs == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, sr in enumerate(speed_results):\n",
    "            frame_indices = np.linspace(0, len(sr['frames']) - 1, n_preview, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(sr['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(sr['label'], fontsize=10, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Vitesse vs Qualite - LTX-Video\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Tableau recapitulatif\n",
    "        print(f\"\\n{'Configuration':<25} {'Steps':<8} {'Temps (s)':<12} {'VRAM (GB)':<12}\")\n",
    "        print(\"-\" * 57)\n",
    "        for sr in speed_results:\n",
    "            print(f\"  {sr['label']:<25} {sr['steps']:<8} {sr['time']:<12.1f} {sr['vram_peak']:<12.1f}\")\n",
    "else:\n",
    "    print(\"Comparaison vitesse/qualite : generation desactivee\")\n",
    "    print(\"\\nGuide vitesse/qualite LTX-Video :\")\n",
    "    print(\"  10 steps : ~5s, preview rapide\")\n",
    "    print(\"  20 steps : ~12s, usage general\")\n",
    "    print(\"  40 steps : ~25s, meilleure qualite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Interpretation : Vitesse vs qualite\n",
    "\n",
    "| Configuration | Temps | Qualite | Cas d'usage |\n",
    "|--------------|-------|---------|-------------|\n",
    "| 10 steps | ~5s | Acceptable | Preview rapide, prototypage |\n",
    "| 20 steps | ~12s | Bonne | Usage general, iterations |\n",
    "| 40 steps | ~25s | Tres bonne | Resultat final, production |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le temps de generation est approximativement proportionnel au nombre de steps\n",
    "2. Au-dela de 40 steps, le gain de qualite est marginal\n",
    "3. Meme a 10 steps, LTX-Video produit des resultats utilisables\n",
    "\n",
    "## Bonnes pratiques et comparaison\n",
    "\n",
    "### Quand utiliser LTX-Video vs HunyuanVideo\n",
    "\n",
    "| Scenario | Modele recommande | Raison |\n",
    "|----------|------------------|--------|\n",
    "| Prototypage rapide | LTX-Video | 2-5x plus rapide, VRAM reduite |\n",
    "| Qualite maximale | HunyuanVideo | Meilleure coherence et details |\n",
    "| GPU 8-12 GB | LTX-Video | Seul modele qui tient en memoire |\n",
    "| Animation d'image | LTX-Video | Mode image-to-video natif |\n",
    "| Video longue (5s+) | HunyuanVideo | Meilleure coherence temporelle |\n",
    "| Iterations multiples | LTX-Video | Plus rapide pour tester des prompts |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Entrez votre propre prompt pour generer une video LTX-Video.\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "    \n",
    "    try:\n",
    "        user_prompt = input(\"\\nVotre prompt : \").strip()\n",
    "        \n",
    "        if user_prompt and run_generation and pipe_t2v is not None:\n",
    "            print(f\"\\nGeneration en cours...\")\n",
    "            result_user = generate_ltx_video(user_prompt, seed=123)\n",
    "            \n",
    "            if result_user['success']:\n",
    "                print(f\"Generation reussie en {result_user['generation_time']:.1f}s\")\n",
    "                \n",
    "                n_display = min(8, len(result_user['frames']))\n",
    "                fig, axes = plt.subplots(1, n_display, figsize=(2.5 * n_display, 3))\n",
    "                if n_display == 1:\n",
    "                    axes = [axes]\n",
    "                indices = np.linspace(0, len(result_user['frames']) - 1, n_display, dtype=int)\n",
    "                for ax, idx in zip(axes, indices):\n",
    "                    ax.imshow(result_user['frames'][idx])\n",
    "                    ax.set_title(f\"Frame {idx+1}\", fontsize=8)\n",
    "                    ax.axis('off')\n",
    "                plt.suptitle(f\"Votre video : {user_prompt[:50]}...\", fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                if save_as_mp4:\n",
    "                    user_mp4 = OUTPUT_DIR / \"user_generation.mp4\"\n",
    "                    export_to_video(result_user['frames'], str(user_mp4), fps=fps_output)\n",
    "                    print(f\"MP4 sauvegarde : {user_mp4.name}\")\n",
    "            else:\n",
    "                print(f\"Erreur : {result_user['error']}\")\n",
    "        elif user_prompt:\n",
    "            print(\"Generation non disponible (pipeline non charge)\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError) as e:\n",
    "        print(f\"\\nMode interactif interrompu ({type(e).__name__})\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"\\nMode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"\\nErreur inattendue : {error_type} - {str(e)[:100]}\")\n",
    "            print(\"Passage a la suite du notebook\")\n",
    "else:\n",
    "    print(\"\\nMode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Modele : {model_id}\")\n",
    "print(f\"Device : {device}\")\n",
    "print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n",
    "print(f\"Resolution : {width}x{height}\")\n",
    "\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "    print(f\"VRAM pic session : {vram_peak:.1f} GB\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "# Liberation VRAM\n",
    "if pipe_t2v is not None:\n",
    "    del pipe_t2v\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nVRAM liberee\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 02-3 : Wan 2.1/2.2 (prompts multilingues FR/EN, motion control)\")\n",
    "print(f\"2. Notebook 02-4 : SVD (animation d'images statiques haute qualite)\")\n",
    "print(f\"3. Module 03-1 : Comparaison benchmark de tous les modeles video\")\n",
    "print(f\"4. Combiner LTX image-to-video avec 01-4 (Real-ESRGAN) pour upscaling\")\n",
    "\n",
    "print(f\"\\nNotebook 02-2 LTX-Video Lightweight termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
