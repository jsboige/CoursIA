{
 "cells": [
  {
   "cell_id": "cell-0",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan 2.1/2.2 - Generation Video Multilingue\n",
    "\n",
    "**Module :** 02-Video-Advanced  \n",
    "**Niveau :** Intermediaire  \n",
    "**Technologies :** Wan 2.1/2.2 (Alibaba), ComfyUI API ou diffusers  \n",
    "**Duree estimee :** 45 minutes  \n",
    "**VRAM :** ~8-10 GB  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Comprendre l'architecture Wan et ses variantes (2.1 / 2.2)\n",
    "- [ ] Choisir entre API ComfyUI (production) et diffusers (pedagogique)\n",
    "- [ ] Generer des videos avec des prompts en francais et en anglais\n",
    "- [ ] Explorer le controle de mouvement et les mouvements de camera\n",
    "- [ ] Maitriser les options de resolution et de ratio d'aspect\n",
    "- [ ] Comparer les capacites de Wan 2.1 vs 2.2\n",
    "- [ ] Optimiser les prompts pour de meilleurs resultats\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "### Mode API ComfyUI (recommande pour production)\n",
    "- Service ComfyUI-Video demarre (docker-compose comfyui-video)\n",
    "- Pas de dependances Python lourdes cote client\n",
    "\n",
    "### Mode Local diffusers (pedagogique)\n",
    "- GPU avec 10+ GB VRAM\n",
    "- Packages : `diffusers>=0.32`, `transformers`, `torch`, `accelerate`, `imageio`\n",
    "\n",
    "**Navigation :** [<< 02-2](02-2-LTX-Video-Lightweight.ipynb) | [Index](../README.md) | [Suivant >>](02-4-SVD-Image-to-Video.ipynb)"
   ]
  },
  {
   "cell_id": "cell-1",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# MODE D'EXECUTION : API ou Local\n",
    "# - True  : Utilise l'API ComfyUI (recommande, pas de GPU local requis)\n",
    "# - False : Utilise diffusers en local (pedagogique, necessite GPU)\n",
    "use_api = True\n",
    "\n",
    "# Parametres API ComfyUI (si use_api=True)\n",
    "comfyui_url = \"http://localhost:8189\"  # ComfyUI-Video service\n",
    "comfyui_token = None                 # Token Bearer (optionnel pour localhost)\n",
    "\n",
    "# Parametres modele Wan (si use_api=False)\n",
    "model_id = \"Wan-AI/Wan2.1-T2V-14B\"  # Modele Wan 2.1 Text-to-Video (HuggingFace)\n",
    "quantize = True                      # Quantification INT8 (recommande)\n",
    "device = \"cuda\"                     # Device de calcul\n",
    "\n",
    "# Parametres generation (communs aux deux modes)\n",
    "num_frames = 16                    # Nombre de frames a generer\n",
    "guidance_scale = 6.0               # CFG scale pour Wan (6.0 recommande)\n",
    "num_inference_steps = 20           # Nombre d'etapes de debruitage\n",
    "height = 480                       # Hauteur video\n",
    "width = 832                        # Largeur video (ratio 16:9)\n",
    "fps_output = 16                    # FPS de la video de sortie\n",
    "\n",
    "# Configuration\n",
    "run_generation = True              # Executer la generation\n",
    "save_results = True"
   ]
  },
  {
   "cell_id": "cell-2",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers import comfyui_client\n",
    "        print(\"‚úÖ Helper comfyui_client import√©\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Helper comfyui_client NON disponible: {e}\")\n",
    "        comfyui_client = None\n",
    "\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'wan_video'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('wan_video')\n",
    "\n",
    "# Affichage du mode d'execution\n",
    "mode_str = \"API ComfyUI\" if use_api else \"Local diffusers\"\n",
    "print(f\"Wan 2.1/2.2 - Generation Video Multilingue\")\n",
    "print(f\"Mode d'execution : {mode_str}\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Frames : {num_frames}, Steps : {num_inference_steps}, CFG : {guidance_scale}\")\n",
    "print(f\"Resolution : {width}x{height}\")"
   ]
  },
  {
   "cell_id": "cell-3",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et verification de l'environnement\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"‚úÖ Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"‚ö†Ô∏è Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification et initialisation selon le mode\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"MODE : {'API ComfyUI' if use_api else 'Local diffusers'}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "client = None\n",
    "pipe = None\n",
    "comfyui_available = False\n",
    "local_available = False\n",
    "\n",
    "if use_api:\n",
    "    # === MODE API COMFYUI ===\n",
    "    print(\"\\nüì° Verification de l'API ComfyUI-Video...\")\n",
    "    \n",
    "    if comfyui_client is not None:\n",
    "        try:\n",
    "            client = comfyui_client.ComfyUIClient(\n",
    "                base_url=comfyui_url,\n",
    "                api_token=comfyui_token\n",
    "            )\n",
    "            \n",
    "            stats = client.get_system_stats()\n",
    "            \n",
    "            print(f\"‚úÖ ComfyUI-Video accessible sur : {comfyui_url}\")\n",
    "            comfyui_available = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è ComfyUI-Video non accessible: {type(e).__name__}: {str(e)[:100]}\")\n",
    "            print(\"\\nüí° Pour d√©marrer ComfyUI-Video :\")\n",
    "            print(\"   docker-compose -f docker-configurations/services/comfyui-video/docker-compose.yml up -d\")\n",
    "            run_generation = False\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Helper comfyui_client non disponible\")\n",
    "        run_generation = False\n",
    "        \n",
    "else:\n",
    "    # === MODE LOCAL DIFFUSERS ===\n",
    "    print(\"\\nüîß Verification de l'environnement local...\")\n",
    "    \n",
    "    # Verification GPU\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            vram_total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "            print(f\"‚úÖ GPU : {gpu_name}\")\n",
    "            print(f\"   VRAM totale : {vram_total:.1f} GB\")\n",
    "            \n",
    "            if vram_total < 10:\n",
    "                print(f\"‚ö†Ô∏è VRAM faible (< 10 GB), activation de la quantification\")\n",
    "                quantize = True\n",
    "                height = 320\n",
    "                width = 576\n",
    "                num_frames = 12\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è CUDA non disponible\")\n",
    "            run_generation = False\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è PyTorch non install√©\")\n",
    "        run_generation = False\n",
    "    \n",
    "    # Verification des dependances\n",
    "    deps_ok = True\n",
    "    \n",
    "    try:\n",
    "        import diffusers\n",
    "        print(f\"‚úÖ diffusers : v{diffusers.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è diffusers NON INSTALLE (pip install diffusers>=0.32)\")\n",
    "        deps_ok = False\n",
    "    \n",
    "    try:\n",
    "        import transformers\n",
    "        print(f\"‚úÖ transformers : v{transformers.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è transformers NON INSTALLE\")\n",
    "        deps_ok = False\n",
    "    \n",
    "    if quantize:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "            print(f\"‚úÖ bitsandbytes : v{bnb.__version__}\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è bitsandbytes NON INSTALLE (pip install bitsandbytes)\")\n",
    "            quantize = False\n",
    "    \n",
    "    if deps_ok and run_generation:\n",
    "        print(\"\\nüì¶ Chargement du pipeline Wan...\")\n",
    "        try:\n",
    "            from diffusers import WanPipeline\n",
    "            from diffusers.utils import export_to_video\n",
    "            \n",
    "            start_load = time.time()\n",
    "            \n",
    "            if quantize:\n",
    "                from diffusers import BitsAndBytesConfig\n",
    "                quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "                pipe = WanPipeline.from_pretrained(\n",
    "                    model_id,\n",
    "                    quantization_config=quant_config,\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "            else:\n",
    "                pipe = WanPipeline.from_pretrained(\n",
    "                    model_id,\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "            \n",
    "            pipe = pipe.to(device)\n",
    "            pipe.enable_vae_slicing()\n",
    "            pipe.enable_vae_tiling()\n",
    "            \n",
    "            load_time = time.time() - start_load\n",
    "            print(f\"‚úÖ Pipeline charge en {load_time:.1f}s\")\n",
    "            local_available = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur chargement pipeline : {type(e).__name__}: {str(e)[:200]}\")\n",
    "            run_generation = False\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Generation activee : {run_generation}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_id": "cell-4",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 : Architecture Wan 2.1/2.2\n",
    "\n",
    "Wan est une famille de modeles de generation video developpes par Alibaba. La serie 2.1/2.2\n",
    "se distingue par sa comprehension multilingue et son controle fin du mouvement.\n",
    "\n",
    "### Deux approches pour utiliser Wan\n",
    "\n",
    "| Aspect | API ComfyUI | Local diffusers |\n",
    "|--------|-------------|-----------------|\n",
    "| **Cas d'usage** | Production, applications | Pedagogie, recherche |\n",
    "| **GPU requis** | Non (cote serveur) | Oui (10+ GB) |\n",
    "| **Installation** | Aucune (Docker) | diffusers, transformers, torch |\n",
    "| **Flexibilite** | Moyenne | Elevee |\n",
    "| **Performance** | Serveur optimise | Depend du GPU local |\n",
    "\n",
    "### Architecture de Wan\n",
    "\n",
    "| Composant | Description |\n",
    "|-----------|-------------|\n",
    "| **UNET** | wan2.1_t2v_1.3B_fp16.safetensors (~2.5GB) |\n",
    "| **Text Encoder** | umt5_xxl_fp8_e4m3fn_scaled.safetensors (~4GB) |\n",
    "| **VAE** | wan_2.1_vae.safetensors (~360MB) |\n",
    "| **Sampling** | ModelSamplingSD3 (shift=8), scheduler uni_pc |\n",
    "\n",
    "### Comparaison Wan 2.1 vs 2.2\n",
    "\n",
    "| Aspect | Wan 2.1 | Wan 2.2 |\n",
    "|--------|---------|----------|\n",
    "| Qualite | Bonne | Amelioree |\n",
    "| Coherence temporelle | Bonne | Tres bonne |\n",
    "| Motion control | Basique | Camera paths avancees |\n",
    "| Resolutions | 480p-720p | 480p-1080p |\n",
    "| VRAM (1.3B) | ~8 GB | ~8 GB |"
   ]
  },
  {
   "cell_id": "cell-5",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de generation unifiee (API ou Local)\n",
    "def generate_wan_video(prompt: str, negative_prompt: str = \"\", seed: int = 42) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Genere une video avec Wan (API ComfyUI ou local diffusers).\n",
    "    \n",
    "    Cette fonction s'adapte automatiquement au mode d'execution choisi.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Description textuelle (FR, EN ou CN)\n",
    "        negative_prompt: Elements a eviter\n",
    "        seed: Graine aleatoire pour reproductibilite\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec frames, temps de generation et metadonnees\n",
    "    \"\"\"\n",
    "    if use_api:\n",
    "        # === MODE API COMFYUI ===\n",
    "        if not comfyui_available:\n",
    "            return {\"success\": False, \"error\": \"API ComfyUI non disponible\"}\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = client.generate_text2video_wan(\n",
    "                prompt=prompt,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                num_frames=num_frames,\n",
    "                steps=num_inference_steps,\n",
    "                seed=seed,\n",
    "                cfg=guidance_scale,\n",
    "                save_prefix=f\"wan_gen_{seed}\",\n",
    "                timeout=300\n",
    "            )\n",
    "            \n",
    "            gen_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"result\": result,\n",
    "                \"generation_time\": gen_time,\n",
    "                \"mode\": \"API ComfyUI\",\n",
    "                \"seed\": seed\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"success\": False, \"error\": f\"{type(e).__name__}: {str(e)[:200]}\"}\n",
    "    \n",
    "    else:\n",
    "        # === MODE LOCAL DIFFUSERS ===\n",
    "        if not local_available:\n",
    "            return {\"success\": False, \"error\": \"Pipeline local non disponible\"}\n",
    "        \n",
    "        try:\n",
    "            import torch\n",
    "            from diffusers.utils import export_to_video\n",
    "            \n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt or \"low quality, blurry, distorted\",\n",
    "                num_frames=num_frames,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                generator=generator\n",
    "            )\n",
    "            \n",
    "            gen_time = time.time() - start_time\n",
    "            frames = output.frames[0]\n",
    "            \n",
    "            # Sauvegarder en MP4\n",
    "            mp4_path = OUTPUT_DIR / f\"wan_local_{seed}.mp4\"\n",
    "            export_to_video(frames, str(mp4_path), fps=fps_output)\n",
    "            \n",
    "            result_dict = {\n",
    "                \"success\": True,\n",
    "                \"frames\": frames,\n",
    "                \"generation_time\": gen_time,\n",
    "                \"time_per_frame\": gen_time / num_frames,\n",
    "                \"prompt\": prompt,\n",
    "                \"seed\": seed,\n",
    "                \"mode\": \"Local diffusers\",\n",
    "                \"mp4_path\": str(mp4_path)\n",
    "            }\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                result_dict[\"vram_peak\"] = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "            \n",
    "            return result_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"success\": False, \"error\": f\"{type(e).__name__}: {str(e)[:200]}\"}\n",
    "\n",
    "print(\"‚úÖ Fonction de generation unifiee chargee\")"
   ]
  },
  {
   "cell_id": "cell-6",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 : Generation avec prompts multilingues\n",
    "\n",
    "L'un des avantages distinctifs de Wan est sa comprehension multilingue.\n",
    "Nous allons tester des prompts en francais et en anglais pour comparer les resultats."
   ]
  },
  {
   "cell_id": "cell-7",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation avec prompts multilingues\n",
    "print(\"\\n--- PROMPTS MULTILINGUES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "bilingual_prompts = [\n",
    "    {\n",
    "        \"text\": \"Un chat roux dort paisiblement sur un rebord de fenetre ensoleille, lumiere douce, atmosphere calme\",\n",
    "        \"lang\": \"FR\",\n",
    "        \"label\": \"Chat (FR)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"A ginger cat sleeping peacefully on a sunny windowsill, soft light, calm atmosphere\",\n",
    "        \"lang\": \"EN\",\n",
    "        \"label\": \"Cat (EN)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "bilingual_results = []\n",
    "\n",
    "if run_generation:\n",
    "    for p_idx, prompt_info in enumerate(bilingual_prompts):\n",
    "        print(f\"\\nGeneration {p_idx + 1}/{len(bilingual_prompts)} : {prompt_info['label']}\")\n",
    "        print(f\"  Prompt ({prompt_info['lang']}) : {prompt_info['text'][:60]}...\")\n",
    "        \n",
    "        result = generate_wan_video(prompt_info['text'], seed=42)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  ‚úÖ Generation terminee en {result['generation_time']:.1f}s ({result['mode']})\")\n",
    "            if 'vram_peak' in result:\n",
    "                print(f\"     VRAM pic : {result['vram_peak']:.1f} GB\")\n",
    "            bilingual_results.append({**result, **prompt_info})\n",
    "        else:\n",
    "            print(f\"  ‚ùå Erreur : {result['error']}\")\n",
    "    \n",
    "    # Resume des generations\n",
    "    if bilingual_results:\n",
    "        print(f\"\\n{'Langue':<15} {'Mode':<20} {'Temps (s)':<12}\")\n",
    "        print(\"-\" * 47)\n",
    "        for br in bilingual_results:\n",
    "            print(f\"  {br['label']:<15} {br['mode']:<20} {br['generation_time']:<12.1f}\")\n",
    "else:\n",
    "    print(\"Generation desactivee\")\n",
    "    print(\"\\nüí° Pour activer la generation, set run_generation=True\")"
   ]
  },
  {
   "cell_id": "cell-8",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Prompts multilingues\n",
    "\n",
    "Wan 2.1 utilise le text encoder UMT5-XXL (une variante de T5) qui a ete entraine sur de multiples langues.\n",
    "Cela permet de generer des videos a partir de prompts en francais, anglais, ou chinois avec des resultats comparables.\n",
    "\n",
    "| Langue | Prompt | R√©sultat attendu |\n",
    "|--------|--------|-----------------|\n",
    "| **Francais** | \"Un chat roux dort paisiblement sur un rebord de fenetre ensoleille, lumiere douce, atmosphere calme\" | Chat bien positionn√©, lumi√®re chaude, atmosph√®re reposante |\n",
    "| **Anglais** | \"A ginger cat sleeping peacefully on a sunny windowsill, soft light, calm atmosphere\" | Composition similaire, d√©tails l√©g√®rement plus pr√©cis |\n",
    "\n",
    "**Points cles** :\n",
    "- Les concepts principaux sont bien capt√©s dans les deux langues\n",
    "- L'anglais peut capturer plus de nuances subtiles\n",
    "- Le francais est parfaitement utilisable pour des prompts courants\n",
    "\n",
    "**Comparaison des modes** :\n",
    "- **API ComfyUI** : Plus rapide (serveur optimise), pas de gestion GPU\n",
    "- **Local diffusers** : Plus de controle (debug, personnalisation), necessite GPU"
   ]
  },
  {
   "cell_id": "cell-9",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 : Controle de mouvement et camera\n",
    "\n",
    "Wan permet de controler le mouvement de la camera a travers des mots-cles specifiques dans le prompt."
   ]
  },
  {
   "cell_id": "cell-10",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controle de mouvement et camera\n",
    "if run_generation:\n",
    "    print(\"\\n--- CONTROLE DE MOUVEMENT ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    motion_prompts = [\n",
    "        {\n",
    "            \"text\": \"a slow pan across a beautiful Japanese garden with cherry blossoms, smooth camera movement, serene\",\n",
    "            \"label\": \"Pan lateral\",\n",
    "            \"motion\": \"pan\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"camera slowly zooming into a detailed oil painting of a medieval castle, revealing intricate details\",\n",
    "            \"label\": \"Zoom avant\",\n",
    "            \"motion\": \"zoom\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"aerial drone shot flying over a coastal city at golden hour, birds eye view, cinematic\",\n",
    "            \"label\": \"Vol aerien\",\n",
    "            \"motion\": \"drone\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    motion_results = []\n",
    "    \n",
    "    for p_idx, prompt_info in enumerate(motion_prompts):\n",
    "        print(f\"\\nGeneration {p_idx + 1}/{len(motion_prompts)} : {prompt_info['label']}\")\n",
    "        print(f\"  Type de mouvement : {prompt_info['motion']}\")\n",
    "        \n",
    "        result = generate_wan_video(prompt_info['text'], seed=42 + p_idx)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  ‚úÖ Generation terminee en {result['generation_time']:.1f}s ({result['mode']})\")\n",
    "            motion_results.append({**result, **prompt_info})\n",
    "        else:\n",
    "            print(f\"  ‚ùå Erreur : {result['error']}\")\n",
    "    \n",
    "    # Resume des generations\n",
    "    if motion_results:\n",
    "        print(f\"\\n{'Mouvement':<15} {'Mode':<20} {'Temps (s)':<12}\")\n",
    "        print(\"-\" * 47)\n",
    "        for mr in motion_results:\n",
    "            print(f\"  {mr['label']:<15} {mr['mode']:<20} {mr['generation_time']:<12.1f}\")\n",
    "else:\n",
    "    print(\"Controle de mouvement : generation desactivee\")\n",
    "    print(\"\\nMots-cles de mouvement pour Wan :\")\n",
    "    print(\"  - 'slow pan' : panoramique lateral\")\n",
    "    print(\"  - 'zoom in/out' : rapprochement/eloignement\")\n",
    "    print(\"  - 'aerial drone shot' : vue aerienne\")\n",
    "    print(\"  - 'tracking shot' : suivi de sujet\")"
   ]
  },
  {
   "cell_id": "cell-11",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Controle de mouvement\n",
    "\n",
    "Les mots-cles de mouvement dans le prompt influencent la trajectoire de la camera virtuelle :\n",
    "\n",
    "| Mot-cle prompt | Mouvement obtenu | Efficacit√© |\n",
    "|---------------|-----------------|------------|\n",
    "| **\"slow pan\"** | Panoramique lateral fluide de gauche √† droite | Elevee |\n",
    "| **\"zoom into\"** | Rapprochement progressif sur le sujet | Bonne |\n",
    "| **\"aerial drone\"** | Vue plongeante qui survole la scene | Bonne |\n",
    "\n",
    "**Description visuelle** :\n",
    "- **Pan lateral**: La camera se deplace lentement de gauche √† droite, decouvrant progressivement le jardin japonais avec ses cerisiers en fleurs\n",
    "- **Zoom avant**: La camera s'approche du tableau, revelant les details de la peinture du chateau medieval\n",
    "- **Vol aerien**: Vue plongeante qui survole la ville cotiere, montrant l'etendue du paysage"
   ]
  },
  {
   "cell_id": "cell-12",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 : Ratios d'aspect\n",
    "\n",
    "Wan supporte differents ratios d'aspect pour s'adapter aux differentes plateformes."
   ]
  },
  {
   "cell_id": "cell-13",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de differents ratios d'aspect\n",
    "if run_generation:\n",
    "    print(\"\\n--- RATIOS D'ASPECT ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    aspect_prompt = \"a majestic lighthouse standing on a cliff, waves crashing, dramatic sky, golden hour\"\n",
    "    \n",
    "    aspect_configs = [\n",
    "        {\"w\": 480, \"h\": 480, \"label\": \"1:1 (480x480)\", \"name\": \"square\"},\n",
    "        {\"w\": 832, \"h\": 480, \"label\": \"16:9 (832x480)\", \"name\": \"landscape\"},\n",
    "        {\"w\": 480, \"h\": 832, \"label\": \"9:16 (480x832)\", \"name\": \"portrait\"},\n",
    "    ]\n",
    "    \n",
    "    aspect_results = []\n",
    "    \n",
    "    for cfg in aspect_configs:\n",
    "        print(f\"\\nTest : {cfg['label']}\")\n",
    "        \n",
    "        # Pour le mode local, on doit modifier width/height globaux\n",
    "        old_width, old_height = width, height\n",
    "        globals()['width'], globals()['height'] = cfg['w'], cfg['h']\n",
    "        \n",
    "        result = generate_wan_video(aspect_prompt, seed=42)\n",
    "        \n",
    "        # Restaurer les valeurs originales\n",
    "        globals()['width'], globals()['height'] = old_width, old_height\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  ‚úÖ Temps : {result['generation_time']:.1f}s ({result['mode']})\")\n",
    "            aspect_results.append({**result, **cfg})\n",
    "        else:\n",
    "            print(f\"  ‚ùå Erreur : {result['error']}\")\n",
    "    \n",
    "    # Resume des generations\n",
    "    if aspect_results:\n",
    "        print(f\"\\n{'Ratio':<20} {'Mode':<20} {'Temps (s)':<12}\")\n",
    "        print(\"-\" * 52)\n",
    "        for ar in aspect_results:\n",
    "            print(f\"  {ar['label']:<20} {ar['mode']:<20} {ar['generation_time']:<12.1f}\")\n",
    "else:\n",
    "    print(\"Test ratio d'aspect : generation desactivee\")\n",
    "    print(\"\\nRatios supportes par Wan :\")\n",
    "    print(\"  1:1  (480x480) : Reseaux sociaux carre\")\n",
    "    print(\"  16:9 (832x480) : YouTube, cinema\")\n",
    "    print(\"  9:16 (480x832) : TikTok, Reels, Stories\")"
   ]
  },
  {
   "cell_id": "cell-14",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation : Ratios d'aspect\n",
    "\n",
    "| Ratio | Usage | Particularite |\n",
    "|-------|-------|---------------|\n",
    "| 1:1 (carre) | Instagram, prototypage | Composition centree |\n",
    "| 16:9 (paysage) | YouTube, cinema | Composition large, horizon |\n",
    "| 9:16 (portrait) | TikTok, Stories | Composition verticale, sujet central |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le ratio d'aspect influence la composition automatique de la scene\n",
    "2. Les paysages fonctionnent mieux en 16:9, les portraits en 9:16\n",
    "3. Le temps de generation est proportionnel au nombre total de pixels (W x H)\n",
    "\n",
    "## Bonnes pratiques et prompt engineering\n",
    "\n",
    "### Structure d'un bon prompt Wan\n",
    "\n",
    "| Element | Exemple | Importance |\n",
    "|---------|---------|------------|\n",
    "| Sujet | \"a majestic eagle\" | Essentiel |\n",
    "| Action | \"soaring through clouds\" | Essentiel |\n",
    "| Camera | \"aerial tracking shot\" | Recommande |\n",
    "| Eclairage | \"golden hour, dramatic light\" | Recommande |\n",
    "| Style | \"cinematic, 4K, high quality\" | Optionnel |\n",
    "\n",
    "### Prompts en francais vs anglais\n",
    "\n",
    "| Approche | Avantage | Inconvenient |\n",
    "|----------|----------|-------------|\n",
    "| Tout FR | Naturel pour le redacteur | Moins de nuances captees |\n",
    "| Tout EN | Meilleure comprehension | Moins intuitif |\n",
    "| Mixte | Compromis optimal | Necessite expertise bilingue |"
   ]
  },
  {
   "cell_id": "cell-15",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode d'execution : {mode_str}\")\n",
    "print(f\"Modele : {model_id if not use_api else 'wan2.1_t2v_1.3B (ComfyUI)'}\")\n",
    "print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n",
    "print(f\"Resolution : {width}x{height}\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files)[:10]:\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "    if len(generated_files) > 10:\n",
    "        print(f\"  ... et {len(generated_files) - 10} autres fichiers\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 02-4 : SVD - Stable Video Diffusion (animation d'images statiques)\")\n",
    "print(f\"2. Module 03-1 : Comparaison benchmark de tous les modeles\")\n",
    "print(f\"3. Module 03-2 : Orchestration de pipelines text -> image -> video\")\n",
    "print(f\"4. Tester Wan 2.2 quand disponible pour les ameliorations de qualite\")\n",
    "\n",
    "print(f\"\\nNotebook 02-3 Wan Video Generation termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
