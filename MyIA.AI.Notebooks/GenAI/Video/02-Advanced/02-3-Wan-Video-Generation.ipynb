{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.005024,
     "end_time": "2026-02-19T09:29:41.800755",
     "exception": false,
     "start_time": "2026-02-19T09:29:41.795731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Wan 2.1/2.2 - Generation Video Multilingue\n",
    "\n",
    "**Module :** 02-Video-Advanced  \n",
    "**Niveau :** Intermediaire  \n",
    "**Technologies :** Wan 2.1/2.2 (Alibaba), diffusers, bitsandbytes  \n",
    "**Duree estimee :** 45 minutes  \n",
    "**VRAM :** ~10 GB  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Comprendre l'architecture Wan et ses variantes (2.1 / 2.2)\n",
    "- [ ] Charger le modele Wan 2.1 T2V avec quantification optionnelle\n",
    "- [ ] Generer des videos avec des prompts en francais et en anglais\n",
    "- [ ] Explorer le controle de mouvement et les mouvements de camera\n",
    "- [ ] Maitriser les options de resolution et de ratio d'aspect\n",
    "- [ ] Comparer les capacites de Wan 2.1 vs 2.2\n",
    "- [ ] Optimiser les prompts pour de meilleurs resultats\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- GPU avec 10+ GB VRAM (RTX 3060 12GB recommande)\n",
    "- Notebooks 02-1 et 02-2 completes pour comparaison\n",
    "- Packages : `diffusers>=0.32`, `transformers`, `torch`, `accelerate`, `imageio`, `imageio-ffmpeg`\n",
    "\n",
    "**Navigation :** [<< 02-2](02-2-LTX-Video-Lightweight.ipynb) | [Index](../README.md) | [Suivant >>](02-4-SVD-Image-to-Video.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:41.807526Z",
     "iopub.status.busy": "2026-02-19T09:29:41.807314Z",
     "iopub.status.idle": "2026-02-19T09:29:41.811558Z",
     "shell.execute_reply": "2026-02-19T09:29:41.810892Z"
    },
    "papermill": {
     "duration": 0.008316,
     "end_time": "2026-02-19T09:29:41.812507",
     "exception": false,
     "start_time": "2026-02-19T09:29:41.804191",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres modele\n",
    "model_id = \"Wan-AI/Wan2.1-T2V-14B\"  # Modele Wan 2.1 Text-to-Video\n",
    "quantize = True                    # Quantification INT8 (recommande)\n",
    "device = \"cuda\"                    # Device de calcul\n",
    "\n",
    "# Parametres generation\n",
    "num_frames = 16                    # Nombre de frames a generer\n",
    "guidance_scale = 5.0               # CFG scale\n",
    "num_inference_steps = 25           # Nombre d'etapes de debruitage\n",
    "height = 480                       # Hauteur video\n",
    "width = 832                        # Largeur video (ratio 16:9)\n",
    "fps_output = 16                    # FPS de la video de sortie\n",
    "\n",
    "# Configuration\n",
    "run_generation = True              # Executer la generation\n",
    "save_as_mp4 = True                 # Sauvegarder en MP4\n",
    "save_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1f5124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:41.817507Z",
     "iopub.status.busy": "2026-02-19T09:29:41.817325Z",
     "iopub.status.idle": "2026-02-19T09:29:41.820300Z",
     "shell.execute_reply": "2026-02-19T09:29:41.819765Z"
    },
    "papermill": {
     "duration": 0.00667,
     "end_time": "2026-02-19T09:29:41.821004",
     "exception": false,
     "start_time": "2026-02-19T09:29:41.814334",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "notebook_mode = \"batch\"\n",
    "skip_widgets = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:41.824990Z",
     "iopub.status.busy": "2026-02-19T09:29:41.824838Z",
     "iopub.status.idle": "2026-02-19T09:29:42.175976Z",
     "shell.execute_reply": "2026-02-19T09:29:42.175392Z"
    },
    "papermill": {
     "duration": 0.354256,
     "end_time": "2026-02-19T09:29:42.176964",
     "exception": false,
     "start_time": "2026-02-19T09:29:41.822708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wan 2.1/2.2 - Generation Video Multilingue\n",
      "Date : 2026-02-19 10:29:42\n",
      "Mode : batch\n",
      "Frames : 16, Steps : 25, CFG : 5.0\n",
      "Quantification : INT8\n"
     ]
    }
   ],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.genai_helpers import setup_genai_logging\n",
    "        print(\"Helpers GenAI importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers GenAI non disponibles - mode autonome\")\n",
    "\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'wan_video'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('wan_video')\n",
    "\n",
    "print(f\"Wan 2.1/2.2 - Generation Video Multilingue\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Frames : {num_frames}, Steps : {num_inference_steps}, CFG : {guidance_scale}\")\n",
    "print(f\"Quantification : {'INT8' if quantize else 'FP16'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:42.182131Z",
     "iopub.status.busy": "2026-02-19T09:29:42.181859Z",
     "iopub.status.idle": "2026-02-19T09:29:46.239067Z",
     "shell.execute_reply": "2026-02-19T09:29:46.238483Z"
    },
    "papermill": {
     "duration": 4.060778,
     "end_time": "2026-02-19T09:29:46.239984",
     "exception": false,
     "start_time": "2026-02-19T09:29:42.179206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucun fichier .env trouve\n",
      "\n",
      "--- VERIFICATION GPU ---\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA non disponible.\n",
      "Wan necessite un GPU. Le notebook montrera le code sans executer.\n",
      "\n",
      "--- VERIFICATION DEPENDANCES ---\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusers : v0.36.0\n",
      "transformers NON INSTALLE\n",
      "bitsandbytes NON INSTALLE (pip install bitsandbytes)\n",
      "  Quantification INT8 non disponible\n",
      "imageio : v2.37.2\n",
      "\n",
      "Dependances manquantes. Le notebook montrera le code sans executer.\n",
      "\n",
      "Device : cpu\n",
      "Generation activee : False\n",
      "Quantification : FP16\n"
     ]
    }
   ],
   "source": [
    "# Chargement .env et verification GPU\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification GPU\n",
    "print(\"\\n--- VERIFICATION GPU ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "    vram_free = (torch.cuda.get_device_properties(0).total_mem - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "    \n",
    "    print(f\"GPU : {gpu_name}\")\n",
    "    print(f\"VRAM totale : {vram_total:.1f} GB\")\n",
    "    print(f\"VRAM libre : {vram_free:.1f} GB\")\n",
    "    print(f\"CUDA : {torch.version.cuda}\")\n",
    "    \n",
    "    if vram_total < 10:\n",
    "        print(f\"\\nAttention : VRAM ({vram_total:.0f} GB) < 10 GB recommandes\")\n",
    "        quantize = True\n",
    "        height = 320\n",
    "        width = 576\n",
    "        num_frames = 12\n",
    "        print(f\"  Resolution reduite a {width}x{height}, {num_frames} frames\")\n",
    "else:\n",
    "    print(\"CUDA non disponible.\")\n",
    "    print(\"Wan necessite un GPU. Le notebook montrera le code sans executer.\")\n",
    "    run_generation = False\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Verification des dependances\n",
    "print(\"\\n--- VERIFICATION DEPENDANCES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "deps_ok = True\n",
    "\n",
    "try:\n",
    "    import diffusers\n",
    "    print(f\"diffusers : v{diffusers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"diffusers NON INSTALLE (pip install diffusers>=0.32)\")\n",
    "    deps_ok = False\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"transformers : v{transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"transformers NON INSTALLE\")\n",
    "    deps_ok = False\n",
    "\n",
    "if quantize:\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "        print(f\"bitsandbytes : v{bnb.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"bitsandbytes NON INSTALLE (pip install bitsandbytes)\")\n",
    "        print(\"  Quantification INT8 non disponible\")\n",
    "        quantize = False\n",
    "\n",
    "try:\n",
    "    import imageio\n",
    "    print(f\"imageio : v{imageio.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"imageio NON INSTALLE\")\n",
    "    deps_ok = False\n",
    "\n",
    "if not deps_ok:\n",
    "    print(\"\\nDependances manquantes. Le notebook montrera le code sans executer.\")\n",
    "    run_generation = False\n",
    "\n",
    "print(f\"\\nDevice : {device}\")\n",
    "print(f\"Generation activee : {run_generation}\")\n",
    "print(f\"Quantification : {'INT8' if quantize else 'FP16'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {
    "papermill": {
     "duration": 0.002037,
     "end_time": "2026-02-19T09:29:46.244204",
     "exception": false,
     "start_time": "2026-02-19T09:29:46.242167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 1 : Architecture Wan 2.1/2.2\n",
    "\n",
    "Wan est une famille de modeles de generation video developpes par Alibaba. La serie 2.1/2.2\n",
    "se distingue par sa comprehension multilingue et son controle fin du mouvement.\n",
    "\n",
    "| Composant | Description |\n",
    "|-----------|-------------|\n",
    "| **Architecture** | Transformer avec attention croisee spatio-temporelle |\n",
    "| **Text encoder** | Multilingue (chinois, anglais, francais) |\n",
    "| **Variantes** | T2V-1.3B (leger) / T2V-14B (haute qualite) |\n",
    "| **Scheduler** | UniPC (rapide) ou DPM-Solver |\n",
    "\n",
    "### Comparaison Wan 2.1 vs 2.2\n",
    "\n",
    "| Aspect | Wan 2.1 | Wan 2.2 |\n",
    "|--------|---------|----------|\n",
    "| Qualite | Bonne | Amelioree |\n",
    "| Coherence temporelle | Bonne | Tres bonne |\n",
    "| Motion control | Basique | Camera paths avancees |\n",
    "| Resolutions | 480p-720p | 480p-1080p |\n",
    "| VRAM (14B, INT8) | ~10 GB | ~12 GB |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:46.249346Z",
     "iopub.status.busy": "2026-02-19T09:29:46.248950Z",
     "iopub.status.idle": "2026-02-19T09:29:46.254103Z",
     "shell.execute_reply": "2026-02-19T09:29:46.253774Z"
    },
    "papermill": {
     "duration": 0.008855,
     "end_time": "2026-02-19T09:29:46.254869",
     "exception": false,
     "start_time": "2026-02-19T09:29:46.246014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement pipeline desactive\n"
     ]
    }
   ],
   "source": [
    "# Chargement du pipeline Wan\n",
    "pipe = None\n",
    "\n",
    "if run_generation:\n",
    "    print(\"\\n--- CHARGEMENT DU PIPELINE ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        from diffusers import WanPipeline\n",
    "        from diffusers.utils import export_to_video\n",
    "        \n",
    "        start_load = time.time()\n",
    "        \n",
    "        if quantize:\n",
    "            from diffusers import BitsAndBytesConfig\n",
    "            \n",
    "            print(f\"Chargement avec quantification INT8...\")\n",
    "            print(f\"  Modele : {model_id}\")\n",
    "            \n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True\n",
    "            )\n",
    "            \n",
    "            pipe = WanPipeline.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=quant_config,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Chargement en FP16...\")\n",
    "            print(f\"  Modele : {model_id}\")\n",
    "            \n",
    "            pipe = WanPipeline.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        \n",
    "        pipe = pipe.to(device)\n",
    "        \n",
    "        # Optimisations memoire\n",
    "        pipe.enable_vae_slicing()\n",
    "        pipe.enable_vae_tiling()\n",
    "        try:\n",
    "            pipe.enable_model_cpu_offload()\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        load_time = time.time() - start_load\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            vram_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            print(f\"  VRAM utilisee : {vram_used:.1f} GB\")\n",
    "        \n",
    "        print(f\"Pipeline charge en {load_time:.1f}s\")\n",
    "        print(f\"  Quantification : {'INT8' if quantize else 'FP16'}\")\n",
    "        print(f\"  Resolution : {width}x{height}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur chargement pipeline : {type(e).__name__}: {str(e)[:200]}\")\n",
    "        print(\"Le notebook continuera sans generation.\")\n",
    "        run_generation = False\n",
    "        pipe = None\n",
    "else:\n",
    "    print(\"Chargement pipeline desactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {
    "papermill": {
     "duration": 0.00182,
     "end_time": "2026-02-19T09:29:46.258625",
     "exception": false,
     "start_time": "2026-02-19T09:29:46.256805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 2 : Generation avec prompts multilingues\n",
    "\n",
    "L'un des avantages distinctifs de Wan est sa comprehension multilingue.\n",
    "Nous allons tester des prompts en francais et en anglais pour comparer les resultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:46.263340Z",
     "iopub.status.busy": "2026-02-19T09:29:46.263111Z",
     "iopub.status.idle": "2026-02-19T09:29:46.270283Z",
     "shell.execute_reply": "2026-02-19T09:29:46.269938Z"
    },
    "papermill": {
     "duration": 0.010643,
     "end_time": "2026-02-19T09:29:46.271024",
     "exception": false,
     "start_time": "2026-02-19T09:29:46.260381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PROMPTS MULTILINGUES ---\n",
      "========================================\n",
      "Generation desactivee\n",
      "\n",
      "Wan supporte les prompts en :\n",
      "  - Francais : 'Un coucher de soleil sur la mer, couleurs chaudes'\n",
      "  - Anglais : 'A sunset over the sea, warm colors'\n",
      "  - Chinois : nativement supporte\n"
     ]
    }
   ],
   "source": [
    "# Generation avec prompts multilingues\n",
    "print(\"\\n--- PROMPTS MULTILINGUES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def generate_wan_video(prompt: str, negative_prompt: str = \"\",\n",
    "                       seed: int = 42) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Genere une video avec Wan.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Description textuelle (FR, EN ou CN)\n",
    "        negative_prompt: Elements a eviter\n",
    "        seed: Graine aleatoire pour reproductibilite\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec frames, temps de generation et metadonnees\n",
    "    \"\"\"\n",
    "    if pipe is None:\n",
    "        return {\"success\": False, \"error\": \"Pipeline non charge\"}\n",
    "    \n",
    "    try:\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        output = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt or \"low quality, blurry, distorted, watermark\",\n",
    "            num_frames=num_frames,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            generator=generator\n",
    "        )\n",
    "        \n",
    "        gen_time = time.time() - start_time\n",
    "        frames = output.frames[0]\n",
    "        \n",
    "        result = {\n",
    "            \"success\": True,\n",
    "            \"frames\": frames,\n",
    "            \"generation_time\": gen_time,\n",
    "            \"time_per_frame\": gen_time / num_frames,\n",
    "            \"prompt\": prompt,\n",
    "            \"seed\": seed,\n",
    "            \"params\": {\n",
    "                \"num_frames\": num_frames,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"num_inference_steps\": num_inference_steps,\n",
    "                \"height\": height,\n",
    "                \"width\": width\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            result[\"vram_peak\"] = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": f\"{type(e).__name__}: {str(e)[:200]}\"}\n",
    "\n",
    "\n",
    "# Test bilingue FR/EN\n",
    "bilingual_prompts = [\n",
    "    {\n",
    "        \"text\": \"Un chat roux dort paisiblement sur un rebord de fenetre ensoleille, lumiere douce, atmosphere calme\",\n",
    "        \"lang\": \"FR\",\n",
    "        \"label\": \"Chat (FR)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"A ginger cat sleeping peacefully on a sunny windowsill, soft light, calm atmosphere\",\n",
    "        \"lang\": \"EN\",\n",
    "        \"label\": \"Cat (EN)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "bilingual_results = []\n",
    "\n",
    "if run_generation:\n",
    "    for p_idx, prompt_info in enumerate(bilingual_prompts):\n",
    "        print(f\"\\nGeneration {p_idx + 1}/{len(bilingual_prompts)} : {prompt_info['label']}\")\n",
    "        print(f\"  Prompt ({prompt_info['lang']}) : {prompt_info['text'][:70]}...\")\n",
    "        \n",
    "        result = generate_wan_video(prompt_info['text'], seed=42)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  Temps : {result['generation_time']:.1f}s\")\n",
    "            bilingual_results.append({\n",
    "                \"label\": prompt_info['label'],\n",
    "                \"lang\": prompt_info['lang'],\n",
    "                \"frames\": result['frames'],\n",
    "                \"time\": result['generation_time']\n",
    "            })\n",
    "            \n",
    "            if save_as_mp4:\n",
    "                mp4_path = OUTPUT_DIR / f\"wan_{prompt_info['lang'].lower()}_demo.mp4\"\n",
    "                export_to_video(result['frames'], str(mp4_path), fps=fps_output)\n",
    "        else:\n",
    "            print(f\"  Erreur : {result['error']}\")\n",
    "    \n",
    "    # Affichage comparatif FR vs EN\n",
    "    if len(bilingual_results) == 2:\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        \n",
    "        for v_idx, br in enumerate(bilingual_results):\n",
    "            frame_indices = np.linspace(0, len(br['frames']) - 1, 4, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(br['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(br['label'], fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Comparaison FR vs EN - Meme sujet\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n{'Langue':<15} {'Temps (s)':<12}\")\n",
    "        print(\"-\" * 27)\n",
    "        for br in bilingual_results:\n",
    "            print(f\"  {br['label']:<15} {br['time']:<12.1f}\")\n",
    "else:\n",
    "    print(\"Generation desactivee\")\n",
    "    print(\"\\nWan supporte les prompts en :\")\n",
    "    print(\"  - Francais : 'Un coucher de soleil sur la mer, couleurs chaudes'\")\n",
    "    print(\"  - Anglais : 'A sunset over the sea, warm colors'\")\n",
    "    print(\"  - Chinois : nativement supporte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {
    "papermill": {
     "duration": 0.001835,
     "end_time": "2026-02-19T09:29:46.274898",
     "exception": false,
     "start_time": "2026-02-19T09:29:46.273063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Prompts multilingues\n",
    "\n",
    "### MODE PEDAGOGIQUE (GPU non disponible)\n",
    "\n",
    "Sur un environnement GPU (RTX 3090, 24GB VRAM), ce code générerait:\n",
    "\n",
    "| Paramètre | Valeur |\n",
    "|-----------|--------|\n",
    "| **Device** | cuda (RTX 3090/4090) |\n",
    "| **VRAM utilisée** | ~10 GB (INT8) |\n",
    "| **Temps par génération** | 15-30 secondes |\n",
    "| **Frames générées** | 16 |\n",
    "| **Comparaison** | FR vs EN |\n",
    "\n",
    "**Résultat attendu:**\n",
    "Wan générerait 2 vidéos du meme chat roux dormant, l'une avec un prompt en français, l'autre en anglais:\n",
    "\n",
    "| Langue | Prompt | Résultat |\n",
    "|--------|--------|----------|\n",
    "| **Francais** | \"Un chat roux dort paisiblement sur un rebord de fenetre ensoleille, lumiere douce, atmosphere calme\" | Chat bien positionné, lumière chaude, atmosphère reposante |\n",
    "| **Anglais** | \"A ginger cat sleeping peacefully on a sunny windowsill, soft light, calm atmosphere\" | Composition similaire, détails légèrement plus précis |\n",
    "\n",
    "**Comparaison qualité:**\n",
    "\n",
    "| Aspect | Francais | Anglais | Avantage |\n",
    "|--------|---------|----------|----------|\n",
    "| **Compréhension** | Concepts principaux captés | Plus de nuances | EN |\n",
    "| **Composition** | Correcte | Idéntique | Égal |\n",
    "| **Détails** | Bon | Légèrement meilleur | EN léger |\n",
    "| **Temps** | Similaire | Similaire | Égal |\n",
    "\n",
    "**Code pour reproduire:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from diffusers import WanPipeline\n",
    "\n",
    "# Pipeline\n",
    "pipe = WanPipeline.from_pretrained(\n",
    "    \"Wan-AI/Wan2.1-T2V-14B\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Test bilingue\n",
    "bilingual_prompts = [\n",
    "    \"Un chat roux dort paisiblement sur un rebord de fenêtre ensoleillé, lumière douce, atmosphère calme\",\n",
    "    \"A ginger cat sleeping peacefully on a sunny windowsill, soft light, calm atmosphere\"\n",
    "]\n",
    "\n",
    "for prompt in bilingual_prompts:\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"low quality, blurry\",\n",
    "        num_frames=16,\n",
    "        guidance_scale=5.0,\n",
    "        num_inference_steps=25,\n",
    "        height=480,\n",
    "        width=832,\n",
    "        generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    "    )\n",
    "    frames = output.frames[0]\n",
    "    # Sauvegarder...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:46.279912Z",
     "iopub.status.busy": "2026-02-19T09:29:46.279683Z",
     "iopub.status.idle": "2026-02-19T09:29:46.286518Z",
     "shell.execute_reply": "2026-02-19T09:29:46.286108Z"
    },
    "papermill": {
     "duration": 0.010736,
     "end_time": "2026-02-19T09:29:46.287433",
     "exception": false,
     "start_time": "2026-02-19T09:29:46.276697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controle de mouvement : generation desactivee\n",
      "\n",
      "Mots-cles de mouvement pour Wan :\n",
      "  - 'slow pan' : panoramique lateral\n",
      "  - 'zoom in/out' : rapprochement/eloignement\n",
      "  - 'aerial drone shot' : vue aerienne\n",
      "  - 'tracking shot' : suivi de sujet\n",
      "  - 'dolly shot' : deplacement lineaire de camera\n"
     ]
    }
   ],
   "source": [
    "# Controle de mouvement et camera\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- CONTROLE DE MOUVEMENT ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    motion_prompts = [\n",
    "        {\n",
    "            \"text\": \"a slow pan across a beautiful Japanese garden with cherry blossoms, smooth camera movement, serene\",\n",
    "            \"label\": \"Pan lateral\",\n",
    "            \"motion\": \"Panoramique\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"camera slowly zooming into a detailed oil painting of a medieval castle, revealing intricate details\",\n",
    "            \"label\": \"Zoom avant\",\n",
    "            \"motion\": \"Zoom\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"aerial drone shot flying over a coastal city at golden hour, birds eye view, cinematic\",\n",
    "            \"label\": \"Vol aerien\",\n",
    "            \"motion\": \"Drone\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    motion_results = []\n",
    "    \n",
    "    for p_idx, prompt_info in enumerate(motion_prompts):\n",
    "        print(f\"\\nGeneration {p_idx + 1}/{len(motion_prompts)} : {prompt_info['label']}\")\n",
    "        print(f\"  Type de mouvement : {prompt_info['motion']}\")\n",
    "        \n",
    "        result = generate_wan_video(prompt_info['text'], seed=42 + p_idx)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  Temps : {result['generation_time']:.1f}s\")\n",
    "            motion_results.append({\n",
    "                \"label\": prompt_info['label'],\n",
    "                \"motion\": prompt_info['motion'],\n",
    "                \"frames\": result['frames'],\n",
    "                \"time\": result['generation_time']\n",
    "            })\n",
    "            \n",
    "            if save_as_mp4:\n",
    "                mp4_path = OUTPUT_DIR / f\"wan_motion_{prompt_info['motion'].lower()}.mp4\"\n",
    "                export_to_video(result['frames'], str(mp4_path), fps=fps_output)\n",
    "        else:\n",
    "            print(f\"  Erreur : {result['error']}\")\n",
    "    \n",
    "    # Affichage comparatif des mouvements\n",
    "    if motion_results:\n",
    "        n_motions = len(motion_results)\n",
    "        n_preview = 4\n",
    "        fig, axes = plt.subplots(n_motions, n_preview, figsize=(3.5 * n_preview, 3 * n_motions))\n",
    "        if n_motions == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, mr in enumerate(motion_results):\n",
    "            frame_indices = np.linspace(0, len(mr['frames']) - 1, n_preview, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(mr['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(mr['label'], fontsize=10, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Controle de mouvement - Wan\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n{'Mouvement':<15} {'Type':<15} {'Temps (s)':<12}\")\n",
    "        print(\"-\" * 42)\n",
    "        for mr in motion_results:\n",
    "            print(f\"  {mr['label']:<15} {mr['motion']:<15} {mr['time']:<12.1f}\")\n",
    "else:\n",
    "    print(\"Controle de mouvement : generation desactivee\")\n",
    "    print(\"\\nMots-cles de mouvement pour Wan :\")\n",
    "    print(\"  - 'slow pan' : panoramique lateral\")\n",
    "    print(\"  - 'zoom in/out' : rapprochement/eloignement\")\n",
    "    print(\"  - 'aerial drone shot' : vue aerienne\")\n",
    "    print(\"  - 'tracking shot' : suivi de sujet\")\n",
    "    print(\"  - 'dolly shot' : deplacement lineaire de camera\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {
    "papermill": {
     "duration": 0.00195,
     "end_time": "2026-02-19T09:29:46.291511",
     "exception": false,
     "start_time": "2026-02-19T09:29:46.289561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Controle de mouvement\n",
    "\n",
    "### MODE PEDAGOGIQUE (GPU non disponible)\n",
    "\n",
    "Sur un environnement GPU (RTX 3090, 24GB VRAM), ce code générerait:\n",
    "\n",
    "| Paramètre | Valeur |\n",
    "|-----------|--------|\n",
    "| **Device** | cuda (RTX 3090/4090) |\n",
    "| **VRAM utilisée** | ~10 GB (INT8) |\n",
    "| **Temps par génération** | 15-25 secondes |\n",
    "| **Frames générées** | 12 |\n",
    "| **Mouvements testés** | Pan, Zoom, Drone |\n",
    "\n",
    "**Résultat attendu:**\n",
    "Wan générerait 3 vidéos avec differents mouvements de camera:\n",
    "\n",
    "| Mot-cle prompt | Mouvement obtenu | Efficacité | Diff. inter-frames |\n",
    "|---------------|-----------------|------------|-------------------|\n",
    "| **\"slow pan\"** | Panoramique lateral fluide de gauche à droite | Elevee | ~10-15 |\n",
    "| **\"zoom into\"** | Rapprochement progressif sur le chateau | Bonne | ~8-12 |\n",
    "| **\"aerial drone\"** | Survol plongeant de la scène | Bonne | ~12-18 |\n",
    "\n",
    "**Description visuelle:**\n",
    "\n",
    "- **Pan lateral**: La camera se deplace lentement de gauche à droite, decouvrant progressivement le jardin japonais avec ses cerisiers en fleurs\n",
    "- **Zoom avant**: La camera s'approche du tableau, revelant les details de la peinture du chateau medieval\n",
    "- **Vol aerien**: Vue plongeante qui survole la ville cotiere, montrant l'etendue du paysage\n",
    "\n",
    "**Code pour reproduire:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from diffusers import WanPipeline\n",
    "\n",
    "pipe = WanPipeline.from_pretrained(\n",
    "    \"Wan-AI/Wan2.1-T2V-14B\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "motion_prompts = [\n",
    "    \"a slow pan across a beautiful Japanese garden with cherry blossoms, smooth camera movement\",\n",
    "    \"camera slowly zooming into a detailed oil painting of a medieval castle\",\n",
    "    \"aerial drone shot flying over a coastal city at golden hour, cinematic\"\n",
    "]\n",
    "\n",
    "for prompt in motion_prompts:\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"low quality\",\n",
    "        num_frames=12,\n",
    "        guidance_scale=5.0,\n",
    "        num_inference_steps=20,\n",
    "        height=480,\n",
    "        width=832,\n",
    "        generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    "    )\n",
    "    frames = output.frames[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:46.296649Z",
     "iopub.status.busy": "2026-02-19T09:29:46.296440Z",
     "iopub.status.idle": "2026-02-19T09:29:46.302706Z",
     "shell.execute_reply": "2026-02-19T09:29:46.302023Z"
    },
    "papermill": {
     "duration": 0.01008,
     "end_time": "2026-02-19T09:29:46.303543",
     "exception": false,
     "start_time": "2026-02-19T09:29:46.293463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ratio d'aspect : generation desactivee\n",
      "\n",
      "Ratios supportes par Wan :\n",
      "  1:1  (480x480) : Reseaux sociaux carre\n",
      "  16:9 (832x480) : YouTube, cinema\n",
      "  9:16 (480x832) : TikTok, Reels, Stories\n"
     ]
    }
   ],
   "source": [
    "# Test de differents ratios d'aspect\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- RATIOS D'ASPECT ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    aspect_prompt = \"a majestic lighthouse standing on a cliff, waves crashing, dramatic sky, golden hour\"\n",
    "    \n",
    "    aspect_configs = [\n",
    "        {\"w\": 480, \"h\": 480, \"label\": \"1:1 (480x480)\"},\n",
    "        {\"w\": 832, \"h\": 480, \"label\": \"16:9 (832x480)\"},\n",
    "        {\"w\": 480, \"h\": 832, \"label\": \"9:16 (480x832)\"},\n",
    "    ]\n",
    "    \n",
    "    aspect_results = []\n",
    "    \n",
    "    for cfg in aspect_configs:\n",
    "        print(f\"\\nTest : {cfg['label']}\")\n",
    "        \n",
    "        try:\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            generator = torch.Generator(device=device).manual_seed(42)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = pipe(\n",
    "                prompt=aspect_prompt,\n",
    "                negative_prompt=\"low quality, blurry, distorted\",\n",
    "                num_frames=12,\n",
    "                guidance_scale=5.0,\n",
    "                num_inference_steps=20,\n",
    "                height=cfg['h'],\n",
    "                width=cfg['w'],\n",
    "                generator=generator\n",
    "            )\n",
    "            \n",
    "            gen_time = time.time() - start_time\n",
    "            frames = output.frames[0]\n",
    "            \n",
    "            vram_peak = 0\n",
    "            if device == \"cuda\":\n",
    "                vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "            \n",
    "            aspect_results.append({\n",
    "                \"label\": cfg['label'],\n",
    "                \"frames\": frames,\n",
    "                \"time\": gen_time,\n",
    "                \"vram_peak\": vram_peak\n",
    "            })\n",
    "            \n",
    "            print(f\"  Temps : {gen_time:.1f}s, VRAM pic : {vram_peak:.1f} GB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur : {type(e).__name__}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Affichage : premiere et derniere frame de chaque ratio\n",
    "    if aspect_results:\n",
    "        fig, axes = plt.subplots(len(aspect_results), 2, figsize=(10, 4 * len(aspect_results)))\n",
    "        if len(aspect_results) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, ar in enumerate(aspect_results):\n",
    "            axes[v_idx][0].imshow(ar['frames'][0])\n",
    "            axes[v_idx][0].set_title(f\"{ar['label']} - Frame 1\", fontsize=10)\n",
    "            axes[v_idx][0].axis('off')\n",
    "            \n",
    "            axes[v_idx][1].imshow(ar['frames'][-1])\n",
    "            axes[v_idx][1].set_title(f\"{ar['label']} - Derniere frame\", fontsize=10)\n",
    "            axes[v_idx][1].axis('off')\n",
    "        \n",
    "        plt.suptitle(\"Ratios d'aspect - Wan\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n{'Ratio':<20} {'Temps (s)':<12} {'VRAM (GB)':<12}\")\n",
    "        print(\"-\" * 44)\n",
    "        for ar in aspect_results:\n",
    "            print(f\"  {ar['label']:<20} {ar['time']:<12.1f} {ar['vram_peak']:<12.1f}\")\n",
    "else:\n",
    "    print(\"Test ratio d'aspect : generation desactivee\")\n",
    "    print(\"\\nRatios supportes par Wan :\")\n",
    "    print(\"  1:1  (480x480) : Reseaux sociaux carre\")\n",
    "    print(\"  16:9 (832x480) : YouTube, cinema\")\n",
    "    print(\"  9:16 (480x832) : TikTok, Reels, Stories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {
    "papermill": {
     "duration": 0.002179,
     "end_time": "2026-02-19T09:29:46.307891",
     "exception": false,
     "start_time": "2026-02-19T09:29:46.305712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Ratios d'aspect\n",
    "\n",
    "| Ratio | Usage | Particularite |\n",
    "|-------|-------|---------------|\n",
    "| 1:1 (carre) | Instagram, prototypage | Composition centree |\n",
    "| 16:9 (paysage) | YouTube, cinema | Composition large, horizon |\n",
    "| 9:16 (portrait) | TikTok, Stories | Composition verticale, sujet central |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le ratio d'aspect influence la composition automatique de la scene\n",
    "2. Les paysages fonctionnent mieux en 16:9, les portraits en 9:16\n",
    "3. La VRAM est proportionnelle au nombre total de pixels (W x H)\n",
    "\n",
    "## Bonnes pratiques et prompt engineering\n",
    "\n",
    "### Structure d'un bon prompt Wan\n",
    "\n",
    "| Element | Exemple | Importance |\n",
    "|---------|---------|------------|\n",
    "| Sujet | \"a majestic eagle\" | Essentiel |\n",
    "| Action | \"soaring through clouds\" | Essentiel |\n",
    "| Camera | \"aerial tracking shot\" | Recommande |\n",
    "| Eclairage | \"golden hour, dramatic light\" | Recommande |\n",
    "| Style | \"cinematic, 4K, high quality\" | Optionnel |\n",
    "\n",
    "### Prompts en francais vs anglais\n",
    "\n",
    "| Approche | Avantage | Inconvenient |\n",
    "|----------|----------|-------------|\n",
    "| Tout FR | Naturel pour le redacteur | Moins de nuances captees |\n",
    "| Tout EN | Meilleure comprehension | Moins intuitif |\n",
    "| Mixte | Compromis optimal | Necessite expertise bilingue |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:46.313395Z",
     "iopub.status.busy": "2026-02-19T09:29:46.313134Z",
     "iopub.status.idle": "2026-02-19T09:29:46.319275Z",
     "shell.execute_reply": "2026-02-19T09:29:46.318866Z"
    },
    "papermill": {
     "duration": 0.010148,
     "end_time": "2026-02-19T09:29:46.320100",
     "exception": false,
     "start_time": "2026-02-19T09:29:46.309952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mode batch - Interface interactive desactivee\n"
     ]
    }
   ],
   "source": [
    "# Mode interactif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Entrez votre propre prompt (francais ou anglais) pour generer une video Wan.\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "    \n",
    "    try:\n",
    "        user_prompt = input(\"\\nVotre prompt (FR ou EN) : \").strip()\n",
    "        \n",
    "        if user_prompt and run_generation and pipe is not None:\n",
    "            print(f\"\\nGeneration en cours...\")\n",
    "            result_user = generate_wan_video(user_prompt, seed=123)\n",
    "            \n",
    "            if result_user['success']:\n",
    "                print(f\"Generation reussie en {result_user['generation_time']:.1f}s\")\n",
    "                \n",
    "                n_display = min(8, len(result_user['frames']))\n",
    "                fig, axes = plt.subplots(1, n_display, figsize=(2.5 * n_display, 3))\n",
    "                if n_display == 1:\n",
    "                    axes = [axes]\n",
    "                indices = np.linspace(0, len(result_user['frames']) - 1, n_display, dtype=int)\n",
    "                for ax, idx in zip(axes, indices):\n",
    "                    ax.imshow(result_user['frames'][idx])\n",
    "                    ax.set_title(f\"Frame {idx+1}\", fontsize=8)\n",
    "                    ax.axis('off')\n",
    "                plt.suptitle(f\"Votre video : {user_prompt[:50]}...\", fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                if save_as_mp4:\n",
    "                    user_mp4 = OUTPUT_DIR / \"user_generation.mp4\"\n",
    "                    export_to_video(result_user['frames'], str(user_mp4), fps=fps_output)\n",
    "                    print(f\"MP4 sauvegarde : {user_mp4.name}\")\n",
    "            else:\n",
    "                print(f\"Erreur : {result_user['error']}\")\n",
    "        elif user_prompt:\n",
    "            print(\"Generation non disponible (pipeline non charge)\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError) as e:\n",
    "        print(f\"\\nMode interactif interrompu ({type(e).__name__})\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"\\nMode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"\\nErreur inattendue : {error_type} - {str(e)[:100]}\")\n",
    "            print(\"Passage a la suite du notebook\")\n",
    "else:\n",
    "    print(\"\\nMode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:46.326026Z",
     "iopub.status.busy": "2026-02-19T09:29:46.325370Z",
     "iopub.status.idle": "2026-02-19T09:29:46.331711Z",
     "shell.execute_reply": "2026-02-19T09:29:46.331389Z"
    },
    "papermill": {
     "duration": 0.010194,
     "end_time": "2026-02-19T09:29:46.332543",
     "exception": false,
     "start_time": "2026-02-19T09:29:46.322349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STATISTIQUES DE SESSION ---\n",
      "========================================\n",
      "Date : 2026-02-19 10:29:46\n",
      "Mode : batch\n",
      "Modele : Wan-AI/Wan2.1-T2V-14B\n",
      "Quantification : FP16\n",
      "Device : cpu\n",
      "Parametres : 16 frames, 25 steps, CFG=5.0\n",
      "Resolution : 832x480\n",
      "\n",
      "Fichiers generes (0) :\n",
      "\n",
      "--- PROCHAINES ETAPES ---\n",
      "1. Notebook 02-4 : SVD - Stable Video Diffusion (animation d'images statiques)\n",
      "2. Module 03-1 : Comparaison benchmark de tous les modeles (AnimateDiff, HunyuanVideo, LTX, Wan, SVD)\n",
      "3. Module 03-2 : Orchestration de pipelines text -> image -> video\n",
      "4. Tester Wan 2.2 quand disponible pour les ameliorations de qualite\n",
      "\n",
      "Notebook 02-3 Wan Video Generation termine - 10:29:46\n"
     ]
    }
   ],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Modele : {model_id}\")\n",
    "print(f\"Quantification : {'INT8' if quantize else 'FP16'}\")\n",
    "print(f\"Device : {device}\")\n",
    "print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n",
    "print(f\"Resolution : {width}x{height}\")\n",
    "\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "    print(f\"VRAM pic session : {vram_peak:.1f} GB\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "# Liberation VRAM\n",
    "if pipe is not None:\n",
    "    del pipe\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nVRAM liberee\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 02-4 : SVD - Stable Video Diffusion (animation d'images statiques)\")\n",
    "print(f\"2. Module 03-1 : Comparaison benchmark de tous les modeles (AnimateDiff, HunyuanVideo, LTX, Wan, SVD)\")\n",
    "print(f\"3. Module 03-2 : Orchestration de pipelines text -> image -> video\")\n",
    "print(f\"4. Tester Wan 2.2 quand disponible pour les ameliorations de qualite\")\n",
    "\n",
    "print(f\"\\nNotebook 02-3 Wan Video Generation termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.785936,
   "end_time": "2026-02-19T09:29:47.233043",
   "environment_variables": {},
   "exception": null,
   "input_path": "MyIA.AI.Notebooks\\GenAI\\Video\\02-Advanced\\02-3-Wan-Video-Generation.ipynb",
   "output_path": "MyIA.AI.Notebooks\\GenAI\\Video\\02-Advanced\\02-3-Wan-Video-Generation.ipynb",
   "parameters": {
    "notebook_mode": "batch",
    "skip_widgets": true
   },
   "start_time": "2026-02-19T09:29:40.447107",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}