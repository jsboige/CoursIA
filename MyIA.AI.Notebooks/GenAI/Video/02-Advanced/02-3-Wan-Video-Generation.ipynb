{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Wan 2.1/2.2 - Generation Video Multilingue\n",
    "\n",
    "**Module :** 02-Video-Advanced  \n",
    "**Niveau :** Intermediaire  \n",
    "**Technologies :** Wan 2.1/2.2 (Alibaba), diffusers, bitsandbytes  \n",
    "**Duree estimee :** 45 minutes  \n",
    "**VRAM :** ~10 GB  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Comprendre l'architecture Wan et ses variantes (2.1 / 2.2)\n",
    "- [ ] Charger le modele Wan 2.1 T2V avec quantification optionnelle\n",
    "- [ ] Generer des videos avec des prompts en francais et en anglais\n",
    "- [ ] Explorer le controle de mouvement et les mouvements de camera\n",
    "- [ ] Maitriser les options de resolution et de ratio d'aspect\n",
    "- [ ] Comparer les capacites de Wan 2.1 vs 2.2\n",
    "- [ ] Optimiser les prompts pour de meilleurs resultats\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- GPU avec 10+ GB VRAM (RTX 3060 12GB recommande)\n",
    "- Notebooks 02-1 et 02-2 completes pour comparaison\n",
    "- Packages : `diffusers>=0.32`, `transformers`, `torch`, `accelerate`, `imageio`, `imageio-ffmpeg`\n",
    "\n",
    "**Navigation :** [<< 02-2](02-2-LTX-Video-Lightweight.ipynb) | [Index](../README.md) | [Suivant >>](02-4-SVD-Image-to-Video.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres modele\n",
    "model_id = \"Wan-AI/Wan2.1-T2V-14B\"  # Modele Wan 2.1 Text-to-Video\n",
    "quantize = True                    # Quantification INT8 (recommande)\n",
    "device = \"cuda\"                    # Device de calcul\n",
    "\n",
    "# Parametres generation\n",
    "num_frames = 16                    # Nombre de frames a generer\n",
    "guidance_scale = 5.0               # CFG scale\n",
    "num_inference_steps = 25           # Nombre d'etapes de debruitage\n",
    "height = 480                       # Hauteur video\n",
    "width = 832                        # Largeur video (ratio 16:9)\n",
    "fps_output = 16                    # FPS de la video de sortie\n",
    "\n",
    "# Configuration\n",
    "run_generation = True              # Executer la generation\n",
    "save_as_mp4 = True                 # Sauvegarder en MP4\n",
    "save_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.genai_helpers import setup_genai_logging\n",
    "        print(\"Helpers GenAI importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers GenAI non disponibles - mode autonome\")\n",
    "\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'wan_video'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('wan_video')\n",
    "\n",
    "print(f\"Wan 2.1/2.2 - Generation Video Multilingue\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Frames : {num_frames}, Steps : {num_inference_steps}, CFG : {guidance_scale}\")\n",
    "print(f\"Quantification : {'INT8' if quantize else 'FP16'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et verification GPU\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification GPU\n",
    "print(\"\\n--- VERIFICATION GPU ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "    vram_free = (torch.cuda.get_device_properties(0).total_mem - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "    \n",
    "    print(f\"GPU : {gpu_name}\")\n",
    "    print(f\"VRAM totale : {vram_total:.1f} GB\")\n",
    "    print(f\"VRAM libre : {vram_free:.1f} GB\")\n",
    "    print(f\"CUDA : {torch.version.cuda}\")\n",
    "    \n",
    "    if vram_total < 10:\n",
    "        print(f\"\\nAttention : VRAM ({vram_total:.0f} GB) < 10 GB recommandes\")\n",
    "        quantize = True\n",
    "        height = 320\n",
    "        width = 576\n",
    "        num_frames = 12\n",
    "        print(f\"  Resolution reduite a {width}x{height}, {num_frames} frames\")\n",
    "else:\n",
    "    print(\"CUDA non disponible.\")\n",
    "    print(\"Wan necessite un GPU. Le notebook montrera le code sans executer.\")\n",
    "    run_generation = False\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Verification des dependances\n",
    "print(\"\\n--- VERIFICATION DEPENDANCES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "deps_ok = True\n",
    "\n",
    "try:\n",
    "    import diffusers\n",
    "    print(f\"diffusers : v{diffusers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"diffusers NON INSTALLE (pip install diffusers>=0.32)\")\n",
    "    deps_ok = False\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"transformers : v{transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"transformers NON INSTALLE\")\n",
    "    deps_ok = False\n",
    "\n",
    "if quantize:\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "        print(f\"bitsandbytes : v{bnb.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"bitsandbytes NON INSTALLE (pip install bitsandbytes)\")\n",
    "        print(\"  Quantification INT8 non disponible\")\n",
    "        quantize = False\n",
    "\n",
    "try:\n",
    "    import imageio\n",
    "    print(f\"imageio : v{imageio.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"imageio NON INSTALLE\")\n",
    "    deps_ok = False\n",
    "\n",
    "if not deps_ok:\n",
    "    print(\"\\nDependances manquantes. Le notebook montrera le code sans executer.\")\n",
    "    run_generation = False\n",
    "\n",
    "print(f\"\\nDevice : {device}\")\n",
    "print(f\"Generation activee : {run_generation}\")\n",
    "print(f\"Quantification : {'INT8' if quantize else 'FP16'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Section 1 : Architecture Wan 2.1/2.2\n",
    "\n",
    "Wan est une famille de modeles de generation video developpes par Alibaba. La serie 2.1/2.2\n",
    "se distingue par sa comprehension multilingue et son controle fin du mouvement.\n",
    "\n",
    "| Composant | Description |\n",
    "|-----------|-------------|\n",
    "| **Architecture** | Transformer avec attention croisee spatio-temporelle |\n",
    "| **Text encoder** | Multilingue (chinois, anglais, francais) |\n",
    "| **Variantes** | T2V-1.3B (leger) / T2V-14B (haute qualite) |\n",
    "| **Scheduler** | UniPC (rapide) ou DPM-Solver |\n",
    "\n",
    "### Comparaison Wan 2.1 vs 2.2\n",
    "\n",
    "| Aspect | Wan 2.1 | Wan 2.2 |\n",
    "|--------|---------|----------|\n",
    "| Qualite | Bonne | Amelioree |\n",
    "| Coherence temporelle | Bonne | Tres bonne |\n",
    "| Motion control | Basique | Camera paths avancees |\n",
    "| Resolutions | 480p-720p | 480p-1080p |\n",
    "| VRAM (14B, INT8) | ~10 GB | ~12 GB |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du pipeline Wan\n",
    "pipe = None\n",
    "\n",
    "if run_generation:\n",
    "    print(\"\\n--- CHARGEMENT DU PIPELINE ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        from diffusers import WanPipeline\n",
    "        from diffusers.utils import export_to_video\n",
    "        \n",
    "        start_load = time.time()\n",
    "        \n",
    "        if quantize:\n",
    "            from diffusers import BitsAndBytesConfig\n",
    "            \n",
    "            print(f\"Chargement avec quantification INT8...\")\n",
    "            print(f\"  Modele : {model_id}\")\n",
    "            \n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True\n",
    "            )\n",
    "            \n",
    "            pipe = WanPipeline.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=quant_config,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Chargement en FP16...\")\n",
    "            print(f\"  Modele : {model_id}\")\n",
    "            \n",
    "            pipe = WanPipeline.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        \n",
    "        pipe = pipe.to(device)\n",
    "        \n",
    "        # Optimisations memoire\n",
    "        pipe.enable_vae_slicing()\n",
    "        pipe.enable_vae_tiling()\n",
    "        try:\n",
    "            pipe.enable_model_cpu_offload()\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        load_time = time.time() - start_load\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            vram_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            print(f\"  VRAM utilisee : {vram_used:.1f} GB\")\n",
    "        \n",
    "        print(f\"Pipeline charge en {load_time:.1f}s\")\n",
    "        print(f\"  Quantification : {'INT8' if quantize else 'FP16'}\")\n",
    "        print(f\"  Resolution : {width}x{height}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur chargement pipeline : {type(e).__name__}: {str(e)[:200]}\")\n",
    "        print(\"Le notebook continuera sans generation.\")\n",
    "        run_generation = False\n",
    "        pipe = None\n",
    "else:\n",
    "    print(\"Chargement pipeline desactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Section 2 : Generation avec prompts multilingues\n",
    "\n",
    "L'un des avantages distinctifs de Wan est sa comprehension multilingue.\n",
    "Nous allons tester des prompts en francais et en anglais pour comparer les resultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation avec prompts multilingues\n",
    "print(\"\\n--- PROMPTS MULTILINGUES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def generate_wan_video(prompt: str, negative_prompt: str = \"\",\n",
    "                       seed: int = 42) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Genere une video avec Wan.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Description textuelle (FR, EN ou CN)\n",
    "        negative_prompt: Elements a eviter\n",
    "        seed: Graine aleatoire pour reproductibilite\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec frames, temps de generation et metadonnees\n",
    "    \"\"\"\n",
    "    if pipe is None:\n",
    "        return {\"success\": False, \"error\": \"Pipeline non charge\"}\n",
    "    \n",
    "    try:\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        output = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt or \"low quality, blurry, distorted, watermark\",\n",
    "            num_frames=num_frames,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            generator=generator\n",
    "        )\n",
    "        \n",
    "        gen_time = time.time() - start_time\n",
    "        frames = output.frames[0]\n",
    "        \n",
    "        result = {\n",
    "            \"success\": True,\n",
    "            \"frames\": frames,\n",
    "            \"generation_time\": gen_time,\n",
    "            \"time_per_frame\": gen_time / num_frames,\n",
    "            \"prompt\": prompt,\n",
    "            \"seed\": seed,\n",
    "            \"params\": {\n",
    "                \"num_frames\": num_frames,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"num_inference_steps\": num_inference_steps,\n",
    "                \"height\": height,\n",
    "                \"width\": width\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            result[\"vram_peak\"] = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": f\"{type(e).__name__}: {str(e)[:200]}\"}\n",
    "\n",
    "\n",
    "# Test bilingue FR/EN\n",
    "bilingual_prompts = [\n",
    "    {\n",
    "        \"text\": \"Un chat roux dort paisiblement sur un rebord de fenetre ensoleille, lumiere douce, atmosphere calme\",\n",
    "        \"lang\": \"FR\",\n",
    "        \"label\": \"Chat (FR)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"A ginger cat sleeping peacefully on a sunny windowsill, soft light, calm atmosphere\",\n",
    "        \"lang\": \"EN\",\n",
    "        \"label\": \"Cat (EN)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "bilingual_results = []\n",
    "\n",
    "if run_generation:\n",
    "    for p_idx, prompt_info in enumerate(bilingual_prompts):\n",
    "        print(f\"\\nGeneration {p_idx + 1}/{len(bilingual_prompts)} : {prompt_info['label']}\")\n",
    "        print(f\"  Prompt ({prompt_info['lang']}) : {prompt_info['text'][:70]}...\")\n",
    "        \n",
    "        result = generate_wan_video(prompt_info['text'], seed=42)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  Temps : {result['generation_time']:.1f}s\")\n",
    "            bilingual_results.append({\n",
    "                \"label\": prompt_info['label'],\n",
    "                \"lang\": prompt_info['lang'],\n",
    "                \"frames\": result['frames'],\n",
    "                \"time\": result['generation_time']\n",
    "            })\n",
    "            \n",
    "            if save_as_mp4:\n",
    "                mp4_path = OUTPUT_DIR / f\"wan_{prompt_info['lang'].lower()}_demo.mp4\"\n",
    "                export_to_video(result['frames'], str(mp4_path), fps=fps_output)\n",
    "        else:\n",
    "            print(f\"  Erreur : {result['error']}\")\n",
    "    \n",
    "    # Affichage comparatif FR vs EN\n",
    "    if len(bilingual_results) == 2:\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        \n",
    "        for v_idx, br in enumerate(bilingual_results):\n",
    "            frame_indices = np.linspace(0, len(br['frames']) - 1, 4, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(br['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(br['label'], fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Comparaison FR vs EN - Meme sujet\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n{'Langue':<15} {'Temps (s)':<12}\")\n",
    "        print(\"-\" * 27)\n",
    "        for br in bilingual_results:\n",
    "            print(f\"  {br['label']:<15} {br['time']:<12.1f}\")\n",
    "else:\n",
    "    print(\"Generation desactivee\")\n",
    "    print(\"\\nWan supporte les prompts en :\")\n",
    "    print(\"  - Francais : 'Un coucher de soleil sur la mer, couleurs chaudes'\")\n",
    "    print(\"  - Anglais : 'A sunset over the sea, warm colors'\")\n",
    "    print(\"  - Chinois : nativement supporte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Interpretation : Prompts multilingues\n",
    "\n",
    "| Aspect | Francais | Anglais |\n",
    "|--------|---------|----------|\n",
    "| Qualite | Bonne | Bonne (legerement meilleure) |\n",
    "| Comprehension | Concepts principaux captes | Plus de nuances |\n",
    "| Temps | Similaire | Similaire |\n",
    "\n",
    "**Points cles** :\n",
    "1. Wan comprend bien le francais grace a son encodeur multilingue\n",
    "2. Les prompts en anglais donnent generalement des resultats legerement meilleurs (donnees d'entrainement)\n",
    "3. Il est possible de melanger les langues dans un meme prompt\n",
    "\n",
    "## Section 3 : Controle de mouvement\n",
    "\n",
    "Wan permet de controler le type de mouvement genere via les prompts.\n",
    "Certains termes cles orientent le modele vers des mouvements de camera specifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controle de mouvement et camera\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- CONTROLE DE MOUVEMENT ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    motion_prompts = [\n",
    "        {\n",
    "            \"text\": \"a slow pan across a beautiful Japanese garden with cherry blossoms, smooth camera movement, serene\",\n",
    "            \"label\": \"Pan lateral\",\n",
    "            \"motion\": \"Panoramique\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"camera slowly zooming into a detailed oil painting of a medieval castle, revealing intricate details\",\n",
    "            \"label\": \"Zoom avant\",\n",
    "            \"motion\": \"Zoom\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"aerial drone shot flying over a coastal city at golden hour, birds eye view, cinematic\",\n",
    "            \"label\": \"Vol aerien\",\n",
    "            \"motion\": \"Drone\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    motion_results = []\n",
    "    \n",
    "    for p_idx, prompt_info in enumerate(motion_prompts):\n",
    "        print(f\"\\nGeneration {p_idx + 1}/{len(motion_prompts)} : {prompt_info['label']}\")\n",
    "        print(f\"  Type de mouvement : {prompt_info['motion']}\")\n",
    "        \n",
    "        result = generate_wan_video(prompt_info['text'], seed=42 + p_idx)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  Temps : {result['generation_time']:.1f}s\")\n",
    "            motion_results.append({\n",
    "                \"label\": prompt_info['label'],\n",
    "                \"motion\": prompt_info['motion'],\n",
    "                \"frames\": result['frames'],\n",
    "                \"time\": result['generation_time']\n",
    "            })\n",
    "            \n",
    "            if save_as_mp4:\n",
    "                mp4_path = OUTPUT_DIR / f\"wan_motion_{prompt_info['motion'].lower()}.mp4\"\n",
    "                export_to_video(result['frames'], str(mp4_path), fps=fps_output)\n",
    "        else:\n",
    "            print(f\"  Erreur : {result['error']}\")\n",
    "    \n",
    "    # Affichage comparatif des mouvements\n",
    "    if motion_results:\n",
    "        n_motions = len(motion_results)\n",
    "        n_preview = 4\n",
    "        fig, axes = plt.subplots(n_motions, n_preview, figsize=(3.5 * n_preview, 3 * n_motions))\n",
    "        if n_motions == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, mr in enumerate(motion_results):\n",
    "            frame_indices = np.linspace(0, len(mr['frames']) - 1, n_preview, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(mr['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(mr['label'], fontsize=10, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Controle de mouvement - Wan\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n{'Mouvement':<15} {'Type':<15} {'Temps (s)':<12}\")\n",
    "        print(\"-\" * 42)\n",
    "        for mr in motion_results:\n",
    "            print(f\"  {mr['label']:<15} {mr['motion']:<15} {mr['time']:<12.1f}\")\n",
    "else:\n",
    "    print(\"Controle de mouvement : generation desactivee\")\n",
    "    print(\"\\nMots-cles de mouvement pour Wan :\")\n",
    "    print(\"  - 'slow pan' : panoramique lateral\")\n",
    "    print(\"  - 'zoom in/out' : rapprochement/eloignement\")\n",
    "    print(\"  - 'aerial drone shot' : vue aerienne\")\n",
    "    print(\"  - 'tracking shot' : suivi de sujet\")\n",
    "    print(\"  - 'dolly shot' : deplacement lineaire de camera\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Interpretation : Controle de mouvement\n",
    "\n",
    "| Mot-cle prompt | Mouvement obtenu | Efficacite |\n",
    "|---------------|-----------------|------------|\n",
    "| \"slow pan\" | Panoramique lateral fluide | Elevee |\n",
    "| \"zoom into\" | Rapprochement progressif | Bonne |\n",
    "| \"aerial drone\" | Survol plongeant | Bonne |\n",
    "| \"tracking shot\" | Suivi lateral d'un sujet | Moyenne |\n",
    "| \"dolly\" | Deplacement lineaire | Moyenne |\n",
    "\n",
    "**Points cles** :\n",
    "1. Les termes cinematographiques en anglais donnent le meilleur controle\n",
    "2. Les mouvements simples (pan, zoom) sont mieux geres que les complexes\n",
    "3. Combiner un mouvement de camera avec une description de scene donne les meilleurs resultats\n",
    "\n",
    "## Section 4 : Resolution et ratio d'aspect\n",
    "\n",
    "Wan supporte differents ratios d'aspect. Le choix du ratio influence la composition de la scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de differents ratios d'aspect\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- RATIOS D'ASPECT ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    aspect_prompt = \"a majestic lighthouse standing on a cliff, waves crashing, dramatic sky, golden hour\"\n",
    "    \n",
    "    aspect_configs = [\n",
    "        {\"w\": 480, \"h\": 480, \"label\": \"1:1 (480x480)\"},\n",
    "        {\"w\": 832, \"h\": 480, \"label\": \"16:9 (832x480)\"},\n",
    "        {\"w\": 480, \"h\": 832, \"label\": \"9:16 (480x832)\"},\n",
    "    ]\n",
    "    \n",
    "    aspect_results = []\n",
    "    \n",
    "    for cfg in aspect_configs:\n",
    "        print(f\"\\nTest : {cfg['label']}\")\n",
    "        \n",
    "        try:\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            generator = torch.Generator(device=device).manual_seed(42)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = pipe(\n",
    "                prompt=aspect_prompt,\n",
    "                negative_prompt=\"low quality, blurry, distorted\",\n",
    "                num_frames=12,\n",
    "                guidance_scale=5.0,\n",
    "                num_inference_steps=20,\n",
    "                height=cfg['h'],\n",
    "                width=cfg['w'],\n",
    "                generator=generator\n",
    "            )\n",
    "            \n",
    "            gen_time = time.time() - start_time\n",
    "            frames = output.frames[0]\n",
    "            \n",
    "            vram_peak = 0\n",
    "            if device == \"cuda\":\n",
    "                vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "            \n",
    "            aspect_results.append({\n",
    "                \"label\": cfg['label'],\n",
    "                \"frames\": frames,\n",
    "                \"time\": gen_time,\n",
    "                \"vram_peak\": vram_peak\n",
    "            })\n",
    "            \n",
    "            print(f\"  Temps : {gen_time:.1f}s, VRAM pic : {vram_peak:.1f} GB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur : {type(e).__name__}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Affichage : premiere et derniere frame de chaque ratio\n",
    "    if aspect_results:\n",
    "        fig, axes = plt.subplots(len(aspect_results), 2, figsize=(10, 4 * len(aspect_results)))\n",
    "        if len(aspect_results) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, ar in enumerate(aspect_results):\n",
    "            axes[v_idx][0].imshow(ar['frames'][0])\n",
    "            axes[v_idx][0].set_title(f\"{ar['label']} - Frame 1\", fontsize=10)\n",
    "            axes[v_idx][0].axis('off')\n",
    "            \n",
    "            axes[v_idx][1].imshow(ar['frames'][-1])\n",
    "            axes[v_idx][1].set_title(f\"{ar['label']} - Derniere frame\", fontsize=10)\n",
    "            axes[v_idx][1].axis('off')\n",
    "        \n",
    "        plt.suptitle(\"Ratios d'aspect - Wan\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n{'Ratio':<20} {'Temps (s)':<12} {'VRAM (GB)':<12}\")\n",
    "        print(\"-\" * 44)\n",
    "        for ar in aspect_results:\n",
    "            print(f\"  {ar['label']:<20} {ar['time']:<12.1f} {ar['vram_peak']:<12.1f}\")\n",
    "else:\n",
    "    print(\"Test ratio d'aspect : generation desactivee\")\n",
    "    print(\"\\nRatios supportes par Wan :\")\n",
    "    print(\"  1:1  (480x480) : Reseaux sociaux carre\")\n",
    "    print(\"  16:9 (832x480) : YouTube, cinema\")\n",
    "    print(\"  9:16 (480x832) : TikTok, Reels, Stories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Interpretation : Ratios d'aspect\n",
    "\n",
    "| Ratio | Usage | Particularite |\n",
    "|-------|-------|---------------|\n",
    "| 1:1 (carre) | Instagram, prototypage | Composition centree |\n",
    "| 16:9 (paysage) | YouTube, cinema | Composition large, horizon |\n",
    "| 9:16 (portrait) | TikTok, Stories | Composition verticale, sujet central |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le ratio d'aspect influence la composition automatique de la scene\n",
    "2. Les paysages fonctionnent mieux en 16:9, les portraits en 9:16\n",
    "3. La VRAM est proportionnelle au nombre total de pixels (W x H)\n",
    "\n",
    "## Bonnes pratiques et prompt engineering\n",
    "\n",
    "### Structure d'un bon prompt Wan\n",
    "\n",
    "| Element | Exemple | Importance |\n",
    "|---------|---------|------------|\n",
    "| Sujet | \"a majestic eagle\" | Essentiel |\n",
    "| Action | \"soaring through clouds\" | Essentiel |\n",
    "| Camera | \"aerial tracking shot\" | Recommande |\n",
    "| Eclairage | \"golden hour, dramatic light\" | Recommande |\n",
    "| Style | \"cinematic, 4K, high quality\" | Optionnel |\n",
    "\n",
    "### Prompts en francais vs anglais\n",
    "\n",
    "| Approche | Avantage | Inconvenient |\n",
    "|----------|----------|-------------|\n",
    "| Tout FR | Naturel pour le redacteur | Moins de nuances captees |\n",
    "| Tout EN | Meilleure comprehension | Moins intuitif |\n",
    "| Mixte | Compromis optimal | Necessite expertise bilingue |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Entrez votre propre prompt (francais ou anglais) pour generer une video Wan.\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "    \n",
    "    try:\n",
    "        user_prompt = input(\"\\nVotre prompt (FR ou EN) : \").strip()\n",
    "        \n",
    "        if user_prompt and run_generation and pipe is not None:\n",
    "            print(f\"\\nGeneration en cours...\")\n",
    "            result_user = generate_wan_video(user_prompt, seed=123)\n",
    "            \n",
    "            if result_user['success']:\n",
    "                print(f\"Generation reussie en {result_user['generation_time']:.1f}s\")\n",
    "                \n",
    "                n_display = min(8, len(result_user['frames']))\n",
    "                fig, axes = plt.subplots(1, n_display, figsize=(2.5 * n_display, 3))\n",
    "                if n_display == 1:\n",
    "                    axes = [axes]\n",
    "                indices = np.linspace(0, len(result_user['frames']) - 1, n_display, dtype=int)\n",
    "                for ax, idx in zip(axes, indices):\n",
    "                    ax.imshow(result_user['frames'][idx])\n",
    "                    ax.set_title(f\"Frame {idx+1}\", fontsize=8)\n",
    "                    ax.axis('off')\n",
    "                plt.suptitle(f\"Votre video : {user_prompt[:50]}...\", fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                if save_as_mp4:\n",
    "                    user_mp4 = OUTPUT_DIR / \"user_generation.mp4\"\n",
    "                    export_to_video(result_user['frames'], str(user_mp4), fps=fps_output)\n",
    "                    print(f\"MP4 sauvegarde : {user_mp4.name}\")\n",
    "            else:\n",
    "                print(f\"Erreur : {result_user['error']}\")\n",
    "        elif user_prompt:\n",
    "            print(\"Generation non disponible (pipeline non charge)\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError) as e:\n",
    "        print(f\"\\nMode interactif interrompu ({type(e).__name__})\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"\\nMode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"\\nErreur inattendue : {error_type} - {str(e)[:100]}\")\n",
    "            print(\"Passage a la suite du notebook\")\n",
    "else:\n",
    "    print(\"\\nMode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Modele : {model_id}\")\n",
    "print(f\"Quantification : {'INT8' if quantize else 'FP16'}\")\n",
    "print(f\"Device : {device}\")\n",
    "print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n",
    "print(f\"Resolution : {width}x{height}\")\n",
    "\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "    print(f\"VRAM pic session : {vram_peak:.1f} GB\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "# Liberation VRAM\n",
    "if pipe is not None:\n",
    "    del pipe\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nVRAM liberee\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 02-4 : SVD - Stable Video Diffusion (animation d'images statiques)\")\n",
    "print(f\"2. Module 03-1 : Comparaison benchmark de tous les modeles (AnimateDiff, HunyuanVideo, LTX, Wan, SVD)\")\n",
    "print(f\"3. Module 03-2 : Orchestration de pipelines text -> image -> video\")\n",
    "print(f\"4. Tester Wan 2.2 quand disponible pour les ameliorations de qualite\")\n",
    "\n",
    "print(f\"\\nNotebook 02-3 Wan Video Generation termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
