{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.003111,
     "end_time": "2026-02-19T09:29:27.212184",
     "exception": false,
     "start_time": "2026-02-19T09:29:27.209073",
     "status": "completed"
    },
    "tags": []
   },
   "source": "# HunyuanVideo - Generation Video Haute Qualite\n\n**Module :** 02-Video-Advanced  \n**Niveau :** Intermediaire  \n**Technologies :** HunyuanVideo 1.5 (Tencent), ComfyUI API ou diffusers  \n**Duree estimee :** 60 minutes  \n**VRAM :** ~12 GB (API) ou ~18 GB (local avec INT8)  \n\n## Objectifs d'Apprentissage\n\n- [ ] Comprendre l'architecture HunyuanVideo et ses avantages\n- [ ] Choisir entre API ComfyUI (production) et diffusers (pedagogique)\n- [ ] Generer des videos text-to-video avec des prompts detailles\n- [ ] Explorer les parametres de generation (steps, guidance_scale, num_frames, fps)\n- [ ] Controler la resolution et la duree des videos\n- [ ] Sauvegarder les resultats en MP4 avec imageio\n- [ ] Analyser la qualite et les metriques de generation\n\n## Prerequis\n\n### Mode API ComfyUI (recommande pour production)\n- Service ComfyUI-Video demarre (docker-compose comfyui-video)\n- Pas de dependances Python lourdes cote client\n\n### Mode Local diffusers (pedagogique)\n- GPU avec 18+ GB VRAM (RTX 3090 / RTX 4090)\n- Packages : `diffusers>=0.32`, `transformers`, `torch`, `accelerate`, `bitsandbytes`, `imageio`\n\n**Navigation :** [<< 01-5](../01-Foundation/01-5-AnimateDiff-Introduction.ipynb) | [Index](../README.md) | [Suivant >>](02-2-LTX-Video-Lightweight.ipynb)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:27.219809Z",
     "iopub.status.busy": "2026-02-19T09:29:27.219270Z",
     "iopub.status.idle": "2026-02-19T09:29:27.225011Z",
     "shell.execute_reply": "2026-02-19T09:29:27.224258Z"
    },
    "papermill": {
     "duration": 0.011136,
     "end_time": "2026-02-19T09:29:27.226419",
     "exception": false,
     "start_time": "2026-02-19T09:29:27.215283",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": "# Parametres Papermill - JAMAIS modifier ce commentaire\n\n# Configuration notebook\nnotebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\nskip_widgets = False               # True pour mode batch MCP\ndebug_level = \"INFO\"\n\n# MODE D'EXECUTION : API ou Local\n# - True  : Utilise l'API ComfyUI (recommande, pas de GPU local requis)\n# - False : Utilise diffusers en local (pedagogique, necessite GPU)\nuse_api = True\n\n# Parametres API ComfyUI (si use_api=True)\ncomfyui_url = \"http://localhost:8189\"  # ComfyUI-Video service\ncomfyui_token = None                 # Token Bearer (optionnel pour localhost)\n\n# Parametres modele HunyuanVideo (si use_api=False)\nmodel_id = \"tencent/HunyuanVideo\"  # Modele HunyuanVideo\nquantize = True                      # Quantification INT8 (recommande)\ndevice = \"cuda\"                     # Device de calcul\n\n# Parametres generation (communs aux deux modes)\nnum_frames = 33                    # Nombre de frames a generer (HunyuanVideo optimal)\nguidance_scale = 7.0               # CFG scale (7.0 recommande pour HunyuanVideo)\nnum_inference_steps = 30           # Nombre d'etapes de debruitage\nheight = 720                       # Hauteur video (720p optimal)\nwidth = 1280                       # Largeur video\nfps_output = 24                    # FPS de la video de sortie\n\n# Configuration\nrun_generation = True              # Executer la generation\nsave_as_mp4 = True                 # Sauvegarder en MP4\nsave_results = True"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c54c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:27.232469Z",
     "iopub.status.busy": "2026-02-19T09:29:27.232054Z",
     "iopub.status.idle": "2026-02-19T09:29:27.235055Z",
     "shell.execute_reply": "2026-02-19T09:29:27.234449Z"
    },
    "papermill": {
     "duration": 0.007031,
     "end_time": "2026-02-19T09:29:27.235844",
     "exception": false,
     "start_time": "2026-02-19T09:29:27.228813",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "notebook_mode = \"batch\"\n",
    "skip_widgets = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:27.241714Z",
     "iopub.status.busy": "2026-02-19T09:29:27.241297Z",
     "iopub.status.idle": "2026-02-19T09:29:27.784103Z",
     "shell.execute_reply": "2026-02-19T09:29:27.783405Z"
    },
    "papermill": {
     "duration": 0.546923,
     "end_time": "2026-02-19T09:29:27.785069",
     "exception": false,
     "start_time": "2026-02-19T09:29:27.238146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "# Setup environnement et imports\nimport os\nimport sys\nimport json\nimport time\nimport warnings\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport logging\nfrom dotenv import load_dotenv\n\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Import helpers GenAI\nGENAI_ROOT = Path.cwd()\nwhile GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n    GENAI_ROOT = GENAI_ROOT.parent\n\nHELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\nif HELPERS_PATH.exists():\n    sys.path.insert(0, str(HELPERS_PATH.parent))\n    try:\n        from helpers import comfyui_client\n        print(\"‚úÖ Helper comfyui_client import√©\")\n    except ImportError as e:\n        print(f\"‚ö†Ô∏è Helper comfyui_client NON disponible: {e}\")\n        comfyui_client = None\n\nOUTPUT_DIR = GENAI_ROOT / 'outputs' / 'hunyuan_video'\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nlogging.basicConfig(level=getattr(logging, debug_level))\nlogger = logging.getLogger('hunyuan_video')\n\n# Affichage du mode d'execution\nmode_str = \"API ComfyUI\" if use_api else \"Local diffusers\"\nprint(f\"HunyuanVideo 1.5 - Generation Video Haute Qualite\")\nprint(f\"Mode d'execution : {mode_str}\")\nprint(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"Frames : {num_frames}, Steps : {num_inference_steps}, CFG : {guidance_scale}\")\nprint(f\"Resolution : {width}x{height}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:27.790477Z",
     "iopub.status.busy": "2026-02-19T09:29:27.790228Z",
     "iopub.status.idle": "2026-02-19T09:29:31.943484Z",
     "shell.execute_reply": "2026-02-19T09:29:31.942563Z"
    },
    "papermill": {
     "duration": 4.157906,
     "end_time": "2026-02-19T09:29:31.945233",
     "exception": false,
     "start_time": "2026-02-19T09:29:27.787327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "# Chargement .env et verification de l'environnement\ncurrent_path = Path.cwd()\nfound_env = False\nfor _ in range(4):\n    env_path = current_path / '.env'\n    if env_path.exists():\n        load_dotenv(env_path)\n        print(f\"‚úÖ Fichier .env charge depuis : {env_path}\")\n        found_env = True\n        break\n    current_path = current_path.parent\n\nif not found_env:\n    print(\"‚ö†Ô∏è Aucun fichier .env trouve\")\n\n# Verification et initialisation selon le mode\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"MODE : {'API ComfyUI' if use_api else 'Local diffusers'}\")\nprint(\"=\" * 50)\n\nclient = None\npipe = None\ncomfyui_available = False\nlocal_available = False\n\nif use_api:\n    # === MODE API COMFYUI ===\n    print(\"\\nüì° Verification de l'API ComfyUI-Video...\")\n    \n    if comfyui_client is not None:\n        try:\n            client = comfyui_client.ComfyUIClient(\n                base_url=comfyui_url,\n                api_token=comfyui_token\n            )\n            \n            stats = client.get_system_stats()\n            \n            print(f\"‚úÖ ComfyUI-Video accessible sur : {comfyui_url}\")\n            comfyui_available = True\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è ComfyUI-Video non accessible: {type(e).__name__}: {str(e)[:100]}\")\n            print(\"\\nüí° Pour d√©marrer ComfyUI-Video :\")\n            print(\"   docker-compose -f docker-configurations/services/comfyui-video/docker-compose.yml up -d\")\n            run_generation = False\n    else:\n        print(\"‚ö†Ô∏è Helper comfyui_client non disponible\")\n        run_generation = False\n        \nelse:\n    # === MODE LOCAL DIFFUSERS ===\n    print(\"\\nüîß Verification de l'environnement local...\")\n    \n    # Verification GPU\n    try:\n        import torch\n        if torch.cuda.is_available():\n            gpu_name = torch.cuda.get_device_name(0)\n            vram_total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n            print(f\"‚úÖ GPU : {gpu_name}\")\n            print(f\"   VRAM totale : {vram_total:.1f} GB\")\n            \n            if vram_total < 18:\n                print(f\"‚ö†Ô∏è VRAM faible (< 18 GB), activation de la quantification\")\n                quantize = True\n                if vram_total < 12:\n                    height = 480\n                    width = 640\n                    num_frames = 24\n                    print(f\"  Resolution reduite a {width}x{height}, {num_frames} frames\")\n        else:\n            print(\"‚ö†Ô∏è CUDA non disponible\")\n            run_generation = False\n    except ImportError:\n        print(\"‚ö†Ô∏è PyTorch non install√©\")\n        run_generation = False\n    \n    # Verification des dependances\n    deps_ok = True\n    \n    try:\n        import diffusers\n        print(f\"‚úÖ diffusers : v{diffusers.__version__}\")\n    except ImportError:\n        print(\"‚ö†Ô∏è diffusers NON INSTALLE (pip install diffusers>=0.32)\")\n        deps_ok = False\n    \n    try:\n        import transformers\n        print(f\"‚úÖ transformers : v{transformers.__version__}\")\n    except ImportError:\n        print(\"‚ö†Ô∏è transformers NON INSTALLE\")\n        deps_ok = False\n    \n    if quantize:\n        try:\n            import bitsandbytes as bnb\n            print(f\"‚úÖ bitsandbytes : v{bnb.__version__}\")\n        except ImportError:\n            print(\"‚ö†Ô∏è bitsandbytes NON INSTALLE (pip install bitsandbytes)\")\n            quantize = False\n    \n    try:\n        import imageio\n        print(f\"‚úÖ imageio : v{imageio.__version__}\")\n    except ImportError:\n        print(\"‚ö†Ô∏è imageio NON INSTALLE\")\n        deps_ok = False\n    \n    if deps_ok and run_generation:\n        print(\"\\nüì¶ Chargement du pipeline HunyuanVideo...\")\n        try:\n            from diffusers import HunyuanVideoPipeline\n            from diffusers.utils import export_to_video\n            \n            start_load = time.time()\n            \n            if quantize:\n                from diffusers import BitsAndBytesConfig\n                quant_config = BitsAndBytesConfig(load_in_8bit=True)\n                pipe = HunyuanVideoPipeline.from_pretrained(\n                    model_id,\n                    quantization_config=quant_config,\n                    torch_dtype=torch.float16\n                )\n            else:\n                pipe = HunyuanVideoPipeline.from_pretrained(\n                    model_id,\n                    torch_dtype=torch.float16\n                )\n            \n            pipe = pipe.to(device)\n            pipe.enable_vae_slicing()\n            pipe.enable_vae_tiling()\n            \n            load_time = time.time() - start_load\n            print(f\"‚úÖ Pipeline charge en {load_time:.1f}s\")\n            local_available = True\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Erreur chargement pipeline : {type(e).__name__}: {str(e)[:200]}\")\n            run_generation = False\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Generation activee : {run_generation}\")\nprint(f\"{'='*50}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {
    "papermill": {
     "duration": 0.003668,
     "end_time": "2026-02-19T09:29:31.952253",
     "exception": false,
     "start_time": "2026-02-19T09:29:31.948585",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Section 1 : Architecture HunyuanVideo\n\nHunyuanVideo est un modele de generation text-to-video open-source developpe par Tencent.\nIl se distingue par sa qualite de generation et sa capacite a produire des videos longues\navec une bonne coherence temporelle.\n\n### Deux approches pour utiliser HunyuanVideo\n\n| Aspect | API ComfyUI | Local diffusers |\n|--------|-------------|-----------------|\n| **Cas d'usage** | Production, applications | Pedagogie, recherche |\n| **GPU requis** | Non (cote serveur) | Oui (18+ GB) |\n| **Installation** | Aucune (Docker) | diffusers, transformers, torch |\n| **Flexibilite** | Moyenne | Elevee |\n| **Performance** | Serveur optimise | Depend du GPU local |\n\n### Architecture de HunyuanVideo\n\n| Composant | Description |\n|-----------|-------------|\n| **Backbone** | Transformer 3D avec attention spatio-temporelle |\n| **Text encoders** | DualCLIP (clip_l + llava_llama3) |\n| **VAE** | Encodeur/decodeur video avec compression temporelle |\n| **Scheduler** | Flow matching pour un debruitage progressif |\n\n### Avantages par rapport a AnimateDiff\n\n| Aspect | AnimateDiff (01-5) | HunyuanVideo |\n|--------|-------------------|---------------|\n| Architecture | SD 1.5 + motion module | Transformer 3D natif |\n| Resolution | 512x512 max | Jusqu'a 720p |\n| Coherence temporelle | Moyenne | Elevee |\n| Duree video | 2-3 secondes | 5+ secondes |\n| VRAM | ~12 GB | ~18 GB (INT8) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:31.958801Z",
     "iopub.status.busy": "2026-02-19T09:29:31.958341Z",
     "iopub.status.idle": "2026-02-19T09:29:31.965370Z",
     "shell.execute_reply": "2026-02-19T09:29:31.964885Z"
    },
    "papermill": {
     "duration": 0.011337,
     "end_time": "2026-02-19T09:29:31.966272",
     "exception": false,
     "start_time": "2026-02-19T09:29:31.954935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "# Fonction de generation unifiee (API ou Local)\ndef generate_hunyuan_video(prompt: str, negative_prompt: str = \"\", seed: int = 42) -> Dict[str, Any]:\n    \"\"\"\n    Genere une video avec HunyuanVideo (API ComfyUI ou local diffusers).\n    \n    Cette fonction s'adapte automatiquement au mode d'execution choisi.\n    \n    Args:\n        prompt: Description textuelle de la video\n        negative_prompt: Elements a eviter\n        seed: Graine aleatoire pour reproductibilite\n    \n    Returns:\n        Dict avec frames, temps de generation et metadonnees\n    \"\"\"\n    if use_api:\n        # === MODE API COMFYUI ===\n        if not comfyui_available:\n            return {\"success\": False, \"error\": \"API ComfyUI non disponible\"}\n        \n        try:\n            start_time = time.time()\n            \n            result = client.generate_text2video_hunyuan(\n                prompt=prompt,\n                width=width,\n                height=height,\n                num_frames=num_frames,\n                steps=num_inference_steps,\n                seed=seed,\n                cfg=guidance_scale,\n                negative_prompt=negative_prompt or \"bad quality, low quality, blurry, distortion\",\n                save_prefix=f\"hunyuan_gen_{seed}\",\n                timeout=600\n            )\n            \n            gen_time = time.time() - start_time\n            \n            return {\n                \"success\": True,\n                \"result\": result,\n                \"generation_time\": gen_time,\n                \"mode\": \"API ComfyUI\",\n                \"seed\": seed\n            }\n            \n        except Exception as e:\n            return {\"success\": False, \"error\": f\"{type(e).__name__}: {str(e)[:200]}\"}\n    \n    else:\n        # === MODE LOCAL DIFFUSERS ===\n        if not local_available:\n            return {\"success\": False, \"error\": \"Pipeline local non disponible\"}\n        \n        try:\n            import torch\n            from diffusers.utils import export_to_video\n            \n            generator = torch.Generator(device=device).manual_seed(seed)\n            \n            if device == \"cuda\":\n                torch.cuda.reset_peak_memory_stats()\n            \n            start_time = time.time()\n            \n            output = pipe(\n                prompt=prompt,\n                negative_prompt=negative_prompt or \"bad quality, low quality, blurry, distortion, artifacts\",\n                num_frames=num_frames,\n                guidance_scale=guidance_scale,\n                num_inference_steps=num_inference_steps,\n                height=height,\n                width=width,\n                generator=generator\n            )\n            \n            gen_time = time.time() - start_time\n            frames = output.frames[0]\n            \n            # Sauvegarder en MP4\n            mp4_path = OUTPUT_DIR / f\"hunyuan_local_{seed}.mp4\"\n            export_to_video(frames, str(mp4_path), fps=fps_output)\n            \n            result_dict = {\n                \"success\": True,\n                \"frames\": frames,\n                \"generation_time\": gen_time,\n                \"time_per_frame\": gen_time / num_frames,\n                \"prompt\": prompt,\n                \"seed\": seed,\n                \"mode\": \"Local diffusers\",\n                \"mp4_path\": str(mp4_path)\n            }\n            \n            if device == \"cuda\":\n                result_dict[\"vram_peak\"] = torch.cuda.max_memory_allocated(0) / 1024**3\n            \n            return result_dict\n            \n        except Exception as e:\n            return {\"success\": False, \"error\": f\"{type(e).__name__}: {str(e)[:200]}\"}\n\nprint(\"‚úÖ Fonction de generation unifiee chargee\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:31.976644Z",
     "iopub.status.busy": "2026-02-19T09:29:31.976248Z",
     "iopub.status.idle": "2026-02-19T09:29:31.987889Z",
     "shell.execute_reply": "2026-02-19T09:29:31.986523Z"
    },
    "papermill": {
     "duration": 0.01635,
     "end_time": "2026-02-19T09:29:31.989602",
     "exception": false,
     "start_time": "2026-02-19T09:29:31.973252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "# Generation text-to-video\nprint(\"\\n--- GENERATION TEXT-TO-VIDEO ---\")\nprint(\"=\" * 40)\n\n# Premier test : prompt cinematographique\nprompt_1 = \"a majestic eagle soaring over snow-capped mountains at golden hour, cinematic aerial shot, smooth camera movement, volumetric clouds\"\n\nif run_generation:\n    print(f\"Prompt : {prompt_1}\")\n    print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n    print(f\"Resolution : {width}x{height}\")\n    print(f\"Mode : {'API ComfyUI' if use_api else 'Local diffusers'}\")\n    print(f\"\\nGeneration en cours...\")\n    \n    result_1 = generate_hunyuan_video(prompt_1, seed=42)\n    \n    if result_1['success']:\n        print(f\"\\n‚úÖ Generation terminee en {result_1['generation_time']:.1f}s ({result_1['mode']})\")\n        if 'vram_peak' in result_1:\n            print(f\"   VRAM pic : {result_1['vram_peak']:.1f} GB\")\n        if 'time_per_frame' in result_1:\n            print(f\"   Temps/frame : {result_1['time_per_frame']:.2f}s\")\n        \n        # Affichage si frames disponibles\n        if 'frames' in result_1:\n            frames = result_1['frames']\n            print(f\"   Frames : {len(frames)}\")\n            \n            n_display = min(8, len(frames))\n            indices = np.linspace(0, len(frames) - 1, n_display, dtype=int)\n            fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n            axes_flat = axes.flatten()\n            for i, idx in enumerate(indices):\n                if i < len(axes_flat):\n                    axes_flat[i].imshow(frames[idx])\n                    axes_flat[i].set_title(f\"Frame {idx + 1}/{len(frames)}\", fontsize=9)\n                    axes_flat[i].axis('off')\n            for i in range(len(indices), len(axes_flat)):\n                axes_flat[i].axis('off')\n            plt.suptitle(f\"HunyuanVideo : {prompt_1[:60]}...\", fontsize=11, fontweight='bold')\n            plt.tight_layout()\n            plt.show()\n        \n        # Sauvegarde MP4 (mode local seulement)\n        if save_as_mp4 and 'mp4_path' in result_1:\n            from pathlib import Path\n            mp4_path = Path(result_1['mp4_path'])\n            if mp4_path.exists():\n                mp4_size_kb = mp4_path.stat().st_size / 1024\n                print(f\"   MP4 sauvegarde : {mp4_path.name} ({mp4_size_kb:.1f} KB)\")\n    else:\n        print(f\"‚ùå Erreur : {result_1['error']}\")\nelse:\n    print(\"Generation desactivee\")\n    print(f\"\\nExemple de code pour generer :\")\n    print(f\"  result = generate_hunyuan_video('{prompt_1[:50]}...', seed=42)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {
    "papermill": {
     "duration": 0.003832,
     "end_time": "2026-02-19T09:29:31.997763",
     "exception": false,
     "start_time": "2026-02-19T09:29:31.993931",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Premiere generation\n",
    "\n",
    "### MODE PEDAGOGIQUE (GPU non disponible)\n",
    "\n",
    "Sur un environnement GPU (RTX 3090, 24GB VRAM), ce code g√©n√©rerait:\n",
    "\n",
    "| Param√®tre | Valeur typique | Signification |\n",
    "|--------|---------------|---------------|\n",
    "| **Temps total** | 60-180s (RTX 3090) | Significativement plus long qu'AnimateDiff |\n",
    "| **VRAM pic** | 16-22 GB | La quantification INT8 maintient la VRAM sous 24 GB |\n",
    "| **Qualit√©** | Haute | Meilleure coh√©rence temporelle que AnimateDiff |\n",
    "| **R√©solution** | 512x320 | Compromis qualit√©/m√©moire |\n",
    "| **Frames** | 24 | ~3 secondes √† 8fps |\n",
    "\n",
    "**R√©sultat visuel attendu:**\n",
    "\n",
    "Pour le prompt \"a majestic eagle soaring over snow-capped mountains at golden hour\", HunyuanVideo g√©n√©rerait:\n",
    "\n",
    "- **Aigle**: Ailes d√©ploy√©es, plumage d√©taill√©, mouvement de vol naturel\n",
    "- **Montagnes**: Pics enneig√©s, ombres dramatiques, profondeur de champ\n",
    "- **Ciel**: Nuages volum√©triques, lumi√®re dor√©e, atmosph√®re cin√©matographique\n",
    "- **Cam√©ra**: Mouvement fluide de suivi de l'aigle, cadrage large\n",
    "\n",
    "**Comparaison avec AnimateDiff (01-5):**\n",
    "\n",
    "| Aspect | AnimateDiff | HunyuanVideo | Avantage |\n",
    "|--------|-------------|--------------|----------|\n",
    "| **Architecture** | SD 1.5 + motion module | Transformer 3D natif | Hunyuan |\n",
    "| **R√©solution max** | 512x512 | 720p | Hunyuan |\n",
    "| **Coh√©rence temporelle** | Moyenne | √âlev√©e | Hunyuan |\n",
    "| **Dur√©e vid√©o** | 2-3s | 5+s | Hunyuan |\n",
    "| **VRAM** | ~12 GB | ~18 GB (INT8) | AnimateDiff |\n",
    "| **Vitesse** | Rapide | Lent | AnimateDiff |\n",
    "\n",
    "**Code pour reproduire:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from diffusers import HunyuanVideoPipeline\n",
    "\n",
    "# Pipeline avec quantification INT8\n",
    "from diffusers import BitsAndBytesConfig\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "pipe = HunyuanVideoPipeline.from_pretrained(\n",
    "    \"tencent/HunyuanVideo\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Optimisations m√©moire\n",
    "pipe.enable_vae_slicing()\n",
    "pipe.enable_vae_tiling()\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# G√©n√©ration\n",
    "prompt = \"a majestic eagle soaring over snow-capped mountains at golden hour, cinematic aerial shot\"\n",
    "output = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=\"low quality, blurry\",\n",
    "    num_frames=24,\n",
    "    guidance_scale=6.0,\n",
    "    num_inference_steps=30,\n",
    "    height=320,\n",
    "    width=512,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    ")\n",
    "\n",
    "frames = output.frames[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:32.005905Z",
     "iopub.status.busy": "2026-02-19T09:29:32.005256Z",
     "iopub.status.idle": "2026-02-19T09:29:32.016502Z",
     "shell.execute_reply": "2026-02-19T09:29:32.015392Z"
    },
    "papermill": {
     "duration": 0.017453,
     "end_time": "2026-02-19T09:29:32.018271",
     "exception": false,
     "start_time": "2026-02-19T09:29:32.000818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploration des parametres : generation desactivee\n",
      "\n",
      "Guide des parametres :\n",
      "  CFG 3-4 : Creatif, plus de liberte\n",
      "  CFG 5-7 : Equilibre (recommande)\n",
      "  CFG 8-10 : Strict, peut introduire des artefacts\n"
     ]
    }
   ],
   "source": [
    "# Exploration des parametres\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- EXPLORATION DES PARAMETRES ---\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Test avec differentes valeurs de guidance_scale\n",
    "    test_prompt = \"a serene waterfall in a lush forest, sunlight filtering through trees, mist rising\"\n",
    "    \n",
    "    cfg_values = [3.0, 6.0, 9.0]\n",
    "    cfg_results = []\n",
    "    \n",
    "    print(f\"Test guidance_scale : {cfg_values}\")\n",
    "    print(f\"Prompt : {test_prompt[:60]}...\")\n",
    "    \n",
    "    for cfg_val in cfg_values:\n",
    "        print(f\"\\n  CFG = {cfg_val}...\")\n",
    "        \n",
    "        # Sauvegarder et modifier temporairement\n",
    "        original_cfg = guidance_scale\n",
    "        original_steps = num_inference_steps\n",
    "        guidance_scale = cfg_val\n",
    "        num_inference_steps = 20  # Reduit pour acceleration\n",
    "        \n",
    "        result = generate_hunyuan_video(test_prompt, seed=42)\n",
    "        \n",
    "        # Restaurer\n",
    "        guidance_scale = original_cfg\n",
    "        num_inference_steps = original_steps\n",
    "        \n",
    "        if result['success']:\n",
    "            cfg_results.append({\n",
    "                \"cfg\": cfg_val,\n",
    "                \"frames\": result['frames'],\n",
    "                \"time\": result['generation_time'],\n",
    "                \"vram_peak\": result.get('vram_peak', 0)\n",
    "            })\n",
    "            print(f\"    Temps : {result['generation_time']:.1f}s\")\n",
    "        else:\n",
    "            print(f\"    Erreur : {result['error']}\")\n",
    "    \n",
    "    # Affichage comparatif\n",
    "    if cfg_results:\n",
    "        n_cfgs = len(cfg_results)\n",
    "        n_preview = 4\n",
    "        fig, axes = plt.subplots(n_cfgs, n_preview, figsize=(3.5 * n_preview, 3 * n_cfgs))\n",
    "        if n_cfgs == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, cr in enumerate(cfg_results):\n",
    "            frame_indices = np.linspace(0, len(cr['frames']) - 1, n_preview, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(cr['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(f\"CFG={cr['cfg']}\", fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Impact de guidance_scale sur la generation\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Tableau recapitulatif\n",
    "        print(f\"\\nRecapitulatif guidance_scale :\")\n",
    "        print(f\"{'CFG':<10} {'Temps (s)':<12} {'VRAM pic (GB)':<15}\")\n",
    "        print(\"-\" * 37)\n",
    "        for cr in cfg_results:\n",
    "            print(f\"  {cr['cfg']:<10} {cr['time']:<12.1f} {cr['vram_peak']:<15.1f}\")\n",
    "else:\n",
    "    print(\"Exploration des parametres : generation desactivee\")\n",
    "    print(\"\\nGuide des parametres :\")\n",
    "    print(\"  CFG 3-4 : Creatif, plus de liberte\")\n",
    "    print(\"  CFG 5-7 : Equilibre (recommande)\")\n",
    "    print(\"  CFG 8-10 : Strict, peut introduire des artefacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {
    "papermill": {
     "duration": 0.002974,
     "end_time": "2026-02-19T09:29:32.024207",
     "exception": false,
     "start_time": "2026-02-19T09:29:32.021233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Impact des parametres\n",
    "\n",
    "| guidance_scale | Comportement | Recommandation |\n",
    "|---------------|-------------|----------------|\n",
    "| 3.0 (bas) | Creatif, variations, parfois hors-sujet | Exploration creative |\n",
    "| 6.0 (moyen) | Bon equilibre fidelite/creativite | Usage general |\n",
    "| 9.0 (haut) | Tres fidele au prompt, risque artefacts | Prompt precis |\n",
    "\n",
    "**Points cles** :\n",
    "1. Contrairement a Stable Diffusion Image, une CFG trop elevee degrade la coherence temporelle\n",
    "2. Pour HunyuanVideo, la plage 5.0-7.0 donne generalement les meilleurs resultats\n",
    "3. Le temps de generation varie peu avec la CFG (meme nombre de steps)\n",
    "\n",
    "## Section 4 : Resolution et duree\n",
    "\n",
    "Nous allons explorer les compromis entre resolution, nombre de frames et consommation memoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:32.032365Z",
     "iopub.status.busy": "2026-02-19T09:29:32.031540Z",
     "iopub.status.idle": "2026-02-19T09:29:32.042365Z",
     "shell.execute_reply": "2026-02-19T09:29:32.041627Z"
    },
    "papermill": {
     "duration": 0.016651,
     "end_time": "2026-02-19T09:29:32.043709",
     "exception": false,
     "start_time": "2026-02-19T09:29:32.027058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test resolution/duree : generation desactivee\n",
      "\n",
      "Guide resolution/VRAM :\n",
      "  384x256 : ~14 GB, rapide, basse qualite\n",
      "  512x320 : ~18 GB, bon compromis (recommande)\n",
      "  640x480 : ~22 GB, haute qualite, lent\n",
      "  720p    : ~28 GB+, necessite quantification avancee\n"
     ]
    }
   ],
   "source": [
    "# Test de resolution et duree\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- RESOLUTION ET DUREE ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    resolution_prompt = \"a golden retriever running through a field of sunflowers, joyful, sunny day, slow motion\"\n",
    "    \n",
    "    # Configurations a tester (resolution, frames)\n",
    "    configs = [\n",
    "        {\"w\": 384, \"h\": 256, \"frames\": 24, \"label\": \"384x256 / 24f\"},\n",
    "        {\"w\": 512, \"h\": 320, \"frames\": 16, \"label\": \"512x320 / 16f\"},\n",
    "        {\"w\": 512, \"h\": 320, \"frames\": 32, \"label\": \"512x320 / 32f\"},\n",
    "    ]\n",
    "    \n",
    "    config_results = []\n",
    "    \n",
    "    for cfg in configs:\n",
    "        print(f\"\\nTest : {cfg['label']}\")\n",
    "        \n",
    "        # Modifier temporairement les parametres globaux\n",
    "        orig_w, orig_h, orig_f = width, height, num_frames\n",
    "        original_steps = num_inference_steps\n",
    "        \n",
    "        # Variables locales pour la generation\n",
    "        gen_width = cfg['w']\n",
    "        gen_height = cfg['h']\n",
    "        gen_frames = cfg['frames']\n",
    "        \n",
    "        try:\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            generator = torch.Generator(device=device).manual_seed(42)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = pipe(\n",
    "                prompt=resolution_prompt,\n",
    "                negative_prompt=\"low quality, blurry, distorted\",\n",
    "                num_frames=gen_frames,\n",
    "                guidance_scale=6.0,\n",
    "                num_inference_steps=20,\n",
    "                height=gen_height,\n",
    "                width=gen_width,\n",
    "                generator=generator\n",
    "            )\n",
    "            \n",
    "            gen_time = time.time() - start_time\n",
    "            frames = output.frames[0]\n",
    "            \n",
    "            vram_peak = 0\n",
    "            if device == \"cuda\":\n",
    "                vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "            \n",
    "            config_results.append({\n",
    "                \"label\": cfg['label'],\n",
    "                \"frames\": frames,\n",
    "                \"time\": gen_time,\n",
    "                \"vram_peak\": vram_peak,\n",
    "                \"n_frames\": gen_frames,\n",
    "                \"resolution\": f\"{gen_width}x{gen_height}\"\n",
    "            })\n",
    "            \n",
    "            print(f\"  Temps : {gen_time:.1f}s, VRAM pic : {vram_peak:.1f} GB\")\n",
    "            \n",
    "            # Sauvegarder en MP4\n",
    "            if save_as_mp4:\n",
    "                mp4_path = OUTPUT_DIR / f\"hunyuan_{cfg['label'].replace(' / ', '_').replace('x', '_')}.mp4\"\n",
    "                export_to_video(frames, str(mp4_path), fps=fps_output)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur : {type(e).__name__}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Tableau recapitulatif\n",
    "    if config_results:\n",
    "        print(f\"\\n{'Configuration':<25} {'Temps (s)':<12} {'VRAM (GB)':<12} {'Duree video':<15}\")\n",
    "        print(\"-\" * 64)\n",
    "        for cr in config_results:\n",
    "            duration = cr['n_frames'] / fps_output\n",
    "            print(f\"  {cr['label']:<25} {cr['time']:<12.1f} {cr['vram_peak']:<12.1f} {duration:.1f}s\")\n",
    "else:\n",
    "    print(\"Test resolution/duree : generation desactivee\")\n",
    "    print(\"\\nGuide resolution/VRAM :\")\n",
    "    print(\"  384x256 : ~14 GB, rapide, basse qualite\")\n",
    "    print(\"  512x320 : ~18 GB, bon compromis (recommande)\")\n",
    "    print(\"  640x480 : ~22 GB, haute qualite, lent\")\n",
    "    print(\"  720p    : ~28 GB+, necessite quantification avancee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {
    "papermill": {
     "duration": 0.00289,
     "end_time": "2026-02-19T09:29:32.049566",
     "exception": false,
     "start_time": "2026-02-19T09:29:32.046676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Resolution et duree\n",
    "\n",
    "### MODE PEDAGOGIQUE (GPU non disponible)\n",
    "\n",
    "Sur un environnement GPU (RTX 3090, 24GB VRAM), ce code g√©n√©rerait:\n",
    "\n",
    "| Param√®tre | Valeur |\n",
    "|-----------|--------|\n",
    "| **Device** | cuda (RTX 3090/4090) |\n",
    "| **VRAM utilis√©e** | ~14-22 GB (selon config) |\n",
    "| **Temps par g√©n√©ration** | 20-60 secondes |\n",
    "| **Configurations test√©es** | 3 r√©solutions/dur√©es |\n",
    "\n",
    "**R√©sultat attendu:**\n",
    "HunyuanVideo g√©n√©rerait 3 vid√©os du golden retriever avec differentes configurations:\n",
    "\n",
    "| Configuration | VRAM | Temps relatif | Dur√©e vid√©o | Qualit√© |\n",
    "|--------------|------|---------------|-------------|---------|\n",
    "| **384x256 / 24f** | ~14 GB | 1x (20s) | 3s @ 8fps | Basique, rapide |\n",
    "| **512x320 / 16f** | ~16 GB | 1.2x (24s) | 2s @ 8fps | Bon compromis |\n",
    "| **512x320 / 32f** | ~20 GB | 2x (40s) | 4s @ 8fps | Haute qualit√© |\n",
    "\n",
    "**Description visuelle:**\n",
    "\n",
    "- **384x256 / 24f**: Chien courant visible, textures simplifi√©es, mouvement fluide mais basse r√©solution\n",
    "- **512x320 / 16f**: Meilleure r√©solution, pelage plus d√©taill√©, dur√©e plus courte\n",
    "- **512x320 / 32f**: Meilleure qualit√© globale, dur√©e plus longue, coh√©rence temporelle excellente\n",
    "\n",
    "**Analyse des compromis:**\n",
    "\n",
    "| Aspect | Augmente avec... | Impact |\n",
    "|--------|-----------------|--------|\n",
    "| **VRAM** | R√©solution (W x H) | Plus de pixels = plus de memoire |\n",
    "| **VRAM** | Nombre de frames | Lineaire avec la dur√©e |\n",
    "| **Temps** | Frames + Steps | Proportionnel |\n",
    "| **Qualit√©** | R√©solution | Details visuels |\n",
    "\n",
    "**Code pour reproduire:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from diffusers import HunyuanVideoPipeline\n",
    "\n",
    "pipe = HunyuanVideoPipeline.from_pretrained(\n",
    "    \"tencent/HunyuanVideo\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "pipe.enable_vae_slicing()\n",
    "pipe.enable_vae_tiling()\n",
    "\n",
    "configs = [\n",
    "    {\"w\": 384, \"h\": 256, \"frames\": 24},\n",
    "    {\"w\": 512, \"h\": 320, \"frames\": 16},\n",
    "    {\"w\": 512, \"h\": 320, \"frames\": 32},\n",
    "]\n",
    "\n",
    "prompt = \"a golden retriever running through a field of sunflowers\"\n",
    "\n",
    "for cfg in configs:\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"low quality\",\n",
    "        num_frames=cfg['frames'],\n",
    "        guidance_scale=6.0,\n",
    "        num_inference_steps=20,\n",
    "        height=cfg['h'],\n",
    "        width=cfg['w'],\n",
    "        generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:32.058546Z",
     "iopub.status.busy": "2026-02-19T09:29:32.058269Z",
     "iopub.status.idle": "2026-02-19T09:29:32.067100Z",
     "shell.execute_reply": "2026-02-19T09:29:32.066543Z"
    },
    "papermill": {
     "duration": 0.015572,
     "end_time": "2026-02-19T09:29:32.068170",
     "exception": false,
     "start_time": "2026-02-19T09:29:32.052598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison de prompts : generation desactivee\n",
      "\n",
      "Types de prompts efficaces pour HunyuanVideo :\n",
      "  - Mouvements naturels : eau, feu, nuages, vent\n",
      "  - Scenes cinematographiques : camera aerienne, slow motion\n",
      "  - Timelapse : nuages, coucher de soleil, fleurs\n",
      "  - Animaux en mouvement : vol d'oiseau, course de chien\n"
     ]
    }
   ],
   "source": [
    "# Comparaison de prompts et analyse qualite\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- COMPARAISON DE PROMPTS ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    prompts = [\n",
    "        {\n",
    "            \"text\": \"a candle flame flickering gently in a dark room, warm light, intimate atmosphere, close-up\",\n",
    "            \"label\": \"Bougie\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"ocean waves rolling onto a sandy beach at sunset, aerial view, golden hour lighting\",\n",
    "            \"label\": \"Ocean\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"a timelapse of clouds moving over a mountain landscape, dramatic sky, epic scale\",\n",
    "            \"label\": \"Timelapse\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for p_idx, prompt_info in enumerate(prompts):\n",
    "        print(f\"\\nGeneration {p_idx + 1}/{len(prompts)} : {prompt_info['label']}\")\n",
    "        print(f\"  Prompt : {prompt_info['text'][:70]}...\")\n",
    "        \n",
    "        result = generate_hunyuan_video(prompt_info['text'], seed=42 + p_idx)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  Temps : {result['generation_time']:.1f}s\")\n",
    "            comparison_results.append({\n",
    "                \"label\": prompt_info['label'],\n",
    "                \"prompt\": prompt_info['text'],\n",
    "                \"frames\": result['frames'],\n",
    "                \"time\": result['generation_time']\n",
    "            })\n",
    "            \n",
    "            if save_as_mp4:\n",
    "                mp4_path = OUTPUT_DIR / f\"hunyuan_{prompt_info['label'].lower()}.mp4\"\n",
    "                export_to_video(result['frames'], str(mp4_path), fps=fps_output)\n",
    "        else:\n",
    "            print(f\"  Erreur : {result['error']}\")\n",
    "    \n",
    "    # Affichage comparatif\n",
    "    if comparison_results:\n",
    "        n_videos = len(comparison_results)\n",
    "        n_preview = 4\n",
    "        fig, axes = plt.subplots(n_videos, n_preview, figsize=(3.5 * n_preview, 3 * n_videos))\n",
    "        if n_videos == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, cr in enumerate(comparison_results):\n",
    "            frame_indices = np.linspace(0, len(cr['frames']) - 1, n_preview, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(cr['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(cr['label'], fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Comparaison de prompts - HunyuanVideo\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyse de coherence temporelle (difference entre frames consecutives)\n",
    "        print(f\"\\nAnalyse de coherence temporelle :\")\n",
    "        print(f\"{'Prompt':<15} {'Temps (s)':<12} {'Diff moy frames':<18} {'Stabilite':<15}\")\n",
    "        print(\"-\" * 60)\n",
    "        for cr in comparison_results:\n",
    "            # Calculer la difference moyenne entre frames consecutives\n",
    "            diffs = []\n",
    "            for i in range(len(cr['frames']) - 1):\n",
    "                f1 = np.array(cr['frames'][i]).astype(float)\n",
    "                f2 = np.array(cr['frames'][i + 1]).astype(float)\n",
    "                diff = np.mean(np.abs(f1 - f2))\n",
    "                diffs.append(diff)\n",
    "            avg_diff = np.mean(diffs)\n",
    "            stability = \"Haute\" if avg_diff < 15 else \"Moyenne\" if avg_diff < 30 else \"Basse\"\n",
    "            print(f\"  {cr['label']:<15} {cr['time']:<12.1f} {avg_diff:<18.2f} {stability:<15}\")\n",
    "else:\n",
    "    print(\"Comparaison de prompts : generation desactivee\")\n",
    "    print(\"\\nTypes de prompts efficaces pour HunyuanVideo :\")\n",
    "    print(\"  - Mouvements naturels : eau, feu, nuages, vent\")\n",
    "    print(\"  - Scenes cinematographiques : camera aerienne, slow motion\")\n",
    "    print(\"  - Timelapse : nuages, coucher de soleil, fleurs\")\n",
    "    print(\"  - Animaux en mouvement : vol d'oiseau, course de chien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:32.075171Z",
     "iopub.status.busy": "2026-02-19T09:29:32.074622Z",
     "iopub.status.idle": "2026-02-19T09:29:32.081564Z",
     "shell.execute_reply": "2026-02-19T09:29:32.081063Z"
    },
    "papermill": {
     "duration": 0.011719,
     "end_time": "2026-02-19T09:29:32.082685",
     "exception": false,
     "start_time": "2026-02-19T09:29:32.070966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mode batch - Interface interactive desactivee\n"
     ]
    }
   ],
   "source": [
    "# Mode interactif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Entrez votre propre prompt pour generer une video HunyuanVideo.\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "    \n",
    "    try:\n",
    "        user_prompt = input(\"\\nVotre prompt : \").strip()\n",
    "        \n",
    "        if user_prompt and run_generation and pipe is not None:\n",
    "            print(f\"\\nGeneration en cours...\")\n",
    "            result_user = generate_hunyuan_video(user_prompt, seed=123)\n",
    "            \n",
    "            if result_user['success']:\n",
    "                print(f\"Generation reussie en {result_user['generation_time']:.1f}s\")\n",
    "                \n",
    "                # Affichage\n",
    "                n_display = min(8, len(result_user['frames']))\n",
    "                fig, axes = plt.subplots(1, n_display, figsize=(2.5 * n_display, 3))\n",
    "                if n_display == 1:\n",
    "                    axes = [axes]\n",
    "                indices = np.linspace(0, len(result_user['frames']) - 1, n_display, dtype=int)\n",
    "                for ax, idx in zip(axes, indices):\n",
    "                    ax.imshow(result_user['frames'][idx])\n",
    "                    ax.set_title(f\"Frame {idx+1}\", fontsize=8)\n",
    "                    ax.axis('off')\n",
    "                plt.suptitle(f\"Votre video : {user_prompt[:50]}...\", fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                if save_as_mp4:\n",
    "                    user_mp4 = OUTPUT_DIR / \"user_generation.mp4\"\n",
    "                    export_to_video(result_user['frames'], str(user_mp4), fps=fps_output)\n",
    "                    print(f\"MP4 sauvegarde : {user_mp4.name}\")\n",
    "            else:\n",
    "                print(f\"Erreur : {result_user['error']}\")\n",
    "        elif user_prompt:\n",
    "            print(\"Generation non disponible (pipeline non charge)\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError) as e:\n",
    "        print(f\"\\nMode interactif interrompu ({type(e).__name__})\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"\\nMode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"\\nErreur inattendue : {error_type} - {str(e)[:100]}\")\n",
    "            print(\"Passage a la suite du notebook\")\n",
    "else:\n",
    "    print(\"\\nMode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {
    "papermill": {
     "duration": 0.003803,
     "end_time": "2026-02-19T09:29:32.090288",
     "exception": false,
     "start_time": "2026-02-19T09:29:32.086485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bonnes pratiques et optimisation HunyuanVideo\n",
    "\n",
    "### Conseils de prompt engineering\n",
    "\n",
    "| Bon prompt | Mauvais prompt | Raison |\n",
    "|-----------|---------------|--------|\n",
    "| \"a bird flying over a lake, aerial shot, cinematic\" | \"bird lake\" | Preciser l'action et le style |\n",
    "| \"timelapse of sunset, clouds moving, warm colors\" | \"nice sunset video\" | Indiquer le type de mouvement |\n",
    "| \"close-up of rain drops on a window\" | \"rain\" | Le cadrage guide la generation |\n",
    "\n",
    "### Comparaison avec les autres modeles du Module 02\n",
    "\n",
    "| Aspect | HunyuanVideo | LTX-Video (02-2) | Wan (02-3) | SVD (02-4) |\n",
    "|--------|-------------|------------------|-----------|------------|\n",
    "| Type | Text-to-video | Text/Img/Vid | Text-to-video | Image-to-video |\n",
    "| VRAM | ~18 GB | ~8 GB | ~10 GB | ~10 GB |\n",
    "| Qualite | Haute | Moyenne | Haute | Haute |\n",
    "| Vitesse | Lente | Rapide | Moyenne | Moyenne |\n",
    "| Resolution max | 720p | 512p | 720p | 576p |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:32.096903Z",
     "iopub.status.busy": "2026-02-19T09:29:32.096655Z",
     "iopub.status.idle": "2026-02-19T09:29:32.103089Z",
     "shell.execute_reply": "2026-02-19T09:29:32.102580Z"
    },
    "papermill": {
     "duration": 0.011144,
     "end_time": "2026-02-19T09:29:32.104207",
     "exception": false,
     "start_time": "2026-02-19T09:29:32.093063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STATISTIQUES DE SESSION ---\n",
      "========================================\n",
      "Date : 2026-02-19 10:29:32\n",
      "Mode : batch\n",
      "Modele : tencent/HunyuanVideo\n",
      "Quantification : FP16\n",
      "Device : cpu\n",
      "Parametres : 24 frames, 30 steps, CFG=6.0\n",
      "Resolution : 512x320\n",
      "\n",
      "Fichiers generes (0) :\n",
      "\n",
      "--- PROCHAINES ETAPES ---\n",
      "1. Notebook 02-2 : LTX-Video (generation rapide et legere, ~8 GB VRAM)\n",
      "2. Notebook 02-3 : Wan 2.1/2.2 (prompts multilingues, motion control)\n",
      "3. Notebook 02-4 : SVD (animation d'images statiques)\n",
      "4. Module 03 : Comparaison multi-modeles et orchestration de pipelines\n",
      "\n",
      "Notebook 02-1 HunyuanVideo Generation termine - 10:29:32\n"
     ]
    }
   ],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Modele : {model_id}\")\n",
    "print(f\"Quantification : {'INT8' if quantize else 'FP16'}\")\n",
    "print(f\"Device : {device}\")\n",
    "print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n",
    "print(f\"Resolution : {width}x{height}\")\n",
    "\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "    print(f\"VRAM pic session : {vram_peak:.1f} GB\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "# Liberation VRAM\n",
    "if pipe is not None:\n",
    "    del pipe\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nVRAM liberee\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 02-2 : LTX-Video (generation rapide et legere, ~8 GB VRAM)\")\n",
    "print(f\"2. Notebook 02-3 : Wan 2.1/2.2 (prompts multilingues, motion control)\")\n",
    "print(f\"3. Notebook 02-4 : SVD (animation d'images statiques)\")\n",
    "print(f\"4. Module 03 : Comparaison multi-modeles et orchestration de pipelines\")\n",
    "\n",
    "print(f\"\\nNotebook 02-1 HunyuanVideo Generation termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.371373,
   "end_time": "2026-02-19T09:29:33.226785",
   "environment_variables": {},
   "exception": null,
   "input_path": "MyIA.AI.Notebooks\\GenAI\\Video\\02-Advanced\\02-1-HunyuanVideo-Generation.ipynb",
   "output_path": "MyIA.AI.Notebooks\\GenAI\\Video\\02-Advanced\\02-1-HunyuanVideo-Generation.ipynb",
   "parameters": {
    "notebook_mode": "batch",
    "skip_widgets": true
   },
   "start_time": "2026-02-19T09:29:25.855412",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}