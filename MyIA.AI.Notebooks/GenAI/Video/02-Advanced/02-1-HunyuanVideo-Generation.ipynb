{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb31133",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [7]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.004941,
     "end_time": "2026-02-26T07:08:13.175488",
     "exception": false,
     "start_time": "2026-02-26T07:08:13.170547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# HunyuanVideo - Generation Video Haute Qualite**Module :** 02-Video-Advanced  **Niveau :** Intermediaire  **Technologies :** HunyuanVideo 1.5 (Tencent), ComfyUI API ou diffusers  **Duree estimee :** 60 minutes  **VRAM :** ~12 GB (API) ou ~18 GB (local avec INT8)  ## Objectifs d'Apprentissage- [ ] Comprendre l'architecture HunyuanVideo et ses avantages- [ ] Choisir entre API ComfyUI (production) et diffusers (pedagogique)- [ ] Generer des videos text-to-video avec des prompts detailles- [ ] Explorer les parametres de generation (steps, guidance_scale, num_frames, fps)- [ ] Controler la resolution et la duree des videos- [ ] Sauvegarder les resultats en MP4 avec imageio- [ ] Analyser la qualite et les metriques de generation## Prerequis### Mode API ComfyUI (recommande pour production)- Service ComfyUI-Video demarre (docker-compose comfyui-video)- Pas de dependances Python lourdes cote client### Mode Local diffusers (pedagogique)- GPU avec 18+ GB VRAM (RTX 3090 / RTX 4090)- Packages : `diffusers>=0.32`, `transformers`, `torch`, `accelerate`, `bitsandbytes`, `imageio`**Navigation :** [<< 01-5](../01-Foundation/01-5-AnimateDiff-Introduction.ipynb) | [Index](../README.md) | [Suivant >>](02-2-LTX-Video-Lightweight.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:08:13.184208Z",
     "iopub.status.busy": "2026-02-26T07:08:13.183982Z",
     "iopub.status.idle": "2026-02-26T07:08:13.188075Z",
     "shell.execute_reply": "2026-02-26T07:08:13.187265Z"
    },
    "papermill": {
     "duration": 0.009753,
     "end_time": "2026-02-26T07:08:13.189528",
     "exception": false,
     "start_time": "2026-02-26T07:08:13.179775",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell was removed - parameters are in Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c54c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:08:13.203154Z",
     "iopub.status.busy": "2026-02-26T07:08:13.202699Z",
     "iopub.status.idle": "2026-02-26T07:08:13.206723Z",
     "shell.execute_reply": "2026-02-26T07:08:13.206037Z"
    },
    "papermill": {
     "duration": 0.010864,
     "end_time": "2026-02-26T07:08:13.208164",
     "exception": false,
     "start_time": "2026-02-26T07:08:13.197300",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters# Configuration notebooknotebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"skip_widgets = False               # True pour mode batch MCPdebug_level = \"INFO\"# MODE D'EXECUTION : API ou Local# - True  : Utilise l'API ComfyUI (recommande, pas de GPU local requis)# - False : Utilise diffusers en local (pedagogique, necessite GPU)use_api = True# Parametres API ComfyUI (si use_api=True)comfyui_url = \"http://localhost:8189\"  # ComfyUI-Video servicecomfyui_token = None                 # Token Bearer (optionnel pour localhost)# Parametres modele HunyuanVideo (si use_api=False)model_id = \"tencent/HunyuanVideo\"  # Modele HunyuanVideoquantize = True                      # Quantification INT8 (recommande)device = \"cuda\"                     # Device de calcul# Parametres generation (communs aux deux modes)num_frames = 33                    # Nombre de frames a generer (HunyuanVideo optimal)guidance_scale = 7.0               # CFG scale (7.0 recommande pour HunyuanVideo)num_inference_steps = 30           # Nombre d'etapes de debruitageheight = 720                       # Hauteur video (720p optimal)width = 1280                       # Largeur videofps_output = 24                    # FPS de la video de sortie# Configurationrun_generation = True              # Executer la generationsave_as_mp4 = True                 # Sauvegarder en MP4save_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:08:13.218599Z",
     "iopub.status.busy": "2026-02-26T07:08:13.218347Z",
     "iopub.status.idle": "2026-02-26T07:08:13.222019Z",
     "shell.execute_reply": "2026-02-26T07:08:13.221442Z"
    },
    "papermill": {
     "duration": 0.0109,
     "end_time": "2026-02-26T07:08:13.223670",
     "exception": false,
     "start_time": "2026-02-26T07:08:13.212770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup environnement et importsimport osimport sysimport jsonimport timeimport warningsfrom pathlib import Pathfrom datetime import datetimefrom typing import Dict, List, Any, Optionalimport numpy as npfrom PIL import Imageimport matplotlib.pyplot as pltimport loggingfrom dotenv import load_dotenvwarnings.filterwarnings('ignore', category=DeprecationWarning)warnings.filterwarnings('ignore', category=FutureWarning)# Import helpers GenAIGENAI_ROOT = Path.cwd()while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:    GENAI_ROOT = GENAI_ROOT.parentHELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'if HELPERS_PATH.exists():    sys.path.insert(0, str(HELPERS_PATH.parent))    try:        from helpers import comfyui_client        print(\"‚úÖ Helper comfyui_client import√©\")    except ImportError as e:        print(f\"‚ö†Ô∏è Helper comfyui_client NON disponible: {e}\")        comfyui_client = NoneOUTPUT_DIR = GENAI_ROOT / 'outputs' / 'hunyuan_video'OUTPUT_DIR.mkdir(parents=True, exist_ok=True)logging.basicConfig(level=getattr(logging, debug_level))logger = logging.getLogger('hunyuan_video')# Affichage du mode d'executionmode_str = \"API ComfyUI\" if use_api else \"Local diffusers\"print(f\"HunyuanVideo 1.5 - Generation Video Haute Qualite\")print(f\"Mode d'execution : {mode_str}\")print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")print(f\"Frames : {num_frames}, Steps : {num_inference_steps}, CFG : {guidance_scale}\")print(f\"Resolution : {width}x{height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:08:13.232321Z",
     "iopub.status.busy": "2026-02-26T07:08:13.231965Z",
     "iopub.status.idle": "2026-02-26T07:08:13.236490Z",
     "shell.execute_reply": "2026-02-26T07:08:13.235644Z"
    },
    "papermill": {
     "duration": 0.01007,
     "end_time": "2026-02-26T07:08:13.237903",
     "exception": false,
     "start_time": "2026-02-26T07:08:13.227833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chargement .env et verification de l'environnementcurrent_path = Path.cwd()found_env = Falsefor _ in range(4):    env_path = current_path / '.env'    if env_path.exists():        load_dotenv(env_path)        print(f\"‚úÖ Fichier .env charge depuis : {env_path}\")        found_env = True        break    current_path = current_path.parentif not found_env:    print(\"‚ö†Ô∏è Aucun fichier .env trouve\")# Verification et initialisation selon le modeprint(\"\\n\" + \"=\" * 50)print(f\"MODE : {'API ComfyUI' if use_api else 'Local diffusers'}\")print(\"=\" * 50)client = Nonepipe = Nonecomfyui_available = Falselocal_available = Falseif use_api:    # === MODE API COMFYUI ===    print(\"\\nüì° Verification de l'API ComfyUI-Video...\")        if comfyui_client is not None:        try:            client = comfyui_client.ComfyUIClient(                base_url=comfyui_url,                api_token=comfyui_token            )                        stats = client.get_system_stats()                        print(f\"‚úÖ ComfyUI-Video accessible sur : {comfyui_url}\")            comfyui_available = True                    except Exception as e:            print(f\"‚ö†Ô∏è ComfyUI-Video non accessible: {type(e).__name__}: {str(e)[:100]}\")            print(\"\\nüí° Pour d√©marrer ComfyUI-Video :\")            print(\"   docker-compose -f docker-configurations/services/comfyui-video/docker-compose.yml up -d\")            run_generation = False    else:        print(\"‚ö†Ô∏è Helper comfyui_client non disponible\")        run_generation = False        else:    # === MODE LOCAL DIFFUSERS ===    print(\"\\nüîß Verification de l'environnement local...\")        # Verification GPU    try:        import torch        if torch.cuda.is_available():            gpu_name = torch.cuda.get_device_name(0)            vram_total = torch.cuda.get_device_properties(0).total_mem / 1024**3            print(f\"‚úÖ GPU : {gpu_name}\")            print(f\"   VRAM totale : {vram_total:.1f} GB\")                        if vram_total < 18:                print(f\"‚ö†Ô∏è VRAM faible (< 18 GB), activation de la quantification\")                quantize = True                if vram_total < 12:                    height = 480                    width = 640                    num_frames = 24                    print(f\"  Resolution reduite a {width}x{height}, {num_frames} frames\")        else:            print(\"‚ö†Ô∏è CUDA non disponible\")            run_generation = False    except ImportError:        print(\"‚ö†Ô∏è PyTorch non install√©\")        run_generation = False        # Verification des dependances    deps_ok = True        try:        import diffusers        print(f\"‚úÖ diffusers : v{diffusers.__version__}\")    except ImportError:        print(\"‚ö†Ô∏è diffusers NON INSTALLE (pip install diffusers>=0.32)\")        deps_ok = False        try:        import transformers        print(f\"‚úÖ transformers : v{transformers.__version__}\")    except ImportError:        print(\"‚ö†Ô∏è transformers NON INSTALLE\")        deps_ok = False        if quantize:        try:            import bitsandbytes as bnb            print(f\"‚úÖ bitsandbytes : v{bnb.__version__}\")        except ImportError:            print(\"‚ö†Ô∏è bitsandbytes NON INSTALLE (pip install bitsandbytes)\")            quantize = False        try:        import imageio        print(f\"‚úÖ imageio : v{imageio.__version__}\")    except ImportError:        print(\"‚ö†Ô∏è imageio NON INSTALLE\")        deps_ok = False        if deps_ok and run_generation:        print(\"\\nüì¶ Chargement du pipeline HunyuanVideo...\")        try:            from diffusers import HunyuanVideoPipeline            from diffusers.utils import export_to_video                        start_load = time.time()                        if quantize:                from diffusers import BitsAndBytesConfig                quant_config = BitsAndBytesConfig(load_in_8bit=True)                pipe = HunyuanVideoPipeline.from_pretrained(                    model_id,                    quantization_config=quant_config,                    torch_dtype=torch.float16                )            else:                pipe = HunyuanVideoPipeline.from_pretrained(                    model_id,                    torch_dtype=torch.float16                )                        pipe = pipe.to(device)            pipe.enable_vae_slicing()            pipe.enable_vae_tiling()                        load_time = time.time() - start_load            print(f\"‚úÖ Pipeline charge en {load_time:.1f}s\")            local_available = True                    except Exception as e:            print(f\"‚ö†Ô∏è Erreur chargement pipeline : {type(e).__name__}: {str(e)[:200]}\")            run_generation = Falseprint(f\"\\n{'='*50}\")print(f\"Generation activee : {run_generation}\")print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {
    "papermill": {
     "duration": 0.002376,
     "end_time": "2026-02-26T07:08:13.244666",
     "exception": false,
     "start_time": "2026-02-26T07:08:13.242290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 1 : Architecture HunyuanVideoHunyuanVideo est un modele de generation text-to-video open-source developpe par Tencent.Il se distingue par sa qualite de generation et sa capacite a produire des videos longuesavec une bonne coherence temporelle.### Deux approches pour utiliser HunyuanVideo| Aspect | API ComfyUI | Local diffusers ||--------|-------------|-----------------|| **Cas d'usage** | Production, applications | Pedagogie, recherche || **GPU requis** | Non (cote serveur) | Oui (18+ GB) || **Installation** | Aucune (Docker) | diffusers, transformers, torch || **Flexibilite** | Moyenne | Elevee || **Performance** | Serveur optimise | Depend du GPU local |### Architecture de HunyuanVideo| Composant | Description ||-----------|-------------|| **Backbone** | Transformer 3D avec attention spatio-temporelle || **Text encoders** | DualCLIP (clip_l + llava_llama3) || **VAE** | Encodeur/decodeur video avec compression temporelle || **Scheduler** | Flow matching pour un debruitage progressif |### Avantages par rapport a AnimateDiff| Aspect | AnimateDiff (01-5) | HunyuanVideo ||--------|-------------------|---------------|| Architecture | SD 1.5 + motion module | Transformer 3D natif || Resolution | 512x512 max | Jusqu'a 720p || Coherence temporelle | Moyenne | Elevee || Duree video | 2-3 secondes | 5+ secondes || VRAM | ~12 GB | ~18 GB (INT8) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:08:13.250828Z",
     "iopub.status.busy": "2026-02-26T07:08:13.250557Z",
     "iopub.status.idle": "2026-02-26T07:08:13.254532Z",
     "shell.execute_reply": "2026-02-26T07:08:13.253816Z"
    },
    "papermill": {
     "duration": 0.008393,
     "end_time": "2026-02-26T07:08:13.255569",
     "exception": false,
     "start_time": "2026-02-26T07:08:13.247176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fonction de generation unifiee (API ou Local)def generate_hunyuan_video(prompt: str, negative_prompt: str = \"\", seed: int = 42) -> Dict[str, Any]:    \"\"\"    Genere une video avec HunyuanVideo (API ComfyUI ou local diffusers).        Cette fonction s'adapte automatiquement au mode d'execution choisi.        Args:        prompt: Description textuelle de la video        negative_prompt: Elements a eviter        seed: Graine aleatoire pour reproductibilite        Returns:        Dict avec frames, temps de generation et metadonnees    \"\"\"    if use_api:        # === MODE API COMFYUI ===        if not comfyui_available:            return {\"success\": False, \"error\": \"API ComfyUI non disponible\"}                try:            start_time = time.time()                        result = client.generate_text2video_hunyuan(                prompt=prompt,                width=width,                height=height,                num_frames=num_frames,                steps=num_inference_steps,                seed=seed,                cfg=guidance_scale,                negative_prompt=negative_prompt or \"bad quality, low quality, blurry, distortion\",                save_prefix=f\"hunyuan_gen_{seed}\",                timeout=600            )                        gen_time = time.time() - start_time                        return {                \"success\": True,                \"result\": result,                \"generation_time\": gen_time,                \"mode\": \"API ComfyUI\",                \"seed\": seed            }                    except Exception as e:            return {\"success\": False, \"error\": f\"{type(e).__name__}: {str(e)[:200]}\"}        else:        # === MODE LOCAL DIFFUSERS ===        if not local_available:            return {\"success\": False, \"error\": \"Pipeline local non disponible\"}                try:            import torch            from diffusers.utils import export_to_video                        generator = torch.Generator(device=device).manual_seed(seed)                        if device == \"cuda\":                torch.cuda.reset_peak_memory_stats()                        start_time = time.time()                        output = pipe(                prompt=prompt,                negative_prompt=negative_prompt or \"bad quality, low quality, blurry, distortion, artifacts\",                num_frames=num_frames,                guidance_scale=guidance_scale,                num_inference_steps=num_inference_steps,                height=height,                width=width,                generator=generator            )                        gen_time = time.time() - start_time            frames = output.frames[0]                        # Sauvegarder en MP4            mp4_path = OUTPUT_DIR / f\"hunyuan_local_{seed}.mp4\"            export_to_video(frames, str(mp4_path), fps=fps_output)                        result_dict = {                \"success\": True,                \"frames\": frames,                \"generation_time\": gen_time,                \"time_per_frame\": gen_time / num_frames,                \"prompt\": prompt,                \"seed\": seed,                \"mode\": \"Local diffusers\",                \"mp4_path\": str(mp4_path)            }                        if device == \"cuda\":                result_dict[\"vram_peak\"] = torch.cuda.max_memory_allocated(0) / 1024**3                        return result_dict                    except Exception as e:            return {\"success\": False, \"error\": f\"{type(e).__name__}: {str(e)[:200]}\"}print(\"‚úÖ Fonction de generation unifiee chargee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:08:13.261532Z",
     "iopub.status.busy": "2026-02-26T07:08:13.261307Z",
     "iopub.status.idle": "2026-02-26T07:08:13.265541Z",
     "shell.execute_reply": "2026-02-26T07:08:13.264716Z"
    },
    "papermill": {
     "duration": 0.009797,
     "end_time": "2026-02-26T07:08:13.268025",
     "exception": false,
     "start_time": "2026-02-26T07:08:13.258228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generation text-to-videoprint(\"\\n--- GENERATION TEXT-TO-VIDEO ---\")print(\"=\" * 40)# Premier test : prompt cinematographiqueprompt_1 = \"a majestic eagle soaring over snow-capped mountains at golden hour, cinematic aerial shot, smooth camera movement, volumetric clouds\"if run_generation:    print(f\"Prompt : {prompt_1}\")    print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")    print(f\"Resolution : {width}x{height}\")    print(f\"Mode : {'API ComfyUI' if use_api else 'Local diffusers'}\")    print(f\"\\nGeneration en cours...\")        result_1 = generate_hunyuan_video(prompt_1, seed=42)        if result_1['success']:        print(f\"\\n‚úÖ Generation terminee en {result_1['generation_time']:.1f}s ({result_1['mode']})\")        if 'vram_peak' in result_1:            print(f\"   VRAM pic : {result_1['vram_peak']:.1f} GB\")        if 'time_per_frame' in result_1:            print(f\"   Temps/frame : {result_1['time_per_frame']:.2f}s\")                # Affichage si frames disponibles        if 'frames' in result_1:            frames = result_1['frames']            print(f\"   Frames : {len(frames)}\")                        n_display = min(8, len(frames))            indices = np.linspace(0, len(frames) - 1, n_display, dtype=int)            fig, axes = plt.subplots(2, 4, figsize=(16, 8))            axes_flat = axes.flatten()            for i, idx in enumerate(indices):                if i < len(axes_flat):                    axes_flat[i].imshow(frames[idx])                    axes_flat[i].set_title(f\"Frame {idx + 1}/{len(frames)}\", fontsize=9)                    axes_flat[i].axis('off')            for i in range(len(indices), len(axes_flat)):                axes_flat[i].axis('off')            plt.suptitle(f\"HunyuanVideo : {prompt_1[:60]}...\", fontsize=11, fontweight='bold')            plt.tight_layout()            plt.show()                # Sauvegarde MP4 (mode local seulement)        if save_as_mp4 and 'mp4_path' in result_1:            from pathlib import Path            mp4_path = Path(result_1['mp4_path'])            if mp4_path.exists():                mp4_size_kb = mp4_path.stat().st_size / 1024                print(f\"   MP4 sauvegarde : {mp4_path.name} ({mp4_size_kb:.1f} KB)\")    else:        print(f\"‚ùå Erreur : {result_1['error']}\")else:    print(\"Generation desactivee\")    print(f\"\\nExemple de code pour generer :\")    print(f\"  result = generate_hunyuan_video('{prompt_1[:50]}...', seed=42)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {
    "papermill": {
     "duration": 0.002507,
     "end_time": "2026-02-26T07:08:13.275048",
     "exception": false,
     "start_time": "2026-02-26T07:08:13.272541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exploration des parametres\n",
    "# Ensure run_generation is defined (fallback for Papermill exception recovery)\n",
    "if 'run_generation' not in dir():\n",
    "    run_generation = False\n",
    "if 'pipe' not in dir():\n",
    "    pipe = None\n",
    "\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- EXPLORATION DES PARAMETRES ---\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Test avec differentes valeurs de guidance_scale\n",
    "    test_prompt = \"a serene waterfall in a lush forest, sunlight filtering through trees, mist rising\"\n",
    "    \n",
    "    cfg_values = [3.0, 6.0, 9.0]\n",
    "    cfg_results = []\n",
    "    \n",
    "    print(f\"Test guidance_scale : {cfg_values}\")\n",
    "    print(f\"Prompt : {test_prompt[:60]}...\")\n",
    "    \n",
    "    for cfg_val in cfg_values:\n",
    "        print(f\"\\n  CFG = {cfg_val}...\")\n",
    "        \n",
    "        # Sauvegarder et modifier temporairement\n",
    "        original_cfg = guidance_scale\n",
    "        original_steps = num_inference_steps\n",
    "        guidance_scale = cfg_val\n",
    "        num_inference_steps = 20  # Reduit pour acceleration\n",
    "        \n",
    "        result = generate_hunyuan_video(test_prompt, seed=42)\n",
    "        \n",
    "        # Restaurer\n",
    "        guidance_scale = original_cfg\n",
    "        num_inference_steps = original_steps\n",
    "        \n",
    "        if result['success']:\n",
    "            cfg_results.append({\n",
    "                \"cfg\": cfg_val,\n",
    "                \"frames\": result['frames'],\n",
    "                \"time\": result['generation_time'],\n",
    "                \"vram_peak\": result.get('vram_peak', 0)\n",
    "            })\n",
    "            print(f\"    Temps : {result['generation_time']:.1f}s\")\n",
    "        else:\n",
    "            print(f\"    Erreur : {result['error']}\")\n",
    "    \n",
    "    # Affichage comparatif\n",
    "    if cfg_results:\n",
    "        n_cfgs = len(cfg_results)\n",
    "        n_preview = 4\n",
    "        fig, axes = plt.subplots(n_cfgs, n_preview, figsize=(3.5 * n_preview, 3 * n_cfgs))\n",
    "        if n_cfgs == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, cr in enumerate(cfg_results):\n",
    "            frame_indices = np.linspace(0, len(cr['frames']) - 1, n_preview, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(cr['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(f\"CFG={cr['cfg']}\", fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Impact de guidance_scale sur la generation\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Tableau recapitulatif\n",
    "        print(f\"\\nRecapitulatif guidance_scale :\")\n",
    "        print(f\"{'CFG':<10} {'Temps (s)':<12} {'VRAM pic (GB)':<15}\")\n",
    "        print(\"-\" * 37)\n",
    "        for cr in cfg_results:\n",
    "            print(f\"  {cr['cfg']:<10} {cr['time']:<12.1f} {cr['vram_peak']:<15.1f}\")\n",
    "else:\n",
    "    print(\"Exploration des parametres : generation desactivee\")\n",
    "    print(\"\\nGuide des parametres :\")\n",
    "    print(\"  CFG 3-4 : Creatif, plus de liberte\")\n",
    "    print(\"  CFG 5-7 : Equilibre (recommande)\")\n",
    "    print(\"  CFG 8-10 : Strict, peut introduire des artefacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f044c",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:08:13.280965Z",
     "iopub.status.busy": "2026-02-26T07:08:13.280606Z",
     "iopub.status.idle": "2026-02-26T07:08:13.464765Z",
     "shell.execute_reply": "2026-02-26T07:08:13.463745Z"
    },
    "papermill": {
     "duration": 0.188621,
     "end_time": "2026-02-26T07:08:13.465906",
     "exception": true,
     "start_time": "2026-02-26T07:08:13.277285",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_generation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Exploration des parametres\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrun_generation\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m pipe \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- EXPLORATION DES PARAMETRES ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m45\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'run_generation' is not defined"
     ]
    }
   ],
   "source": [
    "# Exploration des parametres\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- EXPLORATION DES PARAMETRES ---\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Test avec differentes valeurs de guidance_scale\n",
    "    test_prompt = \"a serene waterfall in a lush forest, sunlight filtering through trees, mist rising\"\n",
    "    \n",
    "    cfg_values = [3.0, 6.0, 9.0]\n",
    "    cfg_results = []\n",
    "    \n",
    "    print(f\"Test guidance_scale : {cfg_values}\")\n",
    "    print(f\"Prompt : {test_prompt[:60]}...\")\n",
    "    \n",
    "    for cfg_val in cfg_values:\n",
    "        print(f\"\\n  CFG = {cfg_val}...\")\n",
    "        \n",
    "        # Sauvegarder et modifier temporairement\n",
    "        original_cfg = guidance_scale\n",
    "        original_steps = num_inference_steps\n",
    "        guidance_scale = cfg_val\n",
    "        num_inference_steps = 20  # Reduit pour acceleration\n",
    "        \n",
    "        result = generate_hunyuan_video(test_prompt, seed=42)\n",
    "        \n",
    "        # Restaurer\n",
    "        guidance_scale = original_cfg\n",
    "        num_inference_steps = original_steps\n",
    "        \n",
    "        if result['success']:\n",
    "            cfg_results.append({\n",
    "                \"cfg\": cfg_val,\n",
    "                \"frames\": result['frames'],\n",
    "                \"time\": result['generation_time'],\n",
    "                \"vram_peak\": result.get('vram_peak', 0)\n",
    "            })\n",
    "            print(f\"    Temps : {result['generation_time']:.1f}s\")\n",
    "        else:\n",
    "            print(f\"    Erreur : {result['error']}\")\n",
    "    \n",
    "    # Affichage comparatif\n",
    "    if cfg_results:\n",
    "        n_cfgs = len(cfg_results)\n",
    "        n_preview = 4\n",
    "        fig, axes = plt.subplots(n_cfgs, n_preview, figsize=(3.5 * n_preview, 3 * n_cfgs))\n",
    "        if n_cfgs == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, cr in enumerate(cfg_results):\n",
    "            frame_indices = np.linspace(0, len(cr['frames']) - 1, n_preview, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(cr['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(f\"CFG={cr['cfg']}\", fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Impact de guidance_scale sur la generation\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Tableau recapitulatif\n",
    "        print(f\"\\nRecapitulatif guidance_scale :\")\n",
    "        print(f\"{'CFG':<10} {'Temps (s)':<12} {'VRAM pic (GB)':<15}\")\n",
    "        print(\"-\" * 37)\n",
    "        for cr in cfg_results:\n",
    "            print(f\"  {cr['cfg']:<10} {cr['time']:<12.1f} {cr['vram_peak']:<15.1f}\")\n",
    "else:\n",
    "    print(\"Exploration des parametres : generation desactivee\")\n",
    "    print(\"\\nGuide des parametres :\")\n",
    "    print(\"  CFG 3-4 : Creatif, plus de liberte\")\n",
    "    print(\"  CFG 5-7 : Equilibre (recommande)\")\n",
    "    print(\"  CFG 8-10 : Strict, peut introduire des artefacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Impact des parametres\n",
    "\n",
    "| guidance_scale | Comportement | Recommandation |\n",
    "|---------------|-------------|----------------|\n",
    "| 3.0 (bas) | Creatif, variations, parfois hors-sujet | Exploration creative |\n",
    "| 6.0 (moyen) | Bon equilibre fidelite/creativite | Usage general |\n",
    "| 9.0 (haut) | Tres fidele au prompt, risque artefacts | Prompt precis |\n",
    "\n",
    "**Points cles** :\n",
    "1. Contrairement a Stable Diffusion Image, une CFG trop elevee degrade la coherence temporelle\n",
    "2. Pour HunyuanVideo, la plage 5.0-7.0 donne generalement les meilleurs resultats\n",
    "3. Le temps de generation varie peu avec la CFG (meme nombre de steps)\n",
    "\n",
    "## Section 4 : Resolution et duree\n",
    "\n",
    "Nous allons explorer les compromis entre resolution, nombre de frames et consommation memoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:32.032365Z",
     "iopub.status.busy": "2026-02-19T09:29:32.031540Z",
     "iopub.status.idle": "2026-02-19T09:29:32.042365Z",
     "shell.execute_reply": "2026-02-19T09:29:32.041627Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test de resolution et duree\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- RESOLUTION ET DUREE ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    resolution_prompt = \"a golden retriever running through a field of sunflowers, joyful, sunny day, slow motion\"\n",
    "    \n",
    "    # Configurations a tester (resolution, frames)\n",
    "    configs = [\n",
    "        {\"w\": 384, \"h\": 256, \"frames\": 24, \"label\": \"384x256 / 24f\"},\n",
    "        {\"w\": 512, \"h\": 320, \"frames\": 16, \"label\": \"512x320 / 16f\"},\n",
    "        {\"w\": 512, \"h\": 320, \"frames\": 32, \"label\": \"512x320 / 32f\"},\n",
    "    ]\n",
    "    \n",
    "    config_results = []\n",
    "    \n",
    "    for cfg in configs:\n",
    "        print(f\"\\nTest : {cfg['label']}\")\n",
    "        \n",
    "        # Modifier temporairement les parametres globaux\n",
    "        orig_w, orig_h, orig_f = width, height, num_frames\n",
    "        original_steps = num_inference_steps\n",
    "        \n",
    "        # Variables locales pour la generation\n",
    "        gen_width = cfg['w']\n",
    "        gen_height = cfg['h']\n",
    "        gen_frames = cfg['frames']\n",
    "        \n",
    "        try:\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            generator = torch.Generator(device=device).manual_seed(42)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = pipe(\n",
    "                prompt=resolution_prompt,\n",
    "                negative_prompt=\"low quality, blurry, distorted\",\n",
    "                num_frames=gen_frames,\n",
    "                guidance_scale=6.0,\n",
    "                num_inference_steps=20,\n",
    "                height=gen_height,\n",
    "                width=gen_width,\n",
    "                generator=generator\n",
    "            )\n",
    "            \n",
    "            gen_time = time.time() - start_time\n",
    "            frames = output.frames[0]\n",
    "            \n",
    "            vram_peak = 0\n",
    "            if device == \"cuda\":\n",
    "                vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "            \n",
    "            config_results.append({\n",
    "                \"label\": cfg['label'],\n",
    "                \"frames\": frames,\n",
    "                \"time\": gen_time,\n",
    "                \"vram_peak\": vram_peak,\n",
    "                \"n_frames\": gen_frames,\n",
    "                \"resolution\": f\"{gen_width}x{gen_height}\"\n",
    "            })\n",
    "            \n",
    "            print(f\"  Temps : {gen_time:.1f}s, VRAM pic : {vram_peak:.1f} GB\")\n",
    "            \n",
    "            # Sauvegarder en MP4\n",
    "            if save_as_mp4:\n",
    "                mp4_path = OUTPUT_DIR / f\"hunyuan_{cfg['label'].replace(' / ', '_').replace('x', '_')}.mp4\"\n",
    "                export_to_video(frames, str(mp4_path), fps=fps_output)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur : {type(e).__name__}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Tableau recapitulatif\n",
    "    if config_results:\n",
    "        print(f\"\\n{'Configuration':<25} {'Temps (s)':<12} {'VRAM (GB)':<12} {'Duree video':<15}\")\n",
    "        print(\"-\" * 64)\n",
    "        for cr in config_results:\n",
    "            duration = cr['n_frames'] / fps_output\n",
    "            print(f\"  {cr['label']:<25} {cr['time']:<12.1f} {cr['vram_peak']:<12.1f} {duration:.1f}s\")\n",
    "else:\n",
    "    print(\"Test resolution/duree : generation desactivee\")\n",
    "    print(\"\\nGuide resolution/VRAM :\")\n",
    "    print(\"  384x256 : ~14 GB, rapide, basse qualite\")\n",
    "    print(\"  512x320 : ~18 GB, bon compromis (recommande)\")\n",
    "    print(\"  640x480 : ~22 GB, haute qualite, lent\")\n",
    "    print(\"  720p    : ~28 GB+, necessite quantification avancee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Resolution et duree\n",
    "\n",
    "### MODE PEDAGOGIQUE (GPU non disponible)\n",
    "\n",
    "Sur un environnement GPU (RTX 3090, 24GB VRAM), ce code g√©n√©rerait:\n",
    "\n",
    "| Param√®tre | Valeur |\n",
    "|-----------|--------|\n",
    "| **Device** | cuda (RTX 3090/4090) |\n",
    "| **VRAM utilis√©e** | ~14-22 GB (selon config) |\n",
    "| **Temps par g√©n√©ration** | 20-60 secondes |\n",
    "| **Configurations test√©es** | 3 r√©solutions/dur√©es |\n",
    "\n",
    "**R√©sultat attendu:**\n",
    "HunyuanVideo g√©n√©rerait 3 vid√©os du golden retriever avec differentes configurations:\n",
    "\n",
    "| Configuration | VRAM | Temps relatif | Dur√©e vid√©o | Qualit√© |\n",
    "|--------------|------|---------------|-------------|---------|\n",
    "| **384x256 / 24f** | ~14 GB | 1x (20s) | 3s @ 8fps | Basique, rapide |\n",
    "| **512x320 / 16f** | ~16 GB | 1.2x (24s) | 2s @ 8fps | Bon compromis |\n",
    "| **512x320 / 32f** | ~20 GB | 2x (40s) | 4s @ 8fps | Haute qualit√© |\n",
    "\n",
    "**Description visuelle:**\n",
    "\n",
    "- **384x256 / 24f**: Chien courant visible, textures simplifi√©es, mouvement fluide mais basse r√©solution\n",
    "- **512x320 / 16f**: Meilleure r√©solution, pelage plus d√©taill√©, dur√©e plus courte\n",
    "- **512x320 / 32f**: Meilleure qualit√© globale, dur√©e plus longue, coh√©rence temporelle excellente\n",
    "\n",
    "**Analyse des compromis:**\n",
    "\n",
    "| Aspect | Augmente avec... | Impact |\n",
    "|--------|-----------------|--------|\n",
    "| **VRAM** | R√©solution (W x H) | Plus de pixels = plus de memoire |\n",
    "| **VRAM** | Nombre de frames | Lineaire avec la dur√©e |\n",
    "| **Temps** | Frames + Steps | Proportionnel |\n",
    "| **Qualit√©** | R√©solution | Details visuels |\n",
    "\n",
    "**Code pour reproduire:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from diffusers import HunyuanVideoPipeline\n",
    "\n",
    "pipe = HunyuanVideoPipeline.from_pretrained(\n",
    "    \"tencent/HunyuanVideo\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "pipe.enable_vae_slicing()\n",
    "pipe.enable_vae_tiling()\n",
    "\n",
    "configs = [\n",
    "    {\"w\": 384, \"h\": 256, \"frames\": 24},\n",
    "    {\"w\": 512, \"h\": 320, \"frames\": 16},\n",
    "    {\"w\": 512, \"h\": 320, \"frames\": 32},\n",
    "]\n",
    "\n",
    "prompt = \"a golden retriever running through a field of sunflowers\"\n",
    "\n",
    "for cfg in configs:\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"low quality\",\n",
    "        num_frames=cfg['frames'],\n",
    "        guidance_scale=6.0,\n",
    "        num_inference_steps=20,\n",
    "        height=cfg['h'],\n",
    "        width=cfg['w'],\n",
    "        generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:32.058546Z",
     "iopub.status.busy": "2026-02-19T09:29:32.058269Z",
     "iopub.status.idle": "2026-02-19T09:29:32.067100Z",
     "shell.execute_reply": "2026-02-19T09:29:32.066543Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comparaison de prompts et analyse qualite\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- COMPARAISON DE PROMPTS ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    prompts = [\n",
    "        {\n",
    "            \"text\": \"a candle flame flickering gently in a dark room, warm light, intimate atmosphere, close-up\",\n",
    "            \"label\": \"Bougie\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"ocean waves rolling onto a sandy beach at sunset, aerial view, golden hour lighting\",\n",
    "            \"label\": \"Ocean\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"a timelapse of clouds moving over a mountain landscape, dramatic sky, epic scale\",\n",
    "            \"label\": \"Timelapse\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for p_idx, prompt_info in enumerate(prompts):\n",
    "        print(f\"\\nGeneration {p_idx + 1}/{len(prompts)} : {prompt_info['label']}\")\n",
    "        print(f\"  Prompt : {prompt_info['text'][:70]}...\")\n",
    "        \n",
    "        result = generate_hunyuan_video(prompt_info['text'], seed=42 + p_idx)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  Temps : {result['generation_time']:.1f}s\")\n",
    "            comparison_results.append({\n",
    "                \"label\": prompt_info['label'],\n",
    "                \"prompt\": prompt_info['text'],\n",
    "                \"frames\": result['frames'],\n",
    "                \"time\": result['generation_time']\n",
    "            })\n",
    "            \n",
    "            if save_as_mp4:\n",
    "                mp4_path = OUTPUT_DIR / f\"hunyuan_{prompt_info['label'].lower()}.mp4\"\n",
    "                export_to_video(result['frames'], str(mp4_path), fps=fps_output)\n",
    "        else:\n",
    "            print(f\"  Erreur : {result['error']}\")\n",
    "    \n",
    "    # Affichage comparatif\n",
    "    if comparison_results:\n",
    "        n_videos = len(comparison_results)\n",
    "        n_preview = 4\n",
    "        fig, axes = plt.subplots(n_videos, n_preview, figsize=(3.5 * n_preview, 3 * n_videos))\n",
    "        if n_videos == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, cr in enumerate(comparison_results):\n",
    "            frame_indices = np.linspace(0, len(cr['frames']) - 1, n_preview, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(cr['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(cr['label'], fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Comparaison de prompts - HunyuanVideo\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyse de coherence temporelle (difference entre frames consecutives)\n",
    "        print(f\"\\nAnalyse de coherence temporelle :\")\n",
    "        print(f\"{'Prompt':<15} {'Temps (s)':<12} {'Diff moy frames':<18} {'Stabilite':<15}\")\n",
    "        print(\"-\" * 60)\n",
    "        for cr in comparison_results:\n",
    "            # Calculer la difference moyenne entre frames consecutives\n",
    "            diffs = []\n",
    "            for i in range(len(cr['frames']) - 1):\n",
    "                f1 = np.array(cr['frames'][i]).astype(float)\n",
    "                f2 = np.array(cr['frames'][i + 1]).astype(float)\n",
    "                diff = np.mean(np.abs(f1 - f2))\n",
    "                diffs.append(diff)\n",
    "            avg_diff = np.mean(diffs)\n",
    "            stability = \"Haute\" if avg_diff < 15 else \"Moyenne\" if avg_diff < 30 else \"Basse\"\n",
    "            print(f\"  {cr['label']:<15} {cr['time']:<12.1f} {avg_diff:<18.2f} {stability:<15}\")\n",
    "else:\n",
    "    print(\"Comparaison de prompts : generation desactivee\")\n",
    "    print(\"\\nTypes de prompts efficaces pour HunyuanVideo :\")\n",
    "    print(\"  - Mouvements naturels : eau, feu, nuages, vent\")\n",
    "    print(\"  - Scenes cinematographiques : camera aerienne, slow motion\")\n",
    "    print(\"  - Timelapse : nuages, coucher de soleil, fleurs\")\n",
    "    print(\"  - Animaux en mouvement : vol d'oiseau, course de chien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:32.075171Z",
     "iopub.status.busy": "2026-02-19T09:29:32.074622Z",
     "iopub.status.idle": "2026-02-19T09:29:32.081564Z",
     "shell.execute_reply": "2026-02-19T09:29:32.081063Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mode interactif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Entrez votre propre prompt pour generer une video HunyuanVideo.\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "    \n",
    "    try:\n",
    "        user_prompt = input(\"\\nVotre prompt : \").strip()\n",
    "        \n",
    "        if user_prompt and run_generation and pipe is not None:\n",
    "            print(f\"\\nGeneration en cours...\")\n",
    "            result_user = generate_hunyuan_video(user_prompt, seed=123)\n",
    "            \n",
    "            if result_user['success']:\n",
    "                print(f\"Generation reussie en {result_user['generation_time']:.1f}s\")\n",
    "                \n",
    "                # Affichage\n",
    "                n_display = min(8, len(result_user['frames']))\n",
    "                fig, axes = plt.subplots(1, n_display, figsize=(2.5 * n_display, 3))\n",
    "                if n_display == 1:\n",
    "                    axes = [axes]\n",
    "                indices = np.linspace(0, len(result_user['frames']) - 1, n_display, dtype=int)\n",
    "                for ax, idx in zip(axes, indices):\n",
    "                    ax.imshow(result_user['frames'][idx])\n",
    "                    ax.set_title(f\"Frame {idx+1}\", fontsize=8)\n",
    "                    ax.axis('off')\n",
    "                plt.suptitle(f\"Votre video : {user_prompt[:50]}...\", fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                if save_as_mp4:\n",
    "                    user_mp4 = OUTPUT_DIR / \"user_generation.mp4\"\n",
    "                    export_to_video(result_user['frames'], str(user_mp4), fps=fps_output)\n",
    "                    print(f\"MP4 sauvegarde : {user_mp4.name}\")\n",
    "            else:\n",
    "                print(f\"Erreur : {result_user['error']}\")\n",
    "        elif user_prompt:\n",
    "            print(\"Generation non disponible (pipeline non charge)\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError) as e:\n",
    "        print(f\"\\nMode interactif interrompu ({type(e).__name__})\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"\\nMode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"\\nErreur inattendue : {error_type} - {str(e)[:100]}\")\n",
    "            print(\"Passage a la suite du notebook\")\n",
    "else:\n",
    "    print(\"\\nMode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Bonnes pratiques et optimisation HunyuanVideo\n",
    "\n",
    "### Conseils de prompt engineering\n",
    "\n",
    "| Bon prompt | Mauvais prompt | Raison |\n",
    "|-----------|---------------|--------|\n",
    "| \"a bird flying over a lake, aerial shot, cinematic\" | \"bird lake\" | Preciser l'action et le style |\n",
    "| \"timelapse of sunset, clouds moving, warm colors\" | \"nice sunset video\" | Indiquer le type de mouvement |\n",
    "| \"close-up of rain drops on a window\" | \"rain\" | Le cadrage guide la generation |\n",
    "\n",
    "### Comparaison avec les autres modeles du Module 02\n",
    "\n",
    "| Aspect | HunyuanVideo | LTX-Video (02-2) | Wan (02-3) | SVD (02-4) |\n",
    "|--------|-------------|------------------|-----------|------------|\n",
    "| Type | Text-to-video | Text/Img/Vid | Text-to-video | Image-to-video |\n",
    "| VRAM | ~18 GB | ~8 GB | ~10 GB | ~10 GB |\n",
    "| Qualite | Haute | Moyenne | Haute | Haute |\n",
    "| Vitesse | Lente | Rapide | Moyenne | Moyenne |\n",
    "| Resolution max | 720p | 512p | 720p | 576p |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T09:29:32.096903Z",
     "iopub.status.busy": "2026-02-19T09:29:32.096655Z",
     "iopub.status.idle": "2026-02-19T09:29:32.103089Z",
     "shell.execute_reply": "2026-02-19T09:29:32.102580Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Modele : {model_id}\")\n",
    "print(f\"Quantification : {'INT8' if quantize else 'FP16'}\")\n",
    "print(f\"Device : {device}\")\n",
    "print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n",
    "print(f\"Resolution : {width}x{height}\")\n",
    "\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "    print(f\"VRAM pic session : {vram_peak:.1f} GB\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "# Liberation VRAM\n",
    "if pipe is not None:\n",
    "    del pipe\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nVRAM liberee\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 02-2 : LTX-Video (generation rapide et legere, ~8 GB VRAM)\")\n",
    "print(f\"2. Notebook 02-3 : Wan 2.1/2.2 (prompts multilingues, motion control)\")\n",
    "print(f\"3. Notebook 02-4 : SVD (animation d'images statiques)\")\n",
    "print(f\"4. Module 03 : Comparaison multi-modeles et orchestration de pipelines\")\n",
    "\n",
    "print(f\"\\nNotebook 02-1 HunyuanVideo Generation termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.835475,
   "end_time": "2026-02-26T07:08:13.701274",
   "environment_variables": {},
   "exception": true,
   "input_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Video\\02-Advanced\\02-1-HunyuanVideo-Generation.ipynb",
   "output_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Video\\02-Advanced\\02-1-HunyuanVideo-Generation.ipynb",
   "parameters": {
    "notebook_mode": "batch",
    "skip_widgets": true
   },
   "start_time": "2026-02-26T07:08:11.865799",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
