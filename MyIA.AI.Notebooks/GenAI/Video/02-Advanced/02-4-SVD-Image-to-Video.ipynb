{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SVD - Stable Video Diffusion (Image-to-Video)\n",
    "\n",
    "**Module :** 02-Video-Advanced  \n",
    "**Niveau :** Intermediaire  \n",
    "**Technologies :** Stable Video Diffusion 1.1 (Stability AI), diffusers  \n",
    "**Duree estimee :** 45 minutes  \n",
    "**VRAM :** ~10 GB  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Comprendre l'architecture SVD et sa specialisation image-to-video\n",
    "- [ ] Charger le pipeline SVD 1.1 (variante XT pour 25 frames)\n",
    "- [ ] Animer des images statiques en videos courtes\n",
    "- [ ] Controler l'intensite du mouvement avec motion_bucket_id\n",
    "- [ ] Ajuster les parametres FPS et noise_aug_strength\n",
    "- [ ] Conditionner la generation sur differents types d'images\n",
    "- [ ] Analyser la qualite avec extraction et comparaison de frames\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- GPU avec 10+ GB VRAM (RTX 3060 12GB recommande)\n",
    "- Notebooks 02-1 a 02-3 completes pour comparaison\n",
    "- Packages : `diffusers>=0.30`, `transformers`, `torch`, `accelerate`, `imageio`, `imageio-ffmpeg`, `Pillow`\n",
    "\n",
    "**Navigation :** [<< 02-3](02-3-Wan-Video-Generation.ipynb) | [Index](../README.md) | [Suivant >>](../03-Orchestration/03-1-Multi-Model-Video-Comparison.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres modele\n",
    "model_id = \"stabilityai/stable-video-diffusion-img2vid-xt\"  # SVD 1.1 XT (25 frames)\n",
    "device = \"cuda\"                    # Device de calcul\n",
    "\n",
    "# Parametres generation\n",
    "num_frames = 25                    # Nombre de frames (XT = 25)\n",
    "fps = 7                            # FPS de conditionnement\n",
    "motion_bucket_id = 127             # Intensite du mouvement (0-255)\n",
    "noise_aug_strength = 0.02          # Force de l'augmentation de bruit\n",
    "num_inference_steps = 25           # Nombre d'etapes de debruitage\n",
    "decode_chunk_size = 8              # Frames decodees par chunk (reduit VRAM)\n",
    "fps_output = 7                     # FPS de la video de sortie\n",
    "\n",
    "# Configuration\n",
    "run_generation = True              # Executer la generation\n",
    "save_as_mp4 = True                 # Sauvegarder en MP4\n",
    "save_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.genai_helpers import setup_genai_logging\n",
    "        print(\"Helpers GenAI importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers GenAI non disponibles - mode autonome\")\n",
    "\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'svd_video'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('svd_video')\n",
    "\n",
    "print(f\"SVD - Stable Video Diffusion (Image-to-Video)\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Frames : {num_frames}, Steps : {num_inference_steps}\")\n",
    "print(f\"Motion bucket : {motion_bucket_id}, Noise aug : {noise_aug_strength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement .env et verification GPU\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification GPU\n",
    "print(\"\\n--- VERIFICATION GPU ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "    vram_free = (torch.cuda.get_device_properties(0).total_mem - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "    \n",
    "    print(f\"GPU : {gpu_name}\")\n",
    "    print(f\"VRAM totale : {vram_total:.1f} GB\")\n",
    "    print(f\"VRAM libre : {vram_free:.1f} GB\")\n",
    "    print(f\"CUDA : {torch.version.cuda}\")\n",
    "    \n",
    "    if vram_total < 10:\n",
    "        print(f\"\\nAttention : VRAM ({vram_total:.0f} GB) < 10 GB recommandes\")\n",
    "        decode_chunk_size = 4\n",
    "        num_frames = 14  # Version non-XT\n",
    "        print(f\"  Frames reduites a {num_frames}, decode chunk = {decode_chunk_size}\")\n",
    "else:\n",
    "    print(\"CUDA non disponible.\")\n",
    "    print(\"SVD necessite un GPU. Le notebook montrera le code sans executer.\")\n",
    "    run_generation = False\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Verification des dependances\n",
    "print(\"\\n--- VERIFICATION DEPENDANCES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "deps_ok = True\n",
    "\n",
    "try:\n",
    "    import diffusers\n",
    "    print(f\"diffusers : v{diffusers.__version__}\")\n",
    "    from diffusers import StableVideoDiffusionPipeline\n",
    "    from diffusers.utils import export_to_video, load_image\n",
    "    print(\"StableVideoDiffusionPipeline : disponible\")\n",
    "except ImportError as e:\n",
    "    print(f\"diffusers NON INSTALLE ou version trop ancienne\")\n",
    "    print(f\"  pip install diffusers>=0.30 transformers accelerate\")\n",
    "    print(f\"  Erreur : {e}\")\n",
    "    deps_ok = False\n",
    "\n",
    "try:\n",
    "    import imageio\n",
    "    print(f\"imageio : v{imageio.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"imageio NON INSTALLE\")\n",
    "    deps_ok = False\n",
    "\n",
    "if not deps_ok:\n",
    "    print(\"\\nDependances manquantes. Le notebook montrera le code sans executer.\")\n",
    "    run_generation = False\n",
    "\n",
    "print(f\"\\nDevice : {device}\")\n",
    "print(f\"Generation activee : {run_generation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Section 1 : Architecture SVD et chargement\n",
    "\n",
    "Stable Video Diffusion (SVD) est un modele image-to-video de Stability AI.\n",
    "Contrairement aux modeles text-to-video (HunyuanVideo, LTX, Wan), SVD prend\n",
    "une image en entree et genere une sequence animee a partir de celle-ci.\n",
    "\n",
    "| Composant | Description |\n",
    "|-----------|-------------|\n",
    "| **Architecture** | U-Net 3D base sur Stable Diffusion 2.1 |\n",
    "| **Entree** | Image PIL unique (pas de prompt textuel) |\n",
    "| **Conditionnement** | Image + FPS + motion_bucket_id |\n",
    "| **Variante XT** | 25 frames au lieu de 14 (version standard) |\n",
    "\n",
    "### Parametres cles de SVD\n",
    "\n",
    "| Parametre | Role | Plage |\n",
    "|-----------|------|-------|\n",
    "| `motion_bucket_id` | Intensite du mouvement | 0-255 (127 = neutre) |\n",
    "| `fps` | FPS de conditionnement (influence la vitesse percue) | 2-30 |\n",
    "| `noise_aug_strength` | Perturbation de l'image d'entree | 0.0-0.1 |\n",
    "| `decode_chunk_size` | Frames decodees par batch (VRAM) | 2-14 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du pipeline SVD\n",
    "pipe = None\n",
    "\n",
    "if run_generation:\n",
    "    print(\"\\n--- CHARGEMENT DU PIPELINE SVD ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        start_load = time.time()\n",
    "        \n",
    "        print(f\"Chargement modele : {model_id}\")\n",
    "        \n",
    "        pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\"\n",
    "        )\n",
    "        pipe = pipe.to(device)\n",
    "        \n",
    "        # Optimisations memoire\n",
    "        pipe.enable_vae_slicing()\n",
    "        try:\n",
    "            pipe.enable_model_cpu_offload()\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        load_time = time.time() - start_load\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            vram_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            print(f\"  VRAM utilisee : {vram_used:.1f} GB\")\n",
    "        \n",
    "        print(f\"Pipeline SVD charge en {load_time:.1f}s\")\n",
    "        print(f\"  Variante : {'XT (25 frames)' if 'xt' in model_id else 'Standard (14 frames)'}\")\n",
    "        print(f\"  VAE slicing : actif\")\n",
    "        print(f\"  Decode chunk size : {decode_chunk_size}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur chargement pipeline : {type(e).__name__}: {str(e)[:200]}\")\n",
    "        print(\"Le notebook continuera sans generation.\")\n",
    "        run_generation = False\n",
    "        pipe = None\n",
    "else:\n",
    "    print(\"Chargement pipeline desactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Section 2 : Creation d'images de test et premiere animation\n",
    "\n",
    "Nous allons creer des images de test variees et les animer avec SVD.\n",
    "SVD ne prend pas de prompt textuel : le mouvement est entierement infere a partir\n",
    "de l'image d'entree et des parametres de conditionnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation d'images de test et premiere animation\n",
    "print(\"\\n--- CREATION D'IMAGES DE TEST ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# SVD attend des images 1024x576\n",
    "svd_width = 1024\n",
    "svd_height = 576\n",
    "\n",
    "def create_test_image(name: str, description: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Cree une image de test pour SVD.\n",
    "    \n",
    "    Args:\n",
    "        name: Nom du type d'image\n",
    "        description: Description pour le titre\n",
    "    \n",
    "    Returns:\n",
    "        Image PIL en RGB\n",
    "    \"\"\"\n",
    "    img = Image.new('RGB', (svd_width, svd_height))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    if name == \"landscape\":\n",
    "        # Paysage : ciel bleu, montagnes, herbe\n",
    "        for y in range(svd_height):\n",
    "            if y < svd_height * 0.4:  # Ciel\n",
    "                r = int(100 + 55 * y / (svd_height * 0.4))\n",
    "                g = int(150 + 50 * y / (svd_height * 0.4))\n",
    "                b = int(230 - 30 * y / (svd_height * 0.4))\n",
    "            elif y < svd_height * 0.6:  # Montagnes\n",
    "                t = (y - svd_height * 0.4) / (svd_height * 0.2)\n",
    "                r = int(120 - 40 * t)\n",
    "                g = int(130 - 30 * t)\n",
    "                b = int(140 - 20 * t)\n",
    "            else:  # Herbe\n",
    "                t = (y - svd_height * 0.6) / (svd_height * 0.4)\n",
    "                r = int(60 + 30 * t)\n",
    "                g = int(140 - 20 * t)\n",
    "                b = int(40 + 20 * t)\n",
    "            draw.line([(0, y), (svd_width, y)], fill=(r, g, b))\n",
    "        # Soleil\n",
    "        draw.ellipse([svd_width - 150, 30, svd_width - 50, 130],\n",
    "                     fill=(255, 220, 100))\n",
    "    \n",
    "    elif name == \"portrait\":\n",
    "        # Fond uni avec forme geometrique (silhouette simplifiee)\n",
    "        draw.rectangle([0, 0, svd_width, svd_height], fill=(80, 70, 90))\n",
    "        # Cercle (tete)\n",
    "        cx, cy = svd_width // 2, svd_height // 3\n",
    "        r = 80\n",
    "        draw.ellipse([cx-r, cy-r, cx+r, cy+r], fill=(200, 170, 140))\n",
    "        # Corps\n",
    "        draw.polygon([(cx-100, cy+r), (cx+100, cy+r),\n",
    "                      (cx+120, svd_height), (cx-120, svd_height)],\n",
    "                     fill=(50, 80, 120))\n",
    "    \n",
    "    elif name == \"nature\":\n",
    "        # Scene nature : eau et reflets\n",
    "        for y in range(svd_height):\n",
    "            if y < svd_height * 0.5:  # Ciel coucher de soleil\n",
    "                t = y / (svd_height * 0.5)\n",
    "                r = int(255 - 60 * t)\n",
    "                g = int(150 - 50 * t)\n",
    "                b = int(100 + 50 * t)\n",
    "            else:  # Eau avec reflets\n",
    "                t = (y - svd_height * 0.5) / (svd_height * 0.5)\n",
    "                r = int(100 - 40 * t)\n",
    "                g = int(120 - 30 * t)\n",
    "                b = int(160 - 20 * t)\n",
    "            draw.line([(0, y), (svd_width, y)], fill=(r, g, b))\n",
    "        # Soleil a l'horizon\n",
    "        sun_y = int(svd_height * 0.45)\n",
    "        draw.ellipse([svd_width//2-60, sun_y-60, svd_width//2+60, sun_y+60],\n",
    "                     fill=(255, 180, 80))\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "# Creer les images de test\n",
    "test_images = [\n",
    "    {\"name\": \"landscape\", \"description\": \"Paysage avec montagnes\"},\n",
    "    {\"name\": \"portrait\", \"description\": \"Silhouette portrait\"},\n",
    "    {\"name\": \"nature\", \"description\": \"Coucher de soleil sur l'eau\"},\n",
    "]\n",
    "\n",
    "created_images = {}\n",
    "for img_info in test_images:\n",
    "    img = create_test_image(img_info['name'], img_info['description'])\n",
    "    img_path = OUTPUT_DIR / f\"test_{img_info['name']}.png\"\n",
    "    img.save(str(img_path))\n",
    "    created_images[img_info['name']] = img\n",
    "    print(f\"Image creee : {img_info['description']} ({svd_width}x{svd_height})\")\n",
    "\n",
    "# Afficher les images de test\n",
    "fig, axes = plt.subplots(1, len(test_images), figsize=(5 * len(test_images), 3))\n",
    "for i, img_info in enumerate(test_images):\n",
    "    axes[i].imshow(created_images[img_info['name']])\n",
    "    axes[i].set_title(img_info['description'], fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle(\"Images de test pour SVD\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Premiere animation : paysage\n",
    "if run_generation and pipe is not None:\n",
    "    print(f\"\\n--- PREMIERE ANIMATION ---\")\n",
    "    print(f\"Image : Paysage avec montagnes\")\n",
    "    print(f\"Motion bucket : {motion_bucket_id}, FPS : {fps}\")\n",
    "    print(f\"Noise aug : {noise_aug_strength}\")\n",
    "    print(f\"\\nGeneration en cours...\")\n",
    "    \n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    output = pipe(\n",
    "        image=created_images['landscape'],\n",
    "        num_frames=num_frames,\n",
    "        fps=fps,\n",
    "        motion_bucket_id=motion_bucket_id,\n",
    "        noise_aug_strength=noise_aug_strength,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        decode_chunk_size=decode_chunk_size,\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    gen_time = time.time() - start_time\n",
    "    frames_landscape = output.frames[0]\n",
    "    \n",
    "    print(f\"\\nGeneration reussie\")\n",
    "    print(f\"  Temps : {gen_time:.1f}s\")\n",
    "    print(f\"  Frames : {len(frames_landscape)}\")\n",
    "    if device == \"cuda\":\n",
    "        vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "        print(f\"  VRAM pic : {vram_peak:.1f} GB\")\n",
    "    \n",
    "    # Affichage\n",
    "    n_display = min(8, len(frames_landscape))\n",
    "    indices = np.linspace(0, len(frames_landscape) - 1, n_display, dtype=int)\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes_flat = axes.flatten()\n",
    "    for i, idx in enumerate(indices):\n",
    "        if i < len(axes_flat):\n",
    "            axes_flat[i].imshow(frames_landscape[idx])\n",
    "            axes_flat[i].set_title(f\"Frame {idx + 1}/{len(frames_landscape)}\", fontsize=9)\n",
    "            axes_flat[i].axis('off')\n",
    "    for i in range(len(indices), len(axes_flat)):\n",
    "        axes_flat[i].axis('off')\n",
    "    plt.suptitle(f\"SVD : Paysage anime ({gen_time:.1f}s)\", fontsize=11, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if save_as_mp4:\n",
    "        mp4_path = OUTPUT_DIR / \"svd_landscape.mp4\"\n",
    "        export_to_video(frames_landscape, str(mp4_path), fps=fps_output)\n",
    "        print(f\"  MP4 sauvegarde : {mp4_path.name}\")\n",
    "else:\n",
    "    print(\"\\nGeneration desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Interpretation : Premiere animation SVD\n",
    "\n",
    "| Aspect | Valeur | Signification |\n",
    "|--------|--------|---------------|\n",
    "| Entree | Image unique (pas de prompt) | Le mouvement est infere automatiquement |\n",
    "| Fidelite | Premiere frame = image d'entree | SVD preserve parfaitement l'image source |\n",
    "| Mouvement | Subtil, naturel | SVD genere des mouvements realistes (nuages, eau, lumiere) |\n",
    "| VRAM | ~10 GB | Plus leger que HunyuanVideo |\n",
    "\n",
    "**Points cles** :\n",
    "1. SVD est le seul modele du Module 02 specialise en image-to-video (pas de texte)\n",
    "2. Le mouvement genere depend du contenu de l'image (eau = ondulations, nuages = deplacement)\n",
    "3. La resolution d'entree optimale est 1024x576\n",
    "\n",
    "## Section 3 : Controle du mouvement avec motion_bucket_id\n",
    "\n",
    "Le parametre `motion_bucket_id` controle l'intensite du mouvement genere.\n",
    "Une valeur basse donne des mouvements subtils, une valeur haute des mouvements plus prononces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de differentes valeurs de motion_bucket_id\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- CONTROLE MOTION BUCKET ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    motion_values = [40, 127, 200]\n",
    "    motion_labels = [\"Subtil (40)\", \"Normal (127)\", \"Fort (200)\"]\n",
    "    motion_results = []\n",
    "    \n",
    "    source_image = created_images['nature']\n",
    "    print(f\"Image source : Coucher de soleil sur l'eau\")\n",
    "    \n",
    "    for m_val, m_label in zip(motion_values, motion_labels):\n",
    "        print(f\"\\nMotion bucket = {m_val} ({m_label})\")\n",
    "        \n",
    "        generator = torch.Generator(device=device).manual_seed(42)\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            output = pipe(\n",
    "                image=source_image,\n",
    "                num_frames=num_frames,\n",
    "                fps=fps,\n",
    "                motion_bucket_id=m_val,\n",
    "                noise_aug_strength=noise_aug_strength,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                decode_chunk_size=decode_chunk_size,\n",
    "                generator=generator\n",
    "            )\n",
    "            \n",
    "            gen_time = time.time() - start_time\n",
    "            frames = output.frames[0]\n",
    "            \n",
    "            # Mesure du mouvement reel (difference entre frames)\n",
    "            diffs = []\n",
    "            for i in range(len(frames) - 1):\n",
    "                f1 = np.array(frames[i]).astype(float)\n",
    "                f2 = np.array(frames[i + 1]).astype(float)\n",
    "                diff = np.mean(np.abs(f1 - f2))\n",
    "                diffs.append(diff)\n",
    "            avg_diff = np.mean(diffs)\n",
    "            \n",
    "            vram_peak = 0\n",
    "            if device == \"cuda\":\n",
    "                vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "            \n",
    "            motion_results.append({\n",
    "                \"bucket\": m_val,\n",
    "                \"label\": m_label,\n",
    "                \"frames\": frames,\n",
    "                \"time\": gen_time,\n",
    "                \"avg_diff\": avg_diff,\n",
    "                \"vram_peak\": vram_peak\n",
    "            })\n",
    "            \n",
    "            print(f\"  Temps : {gen_time:.1f}s, Diff moyenne : {avg_diff:.2f}\")\n",
    "            \n",
    "            if save_as_mp4:\n",
    "                mp4_path = OUTPUT_DIR / f\"svd_motion_{m_val}.mp4\"\n",
    "                export_to_video(frames, str(mp4_path), fps=fps_output)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur : {type(e).__name__}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Affichage comparatif\n",
    "    if motion_results:\n",
    "        n_results = len(motion_results)\n",
    "        n_preview = 4\n",
    "        fig, axes = plt.subplots(n_results, n_preview, figsize=(3.5 * n_preview, 3 * n_results))\n",
    "        if n_results == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, mr in enumerate(motion_results):\n",
    "            frame_indices = np.linspace(0, len(mr['frames']) - 1, n_preview, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx].imshow(mr['frames'][fi])\n",
    "                axes[v_idx][f_idx].axis('off')\n",
    "                if f_idx == 0:\n",
    "                    axes[v_idx][f_idx].set_ylabel(f\"MBI={mr['bucket']}\", fontsize=10, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Impact de motion_bucket_id\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Tableau recapitulatif\n",
    "        print(f\"\\n{'Motion Bucket':<18} {'Diff frames':<15} {'Temps (s)':<12} {'VRAM (GB)':<12}\")\n",
    "        print(\"-\" * 57)\n",
    "        for mr in motion_results:\n",
    "            print(f\"  {mr['label']:<18} {mr['avg_diff']:<15.2f} {mr['time']:<12.1f} {mr['vram_peak']:<12.1f}\")\n",
    "else:\n",
    "    print(\"Test motion bucket : generation desactivee\")\n",
    "    print(\"\\nGuide motion_bucket_id :\")\n",
    "    print(\"  0-60   : Mouvement tres subtil (ideal pour portraits)\")\n",
    "    print(\"  80-150  : Mouvement modere (usage general)\")\n",
    "    print(\"  150-255 : Mouvement fort (scenes dynamiques)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Interpretation : Controle du mouvement\n",
    "\n",
    "| motion_bucket_id | Diff inter-frames | Comportement |\n",
    "|-----------------|-------------------|-------------|\n",
    "| 40 (subtil) | Faible | Micro-mouvements, ideal pour portraits ou scenes calmes |\n",
    "| 127 (neutre) | Moyen | Bon equilibre, mouvements naturels |\n",
    "| 200 (fort) | Eleve | Mouvements prononces, peut introduire des artefacts |\n",
    "\n",
    "**Points cles** :\n",
    "1. La diff inter-frames est une mesure objective du mouvement genere\n",
    "2. Les valeurs extremes (< 20 ou > 230) donnent souvent des resultats imprevisibles\n",
    "3. La plage 80-180 est recommandee pour la plupart des cas d'usage\n",
    "\n",
    "## Section 4 : Animation de differents types d'images\n",
    "\n",
    "Nous allons tester SVD sur nos trois types d'images pour observer comment le modele\n",
    "infere le mouvement a partir du contenu visuel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation de differents types d'images\n",
    "if run_generation and pipe is not None:\n",
    "    print(\"\\n--- ANIMATION MULTI-IMAGES ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    multi_results = []\n",
    "    \n",
    "    for img_info in test_images:\n",
    "        img_name = img_info['name']\n",
    "        img_desc = img_info['description']\n",
    "        source = created_images[img_name]\n",
    "        \n",
    "        print(f\"\\nAnimation : {img_desc}\")\n",
    "        \n",
    "        generator = torch.Generator(device=device).manual_seed(42)\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            output = pipe(\n",
    "                image=source,\n",
    "                num_frames=num_frames,\n",
    "                fps=fps,\n",
    "                motion_bucket_id=motion_bucket_id,\n",
    "                noise_aug_strength=noise_aug_strength,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                decode_chunk_size=decode_chunk_size,\n",
    "                generator=generator\n",
    "            )\n",
    "            \n",
    "            gen_time = time.time() - start_time\n",
    "            frames = output.frames[0]\n",
    "            \n",
    "            # Mesurer le mouvement\n",
    "            diffs = []\n",
    "            for i in range(len(frames) - 1):\n",
    "                f1 = np.array(frames[i]).astype(float)\n",
    "                f2 = np.array(frames[i + 1]).astype(float)\n",
    "                diffs.append(np.mean(np.abs(f1 - f2)))\n",
    "            \n",
    "            multi_results.append({\n",
    "                \"name\": img_name,\n",
    "                \"description\": img_desc,\n",
    "                \"source\": source,\n",
    "                \"frames\": frames,\n",
    "                \"time\": gen_time,\n",
    "                \"avg_diff\": np.mean(diffs),\n",
    "                \"max_diff\": np.max(diffs)\n",
    "            })\n",
    "            \n",
    "            print(f\"  Temps : {gen_time:.1f}s\")\n",
    "            print(f\"  Mouvement moyen : {np.mean(diffs):.2f}\")\n",
    "            \n",
    "            if save_as_mp4:\n",
    "                mp4_path = OUTPUT_DIR / f\"svd_{img_name}.mp4\"\n",
    "                export_to_video(frames, str(mp4_path), fps=fps_output)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur : {type(e).__name__}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Affichage comparatif : source + frames selectionnees\n",
    "    if multi_results:\n",
    "        n_images = len(multi_results)\n",
    "        fig, axes = plt.subplots(n_images, 5, figsize=(18, 3.5 * n_images))\n",
    "        if n_images == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for v_idx, mr in enumerate(multi_results):\n",
    "            # Source\n",
    "            axes[v_idx][0].imshow(mr['source'])\n",
    "            axes[v_idx][0].set_title(\"Source\", fontsize=9, fontweight='bold')\n",
    "            axes[v_idx][0].axis('off')\n",
    "            if v_idx == 0:\n",
    "                axes[v_idx][0].set_ylabel(\"Image\", fontsize=10)\n",
    "            \n",
    "            # Frames animees\n",
    "            frame_indices = np.linspace(0, len(mr['frames']) - 1, 4, dtype=int)\n",
    "            for f_idx, fi in enumerate(frame_indices):\n",
    "                axes[v_idx][f_idx + 1].imshow(mr['frames'][fi])\n",
    "                axes[v_idx][f_idx + 1].set_title(f\"Frame {fi+1}\", fontsize=9)\n",
    "                axes[v_idx][f_idx + 1].axis('off')\n",
    "            \n",
    "            axes[v_idx][0].set_ylabel(mr['description'], fontsize=10, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Animation de differents types d'images - SVD\", fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Tableau comparatif\n",
    "        print(f\"\\n{'Image':<25} {'Temps (s)':<12} {'Mouvement moy':<16} {'Mouvement max':<15}\")\n",
    "        print(\"-\" * 68)\n",
    "        for mr in multi_results:\n",
    "            print(f\"  {mr['description']:<25} {mr['time']:<12.1f} {mr['avg_diff']:<16.2f} {mr['max_diff']:<15.2f}\")\n",
    "else:\n",
    "    print(\"Animation multi-images : generation desactivee\")\n",
    "    print(\"\\nSVD genere des mouvements differents selon le contenu :\")\n",
    "    print(\"  - Paysage : nuages en deplacement, herbe qui ondule\")\n",
    "    print(\"  - Portrait : micro-expressions, legere respiration\")\n",
    "    print(\"  - Eau/Nature : ondulations, reflets en mouvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode interactif\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Entrez le chemin vers une image pour l'animer avec SVD.\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "    \n",
    "    try:\n",
    "        user_image_path = input(\"\\nChemin image (ou vide) : \").strip()\n",
    "        \n",
    "        if user_image_path and Path(user_image_path).exists() and run_generation and pipe is not None:\n",
    "            print(f\"\\nChargement de l'image : {user_image_path}\")\n",
    "            user_image = Image.open(user_image_path).convert('RGB')\n",
    "            user_image = user_image.resize((svd_width, svd_height))\n",
    "            \n",
    "            print(f\"Image redimensionnee a {svd_width}x{svd_height}\")\n",
    "            print(f\"Generation en cours...\")\n",
    "            \n",
    "            generator = torch.Generator(device=device).manual_seed(123)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = pipe(\n",
    "                image=user_image,\n",
    "                num_frames=num_frames,\n",
    "                fps=fps,\n",
    "                motion_bucket_id=motion_bucket_id,\n",
    "                noise_aug_strength=noise_aug_strength,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                decode_chunk_size=decode_chunk_size,\n",
    "                generator=generator\n",
    "            )\n",
    "            \n",
    "            gen_time = time.time() - start_time\n",
    "            user_frames = output.frames[0]\n",
    "            \n",
    "            print(f\"Generation reussie en {gen_time:.1f}s\")\n",
    "            \n",
    "            # Affichage\n",
    "            n_display = min(6, len(user_frames))\n",
    "            fig, axes = plt.subplots(1, n_display + 1, figsize=(2.5 * (n_display + 1), 3))\n",
    "            axes[0].imshow(user_image)\n",
    "            axes[0].set_title(\"Source\", fontsize=9, fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            indices = np.linspace(0, len(user_frames) - 1, n_display, dtype=int)\n",
    "            for i, idx in enumerate(indices):\n",
    "                axes[i + 1].imshow(user_frames[idx])\n",
    "                axes[i + 1].set_title(f\"Frame {idx+1}\", fontsize=8)\n",
    "                axes[i + 1].axis('off')\n",
    "            plt.suptitle(f\"Votre image animee\", fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            if save_as_mp4:\n",
    "                user_mp4 = OUTPUT_DIR / \"user_animation.mp4\"\n",
    "                export_to_video(user_frames, str(user_mp4), fps=fps_output)\n",
    "                print(f\"MP4 sauvegarde : {user_mp4.name}\")\n",
    "        elif user_image_path and not Path(user_image_path).exists():\n",
    "            print(f\"Fichier non trouve : {user_image_path}\")\n",
    "        elif user_image_path:\n",
    "            print(\"Generation non disponible (pipeline non charge)\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError) as e:\n",
    "        print(f\"\\nMode interactif interrompu ({type(e).__name__})\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"\\nMode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"\\nErreur inattendue : {error_type} - {str(e)[:100]}\")\n",
    "            print(\"Passage a la suite du notebook\")\n",
    "else:\n",
    "    print(\"\\nMode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Bonnes pratiques SVD et comparaison Module 02\n",
    "\n",
    "### Conseils pour de bonnes animations SVD\n",
    "\n",
    "| Bon input | Mauvais input | Raison |\n",
    "|-----------|--------------|--------|\n",
    "| Photo nette, bien eclairee | Image floue, sombre | SVD preserve la qualite source |\n",
    "| Scene avec elements mobiles | Image abstraite / texte | SVD detecte les elements a animer |\n",
    "| Resolution 1024x576 | Image tres petite (<256px) | Resolution optimale pour SVD |\n",
    "| Elements naturels (eau, ciel) | Geometrie pure | Mouvement plus naturel |\n",
    "\n",
    "### Recapitulatif Module 02 - Modeles video generatifs\n",
    "\n",
    "| Modele | Type | VRAM | Vitesse | Qualite | Force |\n",
    "|--------|------|------|---------|---------|-------|\n",
    "| HunyuanVideo (02-1) | Text-to-video | ~18 GB | Lente | Haute | Longues sequences |\n",
    "| LTX-Video (02-2) | Text/Img/Vid | ~8 GB | Rapide | Bonne | Polyvalence |\n",
    "| Wan 2.1 (02-3) | Text-to-video | ~10 GB | Moyenne | Haute | Multilingue |\n",
    "| SVD (02-4) | Image-to-video | ~10 GB | Moyenne | Haute | Animation precise |\n",
    "\n",
    "### Pipeline recommande\n",
    "\n",
    "```\n",
    "1. Generer une image avec DALL-E / SDXL / Flux\n",
    "2. Animer avec SVD (motion_bucket_id = 100-150)\n",
    "3. Upscaler avec Real-ESRGAN (notebook 01-4)\n",
    "4. Assembler en video finale avec moviepy (notebook 01-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Modele : {model_id}\")\n",
    "print(f\"Device : {device}\")\n",
    "print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps\")\n",
    "print(f\"Motion bucket : {motion_bucket_id}, FPS : {fps}\")\n",
    "print(f\"Noise aug : {noise_aug_strength}\")\n",
    "\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    vram_peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "    print(f\"VRAM pic session : {vram_peak:.1f} GB\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "# Liberation VRAM\n",
    "if pipe is not None:\n",
    "    del pipe\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nVRAM liberee\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Module 03-1 : Comparaison benchmark de tous les modeles video\")\n",
    "print(f\"2. Module 03-2 : Orchestration de pipelines (text -> image -> video -> upscale)\")\n",
    "print(f\"3. Module 03-3 : Workflows ComfyUI pour la generation video\")\n",
    "print(f\"4. Module 04 : Applications production (education, creatif, bout-en-bout)\")\n",
    "\n",
    "print(f\"\\nNotebook 02-4 SVD Image-to-Video termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
