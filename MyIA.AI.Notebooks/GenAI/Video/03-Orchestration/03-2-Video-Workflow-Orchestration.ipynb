{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-header",
      "metadata": {},
      "source": [
        "# Orchestration de Pipelines Video\n",
        "\n",
        "**Module :** 03-Video-Orchestration  \n",
        "**Niveau :** Avance  \n",
        "**Technologies :** Text->Image->Video pipelines, batch generation, upscale+interpolation  \n",
        "**Duree estimee :** 60 minutes  \n",
        "**VRAM :** ~18 GB (chargement sequentiel)  \n",
        "\n",
        "## Objectifs d'Apprentissage\n",
        "\n",
        "- [ ] Construire un pipeline text -> image (DALL-E/SDXL) -> video (SVD)\n",
        "- [ ] Construire un pipeline text -> video -> upscale -> interpolation\n",
        "- [ ] Orchestrer des generations batch avec plusieurs prompts\n",
        "- [ ] Gerer le chargement sequentiel des modeles (VRAM limitee)\n",
        "- [ ] Assembler les resultats : concatenation de clips, transitions\n",
        "- [ ] Comparer les pipelines en termes de qualite et de cout\n",
        "\n",
        "## Prerequis\n",
        "\n",
        "- GPU avec 18+ GB VRAM (RTX 3090 / RTX 4090)\n",
        "- Notebook 03-1 complete (benchmark multi-modeles)\n",
        "- Packages : `diffusers>=0.32`, `transformers`, `torch`, `accelerate`, `imageio`, `imageio-ffmpeg`, `openai`, `Pillow`\n",
        "\n",
        "**Navigation :** [<< 03-1](03-1-Multi-Model-Video-Comparison.ipynb) | [Index](../README.md) | [Suivant >>](03-3-ComfyUI-Video-Workflows.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-params",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
        "\n",
        "# Configuration notebook\n",
        "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
        "skip_widgets = False               # True pour mode batch MCP\n",
        "debug_level = \"INFO\"\n",
        "\n",
        "# Parametres pipeline\n",
        "pipeline_mode = \"text_image_video\"  # \"text_image_video\" ou \"text_video_upscale\"\n",
        "image_model = \"dall-e-3\"           # Modele image : \"dall-e-3\" ou \"sdxl\"\n",
        "video_model = \"svd\"                # Modele video : \"svd\" ou \"ltx\"\n",
        "upscale = True                     # Activer l'upscaling\n",
        "device = \"cuda\"                    # Device de calcul\n",
        "\n",
        "# Parametres generation\n",
        "num_frames = 25                    # Nombre de frames video\n",
        "num_inference_steps = 25           # Etapes de debruitage\n",
        "guidance_scale = 6.0               # CFG scale\n",
        "fps_output = 8                     # FPS de sortie\n",
        "seed = 42                          # Graine pour reproductibilite\n",
        "\n",
        "# Configuration\n",
        "run_pipeline = True                # Executer les pipelines\n",
        "save_as_mp4 = True                 # Sauvegarder en MP4\n",
        "save_results = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environnement et imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# Import helpers GenAI\n",
        "GENAI_ROOT = Path.cwd()\n",
        "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
        "    GENAI_ROOT = GENAI_ROOT.parent\n",
        "\n",
        "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
        "if HELPERS_PATH.exists():\n",
        "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
        "    try:\n",
        "        from helpers.genai_helpers import setup_genai_logging\n",
        "        print(\"Helpers GenAI importes\")\n",
        "    except ImportError:\n",
        "        print(\"Helpers GenAI non disponibles - mode autonome\")\n",
        "\n",
        "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'pipeline_video'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(level=getattr(logging, debug_level))\n",
        "logger = logging.getLogger('pipeline_video')\n",
        "\n",
        "print(f\"Orchestration de Pipelines Video\")\n",
        "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Mode : {notebook_mode}\")\n",
        "print(f\"Pipeline : {pipeline_mode}\")\n",
        "print(f\"Image : {image_model}, Video : {video_model}\")\n",
        "print(f\"Upscale : {upscale}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-env",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chargement .env et verification GPU\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "current_path = Path.cwd()\n",
        "found_env = False\n",
        "for _ in range(4):\n",
        "    env_path = current_path / '.env'\n",
        "    if env_path.exists():\n",
        "        load_dotenv(env_path)\n",
        "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
        "        found_env = True\n",
        "        break\n",
        "    current_path = current_path.parent\n",
        "\n",
        "if not found_env:\n",
        "    print(\"Aucun fichier .env trouve\")\n",
        "\n",
        "# Verification des API keys\n",
        "print(\"\\n--- VERIFICATION API ---\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "openai_key = os.environ.get('OPENAI_API_KEY', '')\n",
        "if openai_key:\n",
        "    print(f\"OPENAI_API_KEY : configure ({openai_key[:8]}...)\")\n",
        "else:\n",
        "    print(\"OPENAI_API_KEY : non configure\")\n",
        "    if image_model == \"dall-e-3\":\n",
        "        print(\"  Pipeline text->image->video utilisera SDXL en fallback\")\n",
        "        image_model = \"sdxl\"\n",
        "\n",
        "# Verification GPU\n",
        "print(\"\\n--- VERIFICATION GPU ---\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
        "    vram_free = (torch.cuda.get_device_properties(0).total_mem - torch.cuda.memory_allocated(0)) / 1024**3\n",
        "    \n",
        "    print(f\"GPU : {gpu_name}\")\n",
        "    print(f\"VRAM totale : {vram_total:.1f} GB\")\n",
        "    print(f\"VRAM libre : {vram_free:.1f} GB\")\n",
        "    print(f\"CUDA : {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"CUDA non disponible.\")\n",
        "    print(\"Les pipelines locaux necessitent un GPU. Le notebook montrera le code sans executer.\")\n",
        "    run_pipeline = False\n",
        "    device = \"cpu\"\n",
        "\n",
        "# Verification des dependances\n",
        "print(\"\\n--- VERIFICATION DEPENDANCES ---\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "deps_ok = True\n",
        "\n",
        "try:\n",
        "    import diffusers\n",
        "    print(f\"diffusers : v{diffusers.__version__}\")\n",
        "    from diffusers.utils import export_to_video\n",
        "except ImportError:\n",
        "    print(\"diffusers NON INSTALLE\")\n",
        "    deps_ok = False\n",
        "\n",
        "try:\n",
        "    import imageio\n",
        "    print(f\"imageio : v{imageio.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"imageio NON INSTALLE\")\n",
        "    deps_ok = False\n",
        "\n",
        "if not deps_ok:\n",
        "    print(\"\\nDependances manquantes. Le notebook montrera le code sans executer.\")\n",
        "    run_pipeline = False\n",
        "\n",
        "print(f\"\\nDevice : {device}\")\n",
        "print(f\"Pipeline active : {run_pipeline}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-section1-intro",
      "metadata": {},
      "source": [
        "## Section 1 : Pipeline Text -> Image -> Video\n",
        "\n",
        "Ce premier pipeline combine un modele de generation d'images (DALL-E 3 ou SDXL)\n",
        "avec un modele d'animation (SVD) pour creer des videos a partir de descriptions textuelles.\n",
        "\n",
        "```\n",
        "Texte (prompt) --> [DALL-E 3 / SDXL] --> Image haute qualite\n",
        "                                              |\n",
        "                                              v\n",
        "                                        [SVD / LTX-Video] --> Video animee\n",
        "```\n",
        "\n",
        "### Avantages de cette approche\n",
        "\n",
        "| Aspect | Pipeline 2 etapes | Generation directe |\n",
        "|--------|-------------------|---------------------|\n",
        "| Controle visuel | Eleve (image intermediaire) | Faible |\n",
        "| Qualite premiere frame | Excellente (DALL-E 3) | Variable |\n",
        "| Flexibilite | Choix independant image/video | Lie au modele |\n",
        "| Cout VRAM | Sequentiel (un modele a la fois) | Un seul modele |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-pipeline1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline 1 : Text -> Image -> Video\n",
        "print(\"\\n--- PIPELINE TEXT -> IMAGE -> VIDEO ---\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "\n",
        "def release_vram():\n",
        "    \"\"\"Libere la VRAM GPU de facon aggressive.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "def generate_image_dalle(prompt: str) -> Optional[Image.Image]:\n",
        "    \"\"\"Genere une image via DALL-E 3 (API OpenAI).\"\"\"\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        import base64\n",
        "        from io import BytesIO\n",
        "        \n",
        "        client = OpenAI()\n",
        "        response = client.images.generate(\n",
        "            model=\"dall-e-3\",\n",
        "            prompt=prompt,\n",
        "            size=\"1792x1024\",\n",
        "            quality=\"hd\",\n",
        "            n=1,\n",
        "            response_format=\"b64_json\"\n",
        "        )\n",
        "        \n",
        "        img_data = base64.b64decode(response.data[0].b64_json)\n",
        "        img = Image.open(BytesIO(img_data)).convert('RGB')\n",
        "        # Redimensionner pour SVD (1024x576)\n",
        "        img = img.resize((1024, 576), Image.LANCZOS)\n",
        "        return img\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  Erreur DALL-E 3 : {type(e).__name__}: {str(e)[:100]}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def generate_image_sdxl(prompt: str) -> Optional[Image.Image]:\n",
        "    \"\"\"Genere une image via SDXL local.\"\"\"\n",
        "    try:\n",
        "        from diffusers import StableDiffusionXLPipeline\n",
        "        \n",
        "        pipe_sdxl = StableDiffusionXLPipeline.from_pretrained(\n",
        "            \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "            torch_dtype=torch.float16,\n",
        "            variant=\"fp16\"\n",
        "        ).to(device)\n",
        "        pipe_sdxl.enable_vae_slicing()\n",
        "        \n",
        "        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "        img = pipe_sdxl(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=\"low quality, blurry, distorted\",\n",
        "            num_inference_steps=30,\n",
        "            guidance_scale=7.5,\n",
        "            height=576,\n",
        "            width=1024,\n",
        "            generator=generator\n",
        "        ).images[0]\n",
        "        \n",
        "        del pipe_sdxl\n",
        "        release_vram()\n",
        "        return img\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  Erreur SDXL : {type(e).__name__}: {str(e)[:100]}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def animate_with_svd(image: Image.Image) -> Optional[List]:\n",
        "    \"\"\"Anime une image avec SVD.\"\"\"\n",
        "    try:\n",
        "        from diffusers import StableVideoDiffusionPipeline\n",
        "        \n",
        "        pipe_svd = StableVideoDiffusionPipeline.from_pretrained(\n",
        "            \"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
        "            torch_dtype=torch.float16,\n",
        "            variant=\"fp16\"\n",
        "        ).to(device)\n",
        "        pipe_svd.enable_vae_slicing()\n",
        "        \n",
        "        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "        output = pipe_svd(\n",
        "            image=image,\n",
        "            num_frames=num_frames,\n",
        "            fps=7,\n",
        "            motion_bucket_id=127,\n",
        "            noise_aug_strength=0.02,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            decode_chunk_size=8,\n",
        "            generator=generator\n",
        "        )\n",
        "        \n",
        "        frames = output.frames[0]\n",
        "        del pipe_svd\n",
        "        release_vram()\n",
        "        return frames\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  Erreur SVD : {type(e).__name__}: {str(e)[:100]}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Execution du pipeline text -> image -> video\n",
        "pipeline1_prompt = \"a majestic lighthouse on a rocky cliff at sunset, dramatic clouds, golden light, cinematic photography\"\n",
        "pipeline1_result = {\"prompt\": pipeline1_prompt}\n",
        "\n",
        "if run_pipeline:\n",
        "    print(f\"Prompt : {pipeline1_prompt}\")\n",
        "    \n",
        "    # Etape 1 : Generation de l'image\n",
        "    print(f\"\\n  Etape 1 : Generation image ({image_model})...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if image_model == \"dall-e-3\":\n",
        "        source_image = generate_image_dalle(pipeline1_prompt)\n",
        "    else:\n",
        "        source_image = generate_image_sdxl(pipeline1_prompt)\n",
        "    \n",
        "    if source_image is None:\n",
        "        # Fallback : creer une image synthetique\n",
        "        print(\"  Fallback : image synthetique\")\n",
        "        source_image = Image.new('RGB', (1024, 576))\n",
        "        draw = ImageDraw.Draw(source_image)\n",
        "        for y in range(576):\n",
        "            t = y / 576\n",
        "            r = int(255 - 120 * t)\n",
        "            g = int(160 - 60 * t)\n",
        "            b = int(80 + 100 * t)\n",
        "            draw.line([(0, y), (1024, y)], fill=(r, g, b))\n",
        "        # Silhouette de phare\n",
        "        draw.rectangle([480, 100, 544, 400], fill=(60, 55, 50))\n",
        "        draw.ellipse([460, 70, 564, 130], fill=(255, 240, 180))\n",
        "    \n",
        "    img_time = time.time() - start_time\n",
        "    pipeline1_result[\"image_time\"] = img_time\n",
        "    pipeline1_result[\"source_image\"] = source_image\n",
        "    print(f\"  Image generee en {img_time:.1f}s ({source_image.size[0]}x{source_image.size[1]})\")\n",
        "    \n",
        "    # Sauvegarde image intermediaire\n",
        "    img_path = OUTPUT_DIR / \"pipeline1_source.png\"\n",
        "    source_image.save(str(img_path))\n",
        "    \n",
        "    # Etape 2 : Animation avec SVD\n",
        "    print(f\"\\n  Etape 2 : Animation video (SVD)...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    video_frames = animate_with_svd(source_image)\n",
        "    \n",
        "    if video_frames:\n",
        "        video_time = time.time() - start_time\n",
        "        pipeline1_result[\"video_time\"] = video_time\n",
        "        pipeline1_result[\"frames\"] = video_frames\n",
        "        pipeline1_result[\"total_time\"] = img_time + video_time\n",
        "        pipeline1_result[\"success\"] = True\n",
        "        \n",
        "        print(f\"  Video generee en {video_time:.1f}s ({len(video_frames)} frames)\")\n",
        "        print(f\"  Temps total pipeline : {pipeline1_result['total_time']:.1f}s\")\n",
        "        \n",
        "        # Affichage : image source + frames\n",
        "        fig, axes = plt.subplots(1, 5, figsize=(18, 3.5))\n",
        "        axes[0].imshow(source_image)\n",
        "        axes[0].set_title(f\"Source ({image_model})\", fontsize=9, fontweight='bold')\n",
        "        axes[0].axis('off')\n",
        "        \n",
        "        frame_indices = np.linspace(0, len(video_frames) - 1, 4, dtype=int)\n",
        "        for i, fi in enumerate(frame_indices):\n",
        "            axes[i + 1].imshow(video_frames[fi])\n",
        "            axes[i + 1].set_title(f\"Frame {fi + 1}\", fontsize=9)\n",
        "            axes[i + 1].axis('off')\n",
        "        \n",
        "        plt.suptitle(f\"Pipeline Text -> Image -> Video\", fontsize=13, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        if save_as_mp4:\n",
        "            mp4_path = OUTPUT_DIR / \"pipeline1_result.mp4\"\n",
        "            export_to_video(video_frames, str(mp4_path), fps=fps_output)\n",
        "            print(f\"  MP4 : {mp4_path.name}\")\n",
        "    else:\n",
        "        pipeline1_result[\"success\"] = False\n",
        "        print(\"  Animation echouee\")\n",
        "else:\n",
        "    print(\"Pipeline desactive\")\n",
        "    print(\"\\nSchema du pipeline :\")\n",
        "    print(\"  1. DALL-E 3 / SDXL genere une image haute qualite\")\n",
        "    print(\"  2. SVD anime l'image en 25 frames\")\n",
        "    print(\"  3. Export en MP4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-section2-intro",
      "metadata": {},
      "source": [
        "### Interpretation : Pipeline Text -> Image -> Video\n",
        "\n",
        "| Etape | Modele | Temps typique | Cout |\n",
        "|-------|--------|---------------|------|\n",
        "| Generation image | DALL-E 3 (API) | 5-15s | ~0.04$ |\n",
        "| Generation image | SDXL (local) | 10-20s | Gratuit (GPU) |\n",
        "| Animation | SVD | 20-40s | Gratuit (GPU) |\n",
        "\n",
        "**Points cles** :\n",
        "1. L'image intermediaire peut etre verifiee et ajustee avant l'animation\n",
        "2. DALL-E 3 produit des images plus detaillees mais necessite une API key\n",
        "3. Le chargement sequentiel (SDXL puis SVD) permet de rester sous 24 GB VRAM\n",
        "\n",
        "## Section 2 : Pipeline Text -> Video -> Upscale\n",
        "\n",
        "Ce second pipeline genere d'abord une video brute, puis l'ameliore\n",
        "via un upscaling spatial (Real-ESRGAN) et une interpolation temporelle.\n",
        "\n",
        "```\n",
        "Texte --> [LTX-Video] --> Video basse resolution\n",
        "                               |\n",
        "                               v\n",
        "                         [Real-ESRGAN] --> Frames HD\n",
        "                               |\n",
        "                               v\n",
        "                         [Interpolation] --> Video fluide HD\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-pipeline2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline 2 : Text -> Video -> Upscale -> Interpolation\n",
        "print(\"\\n--- PIPELINE TEXT -> VIDEO -> UPSCALE ---\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "\n",
        "def generate_video_ltx(prompt: str) -> Optional[List]:\n",
        "    \"\"\"Genere une video avec LTX-Video (rapide, basse resolution).\"\"\"\n",
        "    try:\n",
        "        from diffusers import LTXPipeline\n",
        "        \n",
        "        pipe_ltx = LTXPipeline.from_pretrained(\n",
        "            \"Lightricks/LTX-Video\",\n",
        "            torch_dtype=torch.float16\n",
        "        ).to(device)\n",
        "        pipe_ltx.enable_vae_slicing()\n",
        "        \n",
        "        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "        output = pipe_ltx(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=\"low quality, blurry, distorted\",\n",
        "            num_frames=num_frames,\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            height=320,\n",
        "            width=512,\n",
        "            generator=generator\n",
        "        )\n",
        "        \n",
        "        frames = output.frames[0]\n",
        "        del pipe_ltx\n",
        "        release_vram()\n",
        "        return frames\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  Erreur LTX-Video : {type(e).__name__}: {str(e)[:100]}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def upscale_frames(frames: List, scale: int = 2) -> List:\n",
        "    \"\"\"\n",
        "    Upscale les frames avec Real-ESRGAN ou fallback bicubique.\n",
        "    \n",
        "    Args:\n",
        "        frames: Liste d'images PIL\n",
        "        scale: Facteur d'agrandissement\n",
        "    \n",
        "    Returns:\n",
        "        Liste d'images PIL upscalees\n",
        "    \"\"\"\n",
        "    upscaled = []\n",
        "    use_esrgan = False\n",
        "    \n",
        "    # Tenter Real-ESRGAN\n",
        "    try:\n",
        "        from realesrgan import RealESRGANer\n",
        "        from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "        \n",
        "        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
        "        upsampler = RealESRGANer(\n",
        "            scale=scale,\n",
        "            model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth',\n",
        "            model=model,\n",
        "            half=True\n",
        "        )\n",
        "        use_esrgan = True\n",
        "        print(f\"  Real-ESRGAN disponible (scale x{scale})\")\n",
        "    except ImportError:\n",
        "        print(f\"  Real-ESRGAN non disponible, fallback bicubique (scale x{scale})\")\n",
        "    \n",
        "    for i, frame in enumerate(frames):\n",
        "        if use_esrgan:\n",
        "            img_array = np.array(frame)\n",
        "            output, _ = upsampler.enhance(img_array, outscale=scale)\n",
        "            upscaled.append(Image.fromarray(output))\n",
        "        else:\n",
        "            w, h = frame.size\n",
        "            upscaled.append(frame.resize((w * scale, h * scale), Image.LANCZOS))\n",
        "    \n",
        "    if use_esrgan:\n",
        "        del upsampler\n",
        "        release_vram()\n",
        "    \n",
        "    return upscaled\n",
        "\n",
        "\n",
        "def interpolate_frames(frames: List, factor: int = 2) -> List:\n",
        "    \"\"\"\n",
        "    Interpole des frames intermediaires par blending lineaire.\n",
        "    \n",
        "    Args:\n",
        "        frames: Liste d'images PIL\n",
        "        factor: Nombre de frames intermediaires a inserer\n",
        "    \n",
        "    Returns:\n",
        "        Liste d'images PIL avec frames interpolees\n",
        "    \"\"\"\n",
        "    interpolated = []\n",
        "    for i in range(len(frames) - 1):\n",
        "        interpolated.append(frames[i])\n",
        "        f1 = np.array(frames[i]).astype(float)\n",
        "        f2 = np.array(frames[i + 1]).astype(float)\n",
        "        for j in range(1, factor):\n",
        "            alpha = j / factor\n",
        "            blended = ((1 - alpha) * f1 + alpha * f2).astype(np.uint8)\n",
        "            interpolated.append(Image.fromarray(blended))\n",
        "    interpolated.append(frames[-1])\n",
        "    return interpolated\n",
        "\n",
        "\n",
        "# Execution du pipeline 2\n",
        "pipeline2_prompt = \"a time-lapse of a flower blooming in a garden, macro photography, soft focus background\"\n",
        "pipeline2_result = {\"prompt\": pipeline2_prompt}\n",
        "\n",
        "if run_pipeline:\n",
        "    print(f\"Prompt : {pipeline2_prompt}\")\n",
        "    \n",
        "    # Etape 1 : Generation video brute\n",
        "    print(f\"\\n  Etape 1 : Generation video brute (LTX-Video)...\")\n",
        "    start_time = time.time()\n",
        "    raw_frames = generate_video_ltx(pipeline2_prompt)\n",
        "    \n",
        "    if raw_frames:\n",
        "        raw_time = time.time() - start_time\n",
        "        pipeline2_result[\"raw_time\"] = raw_time\n",
        "        print(f\"  Video brute : {len(raw_frames)} frames en {raw_time:.1f}s\")\n",
        "        print(f\"  Resolution : {raw_frames[0].size[0]}x{raw_frames[0].size[1]}\")\n",
        "        \n",
        "        # Etape 2 : Upscaling\n",
        "        if upscale:\n",
        "            print(f\"\\n  Etape 2 : Upscaling (x2)...\")\n",
        "            start_time = time.time()\n",
        "            upscaled_frames = upscale_frames(raw_frames, scale=2)\n",
        "            upscale_time = time.time() - start_time\n",
        "            pipeline2_result[\"upscale_time\"] = upscale_time\n",
        "            print(f\"  Upscale en {upscale_time:.1f}s\")\n",
        "            print(f\"  Resolution : {upscaled_frames[0].size[0]}x{upscaled_frames[0].size[1]}\")\n",
        "        else:\n",
        "            upscaled_frames = raw_frames\n",
        "        \n",
        "        # Etape 3 : Interpolation temporelle\n",
        "        print(f\"\\n  Etape 3 : Interpolation temporelle (x2)...\")\n",
        "        start_time = time.time()\n",
        "        final_frames = interpolate_frames(upscaled_frames, factor=2)\n",
        "        interp_time = time.time() - start_time\n",
        "        pipeline2_result[\"interp_time\"] = interp_time\n",
        "        pipeline2_result[\"frames\"] = final_frames\n",
        "        pipeline2_result[\"success\"] = True\n",
        "        \n",
        "        total_time = raw_time + pipeline2_result.get(\"upscale_time\", 0) + interp_time\n",
        "        pipeline2_result[\"total_time\"] = total_time\n",
        "        \n",
        "        print(f\"  Interpolation en {interp_time:.1f}s\")\n",
        "        print(f\"  Frames finales : {len(final_frames)} (x2 par interpolation)\")\n",
        "        print(f\"  Temps total pipeline : {total_time:.1f}s\")\n",
        "        \n",
        "        # Affichage : brut vs upscale\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "        raw_indices = np.linspace(0, len(raw_frames) - 1, 4, dtype=int)\n",
        "        for i, fi in enumerate(raw_indices):\n",
        "            axes[0][i].imshow(raw_frames[fi])\n",
        "            axes[0][i].set_title(f\"Brut - Frame {fi+1}\", fontsize=9)\n",
        "            axes[0][i].axis('off')\n",
        "        \n",
        "        final_indices = np.linspace(0, len(final_frames) - 1, 4, dtype=int)\n",
        "        for i, fi in enumerate(final_indices):\n",
        "            axes[1][i].imshow(final_frames[fi])\n",
        "            axes[1][i].set_title(f\"Final - Frame {fi+1}\", fontsize=9)\n",
        "            axes[1][i].axis('off')\n",
        "        \n",
        "        plt.suptitle(\"Pipeline : Brut vs Upscale + Interpolation\", fontsize=13, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        if save_as_mp4:\n",
        "            mp4_path = OUTPUT_DIR / \"pipeline2_result.mp4\"\n",
        "            export_to_video(final_frames, str(mp4_path), fps=fps_output * 2)\n",
        "            print(f\"  MP4 : {mp4_path.name}\")\n",
        "    else:\n",
        "        pipeline2_result[\"success\"] = False\n",
        "        print(\"  Generation video echouee\")\n",
        "else:\n",
        "    print(\"Pipeline desactive\")\n",
        "    print(\"\\nSchema du pipeline :\")\n",
        "    print(\"  1. LTX-Video genere une video basse resolution\")\n",
        "    print(\"  2. Real-ESRGAN upscale chaque frame\")\n",
        "    print(\"  3. Interpolation temporelle double le nombre de frames\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-section3-intro",
      "metadata": {},
      "source": [
        "### Interpretation : Pipeline Video -> Upscale\n",
        "\n",
        "| Etape | Entree | Sortie | Temps typique |\n",
        "|-------|--------|--------|---------------|\n",
        "| LTX-Video | Prompt texte | 16 frames 512x320 | 15-30s |\n",
        "| Real-ESRGAN x2 | 16 frames 512x320 | 16 frames 1024x640 | 10-20s |\n",
        "| Interpolation x2 | 16 frames | 31 frames | <1s |\n",
        "\n",
        "**Points cles** :\n",
        "1. L'upscaling par frame est simple mais ne gere pas la coherence temporelle\n",
        "2. L'interpolation lineaire produit des transitions douces mais pas de nouveaux mouvements\n",
        "3. Pour une interpolation avancee, des modeles comme RIFE ou FILM seraient preferables\n",
        "\n",
        "## Section 3 : Generation batch multi-prompts\n",
        "\n",
        "Nous allons generer plusieurs videos en batch pour simuler un workflow de production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-batch",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation batch multi-prompts\n",
        "print(\"\\n--- GENERATION BATCH ---\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "batch_prompts = [\n",
        "    {\"text\": \"ocean waves crashing on a rocky shore at sunset, aerial view\", \"label\": \"Ocean\"},\n",
        "    {\"text\": \"a candle flame flickering in a dark room, warm light, close-up\", \"label\": \"Bougie\"},\n",
        "    {\"text\": \"clouds moving over a mountain valley, timelapse, dramatic sky\", \"label\": \"Nuages\"},\n",
        "]\n",
        "\n",
        "batch_results = []\n",
        "\n",
        "if run_pipeline:\n",
        "    # Charger un seul pipeline pour tout le batch\n",
        "    print(f\"Chargement du pipeline LTX-Video pour le batch...\")\n",
        "    try:\n",
        "        from diffusers import LTXPipeline\n",
        "        pipe_batch = LTXPipeline.from_pretrained(\n",
        "            \"Lightricks/LTX-Video\",\n",
        "            torch_dtype=torch.float16\n",
        "        ).to(device)\n",
        "        pipe_batch.enable_vae_slicing()\n",
        "        print(\"Pipeline charge\")\n",
        "        \n",
        "        for p_idx, prompt_info in enumerate(batch_prompts):\n",
        "            print(f\"\\n  Generation {p_idx + 1}/{len(batch_prompts)} : {prompt_info['label']}\")\n",
        "            print(f\"  Prompt : {prompt_info['text'][:60]}...\")\n",
        "            \n",
        "            generator = torch.Generator(device=device).manual_seed(seed + p_idx)\n",
        "            start_time = time.time()\n",
        "            \n",
        "            try:\n",
        "                output = pipe_batch(\n",
        "                    prompt=prompt_info['text'],\n",
        "                    negative_prompt=\"low quality, blurry, distorted\",\n",
        "                    num_frames=num_frames,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    height=320,\n",
        "                    width=512,\n",
        "                    generator=generator\n",
        "                )\n",
        "                \n",
        "                gen_time = time.time() - start_time\n",
        "                frames = output.frames[0]\n",
        "                \n",
        "                batch_results.append({\n",
        "                    \"label\": prompt_info['label'],\n",
        "                    \"prompt\": prompt_info['text'],\n",
        "                    \"frames\": frames,\n",
        "                    \"time\": gen_time,\n",
        "                    \"success\": True\n",
        "                })\n",
        "                \n",
        "                print(f\"  OK en {gen_time:.1f}s ({len(frames)} frames)\")\n",
        "                \n",
        "                if save_as_mp4:\n",
        "                    mp4_path = OUTPUT_DIR / f\"batch_{prompt_info['label'].lower()}.mp4\"\n",
        "                    export_to_video(frames, str(mp4_path), fps=fps_output)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"  Erreur : {type(e).__name__}: {str(e)[:100]}\")\n",
        "                batch_results.append({\"label\": prompt_info['label'], \"success\": False})\n",
        "        \n",
        "        del pipe_batch\n",
        "        release_vram()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Erreur chargement pipeline batch : {type(e).__name__}: {str(e)[:100]}\")\n",
        "    \n",
        "    # Affichage des resultats batch\n",
        "    successful_batch = [r for r in batch_results if r.get('success', False)]\n",
        "    if successful_batch:\n",
        "        n_videos = len(successful_batch)\n",
        "        n_preview = 4\n",
        "        fig, axes = plt.subplots(n_videos, n_preview, figsize=(3.5 * n_preview, 3 * n_videos))\n",
        "        if n_videos == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for v_idx, br in enumerate(successful_batch):\n",
        "            frame_indices = np.linspace(0, len(br['frames']) - 1, n_preview, dtype=int)\n",
        "            for f_idx, fi in enumerate(frame_indices):\n",
        "                axes[v_idx][f_idx].imshow(br['frames'][fi])\n",
        "                axes[v_idx][f_idx].axis('off')\n",
        "                if f_idx == 0:\n",
        "                    axes[v_idx][f_idx].set_ylabel(br['label'], fontsize=11, fontweight='bold')\n",
        "        \n",
        "        plt.suptitle(\"Generation Batch - LTX-Video\", fontsize=13, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        total_batch_time = sum(r.get('time', 0) for r in successful_batch)\n",
        "        print(f\"\\nBatch : {len(successful_batch)} videos en {total_batch_time:.1f}s total\")\n",
        "        print(f\"Moyenne : {total_batch_time / len(successful_batch):.1f}s par video\")\n",
        "else:\n",
        "    print(\"Generation batch desactivee\")\n",
        "    print(\"\\nLe batch reutilise un seul pipeline charge en memoire :\")\n",
        "    print(\"  1. Charger le pipeline une fois\")\n",
        "    print(\"  2. Generer N videos avec des prompts differents\")\n",
        "    print(\"  3. Liberer le pipeline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-section4-intro",
      "metadata": {},
      "source": [
        "### Interpretation : Generation batch\n",
        "\n",
        "| Aspect | Single | Batch |\n",
        "|--------|--------|-------|\n",
        "| Chargement modele | 1 fois par video | 1 fois pour N videos |\n",
        "| Temps total (3 videos) | ~3x (charge + gen) | 1x charge + 3x gen |\n",
        "| VRAM | Variable | Stable |\n",
        "\n",
        "**Points cles** :\n",
        "1. En batch, le temps de chargement du modele est amorti sur toutes les generations\n",
        "2. La VRAM reste stable pendant le batch (meme modele charge)\n",
        "3. La generation batch est la strategie optimale pour produire de nombreuses videos\n",
        "\n",
        "## Comparaison des pipelines\n",
        "\n",
        "| Pipeline | Force | Faiblesse | Cas d'usage |\n",
        "|----------|-------|-----------|-------------|\n",
        "| Text->Image->Video | Controle, qualite premiere frame | 2 modeles, plus lent | Production soignee |\n",
        "| Text->Video->Upscale | Pipeline bout-en-bout | Upscale par frame (pas temporel) | Amelioration qualite |\n",
        "| Batch multi-prompts | Efficace pour N videos | Pipeline unique | Production en serie |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-interactive",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mode interactif : pipeline personnalise\n",
        "if notebook_mode == \"interactive\" and not skip_widgets:\n",
        "    print(\"\\n--- MODE INTERACTIF ---\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"Entrez une description pour generer une video via le pipeline complet.\")\n",
        "    print(\"Pipeline : Text -> Image (synthetique) -> Video (SVD)\")\n",
        "    print(\"(Laissez vide pour passer a la suite)\")\n",
        "    \n",
        "    try:\n",
        "        user_desc = input(\"\\nVotre description : \").strip()\n",
        "        \n",
        "        if user_desc and run_pipeline:\n",
        "            print(f\"\\nGeneration avec : {user_desc}\")\n",
        "            \n",
        "            # Etape 1 : Image synthetique basee sur la description\n",
        "            print(\"  Etape 1 : Generation image (SDXL ou synthetique)...\")\n",
        "            user_image = generate_image_sdxl(user_desc)\n",
        "            if user_image is None:\n",
        "                user_image = Image.new('RGB', (1024, 576), (120, 140, 180))\n",
        "                draw = ImageDraw.Draw(user_image)\n",
        "                for y in range(576):\n",
        "                    t = y / 576\n",
        "                    draw.line([(0, y), (1024, y)], fill=(int(120+80*t), int(140-30*t), int(180-60*t)))\n",
        "                print(\"  Image synthetique utilisee\")\n",
        "            \n",
        "            # Etape 2 : Animation\n",
        "            print(\"  Etape 2 : Animation (SVD)...\")\n",
        "            user_frames = animate_with_svd(user_image)\n",
        "            \n",
        "            if user_frames:\n",
        "                n_display = min(6, len(user_frames))\n",
        "                fig, axes = plt.subplots(1, n_display + 1, figsize=(2.5 * (n_display + 1), 3))\n",
        "                axes[0].imshow(user_image)\n",
        "                axes[0].set_title(\"Source\", fontsize=9, fontweight='bold')\n",
        "                axes[0].axis('off')\n",
        "                \n",
        "                indices = np.linspace(0, len(user_frames) - 1, n_display, dtype=int)\n",
        "                for i, idx in enumerate(indices):\n",
        "                    axes[i + 1].imshow(user_frames[idx])\n",
        "                    axes[i + 1].set_title(f\"Frame {idx+1}\", fontsize=8)\n",
        "                    axes[i + 1].axis('off')\n",
        "                plt.suptitle(f\"Pipeline : {user_desc[:50]}...\", fontweight='bold')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "                \n",
        "                if save_as_mp4:\n",
        "                    user_mp4 = OUTPUT_DIR / \"user_pipeline.mp4\"\n",
        "                    export_to_video(user_frames, str(user_mp4), fps=fps_output)\n",
        "                    print(f\"MP4 sauvegarde : {user_mp4.name}\")\n",
        "            else:\n",
        "                print(\"Animation echouee\")\n",
        "        elif user_desc:\n",
        "            print(\"Pipeline non disponible\")\n",
        "        else:\n",
        "            print(\"Mode interactif ignore\")\n",
        "    \n",
        "    except (KeyboardInterrupt, EOFError) as e:\n",
        "        print(f\"\\nMode interactif interrompu ({type(e).__name__})\")\n",
        "    except Exception as e:\n",
        "        error_type = type(e).__name__\n",
        "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
        "            print(\"\\nMode interactif non disponible (execution automatisee)\")\n",
        "        else:\n",
        "            print(f\"\\nErreur inattendue : {error_type} - {str(e)[:100]}\")\n",
        "            print(\"Passage a la suite du notebook\")\n",
        "else:\n",
        "    print(\"\\nMode batch - Interface interactive desactivee\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-conclusion",
      "metadata": {},
      "source": [
        "## Bonnes pratiques d'orchestration\n",
        "\n",
        "### Guide de choix de pipeline\n",
        "\n",
        "| Besoin | Pipeline recommande | Raison |\n",
        "|--------|-------------------|--------|\n",
        "| Controle maximal | Text->Image->Video | Image intermediaire verifiable |\n",
        "| Qualite HD | Text->Video->Upscale | Amelioration post-generation |\n",
        "| Production en serie | Batch multi-prompts | Efficacite de chargement |\n",
        "| Budget VRAM limite | Pipeline sequentiel | Un modele a la fois |\n",
        "\n",
        "### Strategies avancees\n",
        "\n",
        "| Strategie | Description |\n",
        "|-----------|-------------|\n",
        "| **Pipeline hybride** | Combiner DALL-E 3 (cloud) + SVD (local) pour le meilleur des deux |\n",
        "| **Cache intermediaire** | Sauvegarder les images/videos intermediaires pour re-generation partielle |\n",
        "| **Parallelisation** | Generer les images en parallele (API), animer sequentiellement (GPU) |\n",
        "| **Validation automatique** | Verifier la qualite avant l'etape suivante (seuil de nettete) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-stats",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistiques de session et prochaines etapes\n",
        "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Mode : {notebook_mode}\")\n",
        "print(f\"Pipeline mode : {pipeline_mode}\")\n",
        "print(f\"Image model : {image_model}\")\n",
        "print(f\"Video model : {video_model}\")\n",
        "print(f\"Upscale : {upscale}\")\n",
        "\n",
        "# Recapitulatif des pipelines executes\n",
        "print(f\"\\nResultats des pipelines :\")\n",
        "if pipeline1_result.get('success'):\n",
        "    print(f\"  Pipeline 1 (Text->Image->Video) : OK en {pipeline1_result.get('total_time', 0):.1f}s\")\n",
        "else:\n",
        "    print(f\"  Pipeline 1 (Text->Image->Video) : non execute\")\n",
        "\n",
        "if pipeline2_result.get('success'):\n",
        "    print(f\"  Pipeline 2 (Text->Video->Upscale) : OK en {pipeline2_result.get('total_time', 0):.1f}s\")\n",
        "else:\n",
        "    print(f\"  Pipeline 2 (Text->Video->Upscale) : non execute\")\n",
        "\n",
        "if batch_results:\n",
        "    n_ok = sum(1 for r in batch_results if r.get('success'))\n",
        "    print(f\"  Batch : {n_ok}/{len(batch_results)} videos generees\")\n",
        "\n",
        "if device == \"cuda\" and torch.cuda.is_available():\n",
        "    vram_current = torch.cuda.memory_allocated(0) / 1024**3\n",
        "    print(f\"\\nVRAM actuelle : {vram_current:.1f} GB\")\n",
        "\n",
        "if save_results and OUTPUT_DIR.exists():\n",
        "    generated_files = list(OUTPUT_DIR.glob('*'))\n",
        "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
        "    for f in sorted(generated_files):\n",
        "        size_kb = f.stat().st_size / 1024\n",
        "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
        "\n",
        "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
        "print(f\"1. Notebook 03-3 : Workflows ComfyUI pour la generation video\")\n",
        "print(f\"2. Module 04 : Applications production (education, creatif, bout-en-bout)\")\n",
        "print(f\"3. Combiner les pipelines pour des workflows de production automatises\")\n",
        "\n",
        "print(f\"\\nNotebook 03-2 Orchestration de Pipelines Video termine - {datetime.now().strftime('%H:%M:%S')}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}