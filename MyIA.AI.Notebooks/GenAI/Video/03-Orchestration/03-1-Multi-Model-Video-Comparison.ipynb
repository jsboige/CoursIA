{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {
    "papermill": {
     "duration": 0.002544,
     "end_time": "2026-02-26T07:10:41.287547",
     "exception": false,
     "start_time": "2026-02-26T07:10:41.285003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comparaison Multi-Modeles de Generation Video\n",
    "\n",
    "**Module :** 03-Video-Orchestration  \n",
    "**Niveau :** Avance  \n",
    "**Technologies :** HunyuanVideo vs LTX-Video vs Wan vs SVD (benchmark)  \n",
    "**Duree estimee :** 60 minutes  \n",
    "**VRAM :** ~18 GB (chargement sequentiel)  \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "- [ ] Mettre en place un framework de benchmark pour modeles video generatifs\n",
    "- [ ] Charger et decharger les modeles sequentiellement (gestion VRAM)\n",
    "- [ ] Generer des videos avec le meme prompt sur chaque modele\n",
    "- [ ] Mesurer les metriques : temps de generation, VRAM, coherence temporelle, qualite\n",
    "- [ ] Produire une grille comparative de frames cote-a-cote\n",
    "- [ ] Analyser le compromis cout/qualite entre modeles\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- GPU avec 18+ GB VRAM (RTX 3090 / RTX 4090)\n",
    "- Module 02-Advanced complete (notebooks 02-1 a 02-4)\n",
    "- Packages : `diffusers>=0.32`, `transformers`, `torch`, `accelerate`, `bitsandbytes`, `imageio`, `imageio-ffmpeg`\n",
    "\n",
    "**Navigation :** [<< 02-4](../02-Advanced/02-4-SVD-Image-to-Video.ipynb) | [Index](../README.md) | [Suivant >>](03-2-Video-Workflow-Orchestration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-params",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:10:41.292340Z",
     "iopub.status.busy": "2026-02-26T07:10:41.292141Z",
     "iopub.status.idle": "2026-02-26T07:10:41.296807Z",
     "shell.execute_reply": "2026-02-26T07:10:41.296036Z"
    },
    "papermill": {
     "duration": 0.008212,
     "end_time": "2026-02-26T07:10:41.297883",
     "exception": false,
     "start_time": "2026-02-26T07:10:41.289671",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parametres Papermill - JAMAIS modifier ce commentaire\n",
    "\n",
    "# Configuration notebook\n",
    "notebook_mode = \"interactive\"        # \"interactive\" ou \"batch\"\n",
    "skip_widgets = False               # True pour mode batch MCP\n",
    "debug_level = \"INFO\"\n",
    "\n",
    "# Parametres benchmark\n",
    "models_to_test = [\"hunyuan\", \"ltx\", \"wan\"]  # Modeles a comparer\n",
    "benchmark_prompt = \"a cat walking gracefully through a garden, soft sunlight, cinematic\"  # Prompt commun\n",
    "num_frames = 16                    # Nombre de frames par modele\n",
    "device = \"cuda\"                    # Device de calcul\n",
    "\n",
    "# Parametres generation communs\n",
    "num_inference_steps = 25           # Etapes de debruitage\n",
    "guidance_scale = 6.0               # CFG scale\n",
    "height = 320                       # Hauteur video\n",
    "width = 512                        # Largeur video\n",
    "fps_output = 8                     # FPS de la video de sortie\n",
    "seed = 42                          # Graine pour reproductibilite\n",
    "\n",
    "# Configuration\n",
    "run_benchmark = True               # Executer le benchmark\n",
    "save_as_mp4 = True                 # Sauvegarder en MP4\n",
    "save_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb79e806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:10:41.302779Z",
     "iopub.status.busy": "2026-02-26T07:10:41.302593Z",
     "iopub.status.idle": "2026-02-26T07:10:41.305711Z",
     "shell.execute_reply": "2026-02-26T07:10:41.305072Z"
    },
    "papermill": {
     "duration": 0.006782,
     "end_time": "2026-02-26T07:10:41.306839",
     "exception": false,
     "start_time": "2026-02-26T07:10:41.300057",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "notebook_mode = \"batch\"\n",
    "skip_widgets = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:10:41.312366Z",
     "iopub.status.busy": "2026-02-26T07:10:41.312153Z",
     "iopub.status.idle": "2026-02-26T07:10:41.767497Z",
     "shell.execute_reply": "2026-02-26T07:10:41.766474Z"
    },
    "papermill": {
     "duration": 0.459495,
     "end_time": "2026-02-26T07:10:41.768768",
     "exception": false,
     "start_time": "2026-02-26T07:10:41.309273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers GenAI importes\n",
      "Comparaison Multi-Modeles de Generation Video\n",
      "Date : 2026-02-26 08:10:41\n",
      "Mode : batch\n",
      "Modeles a tester : ['hunyuan', 'ltx', 'wan']\n",
      "Prompt : a cat walking gracefully through a garden, soft sunlight, ci...\n",
      "Frames : 16, Steps : 25, CFG : 6.0\n"
     ]
    }
   ],
   "source": [
    "# Setup environnement et imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Import helpers GenAI\n",
    "GENAI_ROOT = Path.cwd()\n",
    "while GENAI_ROOT.name != 'GenAI' and len(GENAI_ROOT.parts) > 1:\n",
    "    GENAI_ROOT = GENAI_ROOT.parent\n",
    "\n",
    "HELPERS_PATH = GENAI_ROOT / 'shared' / 'helpers'\n",
    "if HELPERS_PATH.exists():\n",
    "    sys.path.insert(0, str(HELPERS_PATH.parent))\n",
    "    try:\n",
    "        from helpers.genai_helpers import setup_genai_logging\n",
    "        print(\"Helpers GenAI importes\")\n",
    "    except ImportError:\n",
    "        print(\"Helpers GenAI non disponibles - mode autonome\")\n",
    "\n",
    "OUTPUT_DIR = GENAI_ROOT / 'outputs' / 'benchmark_video'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=getattr(logging, debug_level))\n",
    "logger = logging.getLogger('benchmark_video')\n",
    "\n",
    "print(f\"Comparaison Multi-Modeles de Generation Video\")\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Modeles a tester : {models_to_test}\")\n",
    "print(f\"Prompt : {benchmark_prompt[:60]}...\")\n",
    "print(f\"Frames : {num_frames}, Steps : {num_inference_steps}, CFG : {guidance_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-env",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:10:41.775929Z",
     "iopub.status.busy": "2026-02-26T07:10:41.775475Z",
     "iopub.status.idle": "2026-02-26T07:10:46.987976Z",
     "shell.execute_reply": "2026-02-26T07:10:46.987113Z"
    },
    "papermill": {
     "duration": 5.216917,
     "end_time": "2026-02-26T07:10:46.988931",
     "exception": false,
     "start_time": "2026-02-26T07:10:41.772014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier .env charge depuis : D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\.env\n",
      "\n",
      "--- VERIFICATION GPU ---\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA non disponible.\n",
      "Le benchmark necessite un GPU. Le notebook montrera le code sans executer.\n",
      "\n",
      "--- VERIFICATION DEPENDANCES ---\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusers : v0.36.0\n",
      "transformers NON INSTALLE\n",
      "imageio : v2.37.2\n",
      "\n",
      "Dependances manquantes. Le notebook montrera le code sans executer.\n",
      "\n",
      "Device : cpu\n",
      "Benchmark active : False\n"
     ]
    }
   ],
   "source": [
    "# Chargement .env et verification GPU\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_path = Path.cwd()\n",
    "found_env = False\n",
    "for _ in range(4):\n",
    "    env_path = current_path / '.env'\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"Fichier .env charge depuis : {env_path}\")\n",
    "        found_env = True\n",
    "        break\n",
    "    current_path = current_path.parent\n",
    "\n",
    "if not found_env:\n",
    "    print(\"Aucun fichier .env trouve\")\n",
    "\n",
    "# Verification GPU\n",
    "print(\"\\n--- VERIFICATION GPU ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "    vram_free = (torch.cuda.get_device_properties(0).total_mem - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "    \n",
    "    print(f\"GPU : {gpu_name}\")\n",
    "    print(f\"VRAM totale : {vram_total:.1f} GB\")\n",
    "    print(f\"VRAM libre : {vram_free:.1f} GB\")\n",
    "    print(f\"CUDA : {torch.version.cuda}\")\n",
    "    \n",
    "    if vram_total < 18:\n",
    "        print(f\"\\nAttention : VRAM ({vram_total:.0f} GB) < 18 GB recommandes\")\n",
    "        print(\"Certains modeles pourraient ne pas charger\")\n",
    "        height = 256\n",
    "        width = 384\n",
    "        num_frames = 12\n",
    "        print(f\"  Resolution reduite a {width}x{height}, {num_frames} frames\")\n",
    "else:\n",
    "    print(\"CUDA non disponible.\")\n",
    "    print(\"Le benchmark necessite un GPU. Le notebook montrera le code sans executer.\")\n",
    "    run_benchmark = False\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Verification des dependances\n",
    "print(\"\\n--- VERIFICATION DEPENDANCES ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "deps_ok = True\n",
    "\n",
    "try:\n",
    "    import diffusers\n",
    "    print(f\"diffusers : v{diffusers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"diffusers NON INSTALLE (pip install diffusers>=0.32)\")\n",
    "    deps_ok = False\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"transformers : v{transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"transformers NON INSTALLE\")\n",
    "    deps_ok = False\n",
    "\n",
    "try:\n",
    "    import imageio\n",
    "    print(f\"imageio : v{imageio.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"imageio NON INSTALLE\")\n",
    "    deps_ok = False\n",
    "\n",
    "if not deps_ok:\n",
    "    print(\"\\nDependances manquantes. Le notebook montrera le code sans executer.\")\n",
    "    run_benchmark = False\n",
    "\n",
    "print(f\"\\nDevice : {device}\")\n",
    "print(f\"Benchmark active : {run_benchmark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section1-intro",
   "metadata": {
    "papermill": {
     "duration": 0.002099,
     "end_time": "2026-02-26T07:10:46.993305",
     "exception": false,
     "start_time": "2026-02-26T07:10:46.991206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 1 : Framework de benchmark\n",
    "\n",
    "Pour comparer equitablement les modeles, nous definissons un framework commun\n",
    "qui garantit les memes conditions de test pour chacun.\n",
    "\n",
    "| Modele | Type | VRAM estimee | Pipeline diffusers |\n",
    "|--------|------|-------------|--------------------|\n",
    "| **HunyuanVideo** | Text-to-video | ~18 GB (INT8) | `HunyuanVideoPipeline` |\n",
    "| **LTX-Video** | Text-to-video | ~8 GB | `LTXPipeline` |\n",
    "| **Wan** | Text-to-video | ~10 GB | `WanPipeline` |\n",
    "| **SVD** | Image-to-video | ~10 GB | `StableVideoDiffusionPipeline` |\n",
    "\n",
    "### Strategie de gestion memoire\n",
    "\n",
    "Les modeles sont charges et decharges **sequentiellement** pour respecter les\n",
    "contraintes VRAM d'une seule carte GPU (24 GB). Apres chaque modele, nous\n",
    "liberons explicitement la memoire GPU avec `torch.cuda.empty_cache()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-framework",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:10:46.999051Z",
     "iopub.status.busy": "2026-02-26T07:10:46.998537Z",
     "iopub.status.idle": "2026-02-26T07:10:47.007044Z",
     "shell.execute_reply": "2026-02-26T07:10:47.006419Z"
    },
    "papermill": {
     "duration": 0.012491,
     "end_time": "2026-02-26T07:10:47.007946",
     "exception": false,
     "start_time": "2026-02-26T07:10:46.995455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FRAMEWORK DE BENCHMARK ---\n",
      "=============================================\n",
      "Modeles demandes : ['hunyuan', 'ltx', 'wan']\n",
      "\n",
      "Registre des modeles :\n",
      "  hunyuan    : HunyuanVideo         (text-to-video, ~18 GB) [SELECTIONNE]\n",
      "  ltx        : LTX-Video            (text-to-video, ~8 GB) [SELECTIONNE]\n",
      "  wan        : Wan 2.1              (text-to-video, ~10 GB) [SELECTIONNE]\n",
      "  svd        : SVD 1.1 XT           (image-to-video, ~10 GB) [disponible]\n"
     ]
    }
   ],
   "source": [
    "# Framework de benchmark\n",
    "print(\"\\n--- FRAMEWORK DE BENCHMARK ---\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "# Registre des modeles supportes\n",
    "MODEL_REGISTRY = {\n",
    "    \"hunyuan\": {\n",
    "        \"name\": \"HunyuanVideo\",\n",
    "        \"model_id\": \"tencent/HunyuanVideo\",\n",
    "        \"pipeline_class\": \"HunyuanVideoPipeline\",\n",
    "        \"type\": \"text-to-video\",\n",
    "        \"vram_estimate_gb\": 18,\n",
    "        \"quantize\": True,\n",
    "        \"supports_negative_prompt\": True\n",
    "    },\n",
    "    \"ltx\": {\n",
    "        \"name\": \"LTX-Video\",\n",
    "        \"model_id\": \"Lightricks/LTX-Video\",\n",
    "        \"pipeline_class\": \"LTXPipeline\",\n",
    "        \"type\": \"text-to-video\",\n",
    "        \"vram_estimate_gb\": 8,\n",
    "        \"quantize\": False,\n",
    "        \"supports_negative_prompt\": True\n",
    "    },\n",
    "    \"wan\": {\n",
    "        \"name\": \"Wan 2.1\",\n",
    "        \"model_id\": \"Wan-AI/Wan2.1-T2V-14B-Diffusers\",\n",
    "        \"pipeline_class\": \"WanPipeline\",\n",
    "        \"type\": \"text-to-video\",\n",
    "        \"vram_estimate_gb\": 10,\n",
    "        \"quantize\": False,\n",
    "        \"supports_negative_prompt\": True\n",
    "    },\n",
    "    \"svd\": {\n",
    "        \"name\": \"SVD 1.1 XT\",\n",
    "        \"model_id\": \"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
    "        \"pipeline_class\": \"StableVideoDiffusionPipeline\",\n",
    "        \"type\": \"image-to-video\",\n",
    "        \"vram_estimate_gb\": 10,\n",
    "        \"quantize\": False,\n",
    "        \"supports_negative_prompt\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def release_vram():\n",
    "    \"\"\"Libere la VRAM GPU de facon aggressive.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        vram_free = (torch.cuda.get_device_properties(0).total_mem - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "        print(f\"  VRAM liberee. Disponible : {vram_free:.1f} GB\")\n",
    "\n",
    "\n",
    "def measure_temporal_coherence(frames: List) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Mesure la coherence temporelle entre les frames generees.\n",
    "    \n",
    "    Metriques :\n",
    "    - avg_diff : difference moyenne inter-frames (mouvement global)\n",
    "    - std_diff : ecart-type des differences (regularite du mouvement)\n",
    "    - max_diff : saut maximal (detecte les artefacts temporels)\n",
    "    \"\"\"\n",
    "    diffs = []\n",
    "    for i in range(len(frames) - 1):\n",
    "        f1 = np.array(frames[i]).astype(float)\n",
    "        f2 = np.array(frames[i + 1]).astype(float)\n",
    "        diff = np.mean(np.abs(f1 - f2))\n",
    "        diffs.append(diff)\n",
    "    \n",
    "    return {\n",
    "        \"avg_diff\": float(np.mean(diffs)),\n",
    "        \"std_diff\": float(np.std(diffs)),\n",
    "        \"max_diff\": float(np.max(diffs)),\n",
    "        \"stability\": \"Haute\" if np.mean(diffs) < 15 else \"Moyenne\" if np.mean(diffs) < 30 else \"Basse\"\n",
    "    }\n",
    "\n",
    "\n",
    "def measure_frame_quality(frames: List) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Mesure des indicateurs de qualite sur les frames individuelles.\n",
    "    \n",
    "    Metriques :\n",
    "    - avg_sharpness : nettete moyenne (variance du Laplacien)\n",
    "    - color_diversity : diversite chromatique (ecart-type couleurs)\n",
    "    \"\"\"\n",
    "    from PIL import ImageFilter\n",
    "    \n",
    "    sharpness_scores = []\n",
    "    color_scores = []\n",
    "    \n",
    "    for frame in frames:\n",
    "        arr = np.array(frame).astype(float)\n",
    "        # Nettete via variance du laplacien approxime\n",
    "        gray = np.mean(arr, axis=2)\n",
    "        laplacian = np.abs(gray[:-2, 1:-1] + gray[2:, 1:-1] + gray[1:-1, :-2] + gray[1:-1, 2:] - 4 * gray[1:-1, 1:-1])\n",
    "        sharpness_scores.append(float(np.var(laplacian)))\n",
    "        # Diversite chromatique\n",
    "        color_scores.append(float(np.std(arr)))\n",
    "    \n",
    "    return {\n",
    "        \"avg_sharpness\": float(np.mean(sharpness_scores)),\n",
    "        \"color_diversity\": float(np.mean(color_scores))\n",
    "    }\n",
    "\n",
    "\n",
    "# Afficher les modeles disponibles\n",
    "print(f\"Modeles demandes : {models_to_test}\")\n",
    "print(f\"\\nRegistre des modeles :\")\n",
    "for key, info in MODEL_REGISTRY.items():\n",
    "    status = \"SELECTIONNE\" if key in models_to_test else \"disponible\"\n",
    "    print(f\"  {key:<10} : {info['name']:<20} ({info['type']}, ~{info['vram_estimate_gb']} GB) [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section2-intro",
   "metadata": {
    "papermill": {
     "duration": 0.002465,
     "end_time": "2026-02-26T07:10:47.013047",
     "exception": false,
     "start_time": "2026-02-26T07:10:47.010582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Framework de benchmark\n",
    "\n",
    "| Composant | Role | Justification |\n",
    "|-----------|------|---------------|\n",
    "| `MODEL_REGISTRY` | Catalogue des modeles | Parametres specifiques a chaque modele |\n",
    "| `release_vram()` | Liberation memoire | Essentiel pour le chargement sequentiel |\n",
    "| `measure_temporal_coherence()` | Qualite temporelle | Detecte les sauts et la regularite |\n",
    "| `measure_frame_quality()` | Qualite spatiale | Nettete et richesse chromatique |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le chargement sequentiel est obligatoire car aucun GPU grand public ne peut contenir tous les modeles simultanement\n",
    "2. La liberation aggressive de VRAM (`gc.collect()` + `empty_cache()` + `synchronize()`) est necessaire entre chaque modele\n",
    "3. Les metriques automatiques ne remplacent pas l'evaluation visuelle mais permettent un classement objectif\n",
    "\n",
    "## Section 2 : Execution du benchmark\n",
    "\n",
    "Nous allons charger chaque modele, generer une video avec le meme prompt,\n",
    "mesurer les performances puis decharger le modele avant de passer au suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-benchmark-run",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:10:47.020178Z",
     "iopub.status.busy": "2026-02-26T07:10:47.019526Z",
     "iopub.status.idle": "2026-02-26T07:10:47.035535Z",
     "shell.execute_reply": "2026-02-26T07:10:47.034174Z"
    },
    "papermill": {
     "duration": 0.021492,
     "end_time": "2026-02-26T07:10:47.037193",
     "exception": false,
     "start_time": "2026-02-26T07:10:47.015701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark desactive\n",
      "\n",
      "Le benchmark charge chaque modele sequentiellement :\n",
      "  1. Charger le pipeline\n",
      "  2. Generer avec le prompt commun\n",
      "  3. Mesurer les metriques\n",
      "  4. Liberer la VRAM\n",
      "  5. Passer au modele suivant\n"
     ]
    }
   ],
   "source": [
    "# Execution du benchmark sequentiel\n",
    "benchmark_results = []\n",
    "\n",
    "if run_benchmark:\n",
    "    print(\"\\n--- EXECUTION DU BENCHMARK ---\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Prompt : {benchmark_prompt}\")\n",
    "    print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n",
    "    print(f\"Resolution : {width}x{height}\")\n",
    "    \n",
    "    for model_key in models_to_test:\n",
    "        if model_key not in MODEL_REGISTRY:\n",
    "            print(f\"\\nModele inconnu : {model_key} (ignore)\")\n",
    "            continue\n",
    "        \n",
    "        model_info = MODEL_REGISTRY[model_key]\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Modele : {model_info['name']} ({model_key})\")\n",
    "        print(f\"Type : {model_info['type']}\")\n",
    "        print(f\"VRAM estimee : ~{model_info['vram_estimate_gb']} GB\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        result = {\n",
    "            \"model_key\": model_key,\n",
    "            \"model_name\": model_info['name'],\n",
    "            \"type\": model_info['type'],\n",
    "            \"success\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 1. Chargement du pipeline\n",
    "            print(f\"  Chargement du pipeline...\")\n",
    "            start_load = time.time()\n",
    "            \n",
    "            if model_key == \"hunyuan\":\n",
    "                from diffusers import HunyuanVideoPipeline, BitsAndBytesConfig\n",
    "                quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "                pipe = HunyuanVideoPipeline.from_pretrained(\n",
    "                    model_info['model_id'],\n",
    "                    quantization_config=quant_config,\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "            elif model_key == \"ltx\":\n",
    "                from diffusers import LTXPipeline\n",
    "                pipe = LTXPipeline.from_pretrained(\n",
    "                    model_info['model_id'],\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "            elif model_key == \"wan\":\n",
    "                from diffusers import WanPipeline\n",
    "                pipe = WanPipeline.from_pretrained(\n",
    "                    model_info['model_id'],\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "            elif model_key == \"svd\":\n",
    "                from diffusers import StableVideoDiffusionPipeline\n",
    "                pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "                    model_info['model_id'],\n",
    "                    torch_dtype=torch.float16,\n",
    "                    variant=\"fp16\"\n",
    "                )\n",
    "            \n",
    "            pipe = pipe.to(device)\n",
    "            pipe.enable_vae_slicing()\n",
    "            try:\n",
    "                pipe.enable_model_cpu_offload()\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            load_time = time.time() - start_load\n",
    "            result[\"load_time\"] = load_time\n",
    "            print(f\"  Pipeline charge en {load_time:.1f}s\")\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                vram_after_load = torch.cuda.memory_allocated(0) / 1024**3\n",
    "                result[\"vram_model\"] = vram_after_load\n",
    "                print(f\"  VRAM apres chargement : {vram_after_load:.1f} GB\")\n",
    "            \n",
    "            # 2. Generation\n",
    "            print(f\"  Generation en cours...\")\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            start_gen = time.time()\n",
    "            \n",
    "            if model_info['type'] == \"text-to-video\":\n",
    "                gen_kwargs = {\n",
    "                    \"prompt\": benchmark_prompt,\n",
    "                    \"num_frames\": num_frames,\n",
    "                    \"guidance_scale\": guidance_scale,\n",
    "                    \"num_inference_steps\": num_inference_steps,\n",
    "                    \"height\": height,\n",
    "                    \"width\": width,\n",
    "                    \"generator\": generator\n",
    "                }\n",
    "                if model_info['supports_negative_prompt']:\n",
    "                    gen_kwargs[\"negative_prompt\"] = \"low quality, blurry, distorted, watermark\"\n",
    "                output = pipe(**gen_kwargs)\n",
    "            else:\n",
    "                # SVD : image-to-video, creer une image placeholder\n",
    "                from PIL import ImageDraw\n",
    "                placeholder = Image.new('RGB', (1024, 576), (100, 150, 200))\n",
    "                draw = ImageDraw.Draw(placeholder)\n",
    "                for y in range(576):\n",
    "                    t = y / 576\n",
    "                    r = int(135 + 80 * t)\n",
    "                    g = int(180 - 40 * t)\n",
    "                    b = int(220 - 60 * t)\n",
    "                    draw.line([(0, y), (1024, y)], fill=(r, g, b))\n",
    "                \n",
    "                output = pipe(\n",
    "                    image=placeholder,\n",
    "                    num_frames=min(num_frames, 25),\n",
    "                    fps=7,\n",
    "                    motion_bucket_id=127,\n",
    "                    noise_aug_strength=0.02,\n",
    "                    num_inference_steps=num_inference_steps,\n",
    "                    decode_chunk_size=8,\n",
    "                    generator=generator\n",
    "                )\n",
    "            \n",
    "            gen_time = time.time() - start_gen\n",
    "            frames = output.frames[0]\n",
    "            \n",
    "            result[\"generation_time\"] = gen_time\n",
    "            result[\"time_per_frame\"] = gen_time / len(frames)\n",
    "            result[\"num_frames_actual\"] = len(frames)\n",
    "            result[\"frames\"] = frames\n",
    "            result[\"success\"] = True\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                result[\"vram_peak\"] = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "            \n",
    "            # Metriques de qualite\n",
    "            result[\"coherence\"] = measure_temporal_coherence(frames)\n",
    "            result[\"quality\"] = measure_frame_quality(frames)\n",
    "            \n",
    "            print(f\"  Generation reussie en {gen_time:.1f}s ({len(frames)} frames)\")\n",
    "            if \"vram_peak\" in result:\n",
    "                print(f\"  VRAM pic : {result['vram_peak']:.1f} GB\")\n",
    "            print(f\"  Coherence temporelle : {result['coherence']['stability']}\")\n",
    "            \n",
    "            # Sauvegarde MP4\n",
    "            if save_as_mp4:\n",
    "                mp4_path = OUTPUT_DIR / f\"benchmark_{model_key}.mp4\"\n",
    "                export_to_video(frames, str(mp4_path), fps=fps_output)\n",
    "                result[\"mp4_path\"] = str(mp4_path)\n",
    "                print(f\"  MP4 : {mp4_path.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result[\"error\"] = f\"{type(e).__name__}: {str(e)[:200]}\"\n",
    "            print(f\"  Erreur : {result['error']}\")\n",
    "        \n",
    "        finally:\n",
    "            # 3. Liberation VRAM\n",
    "            print(f\"  Liberation VRAM...\")\n",
    "            try:\n",
    "                del pipe\n",
    "            except NameError:\n",
    "                pass\n",
    "            release_vram()\n",
    "        \n",
    "        benchmark_results.append(result)\n",
    "    \n",
    "    print(f\"\\nBenchmark termine : {sum(1 for r in benchmark_results if r['success'])}/{len(benchmark_results)} modeles reussis\")\n",
    "\n",
    "else:\n",
    "    print(\"Benchmark desactive\")\n",
    "    print(\"\\nLe benchmark charge chaque modele sequentiellement :\")\n",
    "    print(\"  1. Charger le pipeline\")\n",
    "    print(\"  2. Generer avec le prompt commun\")\n",
    "    print(\"  3. Mesurer les metriques\")\n",
    "    print(\"  4. Liberer la VRAM\")\n",
    "    print(\"  5. Passer au modele suivant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section3-intro",
   "metadata": {
    "papermill": {
     "duration": 0.002771,
     "end_time": "2026-02-26T07:10:47.044408",
     "exception": false,
     "start_time": "2026-02-26T07:10:47.041637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Resultats du benchmark\n",
    "\n",
    "Chaque modele a ete charge puis decharge sequentiellement.\n",
    "Les metriques collectees permettent une comparaison objective.\n",
    "\n",
    "**Metriques mesurees** :\n",
    "- **Temps de chargement** : duree pour telecharger/charger les poids du modele\n",
    "- **Temps de generation** : duree de l'inference seule\n",
    "- **VRAM pic** : consommation memoire GPU maximale pendant la generation\n",
    "- **Coherence temporelle** : regularite du mouvement entre frames consecutives\n",
    "- **Nettete** : variance du Laplacien sur les frames individuelles\n",
    "\n",
    "## Section 3 : Visualisation comparative\n",
    "\n",
    "Nous allons maintenant produire une grille visuelle montrant les frames\n",
    "de chaque modele cote-a-cote pour permettre une evaluation qualitative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-visualization",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:10:47.051119Z",
     "iopub.status.busy": "2026-02-26T07:10:47.050830Z",
     "iopub.status.idle": "2026-02-26T07:10:47.059341Z",
     "shell.execute_reply": "2026-02-26T07:10:47.058285Z"
    },
    "papermill": {
     "duration": 0.014371,
     "end_time": "2026-02-26T07:10:47.061394",
     "exception": false,
     "start_time": "2026-02-26T07:10:47.047023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucun resultat de benchmark disponible pour la visualisation\n",
      "\n",
      "En mode benchmark, cette cellule affiche :\n",
      "  1. Grille de frames (modeles en lignes, frames en colonnes)\n",
      "  2. Courbe de coherence temporelle\n"
     ]
    }
   ],
   "source": [
    "# Visualisation comparative : grille de frames\n",
    "successful_results = [r for r in benchmark_results if r.get('success', False)]\n",
    "\n",
    "if successful_results:\n",
    "    print(\"\\n--- GRILLE COMPARATIVE ---\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    n_models = len(successful_results)\n",
    "    n_preview = 4  # Frames par modele\n",
    "    \n",
    "    fig, axes = plt.subplots(n_models, n_preview, figsize=(3.5 * n_preview, 3 * n_models))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for v_idx, result in enumerate(successful_results):\n",
    "        frames = result['frames']\n",
    "        frame_indices = np.linspace(0, len(frames) - 1, n_preview, dtype=int)\n",
    "        \n",
    "        for f_idx, fi in enumerate(frame_indices):\n",
    "            axes[v_idx][f_idx].imshow(frames[fi])\n",
    "            axes[v_idx][f_idx].axis('off')\n",
    "            if f_idx == 0:\n",
    "                axes[v_idx][f_idx].set_ylabel(\n",
    "                    result['model_name'], fontsize=11, fontweight='bold'\n",
    "                )\n",
    "            axes[v_idx][f_idx].set_title(f\"Frame {fi + 1}\", fontsize=8)\n",
    "    \n",
    "    plt.suptitle(\n",
    "        f\"Benchmark : {benchmark_prompt[:50]}...\",\n",
    "        fontsize=13, fontweight='bold'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Courbe de difference inter-frames pour chaque modele\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    for result in successful_results:\n",
    "        frames = result['frames']\n",
    "        diffs = []\n",
    "        for i in range(len(frames) - 1):\n",
    "            f1 = np.array(frames[i]).astype(float)\n",
    "            f2 = np.array(frames[i + 1]).astype(float)\n",
    "            diffs.append(np.mean(np.abs(f1 - f2)))\n",
    "        ax.plot(range(1, len(diffs) + 1), diffs, marker='o', label=result['model_name'], linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Transition entre frames', fontsize=11)\n",
    "    ax.set_ylabel('Difference moyenne (pixels)', fontsize=11)\n",
    "    ax.set_title('Coherence temporelle par modele', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Aucun resultat de benchmark disponible pour la visualisation\")\n",
    "    print(\"\\nEn mode benchmark, cette cellule affiche :\")\n",
    "    print(\"  1. Grille de frames (modeles en lignes, frames en colonnes)\")\n",
    "    print(\"  2. Courbe de coherence temporelle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section4-intro",
   "metadata": {
    "papermill": {
     "duration": 0.003577,
     "end_time": "2026-02-26T07:10:47.069617",
     "exception": false,
     "start_time": "2026-02-26T07:10:47.066040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Interpretation : Analyse visuelle\n",
    "\n",
    "La grille comparative permet d'evaluer visuellement :\n",
    "\n",
    "| Critere | Meilleur modele typique | Observation |\n",
    "|---------|----------------------|-------------|\n",
    "| Nettete des details | HunyuanVideo | Architecture Transformer 3D plus precise |\n",
    "| Fluidite du mouvement | HunyuanVideo / Wan | Flow matching plus regulier |\n",
    "| Vitesse de generation | LTX-Video | Architecture optimisee pour la vitesse |\n",
    "| Economie VRAM | LTX-Video | ~8 GB contre ~18 GB pour HunyuanVideo |\n",
    "| Fidelite image source | SVD | Specialise image-to-video |\n",
    "\n",
    "**Points cles** :\n",
    "1. La courbe de coherence temporelle revele les modeles avec des sauts brusques (artefacts)\n",
    "2. Un mouvement trop faible (diff ~0) indique un modele qui genere des frames quasi-statiques\n",
    "3. Un mouvement trop fort (diff > 40) indique une instabilite temporelle\n",
    "\n",
    "## Section 4 : Tableau recapitulatif et analyse cout/qualite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-summary-table",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:10:47.077900Z",
     "iopub.status.busy": "2026-02-26T07:10:47.077350Z",
     "iopub.status.idle": "2026-02-26T07:10:47.086447Z",
     "shell.execute_reply": "2026-02-26T07:10:47.085902Z"
    },
    "papermill": {
     "duration": 0.013373,
     "end_time": "2026-02-26T07:10:47.087362",
     "exception": false,
     "start_time": "2026-02-26T07:10:47.073989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucun resultat de benchmark disponible\n",
      "\n",
      "Tableau attendu (valeurs typiques RTX 3090) :\n",
      "Modele             Generation   VRAM pic    Coherence   \n",
      "-------------------------------------------------------\n",
      "  HunyuanVideo       60-180s      ~18 GB      Haute       \n",
      "  LTX-Video          15-30s       ~8 GB       Moyenne     \n",
      "  Wan 2.1            30-90s       ~10 GB      Haute       \n",
      "  SVD 1.1 XT         20-40s       ~10 GB      Haute       \n"
     ]
    }
   ],
   "source": [
    "# Tableau recapitulatif et analyse cout/qualite\n",
    "if successful_results:\n",
    "    print(\"\\n--- TABLEAU RECAPITULATIF ---\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    # En-tete\n",
    "    header = f\"{'Modele':<18} {'Type':<16} {'Chargement':<12} {'Generation':<12} {'VRAM pic':<11} {'Coherence':<12} {'Nettete':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for r in successful_results:\n",
    "        load_t = f\"{r.get('load_time', 0):.0f}s\"\n",
    "        gen_t = f\"{r.get('generation_time', 0):.1f}s\"\n",
    "        vram = f\"{r.get('vram_peak', 0):.1f} GB\"\n",
    "        coherence = r.get('coherence', {}).get('stability', 'N/A')\n",
    "        sharpness = f\"{r.get('quality', {}).get('avg_sharpness', 0):.0f}\"\n",
    "        \n",
    "        print(f\"  {r['model_name']:<18} {r['type']:<16} {load_t:<12} {gen_t:<12} {vram:<11} {coherence:<12} {sharpness:<10}\")\n",
    "    \n",
    "    # Analyse cout/qualite\n",
    "    print(f\"\\n--- ANALYSE COUT / QUALITE ---\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Score composite : qualite / (temps * vram)\n",
    "    print(f\"\\n{'Modele':<18} {'Score efficacite':<20} {'Recommandation':<30}\")\n",
    "    print(\"-\" * 68)\n",
    "    \n",
    "    for r in successful_results:\n",
    "        gen_time = r.get('generation_time', 1)\n",
    "        vram = r.get('vram_peak', 1)\n",
    "        sharpness = r.get('quality', {}).get('avg_sharpness', 1)\n",
    "        coherence_val = r.get('coherence', {}).get('avg_diff', 30)\n",
    "        \n",
    "        # Score : nettete / (temps * vram), normalise par coherence\n",
    "        coherence_factor = 1.0 if coherence_val < 15 else 0.8 if coherence_val < 30 else 0.5\n",
    "        efficiency = (sharpness * coherence_factor) / (gen_time * vram) * 100\n",
    "        \n",
    "        if efficiency > 1.0:\n",
    "            reco = \"Excellent rapport qualite/cout\"\n",
    "        elif efficiency > 0.3:\n",
    "            reco = \"Bon compromis\"\n",
    "        else:\n",
    "            reco = \"Qualite elevee, cout important\"\n",
    "        \n",
    "        print(f\"  {r['model_name']:<18} {efficiency:<20.2f} {reco:<30}\")\n",
    "    \n",
    "    # Sauvegarde du rapport JSON\n",
    "    if save_results:\n",
    "        report = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"prompt\": benchmark_prompt,\n",
    "            \"params\": {\n",
    "                \"num_frames\": num_frames,\n",
    "                \"num_inference_steps\": num_inference_steps,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"height\": height,\n",
    "                \"width\": width\n",
    "            },\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"model\": r['model_name'],\n",
    "                    \"type\": r['type'],\n",
    "                    \"load_time\": r.get('load_time', 0),\n",
    "                    \"generation_time\": r.get('generation_time', 0),\n",
    "                    \"vram_peak\": r.get('vram_peak', 0),\n",
    "                    \"coherence\": r.get('coherence', {}),\n",
    "                    \"quality\": r.get('quality', {})\n",
    "                }\n",
    "                for r in successful_results\n",
    "            ]\n",
    "        }\n",
    "        report_path = OUTPUT_DIR / \"benchmark_report.json\"\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nRapport sauvegarde : {report_path.name}\")\n",
    "\n",
    "else:\n",
    "    print(\"Aucun resultat de benchmark disponible\")\n",
    "    print(\"\\nTableau attendu (valeurs typiques RTX 3090) :\")\n",
    "    print(f\"{'Modele':<18} {'Generation':<12} {'VRAM pic':<11} {'Coherence':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"  {'HunyuanVideo':<18} {'60-180s':<12} {'~18 GB':<11} {'Haute':<12}\")\n",
    "    print(f\"  {'LTX-Video':<18} {'15-30s':<12} {'~8 GB':<11} {'Moyenne':<12}\")\n",
    "    print(f\"  {'Wan 2.1':<18} {'30-90s':<12} {'~10 GB':<11} {'Haute':<12}\")\n",
    "    print(f\"  {'SVD 1.1 XT':<18} {'20-40s':<12} {'~10 GB':<11} {'Haute':<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-interactive",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:10:47.096330Z",
     "iopub.status.busy": "2026-02-26T07:10:47.095624Z",
     "iopub.status.idle": "2026-02-26T07:10:47.105148Z",
     "shell.execute_reply": "2026-02-26T07:10:47.104616Z"
    },
    "papermill": {
     "duration": 0.015453,
     "end_time": "2026-02-26T07:10:47.106118",
     "exception": false,
     "start_time": "2026-02-26T07:10:47.090665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mode batch - Interface interactive desactivee\n"
     ]
    }
   ],
   "source": [
    "# Mode interactif : benchmark avec un prompt utilisateur\n",
    "if notebook_mode == \"interactive\" and not skip_widgets:\n",
    "    print(\"\\n--- MODE INTERACTIF ---\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Entrez votre propre prompt pour tester sur tous les modeles.\")\n",
    "    print(\"(Laissez vide pour passer a la suite)\")\n",
    "    \n",
    "    try:\n",
    "        user_prompt = input(\"\\nVotre prompt : \").strip()\n",
    "        \n",
    "        if user_prompt and run_benchmark:\n",
    "            print(f\"\\nBenchmark avec : {user_prompt}\")\n",
    "            print(\"Note : le benchmark complet prendra plusieurs minutes.\")\n",
    "            print(\"Seul le modele le plus rapide (LTX-Video) sera teste en interactif.\")\n",
    "            \n",
    "            if \"ltx\" in MODEL_REGISTRY:\n",
    "                try:\n",
    "                    from diffusers import LTXPipeline\n",
    "                    pipe_user = LTXPipeline.from_pretrained(\n",
    "                        MODEL_REGISTRY['ltx']['model_id'],\n",
    "                        torch_dtype=torch.float16\n",
    "                    ).to(device)\n",
    "                    pipe_user.enable_vae_slicing()\n",
    "                    \n",
    "                    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "                    output = pipe_user(\n",
    "                        prompt=user_prompt,\n",
    "                        negative_prompt=\"low quality, blurry, distorted\",\n",
    "                        num_frames=num_frames,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                        height=height,\n",
    "                        width=width,\n",
    "                        generator=generator\n",
    "                    )\n",
    "                    \n",
    "                    user_frames = output.frames[0]\n",
    "                    n_display = min(8, len(user_frames))\n",
    "                    indices = np.linspace(0, len(user_frames) - 1, n_display, dtype=int)\n",
    "                    fig, axes = plt.subplots(1, n_display, figsize=(2.5 * n_display, 3))\n",
    "                    if n_display == 1:\n",
    "                        axes = [axes]\n",
    "                    for ax, idx in zip(axes, indices):\n",
    "                        ax.imshow(user_frames[idx])\n",
    "                        ax.set_title(f\"Frame {idx+1}\", fontsize=8)\n",
    "                        ax.axis('off')\n",
    "                    plt.suptitle(f\"LTX-Video : {user_prompt[:50]}...\", fontweight='bold')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    del pipe_user\n",
    "                    release_vram()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur : {type(e).__name__}: {str(e)[:100]}\")\n",
    "        elif user_prompt:\n",
    "            print(\"Benchmark non disponible\")\n",
    "        else:\n",
    "            print(\"Mode interactif ignore\")\n",
    "    \n",
    "    except (KeyboardInterrupt, EOFError) as e:\n",
    "        print(f\"\\nMode interactif interrompu ({type(e).__name__})\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"StdinNotImplemented\" in error_type or \"input\" in str(e).lower():\n",
    "            print(\"\\nMode interactif non disponible (execution automatisee)\")\n",
    "        else:\n",
    "            print(f\"\\nErreur inattendue : {error_type} - {str(e)[:100]}\")\n",
    "            print(\"Passage a la suite du notebook\")\n",
    "else:\n",
    "    print(\"\\nMode batch - Interface interactive desactivee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-conclusion",
   "metadata": {
    "papermill": {
     "duration": 0.003203,
     "end_time": "2026-02-26T07:10:47.111704",
     "exception": false,
     "start_time": "2026-02-26T07:10:47.108501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bonnes pratiques et choix de modele\n",
    "\n",
    "### Guide de selection par cas d'usage\n",
    "\n",
    "| Cas d'usage | Modele recommande | Raison |\n",
    "|-------------|------------------|--------|\n",
    "| Prototypage rapide | LTX-Video | Rapide, leger (~8 GB) |\n",
    "| Qualite maximale | HunyuanVideo | Meilleure coherence et resolution |\n",
    "| Prompts multilingues | Wan 2.1 | Support natif du chinois, francais, etc. |\n",
    "| Animation d'image existante | SVD | Specialise image-to-video |\n",
    "| Production batch | LTX-Video | Rapport vitesse/qualite optimal |\n",
    "| Demo client | HunyuanVideo | Qualite visuelle maximale |\n",
    "\n",
    "### Strategies d'optimisation VRAM\n",
    "\n",
    "| Strategie | Gain VRAM | Impact qualite |\n",
    "|-----------|-----------|----------------|\n",
    "| Quantification INT8 | -40% | Faible |\n",
    "| VAE slicing | -15% pic | Aucun |\n",
    "| VAE tiling | -10% pic | Aucun |\n",
    "| CPU offload | Variable | Augmente le temps |\n",
    "| Resolution reduite | -30-50% | Proportionnel |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-stats",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T07:10:47.118135Z",
     "iopub.status.busy": "2026-02-26T07:10:47.117751Z",
     "iopub.status.idle": "2026-02-26T07:10:47.126423Z",
     "shell.execute_reply": "2026-02-26T07:10:47.125457Z"
    },
    "papermill": {
     "duration": 0.013401,
     "end_time": "2026-02-26T07:10:47.127460",
     "exception": false,
     "start_time": "2026-02-26T07:10:47.114059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STATISTIQUES DE SESSION ---\n",
      "========================================\n",
      "Date : 2026-02-26 08:10:47\n",
      "Mode : batch\n",
      "Modeles testes : ['hunyuan', 'ltx', 'wan']\n",
      "Prompt : a cat walking gracefully through a garden, soft sunlight, ci...\n",
      "Parametres : 16 frames, 25 steps, CFG=6.0\n",
      "Resolution : 512x320\n",
      "\n",
      "Fichiers generes (0) :\n",
      "\n",
      "--- PROCHAINES ETAPES ---\n",
      "1. Notebook 03-2 : Orchestration de pipelines (text -> image -> video -> upscale)\n",
      "2. Notebook 03-3 : Workflows ComfyUI pour la generation video\n",
      "3. Module 04 : Applications production (education, creatif, bout-en-bout)\n",
      "\n",
      "Notebook 03-1 Comparaison Multi-Modeles termine - 08:10:47\n"
     ]
    }
   ],
   "source": [
    "# Statistiques de session et prochaines etapes\n",
    "print(\"\\n--- STATISTIQUES DE SESSION ---\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Mode : {notebook_mode}\")\n",
    "print(f\"Modeles testes : {models_to_test}\")\n",
    "print(f\"Prompt : {benchmark_prompt[:60]}...\")\n",
    "print(f\"Parametres : {num_frames} frames, {num_inference_steps} steps, CFG={guidance_scale}\")\n",
    "print(f\"Resolution : {width}x{height}\")\n",
    "\n",
    "if benchmark_results:\n",
    "    n_success = sum(1 for r in benchmark_results if r.get('success', False))\n",
    "    print(f\"\\nResultats : {n_success}/{len(benchmark_results)} modeles reussis\")\n",
    "    for r in benchmark_results:\n",
    "        status = \"OK\" if r.get('success') else \"ERREUR\"\n",
    "        gen_t = f\"{r.get('generation_time', 0):.1f}s\" if r.get('success') else r.get('error', 'N/A')[:50]\n",
    "        print(f\"  {r['model_name']:<18} [{status}] {gen_t}\")\n",
    "\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    vram_current = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"\\nVRAM actuelle : {vram_current:.1f} GB\")\n",
    "\n",
    "if save_results and OUTPUT_DIR.exists():\n",
    "    generated_files = list(OUTPUT_DIR.glob('*'))\n",
    "    print(f\"\\nFichiers generes ({len(generated_files)}) :\")\n",
    "    for f in sorted(generated_files):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(f\"\\n--- PROCHAINES ETAPES ---\")\n",
    "print(f\"1. Notebook 03-2 : Orchestration de pipelines (text -> image -> video -> upscale)\")\n",
    "print(f\"2. Notebook 03-3 : Workflows ComfyUI pour la generation video\")\n",
    "print(f\"3. Module 04 : Applications production (education, creatif, bout-en-bout)\")\n",
    "\n",
    "print(f\"\\nNotebook 03-1 Comparaison Multi-Modeles termine - {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.651904,
   "end_time": "2026-02-26T07:10:48.127508",
   "environment_variables": {},
   "exception": null,
   "input_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Video\\03-Orchestration\\03-1-Multi-Model-Video-Comparison.ipynb",
   "output_path": "D:\\Dev\\CoursIA.worktrees\\GenAI_Series\\MyIA.AI.Notebooks\\GenAI\\Video\\03-Orchestration\\03-1-Multi-Model-Video-Comparison.ipynb",
   "parameters": {
    "notebook_mode": "batch",
    "skip_widgets": true
   },
   "start_time": "2026-02-26T07:10:39.475604",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}