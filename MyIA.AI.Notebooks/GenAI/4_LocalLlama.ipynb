{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hébergement local de modèles génératifs\n",
    "\n",
    "\n",
    "## 1) Installation & Import\n",
    "\n",
    "On installe/importe ce qui est nécessaire : \n",
    "- `requests` pour les appels HTTP bruts,\n",
    "- `openai` version 1.0.0+,\n",
    "- `semantic-kernel` si on veut tester SK,\n",
    "- d’autres libs selon besoin (json, time, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.11.13)\n",
      "Requirement already satisfied: openai in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.64.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.65.2-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: semantic-kernel in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.21.3)\n",
      "Collecting semantic-kernel\n",
      "  Downloading semantic_kernel-1.22.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.8.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: httpcore in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (1.18.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: cloudevents~=1.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (1.11.0)\n",
      "Requirement already satisfied: pydantic-settings~=2.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (2.7.0)\n",
      "Requirement already satisfied: defusedxml~=0.7 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: azure-identity~=1.13 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (1.19.0)\n",
      "Requirement already satisfied: numpy>=1.25.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (1.26.4)\n",
      "Requirement already satisfied: openapi_core<0.20,>=0.18 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (0.19.4)\n",
      "Requirement already satisfied: opentelemetry-api~=1.24 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.24 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (1.29.0)\n",
      "Requirement already satisfied: prance~=23.6.21.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (23.6.21.0)\n",
      "Requirement already satisfied: pybars4~=0.9 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (0.9.13)\n",
      "Requirement already satisfied: jinja2~=3.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (3.1.4)\n",
      "Requirement already satisfied: nest-asyncio~=1.6 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.15.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semantic-kernel) (1.15.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore) (0.14.0)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from azure-identity~=1.13->semantic-kernel) (1.32.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from azure-identity~=1.13->semantic-kernel) (42.0.8)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from azure-identity~=1.13->semantic-kernel) (1.31.1)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from azure-identity~=1.13->semantic-kernel) (1.2.0)\n",
      "Requirement already satisfied: deprecation<3.0,>=2.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cloudevents~=1.0->semantic-kernel) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2~=3.1->semantic-kernel) (3.0.2)\n",
      "Requirement already satisfied: isodate in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.7.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (4.23.0)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.3.3)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (10.5.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.6.2)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: parse in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (1.20.2)\n",
      "Requirement already satisfied: werkzeug in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (3.1.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-api~=1.24->semantic-kernel) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-api~=1.24->semantic-kernel) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-sdk~=1.24->semantic-kernel) (0.50b0)\n",
      "Requirement already satisfied: chardet>=3.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from prance~=23.6.21.0->semantic-kernel) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from prance~=23.6.21.0->semantic-kernel) (0.18.6)\n",
      "Requirement already satisfied: six~=1.15 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from prance~=23.6.21.0->semantic-kernel) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from prance~=23.6.21.0->semantic-kernel) (24.1)\n",
      "Requirement already satisfied: PyMeta3>=0.5.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pybars4~=0.9->semantic-kernel) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryptography>=2.5->azure-identity~=1.13->semantic-kernel) (1.17.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api~=1.24->semantic-kernel) (1.17.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api~=1.24->semantic-kernel) (3.21.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.22.3)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (6.0.2)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (0.4.3)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity~=1.13->semantic-kernel) (2.10.1)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from msal-extensions>=1.2.0->azure-identity~=1.13->semantic-kernel) (2.10.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.1.4)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel) (1.10.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ruamel.yaml>=0.17.10->prance~=23.6.21.0->semantic-kernel) (0.2.12)\n",
      "Requirement already satisfied: pycparser in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity~=1.13->semantic-kernel) (2.22)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from portalocker<3,>=1.4->msal-extensions>=1.2.0->azure-identity~=1.13->semantic-kernel) (308)\n",
      "Downloading openai-1.65.2-py3-none-any.whl (473 kB)\n",
      "Downloading semantic_kernel-1.22.1-py3-none-any.whl (741 kB)\n",
      "   ---------------------------------------- 0.0/741.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 741.1/741.1 kB 15.0 MB/s eta 0:00:00\n",
      "Installing collected packages: openai, semantic-kernel\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.64.0\n",
      "    Uninstalling openai-1.64.0:\n",
      "      Successfully uninstalled openai-1.64.0\n",
      "  Attempting uninstall: semantic-kernel\n",
      "    Found existing installation: semantic-kernel 1.21.3\n",
      "    Uninstalling semantic-kernel-1.21.3:\n",
      "      Successfully uninstalled semantic-kernel-1.21.3\n",
      "Successfully installed openai-1.65.2 semantic-kernel-1.21.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Importations OK.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade requests aiohttp openai semantic-kernel python-dotenv anyio httpx httpcore\n",
    "\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import getpass\n",
    "import openai\n",
    "\n",
    "\n",
    "print(\"Importations OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journalisation colorée\n",
    "\n",
    "Nous allons utiliser un logger (via le module `logging`) configuré avec un\n",
    "**ColorFormatter** pour afficher les messages en couleur dans la console ou la\n",
    "sortie de Jupyter :\n",
    "\n",
    "- Les **informations** et étapes réussies apparaîtront en vert (niveau `INFO`).\n",
    "- Les **erreurs** seront en rouge (niveau `ERROR`).\n",
    "- Les avertissements (`WARNING`) ou messages de debug (`DEBUG`) auront également leurs couleurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:03:19 [INFO] Local Llama - Configuration initiale terminée.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "class ColorFormatter(logging.Formatter):\n",
    "    \"\"\"\n",
    "    Un formatter coloré pour rendre les logs plus lisibles.\n",
    "    \"\"\"\n",
    "    colors = {\n",
    "        'DEBUG': '\\033[94m',\n",
    "        'INFO': '\\033[92m',\n",
    "        'WARNING': '\\033[93m',\n",
    "        'ERROR': '\\033[91m',\n",
    "        'CRITICAL': '\\033[91m\\033[1m'\n",
    "    }\n",
    "    reset = '\\033[0m'\n",
    "\n",
    "    def format(self, record: logging.LogRecord) -> str:\n",
    "        msg = super().format(record)\n",
    "        return f\"{self.colors.get(record.levelname, '')}{msg}{self.reset}\"\n",
    "\n",
    "logger = logging.getLogger(\"Local Llama\")\n",
    "logger.setLevel(logging.DEBUG)  # Peut être paramétré via .env ou variable\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setLevel(logging.DEBUG)\n",
    "    formatter = ColorFormatter(\n",
    "        fmt=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "logger.info(\"Configuration initiale terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration et définition dynamique des endpoints\n",
    "\n",
    "Pour simplifier la configuration de nos endpoints (URL d’API, clés d’API, modèles, etc.), nous allons externaliser ces informations dans un fichier `.env` placé à la racine de notre projet ou dans un dossier sécurisé. Copiez le fichier .env.example et renommez le fichier résultant en .env avant de le personnaliser.\n",
    "\n",
    "### 1) Déclaration dans `.env`\n",
    "\n",
    "On déclare, par exemple, un premier endpoint dans des variables d’environnement :\n",
    "\n",
    "```\n",
    "OPENAI_ENDPOINT_NAME=OpenAI\n",
    "OPENAI_BASE_URL=https://api.openai.com/v1\n",
    "OPENAI_API_KEY=sk-abcd1234ABCD1423\n",
    "OPENAI_CHAT_MODEL_ID=gpt-4o-mini\n",
    "```\n",
    "\n",
    "Et si l’on souhaite tester plusieurs endpoints (ex. mini, medium, large), on ajoute un suffixe `_2`, `_3`... :\n",
    "\n",
    "```\n",
    "OPENAI_ENDPOINT_NAME_2=local-mini\n",
    "OPENAI_BASE_URL_2=https://api.mini.yourdomain.com/v1\n",
    "OPENAI_API_KEY_2=sk-MINI-SECRET-KEY\n",
    "OPENAI_CHAT_MODEL_ID_2=unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit\n",
    "\n",
    "OPENAI_ENDPOINT_NAME_3=medium\n",
    "OPENAI_BASE_URL_3=https://api.medium.text-generation-webui.myia.io/v1\n",
    "OPENAI_API_KEY_3=sk-MEDIUM-SECRET-KEY\n",
    "OPENAI_CHAT_MODEL_ID_3=unsloth/DeepSeek-R1-Distill-Qwen-14B-bnb-4bit\n",
    "```\n",
    "\n",
    "Ainsi, chacun de ces blocs définit un **endpoint** : un service local ou distant OpenAI-compatible (ex. Oobabooga ou vLLM).  \n",
    "\n",
    "### 2) Lecture et création automatique dans le notebook\n",
    "\n",
    "Dans le notebook, nous allons **lire** ces variables pour construire la liste `endpoints`. Chacun contient :\n",
    "\n",
    "- `name` : un label descriptif (ex. `micro`, `mini`, etc.),\n",
    "- `api_base` : l’URL de base de l’API (ex. `https://api.micro.text-generation-webui.myia.io/v1`),\n",
    "- `api_key` : la clé API (fournie par votre conteneur ou config),\n",
    "- `model` (optionnel) : si le modèle n’est pas fourni, nous pourrons interroger `/models` pour récupérer le nom du (ou des) modèle(s) disponibles.\n",
    "\n",
    "Grâce à cette configuration dynamique, on peut aisément **alterner** entre différents backends (p. ex. Oobabooga ou vLLM) ou **interroger plusieurs endpoints** pour **comparer leurs performances**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:03:41 [INFO] Local Llama - === Endpoints chargés ===\u001b[0m\n",
      "\u001b[92m18:03:41 [INFO] Local Llama - - name=OpenAI, base=https://api.openai.com/v1, key=(len=164), model=gpt-4o-mini\u001b[0m\n",
      "\u001b[92m18:03:41 [INFO] Local Llama - - name=Local Model - Micro, base=https://api.micro.text-generation-webui.myia.io/v1, key=(len=32), model=None\u001b[0m\n",
      "\u001b[92m18:03:41 [INFO] Local Llama - - name=Local Model - Mini, base=https://api.mini.text-generation-webui.myia.io/v1, key=(len=32), model=None\u001b[0m\n",
      "\u001b[92m18:03:41 [INFO] Local Llama - - name=Local Model - Medium, base=https://api.medium.text-generation-webui.myia.io/v1, key=(len=32), model=None\u001b[0m\n",
      "\u001b[92m18:03:41 [INFO] Local Llama - - name=Local Model - Large, base=https://api.large.text-generation-webui.myia.io/v1, key=(len=32), model=None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Charge automatiquement toutes les variables du fichier .env\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def get_optional_env(var_name, default_value=None):\n",
    "    \"\"\"\n",
    "    Récupère la valeur de la variable d'env `var_name`, \n",
    "    ou la valeur par défaut `default_value` si non définie.\n",
    "    \"\"\"\n",
    "    val = os.getenv(var_name)\n",
    "    if val is None or val.strip() == \"\":\n",
    "        return default_value\n",
    "    return val.strip()\n",
    "\n",
    "def load_endpoint(index=1):\n",
    "    \"\"\"\n",
    "    Lit un ensemble de variables d'environnement.\n",
    "    - index=1 => variables : OPENAI_API_KEY, OPENAI_BASE_URL, etc.\n",
    "    - index>1 => on suffixe : OPENAI_API_KEY_{index}, etc.\n",
    "    Retourne un dict {name, api_base, api_key, model} ou None si 'api_key' manquant.\n",
    "    \"\"\"\n",
    "    suffix = \"\" if index == 1 else f\"_{index}\"\n",
    "\n",
    "    # Lecture des variables\n",
    "    api_key = os.getenv(f\"OPENAI_API_KEY{suffix}\")\n",
    "    if not api_key:\n",
    "        return None  # pas de clé => on arrête\n",
    "\n",
    "    name = get_optional_env(f\"OPENAI_ENDPOINT_NAME{suffix}\", default_value=f\"openai{suffix}\")\n",
    "    base_url = get_optional_env(f\"OPENAI_BASE_URL{suffix}\", default_value=\"https://api.openai.com/v1\")\n",
    "    model_id = get_optional_env(f\"OPENAI_CHAT_MODEL_ID{suffix}\", default_value=None)\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"api_base\": base_url,\n",
    "        \"api_key\": api_key,\n",
    "        \"model\": model_id  # On pourra le compléter si None\n",
    "    }\n",
    "\n",
    "\n",
    "endpoints = []\n",
    "\n",
    "# On tente successivement index=1,2,3... jusqu'à ce qu'on ne trouve plus OPENAI_API_KEY_{i}\n",
    "for i in range(1, 10):  # max 9 endpoints, ajustez si besoin\n",
    "    ep = load_endpoint(i)\n",
    "    if ep is None:\n",
    "        break\n",
    "    endpoints.append(ep)\n",
    "\n",
    "# Vérification (simple)\n",
    "logger.info(\"=== Endpoints chargés ===\")\n",
    "for e in endpoints:\n",
    "    logger.info(f\"- name={e['name']}, base={e['api_base']}, key=(len={len(e['api_key'])}), model={e['model']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Inspection des modèles disponibles\n",
    "\n",
    "Nous allons appeler l'endpoint `/models` de chaque service \n",
    "pour récupérer la liste des modèles chargés côté serveur.\n",
    "\n",
    "Si vous avez mis `model=None` dans la config, vous pourrez automatiquement \n",
    "récupérer le `model` à partir des données renvoyées. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:03:55 [INFO] Local Llama - === OpenAI : /models ===\u001b[0m\n",
      "\u001b[94m18:03:56 [DEBUG] Local Llama - Réponse brute: {'object': 'list', 'data': [{'id': 'gpt-4.5-preview', 'object': 'model', 'created': 1740623059, 'owned_by': 'system'}, {'id': 'omni-moderation-2024-09-26', 'object': 'model', 'created': 1732734466, 'owned_by': 'system'}, {'id': 'gpt-4.5-preview-2025-02-27', 'object': 'model', 'created': 1740623304, 'owned_by': 'system'}, {'id': 'gpt-4o-mini-audio-preview-2024-12-17', 'object': 'model', 'created': 1734115920, 'owned_by': 'system'}, {'id': 'dall-e-3', 'object': 'model', 'created': 1698785189, 'owned_by': 'system'}, {'id': 'dall-e-2', 'object': 'model', 'created': 1698798177, 'owned_by': 'system'}, {'id': 'gpt-4o-audio-preview-2024-10-01', 'object': 'model', 'created': 1727389042, 'owned_by': 'system'}, {'id': 'gpt-4o-audio-preview', 'object': 'model', 'created': 1727460443, 'owned_by': 'system'}, {'id': 'gpt-4o-mini-realtime-preview-2024-12-17', 'object': 'model', 'created': 1734112601, 'owned_by': 'system'}, {'id': 'gpt-4o-2024-11-20', 'object': 'model', 'created': 1739331543, 'owned_by': 'system'}, {'id': 'gpt-4o-mini-realtime-preview', 'object': 'model', 'created': 1734387380, 'owned_by': 'system'}, {'id': 'o1-mini-2024-09-12', 'object': 'model', 'created': 1725648979, 'owned_by': 'system'}, {'id': 'o1-preview-2024-09-12', 'object': 'model', 'created': 1725648865, 'owned_by': 'system'}, {'id': 'o1-mini', 'object': 'model', 'created': 1725649008, 'owned_by': 'system'}, {'id': 'o1-preview', 'object': 'model', 'created': 1725648897, 'owned_by': 'system'}, {'id': 'gpt-4o-mini-audio-preview', 'object': 'model', 'created': 1734387424, 'owned_by': 'system'}, {'id': 'whisper-1', 'object': 'model', 'created': 1677532384, 'owned_by': 'openai-internal'}, {'id': 'gpt-4-turbo', 'object': 'model', 'created': 1712361441, 'owned_by': 'system'}, {'id': 'gpt-4o-2024-05-13', 'object': 'model', 'created': 1715368132, 'owned_by': 'system'}, {'id': 'o1', 'object': 'model', 'created': 1734375816, 'owned_by': 'system'}, {'id': 'gpt-4o-realtime-preview-2024-10-01', 'object': 'model', 'created': 1727131766, 'owned_by': 'system'}, {'id': 'gpt-4', 'object': 'model', 'created': 1687882411, 'owned_by': 'openai'}, {'id': 'babbage-002', 'object': 'model', 'created': 1692634615, 'owned_by': 'system'}, {'id': 'o1-2024-12-17', 'object': 'model', 'created': 1734326976, 'owned_by': 'system'}, {'id': 'chatgpt-4o-latest', 'object': 'model', 'created': 1723515131, 'owned_by': 'system'}, {'id': 'tts-1-hd-1106', 'object': 'model', 'created': 1699053533, 'owned_by': 'system'}, {'id': 'gpt-4o-audio-preview-2024-12-17', 'object': 'model', 'created': 1734034239, 'owned_by': 'system'}, {'id': 'gpt-4o', 'object': 'model', 'created': 1715367049, 'owned_by': 'system'}, {'id': 'gpt-4o-2024-08-06', 'object': 'model', 'created': 1722814719, 'owned_by': 'system'}, {'id': 'tts-1-hd', 'object': 'model', 'created': 1699046015, 'owned_by': 'system'}, {'id': 'text-embedding-3-large', 'object': 'model', 'created': 1705953180, 'owned_by': 'system'}, {'id': 'tts-1', 'object': 'model', 'created': 1681940951, 'owned_by': 'openai-internal'}, {'id': 'tts-1-1106', 'object': 'model', 'created': 1699053241, 'owned_by': 'system'}, {'id': 'gpt-4-turbo-2024-04-09', 'object': 'model', 'created': 1712601677, 'owned_by': 'system'}, {'id': 'davinci-002', 'object': 'model', 'created': 1692634301, 'owned_by': 'system'}, {'id': 'gpt-3.5-turbo-1106', 'object': 'model', 'created': 1698959748, 'owned_by': 'system'}, {'id': 'gpt-3.5-turbo-instruct', 'object': 'model', 'created': 1692901427, 'owned_by': 'system'}, {'id': 'gpt-3.5-turbo-instruct-0914', 'object': 'model', 'created': 1694122472, 'owned_by': 'system'}, {'id': 'gpt-3.5-turbo-0125', 'object': 'model', 'created': 1706048358, 'owned_by': 'system'}, {'id': 'gpt-4o-realtime-preview-2024-12-17', 'object': 'model', 'created': 1733945430, 'owned_by': 'system'}, {'id': 'gpt-3.5-turbo', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}, {'id': 'o3-mini-2025-01-31', 'object': 'model', 'created': 1738010200, 'owned_by': 'system'}, {'id': 'gpt-3.5-turbo-16k-0613', 'object': 'model', 'created': 1685474247, 'owned_by': 'openai'}, {'id': 'gpt-4o-realtime-preview', 'object': 'model', 'created': 1727659998, 'owned_by': 'system'}, {'id': 'o3-mini', 'object': 'model', 'created': 1737146383, 'owned_by': 'system'}, {'id': 'gpt-3.5-turbo-16k', 'object': 'model', 'created': 1683758102, 'owned_by': 'openai-internal'}, {'id': 'gpt-4-turbo-preview', 'object': 'model', 'created': 1706037777, 'owned_by': 'system'}, {'id': 'text-embedding-3-small', 'object': 'model', 'created': 1705948997, 'owned_by': 'system'}, {'id': 'gpt-4-0125-preview', 'object': 'model', 'created': 1706037612, 'owned_by': 'system'}, {'id': 'gpt-4-1106-preview', 'object': 'model', 'created': 1698957206, 'owned_by': 'system'}, {'id': 'text-embedding-ada-002', 'object': 'model', 'created': 1671217299, 'owned_by': 'openai-internal'}, {'id': 'gpt-4-0613', 'object': 'model', 'created': 1686588896, 'owned_by': 'openai'}, {'id': 'gpt-4o-mini-2024-07-18', 'object': 'model', 'created': 1721172717, 'owned_by': 'system'}, {'id': 'gpt-4o-mini', 'object': 'model', 'created': 1721172741, 'owned_by': 'system'}, {'id': 'omni-moderation-latest', 'object': 'model', 'created': 1731689265, 'owned_by': 'system'}]}\u001b[0m\n",
      "\u001b[92m18:03:56 [INFO] Local Llama - Réussite: 55 modèle(s) listé(s) (endpoint=OpenAI)\u001b[0m\n",
      "\u001b[92m18:03:56 [INFO] Local Llama -   -> Temps de réponse: 0.41 secondes\u001b[0m\n",
      "\u001b[92m18:03:56 [INFO] Local Llama - === Local Model - Micro : /models ===\u001b[0m\n",
      "\u001b[94m18:03:56 [DEBUG] Local Llama - Réponse brute: {'object': 'list', 'data': [{'id': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'object': 'model', 'created': 1741021436, 'owned_by': 'vllm', 'root': 'Qwen/Qwen2.5-3B-Instruct-AWQ', 'parent': None, 'max_model_len': 32768, 'permission': [{'id': 'modelperm-6e9bacb193d64f97a161d19e354824e0', 'object': 'model_permission', 'created': 1741021436, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}]}]}\u001b[0m\n",
      "\u001b[92m18:03:56 [INFO] Local Llama - Réussite: 1 modèle(s) listé(s) (endpoint=Local Model - Micro)\u001b[0m\n",
      "\u001b[92m18:03:56 [INFO] Local Llama -   -> Temps de réponse: 0.46 secondes\u001b[0m\n",
      "\u001b[92m18:03:56 [INFO] Local Llama -   -> ep['model'] défini à: Qwen/Qwen2.5-3B-Instruct-AWQ\u001b[0m\n",
      "\u001b[92m18:03:56 [INFO] Local Llama - === Local Model - Mini : /models ===\u001b[0m\n",
      "\u001b[94m18:03:57 [DEBUG] Local Llama - Réponse brute: {'object': 'list', 'data': [{'id': 'Qwen/Qwen2.5-7B-Instruct-AWQ', 'object': 'model', 'created': 1741021436, 'owned_by': 'vllm', 'root': 'Qwen/Qwen2.5-7B-Instruct-AWQ', 'parent': None, 'max_model_len': 32768, 'permission': [{'id': 'modelperm-f6d44239e0314a4989ae21cb27e9155f', 'object': 'model_permission', 'created': 1741021436, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}]}]}\u001b[0m\n",
      "\u001b[92m18:03:57 [INFO] Local Llama - Réussite: 1 modèle(s) listé(s) (endpoint=Local Model - Mini)\u001b[0m\n",
      "\u001b[92m18:03:57 [INFO] Local Llama -   -> Temps de réponse: 0.44 secondes\u001b[0m\n",
      "\u001b[92m18:03:57 [INFO] Local Llama -   -> ep['model'] défini à: Qwen/Qwen2.5-7B-Instruct-AWQ\u001b[0m\n",
      "\u001b[92m18:03:57 [INFO] Local Llama - === Local Model - Medium : /models ===\u001b[0m\n",
      "\u001b[94m18:03:57 [DEBUG] Local Llama - Réponse brute: {'object': 'list', 'data': [{'id': 'unsloth/DeepSeek-R1-Distill-Qwen-14B-bnb-4bit', 'object': 'model', 'created': 1741021437, 'owned_by': 'vllm', 'root': 'unsloth/DeepSeek-R1-Distill-Qwen-14B-bnb-4bit', 'parent': None, 'max_model_len': 32768, 'permission': [{'id': 'modelperm-17899d00e8444442ad72fbfd98b486c4', 'object': 'model_permission', 'created': 1741021437, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}]}]}\u001b[0m\n",
      "\u001b[92m18:03:57 [INFO] Local Llama - Réussite: 1 modèle(s) listé(s) (endpoint=Local Model - Medium)\u001b[0m\n",
      "\u001b[92m18:03:57 [INFO] Local Llama -   -> Temps de réponse: 0.45 secondes\u001b[0m\n",
      "\u001b[92m18:03:57 [INFO] Local Llama -   -> ep['model'] défini à: unsloth/DeepSeek-R1-Distill-Qwen-14B-bnb-4bit\u001b[0m\n",
      "\u001b[92m18:03:57 [INFO] Local Llama - === Local Model - Large : /models ===\u001b[0m\n",
      "\u001b[94m18:03:58 [DEBUG] Local Llama - Réponse brute: {'object': 'list', 'data': [{'id': 'Qwen/Qwen2.5-Coder-32B-Instruct-AWQ', 'object': 'model', 'created': 1741021437, 'owned_by': 'vllm', 'root': 'Qwen/Qwen2.5-Coder-32B-Instruct-AWQ', 'parent': None, 'max_model_len': 8192, 'permission': [{'id': 'modelperm-bc86e51b37c648f0a17e4c57eab31c26', 'object': 'model_permission', 'created': 1741021437, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}]}]}\u001b[0m\n",
      "\u001b[92m18:03:58 [INFO] Local Llama - Réussite: 1 modèle(s) listé(s) (endpoint=Local Model - Large)\u001b[0m\n",
      "\u001b[92m18:03:58 [INFO] Local Llama -   -> Temps de réponse: 0.44 secondes\u001b[0m\n",
      "\u001b[92m18:03:58 [INFO] Local Llama -   -> ep['model'] défini à: Qwen/Qwen2.5-Coder-32B-Instruct-AWQ\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def list_models(api_base, api_key):\n",
    "    url = f\"{api_base}/models\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=20)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.json()  # dict\n",
    "        else:\n",
    "            return {\"error\": f\"status={resp.status_code}\", \"text\": resp.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def update_endpoints_with_model():\n",
    "    for ep in endpoints:\n",
    "        logger.info(f\"=== {ep['name']} : /models ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        info = list_models(ep[\"api_base\"], ep[\"api_key\"])\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # On journalise le JSON complet en DEBUG, pour le diagnostic\n",
    "        logger.debug(f\"Réponse brute: {info}\")\n",
    "\n",
    "        if \"error\" in info:\n",
    "            # En cas d'erreur, on logue au niveau ERROR\n",
    "            logger.error(\n",
    "                f\"Échec de récupération /models \"\n",
    "                f\"(endpoint={ep['name']}): {info['error']}, \"\n",
    "                f\"texte={info.get('text', '')}\"\n",
    "            )\n",
    "        else:\n",
    "            # Succès : on peut afficher le nombre de modèles\n",
    "            data_models = info.get(\"data\", [])\n",
    "            logger.info(\n",
    "                f\"Réussite: {len(data_models)} modèle(s) listé(s) \"\n",
    "                f\"(endpoint={ep['name']})\"\n",
    "            )\n",
    "\n",
    "        logger.info(f\"  -> Temps de réponse: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "        # On met à jour le modèle si:\n",
    "        # 1) il n'y a pas d'erreur, et\n",
    "        # 2) ep[\"model\"] n'est pas déjà défini.\n",
    "        if \"error\" not in info and (\"model\" not in ep or not ep[\"model\"]):\n",
    "            data_list = info.get(\"data\", [])\n",
    "            if data_list:\n",
    "                first_model_id = data_list[0].get(\"id\")\n",
    "                ep[\"model\"] = first_model_id\n",
    "                logger.info(f\"  -> ep['model'] défini à: {first_model_id}\")\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    \"  -> Aucune entrée 'data' dans la réponse pour définir ep['model']\"\n",
    "                )\n",
    "\n",
    "# Appel de la fonction pour tester\n",
    "update_endpoints_with_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Test brut via `requests.post`\n",
    "\n",
    "Ce test vérifie le bon fonctionnement de l'endpoint OpenAI-compatible \n",
    "sans passer par la librairie `openai`. \n",
    "\n",
    "On envoie une requête minimaliste en JSON, \n",
    "puis on affiche la réponse brute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:04:24 [INFO] Local Llama - \n",
      "=== Test HTTP brut pour OpenAI ===\u001b[0m\n",
      "\u001b[94m18:04:24 [DEBUG] Local Llama -   -> Envoi de la requête POST...\u001b[0m\n",
      "\u001b[94m18:04:25 [DEBUG] Local Llama -   -> Statut HTTP: 200 (durée=1.42s)\u001b[0m\n",
      "\u001b[94m18:04:25 [DEBUG] Local Llama - Réponse (début): {\n",
      "  \"id\": \"chatcmpl-B73VHwgHpduizlLrb6uTcElasZAPQ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1741021463,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Bonjour ! Je suis une intelligence artificielle con\\u00e7ue pour r\\u00e9pondre \\u00e0 vos questions et vous aider avec des informations sur divers sujets. Comment puis-je vous aider aujourd'hui ?\",\n",
      "        \"refusal\": null\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 13,\n",
      "    \"completion_tokens\": 34,\n",
      "    \"total_tokens\": 47,\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"cached_tokens\": 0,\n",
      "      \"audio_tokens\": 0\n",
      "    },\n",
      "    \"completion_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"service_tier\": \"default\",\n",
      "  \"system_fingerprint\": \"fp_06737a9306\"\n",
      "}...\u001b[0m\n",
      "\u001b[92m18:04:25 [INFO] Local Llama - [OK] Réponse HTTP 200 en 1.42s, Tokens utilisés=47\u001b[0m\n",
      "\u001b[92m18:04:25 [INFO] Local Llama - \n",
      "=== Test HTTP brut pour Local Model - Micro ===\u001b[0m\n",
      "\u001b[94m18:04:25 [DEBUG] Local Llama -   -> Envoi de la requête POST...\u001b[0m\n",
      "\u001b[94m18:04:26 [DEBUG] Local Llama -   -> Statut HTTP: 200 (durée=0.60s)\u001b[0m\n",
      "\u001b[94m18:04:26 [DEBUG] Local Llama - Réponse (début): {\n",
      "  \"id\": \"chatcmpl-f075b5d497f143c1bfee4add9818ff65\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1741021465,\n",
      "  \"model\": \"Qwen/Qwen2.5-3B-Instruct-AWQ\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"reasoning_content\": null,\n",
      "        \"content\": \"Bonjour! Je suis Qwen, un assistant virtuel cri\\u00e9 par Alibaba Cloud. Je suis ici pour vous aider avec diverses t\\u00e2ches et r\\u00e9pondre \\u00e0 vos questions du mieux que je peux.\",\n",
      "        \"tool_calls\": []\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 36,\n",
      "    \"total_tokens\": 78,\n",
      "    \"completion_tokens\": 42,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}...\u001b[0m\n",
      "\u001b[92m18:04:26 [INFO] Local Llama - [OK] Réponse HTTP 200 en 0.60s, Tokens utilisés=78\u001b[0m\n",
      "\u001b[92m18:04:26 [INFO] Local Llama - \n",
      "=== Test HTTP brut pour Local Model - Mini ===\u001b[0m\n",
      "\u001b[94m18:04:26 [DEBUG] Local Llama -   -> Envoi de la requête POST...\u001b[0m\n",
      "\u001b[94m18:04:26 [DEBUG] Local Llama -   -> Statut HTTP: 200 (durée=0.68s)\u001b[0m\n",
      "\u001b[94m18:04:26 [DEBUG] Local Llama - Réponse (début): {\n",
      "  \"id\": \"chatcmpl-7bc7ca8ee2554e2c8648594946fd8ded\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1741021465,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-AWQ\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"reasoning_content\": null,\n",
      "        \"content\": \"Bonjour ! Je suis Qwen, un assistant virtuel cr\\u00e9\\u00e9 par Alibaba Cloud. Mon objectif est de vous aider du mieux que je peux, sur toutes sortes de questions et t\\u00e2ches. Que ce soit pour donner des informations, vous conse\",\n",
      "        \"tool_calls\": []\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 36,\n",
      "    \"total_tokens\": 86,\n",
      "    \"completion_tokens\": 50,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}...\u001b[0m\n",
      "\u001b[92m18:04:26 [INFO] Local Llama - [OK] Réponse HTTP 200 en 0.68s, Tokens utilisés=86\u001b[0m\n",
      "\u001b[92m18:04:26 [INFO] Local Llama - \n",
      "=== Test HTTP brut pour Local Model - Medium ===\u001b[0m\n",
      "\u001b[94m18:04:26 [DEBUG] Local Llama -   -> Envoi de la requête POST...\u001b[0m\n",
      "\u001b[94m18:04:27 [DEBUG] Local Llama -   -> Statut HTTP: 200 (durée=1.23s)\u001b[0m\n",
      "\u001b[94m18:04:27 [DEBUG] Local Llama - Réponse (début): {\n",
      "  \"id\": \"chatcmpl-958527117efa47798d0d714af0946565\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1741021466,\n",
      "  \"model\": \"unsloth/DeepSeek-R1-Distill-Qwen-14B-bnb-4bit\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"reasoning_content\": \"Alright, someone just said \\\"Bonjour, qui es-tu ?\\\" which means \\\"Hello, who are you?\\\" in French.\\n\\nI should respond politely and introduce myself.\\n\\nI need to explain that I'm an AI assistant created by DeepSeek.\\n\\nIt\",\n",
      "        \"content\": null,\n",
      "        \"tool_calls\": []\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 12,\n",
      "    \"total_tokens\": 62,\n",
      "    \"completion_tokens\": 50,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}...\u001b[0m\n",
      "\u001b[92m18:04:27 [INFO] Local Llama - [OK] Réponse HTTP 200 en 1.23s, Tokens utilisés=62\u001b[0m\n",
      "\u001b[92m18:04:27 [INFO] Local Llama - \n",
      "=== Test HTTP brut pour Local Model - Large ===\u001b[0m\n",
      "\u001b[94m18:04:27 [DEBUG] Local Llama -   -> Envoi de la requête POST...\u001b[0m\n",
      "\u001b[94m18:04:29 [DEBUG] Local Llama -   -> Statut HTTP: 200 (durée=1.43s)\u001b[0m\n",
      "\u001b[94m18:04:29 [DEBUG] Local Llama - Réponse (début): {\n",
      "  \"id\": \"chatcmpl-3308dc81817f4645975cad95578da686\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1741021467,\n",
      "  \"model\": \"Qwen/Qwen2.5-Coder-32B-Instruct-AWQ\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"reasoning_content\": null,\n",
      "        \"content\": \"Bonjour ! Je suis Qwen, un grand mod\\u00e8le de langage cr\\u00e9\\u00e9 par Alibaba Cloud. Je suis l\\u00e0 pour vous aider avec toutes vos questions et t\\u00e2ches ! Comment puis-je vous assister aujourd'hui ?\",\n",
      "        \"tool_calls\": []\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 36,\n",
      "    \"total_tokens\": 80,\n",
      "    \"completion_tokens\": 44,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}...\u001b[0m\n",
      "\u001b[92m18:04:29 [INFO] Local Llama - [OK] Réponse HTTP 200 en 1.43s, Tokens utilisés=80\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def test_brut_endpoints():\n",
    "    \"\"\"Test brut via requests.post() sur tous les endpoints.\"\"\"\n",
    "    for ep in endpoints:\n",
    "        logger.info(f\"\\n=== Test HTTP brut pour {ep['name']} ===\")\n",
    "        \n",
    "        url = f\"{ep['api_base']}/chat/completions\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {ep['api_key']}\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": ep[\"model\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"Bonjour, qui es-tu ?\"}\n",
    "            ],\n",
    "            \"max_completion_tokens\": 50\n",
    "        }\n",
    "        \n",
    "        logger.debug(\"  -> Envoi de la requête POST...\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            resp = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            logger.debug(f\"  -> Statut HTTP: {resp.status_code} (durée={elapsed_time:.2f}s)\")\n",
    "            \n",
    "            # On essaie d'obtenir le JSON de la réponse\n",
    "            try:\n",
    "                resp_json = resp.json()\n",
    "            except json.JSONDecodeError:\n",
    "                logger.error(f\"Réponse non-JSON:\\n{resp.text[:200]}\")\n",
    "                continue\n",
    "            \n",
    "            # Affichage d’un extrait du JSON (en DEBUG, car potentiellement verbeux)\n",
    "            logger.debug(f\"Réponse (début): {json.dumps(resp_json, indent=2)[:1000]}...\")\n",
    "\n",
    "            # Nombre de tokens si success\n",
    "            tokens_used = None\n",
    "            if resp.status_code == 200:\n",
    "                if \"usage\" in resp_json:\n",
    "                    tokens_used = resp_json[\"usage\"].get(\"total_tokens\")\n",
    "                logger.info(\n",
    "                    f\"[OK] Réponse HTTP 200 en {elapsed_time:.2f}s, \"\n",
    "                    f\"Tokens utilisés={tokens_used if tokens_used else 'N/A'}\"\n",
    "                )\n",
    "            else:\n",
    "                # On log au niveau ERROR pour signifier un souci\n",
    "                logger.error(\n",
    "                    f\"[ERREUR] HTTP {resp.status_code}, texte={resp_json.get('message', resp.text)}\"\n",
    "                )\n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.error(\"Timeout après 30s.\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Exception lors de la requête: {str(e)}\")\n",
    "\n",
    "# On exécute le test brut\n",
    "test_brut_endpoints()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Test avec la librairie `openai`\n",
    "\n",
    "On reproduit un appel classique OpenAI, \n",
    "mais en changeant `openai.api_base` et `openai.api_key` \n",
    "pour chaque endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:05:50 [INFO] Local Llama - \n",
      "=== Test openai pour gpt-4o-mini (OpenAI) ===\u001b[0m\n",
      "\u001b[94m18:05:50 [DEBUG] Local Llama - Appel de client.chat.completions.create()...\u001b[0m\n",
      "\u001b[92m18:05:52 [INFO] Local Llama -  -> Réponse (début): La philosophie stoïcienne, fondée par Zénon de Kition au IIIe siècle av. J.-C., enseigne que la vertu est la seule véritable source de bonheur. Elle prône l'acceptation des événements extérieurs et la...\u001b[0m\n",
      "\u001b[92m18:05:52 [INFO] Local Llama -  -> Nb tokens: 162\u001b[0m\n",
      "\u001b[92m18:05:52 [INFO] Local Llama -  -> Temps écoulé: 2.35 sec\u001b[0m\n",
      "\u001b[92m18:05:52 [INFO] Local Llama - \n",
      "=== Test openai pour Qwen/Qwen2.5-3B-Instruct-AWQ (Local Model - Micro) ===\u001b[0m\n",
      "\u001b[94m18:05:52 [DEBUG] Local Llama - Appel de client.chat.completions.create()...\u001b[0m\n",
      "\u001b[92m18:05:54 [INFO] Local Llama -  -> Réponse (début): La philosoph stoïcienne soul sur l'acceptation et la contrôle de nos responsabilités, plutôt que sur la possessivité et la résistance aux adversités. Elle incarne la notion que les événements extérieu...\u001b[0m\n",
      "\u001b[92m18:05:54 [INFO] Local Llama -  -> Nb tokens: 211\u001b[0m\n",
      "\u001b[92m18:05:54 [INFO] Local Llama -  -> Temps écoulé: 1.69 sec\u001b[0m\n",
      "\u001b[92m18:05:54 [INFO] Local Llama - \n",
      "=== Test openai pour Qwen/Qwen2.5-7B-Instruct-AWQ (Local Model - Mini) ===\u001b[0m\n",
      "\u001b[94m18:05:54 [DEBUG] Local Llama - Appel de client.chat.completions.create()...\u001b[0m\n",
      "\u001b[92m18:05:56 [INFO] Local Llama -  -> Réponse (début): Certainement ! La philosophie stoïcienne peut être résumée dans quelques principes clés :\n",
      "\n",
      "1. **Acceptation** : Accepter ce qui est hors de notre contrôle et agir de manière raisonnable envers ce qui ...\u001b[0m\n",
      "\u001b[92m18:05:56 [INFO] Local Llama -  -> Nb tokens: 285\u001b[0m\n",
      "\u001b[92m18:05:56 [INFO] Local Llama -  -> Temps écoulé: 2.36 sec\u001b[0m\n",
      "\u001b[92m18:05:56 [INFO] Local Llama - \n",
      "=== Test openai pour unsloth/DeepSeek-R1-Distill-Qwen-14B-bnb-4bit (Local Model - Medium) ===\u001b[0m\n",
      "\u001b[94m18:05:57 [DEBUG] Local Llama - Appel de client.chat.completions.create()...\u001b[0m\n",
      "\u001b[92m18:06:04 [INFO] Local Llama -  -> Réponse (début): \n",
      "\n",
      "Stoic philosophy centers on accepting the things beyond our control and focusing on what we can influence. It emphasizes living virtuously, aligning actions with the natural order of the universe, a...\u001b[0m\n",
      "\u001b[92m18:06:04 [INFO] Local Llama -  -> Nb tokens: 523\u001b[0m\n",
      "\u001b[92m18:06:04 [INFO] Local Llama -  -> Temps écoulé: 7.95 sec\u001b[0m\n",
      "\u001b[92m18:06:04 [INFO] Local Llama - \n",
      "=== Test openai pour Qwen/Qwen2.5-Coder-32B-Instruct-AWQ (Local Model - Large) ===\u001b[0m\n",
      "\u001b[94m18:06:05 [DEBUG] Local Llama - Appel de client.chat.completions.create()...\u001b[0m\n",
      "\u001b[92m18:06:09 [INFO] Local Llama -  -> Réponse (début): La philosophie stoïcienne, fondée par Zénon d'Élis en Grèce autour de 300 av. J.-C., repose sur plusieurs principes clés : l'acceptation de la volonté de Dieu, l'empathie universelle et la maîtrise de...\u001b[0m\n",
      "\u001b[92m18:06:09 [INFO] Local Llama -  -> Nb tokens: 224\u001b[0m\n",
      "\u001b[92m18:06:09 [INFO] Local Llama -  -> Temps écoulé: 4.75 sec\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def test_openai_chat(api_base, api_key, prompt, model):\n",
    "    \"\"\"\n",
    "    Appel classique OpenAI, en utilisant la classe `OpenAI`\n",
    "    et en gérant la journalisation via logger.\n",
    "    \"\"\"\n",
    "    # Création du client OpenAI-compatible\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=api_base\n",
    "    )\n",
    "\n",
    "    if not model:\n",
    "        logger.error(\"[!] Modèle non défini.\")\n",
    "        raise ValueError(\"Modèle non défini\")\n",
    "\n",
    "    try:\n",
    "        # Appel chat.completions\n",
    "        logger.debug(\"Appel de client.chat.completions.create()...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_completion_tokens=500\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        tokens_used = response.usage.total_tokens if response.usage else None\n",
    "        return content, tokens_used\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Exception lors de l'appel OpenAI: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def test_openai_endpoints():\n",
    "    \"\"\"Itère sur tous les endpoints et lance un prompt 'philosophie stoïcienne'.\"\"\"\n",
    "    for ep in endpoints:\n",
    "        label_model = ep.get(\"model\", \"<aucun>\")\n",
    "        logger.info(f\"\\n=== Test openai pour {label_model} ({ep['name']}) ===\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        prompt = \"Peux-tu résumer la philosophie stoïcienne en quelques lignes ?\"\n",
    "        content, tks = test_openai_chat(\n",
    "            ep[\"api_base\"],\n",
    "            ep[\"api_key\"],\n",
    "            prompt,\n",
    "            ep[\"model\"]\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if content:\n",
    "            logger.info(f\" -> Réponse (début): {content[:200]}...\")\n",
    "            logger.info(f\" -> Nb tokens: {tks}\")\n",
    "            logger.info(f\" -> Temps écoulé: {elapsed_time:.2f} sec\")\n",
    "        else:\n",
    "            logger.warning(\" -> Pas de contenu (erreur ou exception).\")\n",
    "\n",
    "# On exécute le test\n",
    "test_openai_endpoints()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Test avec Semantic Kernel (optionnel)\n",
    "\n",
    "Exemple d'intégration avec [Semantic Kernel](https://github.com/microsoft/semantic-kernel).\n",
    "On crée un Kernel, on y ajoute un service chat OpenAI-like avec l'endpoint souhaité, \n",
    "et on exécute un prompt simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     OpenAIChatCompletion,\n\u001b[0;32m      4\u001b[0m     OpenAIChatPromptExecutionSettings\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt_template\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplateConfig\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt_template\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_variable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputVariable\n",
      "File \u001b[1;32mc:\\Users\\Administrateur.000\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\__init__.py:31\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt_execution_settings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai_text_to_audio_execution_settings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     OpenAITextToAudioExecutionSettings,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt_execution_settings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai_text_to_image_execution_settings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     OpenAITextToImageExecutionSettings,\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure_audio_to_text\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureAudioToText\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure_chat_completion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatCompletion\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure_text_completion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureTextCompletion\n",
      "File \u001b[1;32mc:\\Users\\Administrateur.000\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\azure_audio_to_text.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncAzureADTokenProvider\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ValidationError\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure_config_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIConfigBase\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai_audio_to_text_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIAudioToTextBase\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai_model_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIModelTypes\n",
      "File \u001b[1;32mc:\\Users\\Administrateur.000\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\azure_config_base.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigDict, validate_call\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconst\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_AZURE_API_VERSION\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai_handler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIHandler, OpenAIModelTypes\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconst\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m USER_AGENT\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceInitializationError\n",
      "File \u001b[1;32mc:\\Users\\Administrateur.000\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai_model_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIModelTypes\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt_execution_settings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptExecutionSettings\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstructured_output_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_structured_output_response_format_schema\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceResponseException\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceInvalidRequestError\n",
      "File \u001b[1;32mc:\\Users\\Administrateur.000\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\connectors\\utils\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Microsoft. All rights reserved.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentLoader\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocumentLoader\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Administrateur.000\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\connectors\\utils\\document_loader.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncClient, HTTPStatusError, RequestError\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceInvalidRequestError\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtelemetry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTP_USER_AGENT\n\u001b[0;32m     12\u001b[0m logger: logging\u001b[38;5;241m.\u001b[39mLogger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDocumentLoader\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Administrateur.000\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\user_agent.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m HTTP_USER_AGENT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic-kernel-python\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     version_info \u001b[38;5;241m=\u001b[39m \u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msemantic-kernel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PackageNotFoundError:\n\u001b[0;32m     19\u001b[0m     version_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Administrateur.000\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\metadata\\__init__.py:1009\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \n\u001b[0;32m   1005\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrateur.000\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\importlib_metadata\\__init__.py:514\u001b[0m, in \u001b[0;36mDistribution.version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    513\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the 'Version' metadata for the distribution package.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVersion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrateur.000\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\importlib_metadata\\_adapters.py:54\u001b[0m, in \u001b[0;36mMessage.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     52\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(item)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(item)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Version'"
     ]
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    OpenAIChatPromptExecutionSettings\n",
    ")\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "async def test_semantic_kernel():\n",
    "    \"\"\"\n",
    "    Exécute un prompt via Semantic Kernel pour chaque endpoint,\n",
    "    et journalise les résultats en couleur via `logger`.\n",
    "    \"\"\"\n",
    "    for ep in endpoints:\n",
    "        model_id = ep.get(\"model\")\n",
    "        api_key = ep[\"api_key\"]\n",
    "\n",
    "        logger.info(f\"=== Test Semantic Kernel pour endpoint='{ep['name']}', model='{model_id}' ===\")\n",
    "\n",
    "        kernel = sk.Kernel()\n",
    "        async_client = AsyncOpenAI(api_key=api_key, base_url=ep[\"api_base\"])\n",
    "\n",
    "        kernel.add_service(\n",
    "            OpenAIChatCompletion(\n",
    "                service_id=\"default\",\n",
    "                ai_model_id=model_id,\n",
    "                async_client=async_client\n",
    "            )\n",
    "        )\n",
    "        logger.debug(\"Service OpenAI ajouté au Kernel.\")\n",
    "\n",
    "        prompt_template = \"Explique ce qu'est l'apprentissage profond (deep learning) en 500 mots.\"\n",
    "\n",
    "        exec_settings = OpenAIChatPromptExecutionSettings(\n",
    "            service_id=\"default\",\n",
    "            ai_model_id=model_id,\n",
    "            max_completion_tokens=500,\n",
    "        )\n",
    "\n",
    "        pt_config = PromptTemplateConfig(\n",
    "            template=prompt_template,\n",
    "            name=\"deepLearningFunction\",\n",
    "            template_format=\"semantic-kernel\",\n",
    "            input_variables=[],\n",
    "            execution_settings=exec_settings,\n",
    "        )\n",
    "\n",
    "        func = kernel.add_function(\n",
    "            function_name=\"deepLearningFunction\",\n",
    "            plugin_name=\"defaultPlugin\",\n",
    "            prompt_template_config=pt_config\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            logger.info(\"  -> Exécution en cours (Semantic Kernel)...\")\n",
    "            start_time = time.time()\n",
    "            result = await kernel.invoke(func, KernelArguments())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Comptage approximatif de tokens\n",
    "            tokens_count = len(str(result).split())\n",
    "            speed = tokens_count / elapsed if elapsed > 0 else 0\n",
    "\n",
    "            logger.info(f\"  -> Résultat (début): {str(result)[:400]}...\")\n",
    "            logger.info(f\"  -> Durée: {elapsed:.2f}s, Tokens={tokens_count}, speed={speed:.2f} tok/s\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"  [!] Erreur SK sur endpoint='{ep['name']}': {str(e)}\")\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "await test_semantic_kernel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1) Test du Function/Tool Calling sur chaque endpoint\n",
    "\n",
    "Avec vLLM, lorsqu’on démarre les containers avec `--enable-auto-tool-choice` et un `--tool-call-parser` adéquat,\n",
    "le modèle peut déclencher automatiquement un « tool call » s’il juge qu’un outil est pertinent.  \n",
    "On doit alors inclure un paramètre `tools` dans la requête, et indiquer `tool_choice=\"auto\"` (ou un nom de fonction précis).\n",
    "\n",
    "**Note** : Pour exécuter concrètement la fonction côté client Python, on doit définir une fonction Python qui correspond,\n",
    "et réinjecter manuellement le résultat dans la conversation.  \n",
    "Voici un exemple simplifié : on va appeler un `get_weather(location, unit)` sur *tous* les endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:07:07 [INFO] Local Llama - === Test Tool Calling sur endpoint 'OpenAI' ===\u001b[0m\n",
      "\u001b[94m18:07:07 [DEBUG] Local Llama - Appel chat.completions avec tool_choice='auto'\u001b[0m\n",
      "\u001b[94m18:07:08 [DEBUG] Local Llama - Réponse textuelle (début): ''\u001b[0m\n",
      "\u001b[94m18:07:08 [DEBUG] Local Llama - Contenu complet de la réponse: {\n",
      "  \"id\": \"chatcmpl-B73XvBcQlb2vECP0eR7y5eIYzpJhv\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_ftvuhN8WSXaLmTWZG50QyCfc\",\n",
      "            \"function\": {\n",
      "              \"arguments\": \"{\\\"location\\\":\\\"Marseille, France\\\",\\\"unit\\\":\\\"celsius\\\"}\",\n",
      "              \"name\": \"get_weather\"\n",
      "            },\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1741021627,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": \"default\",\n",
      "  \"system_fingerprint\": \"fp_06737a9306\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 23,\n",
      "    \"prompt_tokens\": 82,\n",
      "    \"total_tokens\": 105,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  }\n",
      "}\u001b[0m\n",
      "\u001b[92m18:07:08 [INFO] Local Llama -   -> 1 tool_call(s) détecté(s) sur endpoint='OpenAI'.\u001b[0m\n",
      "\u001b[92m18:07:08 [INFO] Local Llama -      Fonction appelée: get_weather | arguments={'location': 'Marseille, France', 'unit': 'celsius'}\u001b[0m\n",
      "\u001b[92m18:07:08 [INFO] Local Llama -      => Résultat simulé: Simulation: Météo à Marseille, France, unité=celsius, ciel dégagé.\u001b[0m\n",
      "\u001b[92m18:07:08 [INFO] Local Llama - === Test Tool Calling sur endpoint 'Local Model - Micro' ===\u001b[0m\n",
      "\u001b[94m18:07:08 [DEBUG] Local Llama - Appel chat.completions avec tool_choice='auto'\u001b[0m\n",
      "\u001b[94m18:07:09 [DEBUG] Local Llama - Réponse textuelle (début): ''\u001b[0m\n",
      "\u001b[94m18:07:09 [DEBUG] Local Llama - Contenu complet de la réponse: {\n",
      "  \"id\": \"chatcmpl-02e66dd7a0c14a948b020a1fab627374\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"chatcmpl-tool-245882312d3249a2b726e3e94c7de638\",\n",
      "            \"function\": {\n",
      "              \"arguments\": \"{\\\"location\\\": \\\"Marseille, France\\\", \\\"unit\\\": \\\"celsius\\\"}\",\n",
      "              \"name\": \"get_weather\"\n",
      "            },\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ],\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1741021628,\n",
      "  \"model\": \"Qwen/Qwen2.5-3B-Instruct-AWQ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 30,\n",
      "    \"prompt_tokens\": 220,\n",
      "    \"total_tokens\": 250,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}\u001b[0m\n",
      "\u001b[92m18:07:09 [INFO] Local Llama -   -> 1 tool_call(s) détecté(s) sur endpoint='Local Model - Micro'.\u001b[0m\n",
      "\u001b[92m18:07:09 [INFO] Local Llama -      Fonction appelée: get_weather | arguments={'location': 'Marseille, France', 'unit': 'celsius'}\u001b[0m\n",
      "\u001b[92m18:07:09 [INFO] Local Llama -      => Résultat simulé: Simulation: Météo à Marseille, France, unité=celsius, ciel dégagé.\u001b[0m\n",
      "\u001b[92m18:07:09 [INFO] Local Llama - === Test Tool Calling sur endpoint 'Local Model - Mini' ===\u001b[0m\n",
      "\u001b[94m18:07:09 [DEBUG] Local Llama - Appel chat.completions avec tool_choice='auto'\u001b[0m\n",
      "\u001b[94m18:07:09 [DEBUG] Local Llama - Réponse textuelle (début): ''\u001b[0m\n",
      "\u001b[94m18:07:09 [DEBUG] Local Llama - Contenu complet de la réponse: {\n",
      "  \"id\": \"chatcmpl-3f1314f64685497b87da7a2e03d326db\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"chatcmpl-tool-557994bfe77d4298b5c61e3436ee4b0b\",\n",
      "            \"function\": {\n",
      "              \"arguments\": \"{\\\"location\\\": \\\"Marseille, France\\\", \\\"unit\\\": \\\"celsius\\\"}\",\n",
      "              \"name\": \"get_weather\"\n",
      "            },\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ],\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1741021629,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-AWQ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 30,\n",
      "    \"prompt_tokens\": 220,\n",
      "    \"total_tokens\": 250,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}\u001b[0m\n",
      "\u001b[92m18:07:09 [INFO] Local Llama -   -> 1 tool_call(s) détecté(s) sur endpoint='Local Model - Mini'.\u001b[0m\n",
      "\u001b[92m18:07:09 [INFO] Local Llama -      Fonction appelée: get_weather | arguments={'location': 'Marseille, France', 'unit': 'celsius'}\u001b[0m\n",
      "\u001b[92m18:07:09 [INFO] Local Llama -      => Résultat simulé: Simulation: Météo à Marseille, France, unité=celsius, ciel dégagé.\u001b[0m\n",
      "\u001b[92m18:07:09 [INFO] Local Llama - === Test Tool Calling sur endpoint 'Local Model - Medium' ===\u001b[0m\n",
      "\u001b[94m18:07:10 [DEBUG] Local Llama - Appel chat.completions avec tool_choice='auto'\u001b[0m\n",
      "\u001b[91m18:07:10 [ERROR] Local Llama - [!] Erreur lors de l'appel sur endpoint='Local Model - Medium': Error code: 400 - {'object': 'error', 'message': '\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set', 'type': 'BadRequestError', 'param': None, 'code': 400}\u001b[0m\n",
      "\u001b[92m18:07:10 [INFO] Local Llama - === Test Tool Calling sur endpoint 'Local Model - Large' ===\u001b[0m\n",
      "\u001b[94m18:07:10 [DEBUG] Local Llama - Appel chat.completions avec tool_choice='auto'\u001b[0m\n",
      "\u001b[94m18:07:11 [DEBUG] Local Llama - Réponse textuelle (début): '<tools>\\n{\"name\": \"get_weather\", \"arguments\": {\"location\": \"Marseille, France\", \"unit\": \"celsius\"}}\\n</tools>'\u001b[0m\n",
      "\u001b[94m18:07:11 [DEBUG] Local Llama - Contenu complet de la réponse: {\n",
      "  \"id\": \"chatcmpl-952b8688d8d84d11be216ac8b787a869\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"<tools>\\n{\\\"name\\\": \\\"get_weather\\\", \\\"arguments\\\": {\\\"location\\\": \\\"Marseille, France\\\", \\\"unit\\\": \\\"celsius\\\"}}\\n</tools>\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1741021630,\n",
      "  \"model\": \"Qwen/Qwen2.5-Coder-32B-Instruct-AWQ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 33,\n",
      "    \"prompt_tokens\": 220,\n",
      "    \"total_tokens\": 253,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}\u001b[0m\n",
      "\u001b[93m18:07:11 [WARNING] Local Llama -   -> Aucun tool_call détecté dans la réponse.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_weather(location: str, unit: str):\n",
    "    \"\"\"Exemple de fonction locale pour la météo.\"\"\"\n",
    "    return f\"Simulation: Météo à {location}, unité={unit}, ciel dégagé.\"\n",
    "\n",
    "def test_tool_calling():\n",
    "    \"\"\"\n",
    "    Test du Function/Tool Calling pour chaque endpoint, en mode auto (tool_choice='auto').\n",
    "    On journalise les étapes et le résultat.\n",
    "    \"\"\"\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Obtenir la météo pour un lieu donné\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\"type\": \"string\", \"description\": \"Ex: 'Paris, France'\"},\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
    "                    },\n",
    "                    \"required\": [\"location\", \"unit\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    user_message = \"Bonjour, est-ce que tu peux me donner la météo pour Marseille en celsius ?\"\n",
    "\n",
    "    for ep in endpoints:\n",
    "        logger.info(f\"=== Test Tool Calling sur endpoint '{ep['name']}' ===\")\n",
    "\n",
    "        openai.api_base = ep[\"api_base\"]\n",
    "        openai.api_key  = ep[\"api_key\"]\n",
    "\n",
    "        client = OpenAI(\n",
    "            api_key=ep[\"api_key\"],\n",
    "            base_url=ep[\"api_base\"]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            logger.debug(\"Appel chat.completions avec tool_choice='auto'\")\n",
    "            response = client.chat.completions.create(\n",
    "                model=ep.get(\"model\"),\n",
    "                messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "                tools=tools,\n",
    "                tool_choice=\"auto\",\n",
    "            )\n",
    "        except Exception as ex:\n",
    "            logger.error(f\"[!] Erreur lors de l'appel sur endpoint='{ep['name']}': {ex}\")\n",
    "            continue\n",
    "\n",
    "        choice = response.choices[0]\n",
    "        msg = choice.message\n",
    "\n",
    "        text_content = msg.content if msg.content else \"\"\n",
    "        tool_calls = msg.tool_calls\n",
    "\n",
    "        logger.debug(f\"Réponse textuelle (début): {text_content[:400]!r}\")\n",
    "        logger.debug(f\"Contenu complet de la réponse: {json.dumps(response.to_dict(), indent=2)}\")\n",
    "\n",
    "        if tool_calls:\n",
    "            logger.info(f\"  -> {len(tool_calls)} tool_call(s) détecté(s) sur endpoint='{ep['name']}'.\")\n",
    "            for call in tool_calls:\n",
    "                func_name = call.function.name\n",
    "                args_str  = call.function.arguments or \"{}\"\n",
    "                args_dict = json.loads(args_str)\n",
    "\n",
    "                logger.info(f\"     Fonction appelée: {func_name} | arguments={args_dict}\")\n",
    "\n",
    "                # Exécution locale simulée\n",
    "                if func_name == \"get_weather\":\n",
    "                    result = get_weather(**args_dict)\n",
    "                    logger.info(f\"     => Résultat simulé: {result}\")\n",
    "                else:\n",
    "                    logger.warning(f\"     => Fonction inconnue: {func_name}\")\n",
    "        else:\n",
    "            logger.warning(\"  -> Aucun tool_call détecté dans la réponse.\")\n",
    "\n",
    "# On l’exécute\n",
    "test_tool_calling()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2) Test du mode « Reasoning Outputs »\n",
    "\n",
    "Certains modèles (p. ex. DeepSeek R1) sont lancés avec `--enable-reasoning --reasoning-parser deepseek_r1`.\n",
    "Cela permet de renvoyer, en plus du `content` final, un champ `reasoning_content` qui détaille la chaîne de raisonnement.\n",
    "\n",
    "Voici un exemple d’appel sur *tous* les endpoints (certains n’auront pas de champ `reasoning_content` si le modèle ne supporte pas le raisonnement).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:08:40 [INFO] Local Llama - === Test Reasoning Output sur endpoint='OpenAI' ===\u001b[0m\n",
      "\u001b[94m18:08:42 [DEBUG] Local Llama -   -> Réponse (finale) : Pour résoudre l'expression \\( 253 \\times 73 - 287 \\), nous procédons comme suit :\n",
      "\n",
      "1. Calculons \\( 253 \\times 73 \\) :\n",
      "   \\[\n",
      "   253 \\times 73 = 18469\n",
      "   \\]\n",
      "\n",
      "2. Maintenant, soustrayons 287 de ce résultat :\n",
      "   \\[\n",
      "   18469 - 287 = 18182\n",
      "   \\]\n",
      "\n",
      "Donc, \\( 2\u001b[0m\n",
      "\u001b[93m18:08:42 [WARNING] Local Llama -   -> Pas de 'reasoning_content' pour ce modèle/parsing.\u001b[0m\n",
      "\u001b[92m18:08:42 [INFO] Local Llama - === Test Reasoning Output sur endpoint='Local Model - Micro' ===\u001b[0m\n",
      "\u001b[94m18:08:44 [DEBUG] Local Llama -   -> Réponse (finale) : Pour résoudre l'expression 253 * 73 - 287, nous allons procéder comme ceci :\n",
      "\n",
      "1) Calculer le produit 253 * 73\n",
      "2) Retir la valeur 287 du résultat obtenu\n",
      "\n",
      "Premièrement, calculons 253 * 73 :\n",
      "\n",
      "\\[ 253 \\times 73 = 18559 \\]\n",
      "\n",
      "Deuxièmement, soustrons 287 de 1\u001b[0m\n",
      "\u001b[93m18:08:44 [WARNING] Local Llama -   -> Pas de 'reasoning_content' pour ce modèle/parsing.\u001b[0m\n",
      "\u001b[92m18:08:44 [INFO] Local Llama - === Test Reasoning Output sur endpoint='Local Model - Mini' ===\u001b[0m\n",
      "\u001b[94m18:08:45 [DEBUG] Local Llama -   -> Réponse (finale) : Pour calculer cette expression, nous allons la résoudre étape par étape :\n",
      "\n",
      "1) D'abord, multiplions 253 par 73 :\n",
      "   253 * 73 = 18369\n",
      "\n",
      "2) Ensuite, soustrayons 287 de ce résultat :\n",
      "   18369 - 287 = 18082\n",
      "\n",
      "Donc, la réponse à la question 253 * 73 - 287 es\u001b[0m\n",
      "\u001b[93m18:08:45 [WARNING] Local Llama -   -> Pas de 'reasoning_content' pour ce modèle/parsing.\u001b[0m\n",
      "\u001b[92m18:08:45 [INFO] Local Llama - === Test Reasoning Output sur endpoint='Local Model - Medium' ===\u001b[0m\n",
      "\u001b[94m18:08:49 [DEBUG] Local Llama -   -> Réponse (finale) : \n",
      "\n",
      "Calculons \\( 253 \\times 73 - 287 \\) étape par étape :\n",
      "\n",
      "**1. Multiplication :**\n",
      "\\[\n",
      "253 \\times 73 = 18\\,429\n",
      "\\]\n",
      "\n",
      "**2. Soustraction :**\n",
      "\\[\n",
      "18\\,429 - 287 = 18\\,142\n",
      "\\]\n",
      "\n",
      "Ainsi, le résultat est :\n",
      "\\[\n",
      "\\boxed{18\\,142}\n",
      "\\]\u001b[0m\n",
      "\u001b[92m18:08:49 [INFO] Local Llama -   -> Raisonnement   : Pour résoudre l'expression \\(253 \\times 73 - 287\\), je commence par calculer le produit de 253 et 73.\n",
      "\n",
      "Je multiplie 253 par 73, ce qui donne 18 429.\n",
      "\n",
      "Ensuite, je soustrais 287 de ce résultat, ce qui donne 18 429 - 287 = 18 142.\n",
      "\n",
      "Ainsi, le résultat fi\u001b[0m\n",
      "\u001b[92m18:08:49 [INFO] Local Llama - === Test Reasoning Output sur endpoint='Local Model - Large' ===\u001b[0m\n",
      "\u001b[94m18:08:53 [DEBUG] Local Llama -   -> Réponse (finale) : Pour résoudre cette expression, nous allons suivre l'ordre des opérations, c'est-à-dire la multiplication avant la soustraction.\n",
      "\n",
      "1. Calculer la multiplication : 253 * 73\n",
      "2. Soustraire 287 du résultat de la multiplication\n",
      "\n",
      "**Calcul de la multiplicati\u001b[0m\n",
      "\u001b[93m18:08:53 [WARNING] Local Llama -   -> Pas de 'reasoning_content' pour ce modèle/parsing.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def test_reasoning():\n",
    "    \"\"\"\n",
    "    Test du mode reasoning_output (champ 'reasoning_content') \n",
    "    s'il est activé sur certains modèles. \n",
    "    \"\"\"\n",
    "    question = \"Combien font 253 * 73 - 287 ?\"\n",
    "\n",
    "    for ep in endpoints:\n",
    "        logger.info(f\"=== Test Reasoning Output sur endpoint='{ep['name']}' ===\")\n",
    "\n",
    "        openai.api_base = ep[\"api_base\"]\n",
    "        openai.api_key  = ep[\"api_key\"]\n",
    "\n",
    "        client = OpenAI(\n",
    "            api_key=ep[\"api_key\"],\n",
    "            base_url=ep[\"api_base\"]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=ep.get(\"model\"),\n",
    "                messages=[{\"role\": \"user\", \"content\": question}]\n",
    "            )\n",
    "        except Exception as ex:\n",
    "            logger.error(f\"  [!] Erreur lors de l'appel endpoint='{ep['name']}': {ex}\")\n",
    "            continue\n",
    "\n",
    "        choice = response.choices[0]\n",
    "        msg = choice.message\n",
    "\n",
    "        reasoning_part = getattr(msg, \"reasoning_content\", None)\n",
    "        final_content = msg.content or \"\"\n",
    "\n",
    "        logger.debug(f\"  -> Réponse (finale) : {final_content[:250]}\")\n",
    "        if reasoning_part:\n",
    "            logger.info(f\"  -> Raisonnement   : {reasoning_part[:250]}\")\n",
    "        else:\n",
    "            logger.warning(\"  -> Pas de 'reasoning_content' pour ce modèle/parsing.\")\n",
    "\n",
    "# Lancement\n",
    "test_reasoning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Benchmark final (avec journaux réguliers)\n",
    "\n",
    "Cette étape exécute un warm-up + N itérations par endpoint.\n",
    "On calcule ensuite la vitesse tokens/s. \n",
    "\n",
    "**Important** : Le prompt est un peu plus long, et la génération peut \n",
    "prendre du temps selon la taille du modèle ou la quantization.\n",
    "\n",
    "Pour ne pas paraître figé, on ajoute des `print` avant et après l'appel, \n",
    "pour indiquer la progression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:09:12 [INFO] Local Llama - === Benchmark: endpoint=OpenAI, label=auto_benchmark1 ===\u001b[0m\n",
      "\u001b[92m18:09:12 [INFO] Local Llama -    (warm-up) Lancement d'un appel simple.\u001b[0m\n",
      "\u001b[94m18:09:12 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n",
      "\u001b[92m18:09:13 [INFO] Local Llama -    (benchmark) On va faire 1 itérations.\u001b[0m\n",
      "\u001b[92m18:09:13 [INFO] Local Llama -    -> Iteration 1/1\u001b[0m\n",
      "\u001b[94m18:09:13 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n",
      "\u001b[92m18:09:26 [INFO] Local Llama -       => Durée: 12.81s, tokens=1053\u001b[0m\n",
      "\u001b[92m18:09:26 [INFO] Local Llama -    => OpenAI OK: 1/1 calls, avg_time=12.81s, speed=82.22 tok/s\u001b[0m\n",
      "\u001b[92m18:09:26 [INFO] Local Llama - === Benchmark: endpoint=Local Model - Micro, label=auto_benchmark1 ===\u001b[0m\n",
      "\u001b[92m18:09:26 [INFO] Local Llama -    (warm-up) Lancement d'un appel simple.\u001b[0m\n",
      "\u001b[94m18:09:26 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n",
      "\u001b[92m18:09:27 [INFO] Local Llama -    (benchmark) On va faire 1 itérations.\u001b[0m\n",
      "\u001b[92m18:09:27 [INFO] Local Llama -    -> Iteration 1/1\u001b[0m\n",
      "\u001b[94m18:09:27 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n",
      "\u001b[92m18:09:34 [INFO] Local Llama -       => Durée: 6.81s, tokens=1064\u001b[0m\n",
      "\u001b[92m18:09:34 [INFO] Local Llama -    => Local Model - Micro OK: 1/1 calls, avg_time=6.81s, speed=156.23 tok/s\u001b[0m\n",
      "\u001b[92m18:09:34 [INFO] Local Llama - === Benchmark: endpoint=Local Model - Mini, label=auto_benchmark1 ===\u001b[0m\n",
      "\u001b[92m18:09:34 [INFO] Local Llama -    (warm-up) Lancement d'un appel simple.\u001b[0m\n",
      "\u001b[94m18:09:34 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n",
      "\u001b[92m18:09:35 [INFO] Local Llama -    (benchmark) On va faire 1 itérations.\u001b[0m\n",
      "\u001b[92m18:09:35 [INFO] Local Llama -    -> Iteration 1/1\u001b[0m\n",
      "\u001b[94m18:09:35 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n",
      "\u001b[92m18:09:43 [INFO] Local Llama -       => Durée: 8.37s, tokens=1064\u001b[0m\n",
      "\u001b[92m18:09:43 [INFO] Local Llama -    => Local Model - Mini OK: 1/1 calls, avg_time=8.37s, speed=127.18 tok/s\u001b[0m\n",
      "\u001b[92m18:09:43 [INFO] Local Llama - === Benchmark: endpoint=Local Model - Medium, label=auto_benchmark1 ===\u001b[0m\n",
      "\u001b[92m18:09:43 [INFO] Local Llama -    (warm-up) Lancement d'un appel simple.\u001b[0m\n",
      "\u001b[94m18:09:44 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n",
      "\u001b[92m18:09:47 [INFO] Local Llama -    (benchmark) On va faire 1 itérations.\u001b[0m\n",
      "\u001b[92m18:09:47 [INFO] Local Llama -    -> Iteration 1/1\u001b[0m\n",
      "\u001b[94m18:09:47 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n",
      "\u001b[92m18:10:02 [INFO] Local Llama -       => Durée: 14.94s, tokens=1056\u001b[0m\n",
      "\u001b[92m18:10:02 [INFO] Local Llama -    => Local Model - Medium OK: 1/1 calls, avg_time=14.94s, speed=70.67 tok/s\u001b[0m\n",
      "\u001b[92m18:10:02 [INFO] Local Llama - === Benchmark: endpoint=Local Model - Large, label=auto_benchmark1 ===\u001b[0m\n",
      "\u001b[92m18:10:02 [INFO] Local Llama -    (warm-up) Lancement d'un appel simple.\u001b[0m\n",
      "\u001b[94m18:10:02 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n",
      "\u001b[92m18:10:03 [INFO] Local Llama -    (benchmark) On va faire 1 itérations.\u001b[0m\n",
      "\u001b[92m18:10:03 [INFO] Local Llama -    -> Iteration 1/1\u001b[0m\n",
      "\u001b[94m18:10:03 [DEBUG] Local Llama -       ... appel client.chat.completions.create() en cours ...\u001b[0m\n",
      "\u001b[92m18:10:27 [INFO] Local Llama -       => Durée: 23.52s, tokens=1064\u001b[0m\n",
      "\u001b[92m18:10:27 [INFO] Local Llama -    => Local Model - Large OK: 1/1 calls, avg_time=23.52s, speed=45.23 tok/s\u001b[0m\n",
      "\u001b[92m18:10:27 [INFO] Local Llama - [INFO] Fin du benchmark pour label='auto_benchmark1'.\u001b[0m\n",
      "\u001b[92m18:10:27 [INFO] Local Llama - \n",
      "=== Résultats de ce premier benchmark (label='auto_benchmark1') ===\u001b[0m\n",
      "\u001b[92m18:10:27 [INFO] Local Llama - OpenAI -> 1/1 ok, avg time=12.81s, speed=82.22 tok/s\u001b[0m\n",
      "\u001b[92m18:10:27 [INFO] Local Llama - Local Model - Micro -> 1/1 ok, avg time=6.81s, speed=156.23 tok/s\u001b[0m\n",
      "\u001b[92m18:10:27 [INFO] Local Llama - Local Model - Mini -> 1/1 ok, avg time=8.37s, speed=127.18 tok/s\u001b[0m\n",
      "\u001b[92m18:10:27 [INFO] Local Llama - Local Model - Medium -> 1/1 ok, avg time=14.94s, speed=70.67 tok/s\u001b[0m\n",
      "\u001b[92m18:10:27 [INFO] Local Llama - Local Model - Large -> 1/1 ok, avg time=23.52s, speed=45.23 tok/s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Paramètre: combien de fois on répète la requête pour la mesure.\n",
    "N_REPEATS = 1\n",
    "all_results = []\n",
    "\n",
    "def query_once(api_base, api_key, prompt, model=None):\n",
    "    \"\"\"\n",
    "    Appel unique via la nouvelle API (client = OpenAI(...)) + log.\n",
    "    Retourne (elapsed_seconds, total_tokens).\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    logger.debug(\"      ... appel client.chat.completions.create() en cours ...\")\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "            max_completion_tokens=1000,\n",
    "            stream=False,\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        tokens = resp.usage.total_tokens if resp.usage else None\n",
    "        return elapsed, tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"      [!] Exception: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def run_benchmark(label, prompt, repeats=N_REPEATS):\n",
    "    \"\"\"\n",
    "    1) Warm-up (unique appel) + N appels chronométrés par endpoint.\n",
    "    2) On log avant/après chaque appel via logger.\n",
    "    3) On stocke les métriques dans all_results.\n",
    "    \"\"\"\n",
    "    global all_results\n",
    "\n",
    "    for ep in endpoints:\n",
    "        logger.info(f\"=== Benchmark: endpoint={ep['name']}, label={label} ===\")\n",
    "\n",
    "        # WARM-UP\n",
    "        logger.info(\"   (warm-up) Lancement d'un appel simple.\")\n",
    "        w_elapsed, w_tokens = query_once(\n",
    "            ep[\"api_base\"], ep[\"api_key\"],\n",
    "            \"Warm up. Ignorez ce message.\",\n",
    "            model=ep.get(\"model\", None)\n",
    "        )\n",
    "        if w_elapsed is None:\n",
    "            logger.warning(f\"   [!] Warm-up échoué => skip {ep['name']}.\")\n",
    "            continue\n",
    "\n",
    "        total_time = 0.0\n",
    "        total_tokens = 0\n",
    "        success_count = 0\n",
    "\n",
    "        logger.info(f\"   (benchmark) On va faire {repeats} itérations.\")\n",
    "        for i in range(repeats):\n",
    "            logger.info(f\"   -> Iteration {i+1}/{repeats}\")\n",
    "            e, tks = query_once(\n",
    "                ep[\"api_base\"], ep[\"api_key\"],\n",
    "                prompt,\n",
    "                model=ep.get('model', None)\n",
    "            )\n",
    "            if e is None:\n",
    "                logger.error(\"      Echec de l'appel, on continue avec le suivant.\")\n",
    "                continue\n",
    "            logger.info(f\"      => Durée: {e:.2f}s, tokens={tks}\")\n",
    "            total_time += e\n",
    "            if tks is not None:\n",
    "                total_tokens += tks\n",
    "            success_count += 1\n",
    "\n",
    "        if success_count == 0:\n",
    "            logger.warning(\"   [!] Aucune itération réussie pour ce endpoint.\")\n",
    "            continue\n",
    "\n",
    "        avg_time = total_time / success_count\n",
    "        tok_per_sec = 0.0\n",
    "        if total_time > 0:\n",
    "            tok_per_sec = total_tokens / total_time\n",
    "\n",
    "        res = {\n",
    "            \"label\": label,\n",
    "            \"endpoint\": ep[\"name\"],\n",
    "            \"repeats\": repeats,\n",
    "            \"success_count\": success_count,\n",
    "            \"total_time_s\": total_time,\n",
    "            \"avg_time_s\": avg_time,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"tokens_per_sec\": tok_per_sec\n",
    "        }\n",
    "        all_results.append(res)\n",
    "        logger.info(\n",
    "            f\"   => {ep['name']} OK: {success_count}/{repeats} calls, \"\n",
    "            f\"avg_time={avg_time:.2f}s, speed={tok_per_sec:.2f} tok/s\"\n",
    "        )\n",
    "\n",
    "    logger.info(f\"[INFO] Fin du benchmark pour label='{label}'.\")\n",
    "\n",
    "# Exemple d’appel:\n",
    "label_run = \"auto_benchmark1\"\n",
    "USER_PROMPT = (\n",
    "    \"Rédige un texte d'environ 1000 mots sur l'IA, \"\n",
    "    \"en évoquant l'apprentissage machine, les grands modèles de langage, \"\n",
    "    \"et quelques perspectives d'évolution.\"\n",
    ")\n",
    "\n",
    "run_benchmark(label_run, USER_PROMPT, repeats=1)\n",
    "\n",
    "logger.info(f\"\\n=== Résultats de ce premier benchmark (label='{label_run}') ===\")\n",
    "for r in all_results:\n",
    "    if r[\"label\"] == label_run:\n",
    "        logger.info(\n",
    "            f\"{r['endpoint']} -> {r['success_count']}/{r['repeats']} ok, \"\n",
    "            f\"avg time={r['avg_time_s']:.2f}s, speed={r['tokens_per_sec']:.2f} tok/s\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Relancer le benchmark après mise à jour des containers\n",
    "\n",
    "Pour passer de Oobabooga à vLLM (ou inversement), \n",
    "il faut arrêter le premier groupe de containers \n",
    "et démarrer le second (avec `docker-compose.*.yml`).\n",
    "\n",
    "Ensuite, on ré-exécute ce notebook (ou au moins les cellules de configuration + benchmark).\n",
    "\n",
    "Ici, on propose un prompt qui demande si tu veux relancer un benchmark \n",
    "avec un nouveau label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "do_rerun = input(\"Voulez-vous exécuter un nouveau benchmark ? (y/n) \").strip().lower()\n",
    "\n",
    "if do_rerun == \"y\":\n",
    "    rename_label = input(\"Entrez un label pour le 1er benchmark (actuellement 'auto_benchmark1'): \").strip()\n",
    "    for r in all_results:\n",
    "        if r[\"label\"] == \"auto_benchmark1\":\n",
    "            r[\"label\"] = rename_label\n",
    "    second_label = input(\"Entrez un label pour le 2e benchmark : \").strip()\n",
    "    run_benchmark(second_label, USER_PROMPT, repeats=N_REPEATS)\n",
    "\n",
    "logger.info(\"\\n=== Récapitulatif complet de tous les benchmarks ===\")\n",
    "for r in all_results:\n",
    "    logger.info(\n",
    "        f\"{r['label']} | {r['endpoint']} -> {r['success_count']}/{r['repeats']} ok, \"\n",
    "        f\"time={r['avg_time_s']:.2f}s, speed={r['tokens_per_sec']:.2f} tok/s\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Test de traitement parallèle (batching)\n",
    "\n",
    "Dans cette cellule, nous allons :\n",
    "- Définir un nombre de requêtes à envoyer en parallèle (`N_PARALLEL`).\n",
    "- Pour chaque endpoint, lancer ces requêtes en **concurrence**.\n",
    "- Mesurer le temps total écoulé et le nombre total de tokens.\n",
    "- Calculer la **vitesse globale** de traitement (tokens / seconde) lorsque plusieurs requêtes arrivent simultanément.\n",
    "\n",
    "**vLLM** est réputé supporter le batching token-level et donc bénéficier d'une meilleure latence moyenne et d'un meilleur débit lorsqu'il y a plusieurs requêtes en parallèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:10:46 [INFO] Local Llama - ==== Lancement du test parallèle: 25 requêtes simultanées par endpoint ====\u001b[0m\n",
      "\u001b[92m18:10:46 [INFO] Local Llama - === Parallel test sur 'OpenAI' ===\u001b[0m\n",
      "\u001b[92m18:10:52 [INFO] Local Llama -   -> 25/25 appels OK\u001b[0m\n",
      "\u001b[92m18:10:52 [INFO] Local Llama -   -> Durée totale (concurrente): 6.35 s\u001b[0m\n",
      "\u001b[92m18:10:52 [INFO] Local Llama -   -> Tokens cumulés: 5900\u001b[0m\n",
      "\u001b[92m18:10:52 [INFO] Local Llama -   -> Vitesse globale: 929.50 tok/s\u001b[0m\n",
      "\u001b[92m18:10:52 [INFO] Local Llama - === Parallel test sur 'Local Model - Micro' ===\u001b[0m\n",
      "\u001b[92m18:10:57 [INFO] Local Llama -   -> 25/25 appels OK\u001b[0m\n",
      "\u001b[92m18:10:57 [INFO] Local Llama -   -> Durée totale (concurrente): 4.86 s\u001b[0m\n",
      "\u001b[92m18:10:57 [INFO] Local Llama -   -> Tokens cumulés: 6575\u001b[0m\n",
      "\u001b[92m18:10:57 [INFO] Local Llama -   -> Vitesse globale: 1351.52 tok/s\u001b[0m\n",
      "\u001b[92m18:10:57 [INFO] Local Llama - === Parallel test sur 'Local Model - Mini' ===\u001b[0m\n",
      "\u001b[92m18:11:02 [INFO] Local Llama -   -> 25/25 appels OK\u001b[0m\n",
      "\u001b[92m18:11:02 [INFO] Local Llama -   -> Durée totale (concurrente): 5.35 s\u001b[0m\n",
      "\u001b[92m18:11:02 [INFO] Local Llama -   -> Tokens cumulés: 6531\u001b[0m\n",
      "\u001b[92m18:11:02 [INFO] Local Llama -   -> Vitesse globale: 1220.33 tok/s\u001b[0m\n",
      "\u001b[92m18:11:02 [INFO] Local Llama - === Parallel test sur 'Local Model - Medium' ===\u001b[0m\n",
      "\u001b[92m18:11:45 [INFO] Local Llama -   -> 25/25 appels OK\u001b[0m\n",
      "\u001b[92m18:11:45 [INFO] Local Llama -   -> Durée totale (concurrente): 42.22 s\u001b[0m\n",
      "\u001b[92m18:11:45 [INFO] Local Llama -   -> Tokens cumulés: 5975\u001b[0m\n",
      "\u001b[92m18:11:45 [INFO] Local Llama -   -> Vitesse globale: 141.53 tok/s\u001b[0m\n",
      "\u001b[92m18:11:45 [INFO] Local Llama - === Parallel test sur 'Local Model - Large' ===\u001b[0m\n",
      "\u001b[92m18:12:00 [INFO] Local Llama -   -> 25/25 appels OK\u001b[0m\n",
      "\u001b[92m18:12:00 [INFO] Local Llama -   -> Durée totale (concurrente): 15.08 s\u001b[0m\n",
      "\u001b[92m18:12:00 [INFO] Local Llama -   -> Tokens cumulés: 6575\u001b[0m\n",
      "\u001b[92m18:12:00 [INFO] Local Llama -   -> Vitesse globale: 436.06 tok/s\u001b[0m\n",
      "\u001b[92m18:12:00 [INFO] Local Llama - \n",
      "=== Récapitulatif du test parallèle ===\u001b[0m\n",
      "\u001b[92m18:12:00 [INFO] Local Llama - OpenAI: 25/25 ok, total_time=6.35s, speed=929.50 tok/s\u001b[0m\n",
      "\u001b[92m18:12:00 [INFO] Local Llama - Local Model - Micro: 25/25 ok, total_time=4.86s, speed=1351.52 tok/s\u001b[0m\n",
      "\u001b[92m18:12:00 [INFO] Local Llama - Local Model - Mini: 25/25 ok, total_time=5.35s, speed=1220.33 tok/s\u001b[0m\n",
      "\u001b[92m18:12:00 [INFO] Local Llama - Local Model - Medium: 25/25 ok, total_time=42.22s, speed=141.53 tok/s\u001b[0m\n",
      "\u001b[92m18:12:00 [INFO] Local Llama - Local Model - Large: 25/25 ok, total_time=15.08s, speed=436.06 tok/s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "N_PARALLEL = 25\n",
    "PARALLEL_PROMPT = (\n",
    "    \"Bonjour, ceci est un test de requêtes parallèles. \"\n",
    "    \"Peux-tu me donner quelques idées créatives pour un week-end ?\"\n",
    ")\n",
    "\n",
    "async def async_chat_completion(api_base: str, api_key: str, model: str, prompt: str):\n",
    "    url = f\"{api_base}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 200\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        start_t = time.time()\n",
    "        try:\n",
    "            async with session.post(url, headers=headers, json=payload, timeout=60) as resp:\n",
    "                elapsed = time.time() - start_t\n",
    "                if resp.status == 200:\n",
    "                    data = await resp.json()\n",
    "                    tokens = None\n",
    "                    if \"usage\" in data and data[\"usage\"].get(\"total_tokens\"):\n",
    "                        tokens = data[\"usage\"][\"total_tokens\"]\n",
    "                    return (elapsed, tokens)\n",
    "                else:\n",
    "                    return (None, None)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[!] Exception asynchrone: {e}\")\n",
    "            return (None, None)\n",
    "\n",
    "async def run_parallel_test(endpoint, n_parallel, prompt):\n",
    "    api_base = endpoint[\"api_base\"]\n",
    "    api_key  = endpoint[\"api_key\"]\n",
    "    model    = endpoint.get(\"model\", None)\n",
    "\n",
    "    tasks = []\n",
    "    for _ in range(n_parallel):\n",
    "        tasks.append(asyncio.create_task(async_chat_completion(api_base, api_key, model, prompt)))\n",
    "\n",
    "    start_time = time.time()\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    nb_ok = 0\n",
    "    sum_tokens = 0\n",
    "    for (elapsed, tokens) in results:\n",
    "        if elapsed is not None and tokens is not None:\n",
    "            nb_ok += 1\n",
    "            sum_tokens += tokens\n",
    "\n",
    "    return {\n",
    "        \"endpoint\": endpoint[\"name\"],\n",
    "        \"n_req\": n_parallel,\n",
    "        \"n_ok\": nb_ok,\n",
    "        \"total_time_s\": total_time,\n",
    "        \"sum_tokens\": sum_tokens\n",
    "    }\n",
    "\n",
    "async def parallel_benchmark(endpoints_list, n_parallel, prompt, use_random_prefix=True):\n",
    "    summary = []\n",
    "    logger.info(f\"==== Lancement du test parallèle: {n_parallel} requêtes simultanées par endpoint ====\")\n",
    "    \n",
    "    for ep in endpoints_list:\n",
    "        logger.info(f\"=== Parallel test sur '{ep['name']}' ===\")\n",
    "        \n",
    "        if use_random_prefix:\n",
    "            prefix = ''.join(random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZ', k=5))\n",
    "            modified_prompt = f\"{prefix} {prompt}\"\n",
    "        else:\n",
    "            modified_prompt = prompt\n",
    "        \n",
    "        res = await run_parallel_test(ep, n_parallel, modified_prompt)\n",
    "\n",
    "        nb_ok = res[\"n_ok\"]\n",
    "        total_time = res[\"total_time_s\"]\n",
    "        sum_tokens = res[\"sum_tokens\"]\n",
    "        speed = sum_tokens / total_time if total_time > 0 else 0\n",
    "\n",
    "        logger.info(f\"  -> {nb_ok}/{n_parallel} appels OK\")\n",
    "        logger.info(f\"  -> Durée totale (concurrente): {total_time:.2f} s\")\n",
    "        logger.info(f\"  -> Tokens cumulés: {sum_tokens}\")\n",
    "        logger.info(f\"  -> Vitesse globale: {speed:.2f} tok/s\")\n",
    "\n",
    "        summary.append(res)\n",
    "\n",
    "    logger.info(\"\\n=== Récapitulatif du test parallèle ===\")\n",
    "    for s in summary:\n",
    "        speed = 0.0\n",
    "        if s[\"total_time_s\"] > 0:\n",
    "            speed = s[\"sum_tokens\"] / s[\"total_time_s\"]\n",
    "        logger.info(\n",
    "            f\"{s['endpoint']}: {s['n_ok']}/{s['n_req']} ok, \"\n",
    "            f\"total_time={s['total_time_s']:.2f}s, speed={speed:.2f} tok/s\"\n",
    "        )\n",
    "\n",
    "import nest_asyncio\n",
    "import random\n",
    "nest_asyncio.apply()\n",
    "\n",
    "await parallel_benchmark(endpoints, N_PARALLEL, PARALLEL_PROMPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Test de parallélisme global (MAJ) : mesures de débits individuels et ordre d’exécution aléatoire\n",
    "\n",
    "Ici, nous souhaitons :\n",
    "1. **Mesurer** le débit (tokens/s) **individuellement** pour chaque endpoint.\n",
    "2. **Lancer** toutes les requêtes (pour tous les endpoints) **en ordre aléatoire**, sans rajouter de délais artificiels.\n",
    "\n",
    "**Principe** :\n",
    "- On construit d’abord **la liste** complète des appels (ex. `N_PARALLEL_GLOBAL` requêtes pour chaque endpoint).\n",
    "- On associe à chaque appel l’endpoint correspondant, puis on **randomise** l’ordre de cette liste.\n",
    "- On déclenche **en simultané** l’ensemble des requêtes (via `asyncio.gather`).\n",
    "- Après exécution, on calcule :\n",
    "  - **Durée totale** (début → fin) pour l’ensemble des requêtes,\n",
    "  - **Résultats individuels** (tokens cumulés par endpoint, nombre de requêtes OK, etc.),\n",
    "  - **Débit** de chaque endpoint : (somme des tokens pour cet endpoint) / (durée totale),\n",
    "  - **Débit global** : (somme de tous les tokens) / (durée totale).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:29:48 [INFO] Local Llama - === Lancement du test de parallélisme global sur 5 endpoints ===\u001b[0m\n",
      "\u001b[92m10:29:48 [INFO] Local Llama -    -> 25 requêtes par endpoint (ordre aléatoire).\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama - \u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama - === Résultats du test global (tous endpoints en même temps) ===\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -   -> Nombre total de requêtes : 125\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -   -> Nombre de requêtes OK   : 125\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -   -> Fenêtre de temps (global) : 35.15 s\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -   -> Cumul de tokens (global)  : 29476\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -   -> Débit global (tous endpoints) : 838.57 tok/s\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama - \n",
      "=== Détails par endpoint ===\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama - - OpenAI : 25/25 OK, tokens=5273\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -     => Fenêtre concurrency = 5.30s\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -     => Débit effectif ~ 994.24 tok/s\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama - - Local Model - Micro : 25/25 OK, tokens=6201\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -     => Fenêtre concurrency = 9.84s\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -     => Débit effectif ~ 630.24 tok/s\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama - - Local Model - Mini : 25/25 OK, tokens=6198\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -     => Fenêtre concurrency = 9.99s\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -     => Débit effectif ~ 620.25 tok/s\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama - - Local Model - Medium : 25/25 OK, tokens=5601\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -     => Fenêtre concurrency = 35.15s\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -     => Débit effectif ~ 159.34 tok/s\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama - - Local Model - Large : 25/25 OK, tokens=6203\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -     => Fenêtre concurrency = 15.28s\u001b[0m\n",
      "\u001b[92m10:30:23 [INFO] Local Llama -     => Débit effectif ~ 405.91 tok/s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "\n",
    "N_PARALLEL_GLOBAL = 25  # Nombre de requêtes à envoyer par endpoint\n",
    "GLOBAL_PROMPT = (\n",
    "    \"Bonjour, ceci est un test de parallélisme global. \"\n",
    "    \"Peux-tu me détailler en 500 mots les avantages et inconvénients de travailler \"\n",
    "    \"avec plusieurs grands modèles (Llama, Qwen, GPT, etc.) en parallèle sur un même serveur ?\"\n",
    ")\n",
    "\n",
    "async def async_chat_completion(api_base: str, api_key: str, model: str, prompt: str):\n",
    "    \"\"\"\n",
    "    Effectue une requête unique (POST /chat/completions) en asynchrone.\n",
    "    Retourne un tuple (start_t, end_t, tokens, success).\n",
    "      - start_t  : l'instant du début effectif (time.time()) juste avant l'appel\n",
    "      - end_t    : l'instant de fin (time.time()) juste après la réception de la réponse\n",
    "      - tokens   : nombre de tokens dans la réponse (None si échec)\n",
    "      - success  : booléen (True si statut=200 et parse JSON OK)\n",
    "    \"\"\"\n",
    "    # On note l'instant de démarrage avant de créer la session et d'envoyer la requête\n",
    "    start_t = time.time()\n",
    "\n",
    "    url = f\"{api_base}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 150\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url, headers=headers, json=payload, timeout=60) as resp:\n",
    "                end_t = time.time()\n",
    "                if resp.status == 200:\n",
    "                    data = await resp.json()\n",
    "                    tokens = data.get(\"usage\", {}).get(\"total_tokens\", None)\n",
    "                    return (start_t, end_t, tokens, True)\n",
    "                else:\n",
    "                    return (start_t, end_t, None, False)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[!] Exception asynchrone: {e}\")\n",
    "            end_t = time.time()\n",
    "            return (start_t, end_t, None, False)\n",
    "\n",
    "async def run_full_parallel_on_all_endpoints(endpoints, n_parallel, prompt):\n",
    "    \"\"\"\n",
    "    Lance n_parallel requêtes pour chaque endpoint (donc total = n_parallel * len(endpoints)),\n",
    "    en un seul grand batch concurrent *et dans un ordre aléatoire*.\n",
    "\n",
    "    Retourne un dict contenant :\n",
    "\n",
    "    - global_min_start  : le plus petit start_t (sur toutes les requêtes, tous endpoints)\n",
    "    - global_max_end    : le plus grand end_t   (sur toutes les requêtes)\n",
    "    - n_req_total       : total de requêtes\n",
    "    - n_ok_global       : total de requêtes ayant répondu 200 OK\n",
    "    - sum_tokens_global : total de tokens (toutes requêtes OK)\n",
    "    - stats_endpoints   : dict par endpoint, contenant :\n",
    "         {\n",
    "           \"calls\": int,\n",
    "           \"ok\": int,\n",
    "           \"sum_tokens\": int,\n",
    "           \"min_start\": float,\n",
    "           \"max_end\": float\n",
    "         }\n",
    "      (on calcule ensuite un débit = sum_tokens / (max_end - min_start))\n",
    "    \"\"\"\n",
    "\n",
    "    # Prépare tous les appels (endpoint, prompt)\n",
    "    tasks_info = []\n",
    "    for ep in endpoints:\n",
    "        for _ in range(n_parallel):\n",
    "            # Optionnel : préfixe aléatoire pour \"casser\" un éventuel cache\n",
    "            prefix = \"\".join(random.choices(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", k=3))\n",
    "            modified_prompt = f\"{prefix} {prompt}\"\n",
    "            tasks_info.append((ep, modified_prompt))\n",
    "\n",
    "    # Mélange l’ordre des requêtes\n",
    "    random.shuffle(tasks_info)\n",
    "\n",
    "    # Transforme chaque (ep, prompt) en coroutine\n",
    "    coroutines = []\n",
    "    for (ep, pr) in tasks_info:\n",
    "        coroutines.append(async_chat_completion(ep[\"api_base\"], ep[\"api_key\"], ep[\"model\"], pr))\n",
    "\n",
    "    # Exécution en parallèle\n",
    "    results = await asyncio.gather(*coroutines)\n",
    "\n",
    "    # Rassemble toutes les stats\n",
    "    # On calcule le min de start_t et le max de end_t global\n",
    "    all_starts = []\n",
    "    all_ends = []\n",
    "\n",
    "    stats_per_endpoint = {}\n",
    "    for ep in endpoints:\n",
    "        stats_per_endpoint[ep[\"name\"]] = {\n",
    "            \"calls\": 0,\n",
    "            \"ok\": 0,\n",
    "            \"sum_tokens\": 0,\n",
    "            \"min_start\": float(\"inf\"),\n",
    "            \"max_end\": float(\"-inf\"),\n",
    "        }\n",
    "\n",
    "    global_ok = 0\n",
    "    global_tokens = 0\n",
    "\n",
    "    for i, (start_t, end_t, tokens, success) in enumerate(results):\n",
    "        ep_name = tasks_info[i][0][\"name\"]\n",
    "        ep_stats = stats_per_endpoint[ep_name]\n",
    "        ep_stats[\"calls\"] += 1\n",
    "\n",
    "        # On enrichit le min/max local à l'endpoint\n",
    "        if start_t < ep_stats[\"min_start\"]:\n",
    "            ep_stats[\"min_start\"] = start_t\n",
    "        if end_t > ep_stats[\"max_end\"]:\n",
    "            ep_stats[\"max_end\"] = end_t\n",
    "\n",
    "        # Idem pour le global\n",
    "        all_starts.append(start_t)\n",
    "        all_ends.append(end_t)\n",
    "\n",
    "        # On comptabilise tokens si success\n",
    "        if success and tokens is not None:\n",
    "            ep_stats[\"ok\"] += 1\n",
    "            ep_stats[\"sum_tokens\"] += tokens\n",
    "            global_ok += 1\n",
    "            global_tokens += tokens\n",
    "\n",
    "    # min/max global\n",
    "    global_min_start = min(all_starts) if all_starts else None\n",
    "    global_max_end = max(all_ends) if all_ends else None\n",
    "\n",
    "    summary = {\n",
    "        \"global_min_start\": global_min_start,\n",
    "        \"global_max_end\": global_max_end,\n",
    "        \"n_req_total\": len(results),\n",
    "        \"n_ok_global\": global_ok,\n",
    "        \"sum_tokens_global\": global_tokens,\n",
    "        \"stats_endpoints\": stats_per_endpoint\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "\n",
    "# --- Lancement effectif du test ---\n",
    "logger.info(f\"=== Lancement du test de parallélisme global sur {len(endpoints)} endpoints ===\")\n",
    "logger.info(f\"   -> {N_PARALLEL_GLOBAL} requêtes par endpoint (ordre aléatoire).\")\n",
    "\n",
    "summary_global = await run_full_parallel_on_all_endpoints(endpoints, N_PARALLEL_GLOBAL, GLOBAL_PROMPT)\n",
    "\n",
    "# Récapitulatif\n",
    "global_min_start = summary_global[\"global_min_start\"]\n",
    "global_max_end = summary_global[\"global_max_end\"]\n",
    "n_req_total = summary_global[\"n_req_total\"]\n",
    "n_ok_global = summary_global[\"n_ok_global\"]\n",
    "sum_tokens_global = summary_global[\"sum_tokens_global\"]\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"=== Résultats du test global (tous endpoints en même temps) ===\")\n",
    "logger.info(f\"  -> Nombre total de requêtes : {n_req_total}\")\n",
    "logger.info(f\"  -> Nombre de requêtes OK   : {n_ok_global}\")\n",
    "\n",
    "if global_min_start is not None and global_max_end is not None:\n",
    "    global_duration = global_max_end - global_min_start\n",
    "    logger.info(f\"  -> Fenêtre de temps (global) : {global_duration:.2f} s\")\n",
    "    logger.info(f\"  -> Cumul de tokens (global)  : {sum_tokens_global}\")\n",
    "    if global_duration > 0:\n",
    "        global_speed = sum_tokens_global / global_duration\n",
    "        logger.info(f\"  -> Débit global (tous endpoints) : {global_speed:.2f} tok/s\")\n",
    "\n",
    "logger.info(\"\\n=== Détails par endpoint ===\")\n",
    "for ep_name, ep_stats in summary_global[\"stats_endpoints\"].items():\n",
    "    calls = ep_stats[\"calls\"]\n",
    "    ok = ep_stats[\"ok\"]\n",
    "    sum_tks = ep_stats[\"sum_tokens\"]\n",
    "    ep_min_start = ep_stats[\"min_start\"]\n",
    "    ep_max_end   = ep_stats[\"max_end\"]\n",
    "\n",
    "    logger.info(f\"- {ep_name} : {ok}/{calls} OK, tokens={sum_tks}\")\n",
    "    if ok > 0 and ep_min_start < ep_max_end:  # au moins 1 requête\n",
    "        ep_concurrency_window = ep_max_end - ep_min_start\n",
    "        # 'ep_concurrency_window' = fenêtrage du 1er démarrage -> dernier aboutissement\n",
    "        speed_ep = sum_tks / ep_concurrency_window if ep_concurrency_window > 0 else 0\n",
    "        logger.info(f\"    => Fenêtre concurrency = {ep_concurrency_window:.2f}s\")\n",
    "        logger.info(f\"    => Débit effectif ~ {speed_ep:.2f} tok/s\")\n",
    "    else:\n",
    "        logger.info(\"    => Pas de requêtes OK ou pas de fenêtre exploitable.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
