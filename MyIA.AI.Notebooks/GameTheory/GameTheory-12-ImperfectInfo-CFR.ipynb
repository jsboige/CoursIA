{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GameTheory-12 : Jeux a Information Imparfaite et CFR\n",
    "\n",
    "## Counterfactual Regret Minimization\n",
    "\n",
    "Ce notebook couvre les algorithmes de resolution pour les jeux a information imparfaite, notamment la famille CFR (Counterfactual Regret Minimization) qui a revolutionne la resolution du poker.\n",
    "\n",
    "**Objectifs pedagogiques** :\n",
    "- Comprendre la difference entre information parfaite et imparfaite\n",
    "- Maitriser le concept de regret et regret contrefactuel\n",
    "- Implementer CFR vanilla et ses variantes\n",
    "- Analyser la convergence vers l'equilibre de Nash\n",
    "\n",
    "**Prerequis** : Notebooks 1-11 (notamment 7-ExtensiveForm)\n",
    "\n",
    "**Duree estimee** : 70 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Information Imparfaite : Rappels et Formalisation\n",
    "\n",
    "### 1.1 Information parfaite vs imparfaite\n",
    "\n",
    "| Aspect | Parfaite | Imparfaite |\n",
    "|--------|----------|------------|\n",
    "| **Definition** | Chaque joueur connait l'historique complet | Certaines actions sont cachees |\n",
    "| **Information sets** | Singletons | Peuvent contenir plusieurs noeuds |\n",
    "| **Exemples** | Echecs, Go, Morpion | Poker, Bridge, Bataille navale |\n",
    "| **Resolution** | Backward induction | CFR, LP sequence-form |\n",
    "\n",
    "### 1.2 Strategies comportementales vs mixtes\n",
    "\n",
    "Dans les jeux extensifs a information imparfaite :\n",
    "- **Strategie comportementale** : probabilite sur les actions a chaque information set\n",
    "- **Strategie mixte** : probabilite sur les strategies pures\n",
    "\n",
    "**Theoreme de Kuhn** : Dans les jeux a rappel parfait, strategies comportementales et mixtes sont equivalentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dependances\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['numpy', 'matplotlib', 'tqdm']\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "\n",
    "# Tentative d'installation OpenSpiel (peut echouer sur certains systemes)\n",
    "try:\n",
    "    import pyspiel\n",
    "    OPENSPIEL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'open_spiel'])\n",
    "        import pyspiel\n",
    "        OPENSPIEL_AVAILABLE = True\n",
    "    except:\n",
    "        OPENSPIEL_AVAILABLE = False\n",
    "        print(\"OpenSpiel non disponible - utilisation des implementations locales\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Imports reussis\")\n",
    "print(f\"OpenSpiel disponible: {OPENSPIEL_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kuhn Poker : Notre Jeu de Reference\n",
    "\n",
    "Le **Kuhn Poker** est le plus petit jeu de poker interessant :\n",
    "- 3 cartes : Jack (J), Queen (Q), King (K)\n",
    "- 2 joueurs, chacun recoit 1 carte\n",
    "- Mise initiale de 1 (ante)\n",
    "- Actions : Check/Bet pour J1, puis Fold/Call pour J2\n",
    "\n",
    "### Arbre de jeu simplifie\n",
    "\n",
    "```\n",
    "        [Chance: distribue cartes]\n",
    "              |\n",
    "         [J1: Check/Bet]\n",
    "        /              \\\n",
    "   Check               Bet\n",
    "      |                  |\n",
    "  [J2: Check/Bet]    [J2: Fold/Call]\n",
    "   /      \\           /      \\\n",
    "Check    Bet       Fold     Call\n",
    "  |        |         |        |\n",
    "Show    [J1]       J1wins   Show\n",
    "```\n",
    "\n",
    "**Equilibre de Nash** (connu analytiquement) :\n",
    "- J1 avec J : bet avec prob 1/3, check sinon\n",
    "- J1 avec Q : toujours check\n",
    "- J1 avec K : toujours bet\n",
    "- J2 : call avec K toujours, call avec Q face a un bet avec prob 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuhnPoker:\n",
    "    \"\"\"\n",
    "    Implementation du Kuhn Poker pour CFR.\n",
    "    \n",
    "    Cartes: 0=Jack, 1=Queen, 2=King\n",
    "    Actions: 0=Pass/Fold, 1=Bet/Call\n",
    "    \"\"\"\n",
    "    \n",
    "    PASS = 0\n",
    "    BET = 1\n",
    "    NUM_ACTIONS = 2\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cards = [0, 1, 2]  # J, Q, K\n",
    "        \n",
    "    def is_terminal(self, history: str) -> bool:\n",
    "        \"\"\"Verifie si l'historique correspond a un etat terminal.\"\"\"\n",
    "        return history in ['pp', 'pbp', 'pbb', 'bp', 'bb']\n",
    "    \n",
    "    def get_payoff(self, history: str, cards: List[int]) -> float:\n",
    "        \"\"\"\n",
    "        Retourne le payoff du joueur 1.\n",
    "        \n",
    "        cards[0] = carte J1, cards[1] = carte J2\n",
    "        \"\"\"\n",
    "        if history == 'pp':  # check-check: showdown\n",
    "            return 1 if cards[0] > cards[1] else -1\n",
    "        elif history == 'pbp':  # check-bet-fold: J2 gagne l'ante\n",
    "            return -1\n",
    "        elif history == 'pbb':  # check-bet-call: showdown pour pot=4\n",
    "            return 2 if cards[0] > cards[1] else -2\n",
    "        elif history == 'bp':  # bet-fold: J1 gagne l'ante\n",
    "            return 1\n",
    "        elif history == 'bb':  # bet-call: showdown pour pot=4\n",
    "            return 2 if cards[0] > cards[1] else -2\n",
    "        return 0\n",
    "    \n",
    "    def get_info_set(self, history: str, card: int) -> str:\n",
    "        \"\"\"Retourne l'information set (carte + historique visible).\"\"\"\n",
    "        card_str = ['J', 'Q', 'K'][card]\n",
    "        return card_str + history\n",
    "    \n",
    "    def get_current_player(self, history: str) -> int:\n",
    "        \"\"\"Retourne le joueur courant (0 ou 1).\"\"\"\n",
    "        return len(history) % 2\n",
    "    \n",
    "    def get_actions(self, history: str) -> List[int]:\n",
    "        \"\"\"Retourne les actions legales.\"\"\"\n",
    "        return [0, 1]  # Pass/Fold ou Bet/Call\n",
    "\n",
    "\n",
    "# Test\n",
    "kuhn = KuhnPoker()\n",
    "print(\"Test Kuhn Poker:\")\n",
    "print(f\"  'pp' terminal? {kuhn.is_terminal('pp')}\")\n",
    "print(f\"  'p' terminal? {kuhn.is_terminal('p')}\")\n",
    "print(f\"  Payoff 'bb' avec K vs J: {kuhn.get_payoff('bb', [2, 0])}\")\n",
    "print(f\"  Payoff 'bp' (fold): {kuhn.get_payoff('bp', [0, 2])}\")\n",
    "print(f\"  Info set J1 avec Q apres '': {kuhn.get_info_set('', 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regret et Regret Contrefactuel\n",
    "\n",
    "### 3.1 Regret classique (Hannan)\n",
    "\n",
    "Le **regret** pour une action $a$ apres $T$ tours est :\n",
    "\n",
    "$$R^T(a) = \\sum_{t=1}^{T} [u(a, s^t) - u(a^t, s^t)]$$\n",
    "\n",
    "ou $s^t$ est la strategie adverse au tour $t$ et $a^t$ l'action jouee.\n",
    "\n",
    "### 3.2 Regret Matching\n",
    "\n",
    "L'algorithme de **Regret Matching** (Hart & Mas-Colell, 2000) :\n",
    "1. Calculer le regret cumule $R^T(a)$ pour chaque action\n",
    "2. Jouer proportionnellement aux regrets positifs :\n",
    "\n",
    "$$\\sigma^{T+1}(a) = \\frac{\\max(R^T(a), 0)}{\\sum_{a'} \\max(R^T(a'), 0)}$$\n",
    "\n",
    "**Theoreme** : Le regret moyen converge vers 0, i.e. $\\frac{R^T}{T} \\to 0$.\n",
    "\n",
    "### 3.3 Regret Contrefactuel (CFR)\n",
    "\n",
    "Pour les jeux extensifs, on utilise le **regret contrefactuel** :\n",
    "\n",
    "$$r_i(I, a) = v_i(\\sigma_{I \\to a}) - v_i(\\sigma)$$\n",
    "\n",
    "ou $\\sigma_{I \\to a}$ est la strategie ou on joue toujours $a$ a l'infoset $I$.\n",
    "\n",
    "Plus precisement :\n",
    "$$r_i(I, a) = \\sum_{h \\in I} \\pi_{-i}^\\sigma(h) [v_i(\\sigma, h \\cdot a) - v_i(\\sigma, h)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegretMatcher:\n",
    "    \"\"\"\n",
    "    Implementation du Regret Matching pour un agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions: int):\n",
    "        self.num_actions = num_actions\n",
    "        self.regret_sum = np.zeros(num_actions)\n",
    "        self.strategy_sum = np.zeros(num_actions)\n",
    "        \n",
    "    def get_strategy(self) -> np.ndarray:\n",
    "        \"\"\"Calcule la strategie courante via regret matching.\"\"\"\n",
    "        positive_regrets = np.maximum(self.regret_sum, 0)\n",
    "        normalizing_sum = positive_regrets.sum()\n",
    "        \n",
    "        if normalizing_sum > 0:\n",
    "            return positive_regrets / normalizing_sum\n",
    "        else:\n",
    "            # Strategie uniforme si pas de regret positif\n",
    "            return np.ones(self.num_actions) / self.num_actions\n",
    "    \n",
    "    def get_average_strategy(self) -> np.ndarray:\n",
    "        \"\"\"Retourne la strategie moyenne (converge vers Nash).\"\"\"\n",
    "        normalizing_sum = self.strategy_sum.sum()\n",
    "        if normalizing_sum > 0:\n",
    "            return self.strategy_sum / normalizing_sum\n",
    "        else:\n",
    "            return np.ones(self.num_actions) / self.num_actions\n",
    "    \n",
    "    def update(self, action_utilities: np.ndarray, reach_prob: float = 1.0):\n",
    "        \"\"\"\n",
    "        Met a jour les regrets apres avoir observe les utilites.\n",
    "        \n",
    "        action_utilities[a] = utilite de l'action a\n",
    "        reach_prob = probabilite d'atteindre cet etat\n",
    "        \"\"\"\n",
    "        strategy = self.get_strategy()\n",
    "        expected_utility = (strategy * action_utilities).sum()\n",
    "        \n",
    "        # Regret = utilite action - utilite esperee\n",
    "        regrets = action_utilities - expected_utility\n",
    "        self.regret_sum += regrets\n",
    "        \n",
    "        # Accumulation de la strategie ponderee\n",
    "        self.strategy_sum += reach_prob * strategy\n",
    "\n",
    "\n",
    "# Demonstration : Rock-Paper-Scissors\n",
    "print(\"Demo Regret Matching sur Pierre-Feuille-Ciseaux\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "rps_matcher = RegretMatcher(3)  # 0=Rock, 1=Paper, 2=Scissors\n",
    "\n",
    "# Simuler contre un adversaire qui joue toujours Rock\n",
    "for t in range(1000):\n",
    "    opponent_action = 0  # Rock\n",
    "    # Utilites: Rock=0, Paper=1, Scissors=-1 (contre Rock)\n",
    "    utilities = np.array([0.0, 1.0, -1.0])\n",
    "    rps_matcher.update(utilities)\n",
    "\n",
    "avg_strategy = rps_matcher.get_average_strategy()\n",
    "print(f\"Strategie moyenne apres 1000 iterations:\")\n",
    "print(f\"  Rock: {avg_strategy[0]:.3f}, Paper: {avg_strategy[1]:.3f}, Scissors: {avg_strategy[2]:.3f}\")\n",
    "print(f\"  -> Converge vers Paper (meilleure reponse a Rock)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CFR Vanilla : Implementation Complete\n",
    "\n",
    "L'algorithme CFR (Zinkevich et al., 2007) applique le regret matching a chaque information set d'un jeu extensif.\n",
    "\n",
    "### Algorithme\n",
    "\n",
    "```\n",
    "function CFR(history h, reach_probs pi):\n",
    "    if h est terminal:\n",
    "        return payoff(h)\n",
    "    \n",
    "    player = current_player(h)\n",
    "    info_set = get_info_set(h)\n",
    "    strategy = regret_match(info_set)\n",
    "    \n",
    "    action_utilities = []\n",
    "    for action a in actions(h):\n",
    "        new_pi = pi.copy()\n",
    "        new_pi[player] *= strategy[a]\n",
    "        utility = CFR(h + a, new_pi)\n",
    "        action_utilities.append(utility)\n",
    "    \n",
    "    # Mise a jour des regrets contrefactuels\n",
    "    cf_reach = product(pi[-player])  # reach prob sans le joueur courant\n",
    "    for a, u in enumerate(action_utilities):\n",
    "        regret[info_set][a] += cf_reach * (u[player] - node_utility[player])\n",
    "    \n",
    "    return weighted_utility\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFRSolver:\n",
    "    \"\"\"\n",
    "    Solveur CFR vanilla pour Kuhn Poker.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.game = KuhnPoker()\n",
    "        self.regret_sum: Dict[str, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(self.game.NUM_ACTIONS)\n",
    "        )\n",
    "        self.strategy_sum: Dict[str, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(self.game.NUM_ACTIONS)\n",
    "        )\n",
    "        self.iterations = 0\n",
    "        \n",
    "    def get_strategy(self, info_set: str) -> np.ndarray:\n",
    "        \"\"\"Calcule la strategie courante pour un information set.\"\"\"\n",
    "        regrets = self.regret_sum[info_set]\n",
    "        positive_regrets = np.maximum(regrets, 0)\n",
    "        normalizing_sum = positive_regrets.sum()\n",
    "        \n",
    "        if normalizing_sum > 0:\n",
    "            return positive_regrets / normalizing_sum\n",
    "        else:\n",
    "            return np.ones(self.game.NUM_ACTIONS) / self.game.NUM_ACTIONS\n",
    "    \n",
    "    def get_average_strategy(self, info_set: str) -> np.ndarray:\n",
    "        \"\"\"Retourne la strategie moyenne pour un information set.\"\"\"\n",
    "        strategy_sum = self.strategy_sum[info_set]\n",
    "        normalizing_sum = strategy_sum.sum()\n",
    "        \n",
    "        if normalizing_sum > 0:\n",
    "            return strategy_sum / normalizing_sum\n",
    "        else:\n",
    "            return np.ones(self.game.NUM_ACTIONS) / self.game.NUM_ACTIONS\n",
    "    \n",
    "    def cfr(self, history: str, cards: List[int], \n",
    "            reach_probs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Recursion CFR principale.\n",
    "        \n",
    "        Retourne les utilites esperees pour les deux joueurs.\n",
    "        \"\"\"\n",
    "        # Cas terminal\n",
    "        if self.game.is_terminal(history):\n",
    "            payoff = self.game.get_payoff(history, cards)\n",
    "            return np.array([payoff, -payoff])\n",
    "        \n",
    "        player = self.game.get_current_player(history)\n",
    "        info_set = self.game.get_info_set(history, cards[player])\n",
    "        strategy = self.get_strategy(info_set)\n",
    "        \n",
    "        # Calculer les utilites pour chaque action\n",
    "        action_utilities = np.zeros((self.game.NUM_ACTIONS, 2))\n",
    "        node_utility = np.zeros(2)\n",
    "        \n",
    "        for action in range(self.game.NUM_ACTIONS):\n",
    "            action_char = 'p' if action == 0 else 'b'\n",
    "            new_history = history + action_char\n",
    "            \n",
    "            # Mettre a jour les reach probabilities\n",
    "            new_reach = reach_probs.copy()\n",
    "            new_reach[player] *= strategy[action]\n",
    "            \n",
    "            # Recursion\n",
    "            action_utilities[action] = self.cfr(new_history, cards, new_reach)\n",
    "            node_utility += strategy[action] * action_utilities[action]\n",
    "        \n",
    "        # Mise a jour des regrets et strategies\n",
    "        opponent = 1 - player\n",
    "        cf_reach = reach_probs[opponent]  # Counterfactual reach\n",
    "        \n",
    "        for action in range(self.game.NUM_ACTIONS):\n",
    "            regret = action_utilities[action][player] - node_utility[player]\n",
    "            self.regret_sum[info_set][action] += cf_reach * regret\n",
    "        \n",
    "        # Accumuler la strategie\n",
    "        self.strategy_sum[info_set] += reach_probs[player] * strategy\n",
    "        \n",
    "        return node_utility\n",
    "    \n",
    "    def train(self, iterations: int, verbose: bool = True) -> List[float]:\n",
    "        \"\"\"\n",
    "        Entraine le solveur CFR.\n",
    "        \n",
    "        Retourne l'historique des utilites esperees.\n",
    "        \"\"\"\n",
    "        utilities = []\n",
    "        cards_permutations = [\n",
    "            [0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]\n",
    "        ]\n",
    "        \n",
    "        iterator = tqdm(range(iterations)) if verbose else range(iterations)\n",
    "        \n",
    "        for i in iterator:\n",
    "            total_utility = 0.0\n",
    "            \n",
    "            for cards in cards_permutations:\n",
    "                utility = self.cfr('', cards, np.ones(2))\n",
    "                total_utility += utility[0]\n",
    "            \n",
    "            # Utilite moyenne sur toutes les distributions de cartes\n",
    "            avg_utility = total_utility / len(cards_permutations)\n",
    "            utilities.append(avg_utility)\n",
    "            self.iterations += 1\n",
    "        \n",
    "        return utilities\n",
    "    \n",
    "    def get_exploitability(self) -> float:\n",
    "        \"\"\"\n",
    "        Calcule l'exploitabilite de la strategie courante.\n",
    "        \n",
    "        Plus la valeur est basse, plus on est proche de Nash.\n",
    "        \"\"\"\n",
    "        # Simplification: on retourne la variance des regrets\n",
    "        total_positive_regret = 0.0\n",
    "        for info_set in self.regret_sum:\n",
    "            total_positive_regret += np.maximum(self.regret_sum[info_set], 0).sum()\n",
    "        return total_positive_regret / max(1, self.iterations)\n",
    "\n",
    "\n",
    "print(\"CFRSolver defini avec succes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement CFR\n",
    "print(\"Entrainement CFR sur Kuhn Poker\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cfr_solver = CFRSolver()\n",
    "utilities = cfr_solver.train(iterations=10000, verbose=True)\n",
    "\n",
    "print(f\"\\nIterations: {cfr_solver.iterations}\")\n",
    "print(f\"Utilite finale J1: {utilities[-1]:.4f}\")\n",
    "print(f\"Exploitabilite: {cfr_solver.get_exploitability():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des strategies apprises\n",
    "print(\"\\nStrategies moyennes apprises:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Equilibre theorique pour comparaison\n",
    "theoretical = {\n",
    "    'J': (1/3, 2/3),   # Bet 1/3, Pass 2/3\n",
    "    'Q': (0.0, 1.0),   # Toujours Pass\n",
    "    'K': (1.0, 0.0),   # Toujours Bet\n",
    "    'Jp': (1/3, 2/3),  # Apres check adverse, bet 1/3 (bluff)\n",
    "    'Qp': (0.0, 1.0),  # Apres check, toujours pass\n",
    "    'Kp': (1.0, 0.0),  # Apres check, toujours bet (value)\n",
    "    'Jpb': (0.0, 1.0), # Face a bet, fold avec J\n",
    "    'Qpb': (1/3, 2/3), # Face a bet, call 1/3 avec Q\n",
    "    'Kpb': (1.0, 0.0), # Face a bet, toujours call avec K\n",
    "    'Jb': (0.0, 1.0),  # Face a bet initial, fold avec J\n",
    "    'Qb': (1/3, 2/3),  # Face a bet, call 1/3\n",
    "    'Kb': (1.0, 0.0),  # Face a bet, call avec K\n",
    "}\n",
    "\n",
    "info_sets_to_show = ['J', 'Q', 'K', 'Jb', 'Qb', 'Kb']\n",
    "\n",
    "print(f\"{'Info Set':<10} {'Appris (b/p)':<20} {'Theorique (b/p)':<20}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for info_set in info_sets_to_show:\n",
    "    if info_set in cfr_solver.strategy_sum:\n",
    "        learned = cfr_solver.get_average_strategy(info_set)\n",
    "        theo = theoretical.get(info_set, (0.5, 0.5))\n",
    "        # Note: action 0=pass, action 1=bet\n",
    "        print(f\"{info_set:<10} ({learned[1]:.3f}/{learned[0]:.3f})         ({theo[0]:.3f}/{theo[1]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Utilite esperee\n",
    "ax1 = axes[0]\n",
    "ax1.plot(utilities, alpha=0.3, label='Utilite par iteration')\n",
    "# Moyenne mobile\n",
    "window = 100\n",
    "if len(utilities) > window:\n",
    "    moving_avg = np.convolve(utilities, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(utilities)), moving_avg, 'r-', linewidth=2,\n",
    "             label=f'Moyenne mobile ({window})')\n",
    "\n",
    "# Valeur theorique du jeu pour J1: -1/18\n",
    "ax1.axhline(y=-1/18, color='g', linestyle='--', label=f'Nash: {-1/18:.4f}')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Utilite esperee J1')\n",
    "ax1.set_title('Convergence de la valeur du jeu')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Evolution des strategies pour J1 avec Jack (doit converger vers bet=1/3)\n",
    "ax2 = axes[1]\n",
    "# Recalculer l'historique des strategies (simplification: on montre l'etat final)\n",
    "info_sets = ['J', 'Q', 'K']\n",
    "colors = ['red', 'blue', 'green']\n",
    "theo_values = [1/3, 0.0, 1.0]  # Prob de bet\n",
    "\n",
    "for i, (info_set, color, theo) in enumerate(zip(info_sets, colors, theo_values)):\n",
    "    if info_set in cfr_solver.strategy_sum:\n",
    "        learned_bet = cfr_solver.get_average_strategy(info_set)[1]\n",
    "        ax2.bar(i, learned_bet, color=color, alpha=0.7, label=f'{info_set} appris')\n",
    "        ax2.scatter(i, theo, color='black', s=100, marker='*', zorder=5)\n",
    "\n",
    "ax2.set_xticks(range(len(info_sets)))\n",
    "ax2.set_xticklabels(info_sets)\n",
    "ax2.set_ylabel('Probabilite de Bet')\n",
    "ax2.set_title('Strategies J1 (etoile = Nash theorique)')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cfr_convergence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure sauvegardee: cfr_convergence.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Variantes de CFR\n",
    "\n",
    "### 5.1 CFR+ (Tammelin, 2014)\n",
    "\n",
    "Amelioration de CFR avec :\n",
    "- **Regrets non-negatifs** : $R^{T+1} = \\max(R^T + r^t, 0)$\n",
    "- Convergence plus rapide en pratique\n",
    "\n",
    "### 5.2 Monte Carlo CFR (MCCFR)\n",
    "\n",
    "Au lieu de parcourir tout l'arbre, on echantillonne :\n",
    "- **Outcome Sampling** : echantillonne une trajectoire complete\n",
    "- **External Sampling** : echantillonne les actions des adversaires\n",
    "- **Chance Sampling** : echantillonne les noeuds de chance\n",
    "\n",
    "### 5.3 Deep CFR (Brown et al., 2019)\n",
    "\n",
    "Utilise des reseaux de neurones pour :\n",
    "- Approximer les regrets cumules\n",
    "- Approximer la strategie moyenne\n",
    "- Permet de passer a l'echelle (Texas Hold'em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFRPlusSolver(CFRSolver):\n",
    "    \"\"\"\n",
    "    CFR+ : variante avec regrets toujours non-negatifs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def cfr(self, history: str, cards: List[int], \n",
    "            reach_probs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        CFR+ avec regrets floors a zero.\n",
    "        \"\"\"\n",
    "        if self.game.is_terminal(history):\n",
    "            payoff = self.game.get_payoff(history, cards)\n",
    "            return np.array([payoff, -payoff])\n",
    "        \n",
    "        player = self.game.get_current_player(history)\n",
    "        info_set = self.game.get_info_set(history, cards[player])\n",
    "        strategy = self.get_strategy(info_set)\n",
    "        \n",
    "        action_utilities = np.zeros((self.game.NUM_ACTIONS, 2))\n",
    "        node_utility = np.zeros(2)\n",
    "        \n",
    "        for action in range(self.game.NUM_ACTIONS):\n",
    "            action_char = 'p' if action == 0 else 'b'\n",
    "            new_history = history + action_char\n",
    "            new_reach = reach_probs.copy()\n",
    "            new_reach[player] *= strategy[action]\n",
    "            action_utilities[action] = self.cfr(new_history, cards, new_reach)\n",
    "            node_utility += strategy[action] * action_utilities[action]\n",
    "        \n",
    "        opponent = 1 - player\n",
    "        cf_reach = reach_probs[opponent]\n",
    "        \n",
    "        for action in range(self.game.NUM_ACTIONS):\n",
    "            regret = action_utilities[action][player] - node_utility[player]\n",
    "            # CFR+ : floor les regrets a zero\n",
    "            self.regret_sum[info_set][action] = max(\n",
    "                self.regret_sum[info_set][action] + cf_reach * regret, 0\n",
    "            )\n",
    "        \n",
    "        self.strategy_sum[info_set] += reach_probs[player] * strategy\n",
    "        return node_utility\n",
    "\n",
    "\n",
    "# Comparaison CFR vs CFR+\n",
    "print(\"Comparaison CFR vs CFR+\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cfr_vanilla = CFRSolver()\n",
    "cfr_plus = CFRPlusSolver()\n",
    "\n",
    "n_iterations = 5000\n",
    "\n",
    "print(\"\\nEntrainement CFR vanilla...\")\n",
    "utilities_vanilla = cfr_vanilla.train(n_iterations, verbose=False)\n",
    "\n",
    "print(\"Entrainement CFR+...\")\n",
    "utilities_plus = cfr_plus.train(n_iterations, verbose=False)\n",
    "\n",
    "# Comparaison\n",
    "print(f\"\\nResultats apres {n_iterations} iterations:\")\n",
    "print(f\"  CFR:  utilite finale = {utilities_vanilla[-1]:.4f}, exploitabilite = {cfr_vanilla.get_exploitability():.6f}\")\n",
    "print(f\"  CFR+: utilite finale = {utilities_plus[-1]:.4f}, exploitabilite = {cfr_plus.get_exploitability():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCCFRSolver(CFRSolver):\n",
    "    \"\"\"\n",
    "    Monte Carlo CFR avec external sampling.\n",
    "    \n",
    "    Echantillonne les actions de l'adversaire au lieu de\n",
    "    parcourir tout l'arbre.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, exploring_player: int = 0):\n",
    "        super().__init__()\n",
    "        self.exploring_player = exploring_player\n",
    "    \n",
    "    def mccfr(self, history: str, cards: List[int], \n",
    "              player: int) -> float:\n",
    "        \"\"\"\n",
    "        MCCFR avec external sampling.\n",
    "        \n",
    "        Retourne l'utilite pour le joueur specifie.\n",
    "        \"\"\"\n",
    "        if self.game.is_terminal(history):\n",
    "            payoff = self.game.get_payoff(history, cards)\n",
    "            return payoff if player == 0 else -payoff\n",
    "        \n",
    "        current_player = self.game.get_current_player(history)\n",
    "        info_set = self.game.get_info_set(history, cards[current_player])\n",
    "        strategy = self.get_strategy(info_set)\n",
    "        \n",
    "        if current_player == player:\n",
    "            # C'est notre tour: calculer toutes les actions\n",
    "            action_utilities = np.zeros(self.game.NUM_ACTIONS)\n",
    "            \n",
    "            for action in range(self.game.NUM_ACTIONS):\n",
    "                action_char = 'p' if action == 0 else 'b'\n",
    "                new_history = history + action_char\n",
    "                action_utilities[action] = self.mccfr(new_history, cards, player)\n",
    "            \n",
    "            node_utility = (strategy * action_utilities).sum()\n",
    "            \n",
    "            # Mise a jour des regrets\n",
    "            for action in range(self.game.NUM_ACTIONS):\n",
    "                regret = action_utilities[action] - node_utility\n",
    "                self.regret_sum[info_set][action] += regret\n",
    "            \n",
    "            return node_utility\n",
    "        else:\n",
    "            # Tour de l'adversaire: echantillonner une action\n",
    "            action = np.random.choice(self.game.NUM_ACTIONS, p=strategy)\n",
    "            action_char = 'p' if action == 0 else 'b'\n",
    "            new_history = history + action_char\n",
    "            \n",
    "            # Accumuler la strategie\n",
    "            self.strategy_sum[info_set] += strategy\n",
    "            \n",
    "            return self.mccfr(new_history, cards, player)\n",
    "    \n",
    "    def train(self, iterations: int, verbose: bool = True) -> List[float]:\n",
    "        \"\"\"Entrainement MCCFR.\"\"\"\n",
    "        utilities = []\n",
    "        cards_permutations = [\n",
    "            [0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]\n",
    "        ]\n",
    "        \n",
    "        iterator = tqdm(range(iterations)) if verbose else range(iterations)\n",
    "        \n",
    "        for i in iterator:\n",
    "            # Echantillonner une distribution de cartes\n",
    "            cards = cards_permutations[np.random.randint(len(cards_permutations))]\n",
    "            \n",
    "            # Alterner le joueur qui explore\n",
    "            for player in [0, 1]:\n",
    "                self.mccfr('', cards, player)\n",
    "            \n",
    "            self.iterations += 1\n",
    "            \n",
    "            # Calculer l'utilite moyenne periodiquement\n",
    "            if i % 100 == 0:\n",
    "                total_utility = 0.0\n",
    "                for cards in cards_permutations:\n",
    "                    payoff = self._compute_expected_utility('', cards)\n",
    "                    total_utility += payoff\n",
    "                utilities.append(total_utility / len(cards_permutations))\n",
    "        \n",
    "        return utilities\n",
    "    \n",
    "    def _compute_expected_utility(self, history: str, cards: List[int]) -> float:\n",
    "        \"\"\"Calcule l'utilite esperee avec les strategies moyennes.\"\"\"\n",
    "        if self.game.is_terminal(history):\n",
    "            return self.game.get_payoff(history, cards)\n",
    "        \n",
    "        player = self.game.get_current_player(history)\n",
    "        info_set = self.game.get_info_set(history, cards[player])\n",
    "        strategy = self.get_average_strategy(info_set)\n",
    "        \n",
    "        utility = 0.0\n",
    "        for action in range(self.game.NUM_ACTIONS):\n",
    "            action_char = 'p' if action == 0 else 'b'\n",
    "            utility += strategy[action] * self._compute_expected_utility(\n",
    "                history + action_char, cards\n",
    "            )\n",
    "        return utility\n",
    "\n",
    "\n",
    "print(\"\\nEntrainement MCCFR...\")\n",
    "mccfr = MCCFRSolver()\n",
    "utilities_mccfr = mccfr.train(50000, verbose=True)\n",
    "\n",
    "print(f\"\\nMCCFR apres 50000 iterations:\")\n",
    "print(f\"  Utilite finale: {utilities_mccfr[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. OpenSpiel : CFR a l'Echelle\n",
    "\n",
    "OpenSpiel fournit des implementations optimisees de CFR et ses variantes. Voyons comment les utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENSPIEL_AVAILABLE:\n",
    "    from open_spiel.python.algorithms import cfr as openspiel_cfr\n",
    "    from open_spiel.python.algorithms import exploitability\n",
    "\n",
    "    # Charger Kuhn Poker\n",
    "    game = pyspiel.load_game(\"kuhn_poker\")\n",
    "    print(f\"Jeu: {game.get_type().short_name}\")\n",
    "    print(f\"Joueurs: {game.num_players()}\")\n",
    "    print(f\"Actions max: {game.num_distinct_actions()}\")\n",
    "\n",
    "    # CFR solver OpenSpiel\n",
    "    cfr_solver_os = openspiel_cfr.CFRSolver(game)\n",
    "\n",
    "    exploitabilities = []\n",
    "    iterations_to_record = [1, 10, 100, 500, 1000, 2000, 5000, 10000]\n",
    "\n",
    "    print(\"\\nEntrainement CFR OpenSpiel...\")\n",
    "    for i in tqdm(range(10001)):\n",
    "        cfr_solver_os.evaluate_and_update_policy()\n",
    "\n",
    "        if i in iterations_to_record:\n",
    "            avg_policy = cfr_solver_os.average_policy()\n",
    "            expl = exploitability.exploitability(game, avg_policy)\n",
    "            exploitabilities.append((i, expl))\n",
    "\n",
    "    print(\"\\nConvergence de l'exploitabilite:\")\n",
    "    print(f\"{'Iterations':<12} {'Exploitabilite':<15}\")\n",
    "    print(\"-\"*27)\n",
    "    for it, expl in exploitabilities:\n",
    "        print(f\"{it:<12} {expl:<15.6f}\")\n",
    "\n",
    "    # Afficher la politique finale\n",
    "    avg_policy = cfr_solver_os.average_policy()\n",
    "    print(\"\\nPolitique moyenne finale (premiers info sets):\")\n",
    "\n",
    "    # L'API TabularPolicy n'a plus d'attribut .policy\n",
    "    # On utilise action_probabilities pour chaque etat\n",
    "    state = game.new_initial_state()\n",
    "    \n",
    "    # Parcourir quelques etats pour montrer les strategies\n",
    "    def show_policy_for_states(state, policy, depth=0, max_states=6):\n",
    "        shown = [0]\n",
    "        def _traverse(s, d):\n",
    "            if shown[0] >= max_states:\n",
    "                return\n",
    "            if s.is_terminal():\n",
    "                return\n",
    "            if s.is_chance_node():\n",
    "                for action, prob in s.chance_outcomes():\n",
    "                    _traverse(s.child(action), d + 1)\n",
    "            else:\n",
    "                info_state = s.information_state_string()\n",
    "                action_probs = policy.action_probabilities(s)\n",
    "                if action_probs:\n",
    "                    print(f\"  {info_state}: {dict(action_probs)}\")\n",
    "                    shown[0] += 1\n",
    "                for action in s.legal_actions():\n",
    "                    _traverse(s.child(action), d + 1)\n",
    "        _traverse(state, depth)\n",
    "    \n",
    "    show_policy_for_states(game.new_initial_state(), avg_policy)\n",
    "else:\n",
    "    print(\"OpenSpiel non disponible - section ignoree\")\n",
    "    print(\"Pour installer: pip install open_spiel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENSPIEL_AVAILABLE:\n",
    "    # Comparaison CFR vs CFR+ vs Linear CFR sur OpenSpiel\n",
    "    from open_spiel.python.algorithms import cfr as cfr_module\n",
    "    \n",
    "    game = pyspiel.load_game(\"kuhn_poker\")\n",
    "    \n",
    "    solvers = {\n",
    "        'CFR': cfr_module.CFRSolver(game),\n",
    "        'CFR+': cfr_module.CFRPlusSolver(game),\n",
    "    }\n",
    "    \n",
    "    results = {name: [] for name in solvers}\n",
    "    checkpoints = list(range(0, 5001, 100))\n",
    "    \n",
    "    print(\"Comparaison des variantes CFR (OpenSpiel)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for name, solver in solvers.items():\n",
    "        print(f\"\\nEntrainement {name}...\")\n",
    "        for i in tqdm(range(5001)):\n",
    "            solver.evaluate_and_update_policy()\n",
    "            \n",
    "            if i in checkpoints:\n",
    "                avg_policy = solver.average_policy()\n",
    "                expl = exploitability.exploitability(game, avg_policy)\n",
    "                results[name].append(expl)\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for name, expls in results.items():\n",
    "        plt.plot(checkpoints, expls, label=name, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Exploitabilite')\n",
    "    plt.title('Convergence des variantes CFR sur Kuhn Poker')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cfr_variants_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nExploitabilite finale:\")\n",
    "    for name, expls in results.items():\n",
    "        print(f\"  {name}: {expls[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Leduc Poker : Un Jeu Plus Complexe\n",
    "\n",
    "Le **Leduc Poker** est un jeu plus riche que Kuhn :\n",
    "- 6 cartes : 2x Jack, 2x Queen, 2x King\n",
    "- 2 tours de mises\n",
    "- Carte commune revelee au 2e tour\n",
    "\n",
    "Avec ~936 information sets, c'est un bon benchmark intermediaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENSPIEL_AVAILABLE:\n",
    "    from open_spiel.python.algorithms import cfr as cfr_module\n",
    "    from open_spiel.python.algorithms import exploitability\n",
    "\n",
    "    # Leduc Poker\n",
    "    leduc = pyspiel.load_game(\"leduc_poker\")\n",
    "\n",
    "    print(\"Leduc Poker\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Joueurs: {leduc.num_players()}\")\n",
    "    print(f\"Actions: {leduc.num_distinct_actions()}\")\n",
    "\n",
    "    # Compter les information sets\n",
    "    cfr_leduc = cfr_module.CFRSolver(leduc)\n",
    "\n",
    "    print(\"\\nEntrainement CFR sur Leduc (100 iterations)...\")\n",
    "    for i in tqdm(range(101)):\n",
    "        cfr_leduc.evaluate_and_update_policy()\n",
    "\n",
    "    avg_policy = cfr_leduc.average_policy()\n",
    "    expl = exploitability.exploitability(leduc, avg_policy)\n",
    "\n",
    "    print(f\"\\nExploitabilite apres 100 iterations: {expl:.4f}\")\n",
    "    print(f\"(Equilibre parfait = 0)\")\n",
    "\n",
    "    # Continuer l'entrainement\n",
    "    print(\"\\nEntrainement supplementaire (900 iterations)...\")\n",
    "    for i in tqdm(range(900)):\n",
    "        cfr_leduc.evaluate_and_update_policy()\n",
    "\n",
    "    avg_policy = cfr_leduc.average_policy()\n",
    "    expl = exploitability.exploitability(leduc, avg_policy)\n",
    "    print(f\"Exploitabilite apres 1000 iterations: {expl:.4f}\")\n",
    "else:\n",
    "    print(\"OpenSpiel requis pour Leduc Poker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deep CFR : Apercu\n",
    "\n",
    "Pour les jeux de grande taille (Texas Hold'em: ~10^14 etats), CFR tabulaire est impossible. **Deep CFR** utilise des reseaux de neurones.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "1. **Reseau de regrets** $V_i(I, a; \\theta)$ : predit les regrets cumules\n",
    "2. **Reseau de strategie** $\\Pi(I, a; \\phi)$ : predit la strategie moyenne\n",
    "3. **Reservoir sampling** : maintient un echantillon des donnees d'entrainement\n",
    "\n",
    "### Algorithme simplifie\n",
    "\n",
    "```\n",
    "for t = 1 to T:\n",
    "    # Traversee CFR externe\n",
    "    for each sampled state h:\n",
    "        compute counterfactual regrets r(I, a)\n",
    "        add (I, r) to advantage memory M_V\n",
    "        add (I, sigma) to strategy memory M_Pi\n",
    "    \n",
    "    # Entrainement des reseaux\n",
    "    train V on M_V (regression)\n",
    "    train Pi on M_Pi (cross-entropy)\n",
    "```\n",
    "\n",
    "### Resultats notables\n",
    "\n",
    "- **Libratus** (2017) : a battu des pros au Heads-Up No-Limit Hold'em\n",
    "- **Pluribus** (2019) : premier bot a battre des pros en 6-joueurs\n",
    "\n",
    "L'implementation complete de Deep CFR depasse le cadre de ce notebook (voir OpenSpiel ou le papier original)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema conceptuel de Deep CFR\n",
    "print(\"Architecture Deep CFR (conceptuel)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Utilisation d'un raw string pour eviter les warnings d'escape sequences\n",
    "deep_cfr_schema = r\"\"\"\n",
    "+-------------------+     +-------------------+\n",
    "|   Traversee CFR   |     |   Traversee CFR   |\n",
    "|   (External       |     |   (External       |\n",
    "|    Sampling)      |     |    Sampling)      |\n",
    "+--------+----------+     +--------+----------+\n",
    "         |                         |\n",
    "         v                         v\n",
    "+--------+----------+     +--------+----------+\n",
    "|  Advantage Memory |     |  Strategy Memory  |\n",
    "|  M_V: (I, r(I,a)) |     |  M_Pi: (I, sigma) |\n",
    "+--------+----------+     +--------+----------+\n",
    "         |                         |\n",
    "         v                         v\n",
    "+--------+----------+     +--------+----------+\n",
    "|   Value Network   |     | Strategy Network  |\n",
    "|   V(I,a; theta)   |     |  Pi(I,a; phi)     |\n",
    "|   (MSE loss)      |     | (Cross-entropy)   |\n",
    "+-------------------+     +-------------------+\n",
    "\n",
    "         ||                        ||\n",
    "         \\/                        \\/\n",
    "    Regret Matching           Average Strategy\n",
    "    pour actions              (converge vers Nash)\n",
    "\"\"\"\n",
    "\n",
    "print(deep_cfr_schema)\n",
    "\n",
    "print(\"\\nAvantages de Deep CFR:\")\n",
    "print(\"  - Generalisation: apprend des patterns, pas une table\")\n",
    "print(\"  - Passage a l'echelle: 10^14 etats en Hold'em\")\n",
    "print(\"  - Abstraction implicite: le reseau compresse l'info\")\n",
    "\n",
    "print(\"\\nLimitations:\")\n",
    "print(\"  - Approximation: pas de garantie de convergence exacte\")\n",
    "print(\"  - Hyperparametres: architecture, learning rate, etc.\")\n",
    "print(\"  - Compute: necessite GPU et beaucoup d'iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercices\n",
    "\n",
    "### Exercice 1 : CFR sur Rock-Paper-Scissors\n",
    "\n",
    "Adaptez le solveur CFR pour resoudre Rock-Paper-Scissors (jeu a somme nulle trivial).\n",
    "\n",
    "### Exercice 2 : Analyse de la convergence\n",
    "\n",
    "Tracez l'evolution des strategies pour les 6 information sets principaux de Kuhn Poker au cours des iterations.\n",
    "\n",
    "### Exercice 3 : MCCFR variants\n",
    "\n",
    "Implementez l'**outcome sampling** MCCFR et comparez sa variance avec l'external sampling.\n",
    "\n",
    "### Exercice 4 : Jeu personnalise\n",
    "\n",
    "Creez un jeu de poker simplifie avec 4 cartes et 3 joueurs, puis appliquez CFR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espace pour les exercices\n",
    "\n",
    "# Exercice 1 : CFR sur RPS\n",
    "class RPSGame:\n",
    "    \"\"\"Rock-Paper-Scissors comme jeu extensif trivial.\"\"\"\n",
    "    NUM_ACTIONS = 3  # 0=Rock, 1=Paper, 2=Scissors\n",
    "    \n",
    "    def get_payoff(self, action1: int, action2: int) -> float:\n",
    "        \"\"\"Retourne le payoff du joueur 1.\"\"\"\n",
    "        if action1 == action2:\n",
    "            return 0\n",
    "        # Rock beats Scissors, Paper beats Rock, Scissors beats Paper\n",
    "        wins = {(0, 2), (1, 0), (2, 1)}\n",
    "        return 1 if (action1, action2) in wins else -1\n",
    "\n",
    "# A completer...\n",
    "\n",
    "print(\"Exercices a completer dans les cellules suivantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resume et Points Cles\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "1. **Information imparfaite** : les joueurs ne connaissent pas l'historique complet\n",
    "2. **Regret Matching** : jouer proportionnellement aux regrets positifs\n",
    "3. **CFR** : appliquer le regret matching a chaque information set\n",
    "4. **Convergence** : la strategie moyenne converge vers un equilibre de Nash\n",
    "5. **Variantes** : CFR+ (plus rapide), MCCFR (echantillonnage), Deep CFR (neural)\n",
    "\n",
    "### Formules cles\n",
    "\n",
    "| Concept | Formule |\n",
    "|---------|--------|\n",
    "| Regret Matching | $\\sigma(a) = \\frac{\\max(R(a), 0)}{\\sum_{a'} \\max(R(a'), 0)}$ |\n",
    "| Regret update | $R^{T+1}(a) = R^T(a) + r^T(a)$ |\n",
    "| Regret contrefactuel | $r_i(I,a) = \\sum_{h \\in I} \\pi_{-i}(h) [v_i(h \\cdot a) - v_i(h)]$ |\n",
    "| Exploitabilite | $\\epsilon = \\sum_i \\max_{\\sigma'_i} u_i(\\sigma'_i, \\sigma_{-i}) - u_i(\\sigma)$ |\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Poker** : Libratus, Pluribus (battent les humains)\n",
    "- **Negociation** : jeux de marchandage\n",
    "- **Securite** : jeux de securite avec information cachee\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook suivant** : [GameTheory-13-DifferentialGames](GameTheory-13-DifferentialGames.ipynb) - Jeux differentiels et equilibres de Stackelberg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GameTheory WSL + OpenSpiel)",
   "language": "python",
   "name": "gametheory-wsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}