{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GameTheory-15 : Apprentissage par Renforcement Multi-Agent\n",
    "\n",
    "## Self-Play, NFSP et PSRO\n",
    "\n",
    "Ce notebook explore l'intersection entre la theorie des jeux et l'apprentissage par renforcement (RL) dans les environnements multi-agents.\n",
    "\n",
    "**Objectifs pedagogiques** :\n",
    "- Comprendre les defis specifiques du MARL (Multi-Agent RL)\n",
    "- Maitriser le concept de self-play et ses variantes\n",
    "- Implementer Fictitious Play et ses extensions neurales (NFSP)\n",
    "- Decouvrir PSRO (Policy-Space Response Oracles)\n",
    "\n",
    "**Prerequis** : Notebooks 1-14, notions de base en RL\n",
    "\n",
    "**Duree estimee** : 55 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defis du Multi-Agent RL\n",
    "\n",
    "### 1.1 Pourquoi le MARL est difficile\n",
    "\n",
    "| Defi | Description |\n",
    "|------|-------------|\n",
    "| **Non-stationnarite** | L'environnement change car les autres agents apprennent |\n",
    "| **Credit assignment** | Difficult de savoir qui est responsable du resultat |\n",
    "| **Espace d'action conjoint** | Explose exponentiellement avec le nombre d'agents |\n",
    "| **Equilibres multiples** | Plusieurs Nash possibles, convergence incertaine |\n",
    "| **Exploration** | Coordination implicite necessaire |\n",
    "\n",
    "### 1.2 Approches principales\n",
    "\n",
    "1. **Independent Learning** : chaque agent apprend comme si les autres etaient statiques\n",
    "2. **Self-Play** : l'agent joue contre des versions de lui-meme\n",
    "3. **Population-based Training** : ensemble d'agents qui co-evoluent\n",
    "4. **Centralized Training, Decentralized Execution** (CTDE)\n",
    "\n",
    "### 1.3 Objectifs d'apprentissage\n",
    "\n",
    "- **Jeux a somme nulle** : trouver un equilibre de Nash\n",
    "- **Jeux cooperatifs** : maximiser le gain collectif\n",
    "- **Jeux generaux** : trouver un equilibre stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dependances\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['numpy', 'matplotlib', 'tqdm']\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "\n",
    "# Tentative OpenSpiel\n",
    "try:\n",
    "    import pyspiel\n",
    "    OPENSPIEL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'open_spiel'])\n",
    "        import pyspiel\n",
    "        OPENSPIEL_AVAILABLE = True\n",
    "    except:\n",
    "        OPENSPIEL_AVAILABLE = False\n",
    "        print(\"OpenSpiel non disponible - utilisation des implementations locales\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Imports reussis\")\n",
    "print(f\"OpenSpiel disponible: {OPENSPIEL_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-Play : Apprentissage par Auto-Competition\n",
    "\n",
    "### 2.1 Principe\n",
    "\n",
    "Au lieu d'avoir un adversaire fixe, l'agent joue contre :\n",
    "- Lui-meme (self-play naif)\n",
    "- Des versions anterieures (historical self-play)\n",
    "- Un melange (fictitious self-play)\n",
    "\n",
    "### 2.2 Problemes du self-play naif\n",
    "\n",
    "- **Forgetting catastrophique** : oublie comment battre les anciennes strategies\n",
    "- **Cycles** : oscillations entre strategies non-transitives (RPS)\n",
    "- **Exploitation locale** : sur-optimise contre la version courante\n",
    "\n",
    "### 2.3 AlphaGo/AlphaZero\n",
    "\n",
    "Succes spectaculaire du self-play avec MCTS + reseaux de neurones dans les jeux a information parfaite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixGame:\n",
    "    \"\"\"Jeu matriciel a 2 joueurs.\"\"\"\n",
    "    \n",
    "    def __init__(self, payoff_matrix_p1: np.ndarray, name: str = \"Game\"):\n",
    "        \"\"\"\n",
    "        payoff_matrix_p1[i, j] = payoff de P1 si P1 joue i et P2 joue j.\n",
    "        Jeu a somme nulle : payoff P2 = -payoff P1.\n",
    "        \"\"\"\n",
    "        self.A = payoff_matrix_p1\n",
    "        self.name = name\n",
    "        self.n_actions_p1 = self.A.shape[0]\n",
    "        self.n_actions_p2 = self.A.shape[1]\n",
    "    \n",
    "    def get_payoff(self, action_p1: int, action_p2: int) -> Tuple[float, float]:\n",
    "        \"\"\"Retourne (payoff P1, payoff P2).\"\"\"\n",
    "        p1 = self.A[action_p1, action_p2]\n",
    "        return (p1, -p1)\n",
    "    \n",
    "    def expected_payoff(self, strategy_p1: np.ndarray, \n",
    "                        strategy_p2: np.ndarray) -> Tuple[float, float]:\n",
    "        \"\"\"Payoff espere pour des strategies mixtes.\"\"\"\n",
    "        p1 = strategy_p1 @ self.A @ strategy_p2\n",
    "        return (p1, -p1)\n",
    "    \n",
    "    def best_response(self, opponent_strategy: np.ndarray, \n",
    "                      player: int) -> np.ndarray:\n",
    "        \"\"\"Meilleure reponse pure a une strategie mixte adverse.\"\"\"\n",
    "        if player == 0:  # P1\n",
    "            payoffs = self.A @ opponent_strategy\n",
    "            best_action = np.argmax(payoffs)\n",
    "            br = np.zeros(self.n_actions_p1)\n",
    "            br[best_action] = 1.0\n",
    "        else:  # P2 (minimise payoff P1)\n",
    "            payoffs = self.A.T @ opponent_strategy\n",
    "            best_action = np.argmin(payoffs)  # Min car somme nulle\n",
    "            br = np.zeros(self.n_actions_p2)\n",
    "            br[best_action] = 1.0\n",
    "        return br\n",
    "    \n",
    "    def exploitability(self, strategy_p1: np.ndarray, \n",
    "                       strategy_p2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Mesure de la distance a l'equilibre de Nash.\n",
    "        \n",
    "        = gain max de P1 en deviant + gain max de P2 en deviant\n",
    "        \"\"\"\n",
    "        # Meilleure reponse de P1\n",
    "        br_p1 = self.best_response(strategy_p2, 0)\n",
    "        gain_p1 = self.expected_payoff(br_p1, strategy_p2)[0] - \\\n",
    "                  self.expected_payoff(strategy_p1, strategy_p2)[0]\n",
    "        \n",
    "        # Meilleure reponse de P2\n",
    "        br_p2 = self.best_response(strategy_p1, 1)\n",
    "        gain_p2 = self.expected_payoff(strategy_p1, br_p2)[1] - \\\n",
    "                  self.expected_payoff(strategy_p1, strategy_p2)[1]\n",
    "        \n",
    "        return gain_p1 + gain_p2\n",
    "\n",
    "\n",
    "# Jeux classiques\n",
    "def create_rps() -> MatrixGame:\n",
    "    \"\"\"Rock-Paper-Scissors.\"\"\"\n",
    "    A = np.array([\n",
    "        [0, -1, 1],   # Rock\n",
    "        [1, 0, -1],   # Paper\n",
    "        [-1, 1, 0]    # Scissors\n",
    "    ])\n",
    "    return MatrixGame(A, \"Rock-Paper-Scissors\")\n",
    "\n",
    "\n",
    "def create_matching_pennies() -> MatrixGame:\n",
    "    \"\"\"Matching Pennies.\"\"\"\n",
    "    A = np.array([\n",
    "        [1, -1],   # Heads\n",
    "        [-1, 1]    # Tails\n",
    "    ])\n",
    "    return MatrixGame(A, \"Matching Pennies\")\n",
    "\n",
    "\n",
    "# Test\n",
    "rps = create_rps()\n",
    "print(f\"Jeu: {rps.name}\")\n",
    "print(f\"Matrice de gains P1:\")\n",
    "print(rps.A)\n",
    "\n",
    "# Equilibre de Nash = (1/3, 1/3, 1/3)\n",
    "nash = np.array([1/3, 1/3, 1/3])\n",
    "print(f\"\\nExploitabilite a Nash: {rps.exploitability(nash, nash):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSelfPlay:\n",
    "    \"\"\"\n",
    "    Self-play naif : chaque joueur optimise contre la strategie\n",
    "    courante de l'adversaire.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, game: MatrixGame, learning_rate: float = 0.1):\n",
    "        self.game = game\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Strategies initiales uniformes\n",
    "        self.strategy_p1 = np.ones(game.n_actions_p1) / game.n_actions_p1\n",
    "        self.strategy_p2 = np.ones(game.n_actions_p2) / game.n_actions_p2\n",
    "        \n",
    "        self.history = {\n",
    "            'exploitability': [],\n",
    "            'strategy_p1': [],\n",
    "            'strategy_p2': []\n",
    "        }\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Une iteration de self-play naif.\"\"\"\n",
    "        # Meilleure reponse de chaque joueur\n",
    "        br_p1 = self.game.best_response(self.strategy_p2, 0)\n",
    "        br_p2 = self.game.best_response(self.strategy_p1, 1)\n",
    "        \n",
    "        # Mise a jour graduelle (moving average)\n",
    "        self.strategy_p1 = (1 - self.lr) * self.strategy_p1 + self.lr * br_p1\n",
    "        self.strategy_p2 = (1 - self.lr) * self.strategy_p2 + self.lr * br_p2\n",
    "        \n",
    "        # Normaliser\n",
    "        self.strategy_p1 /= self.strategy_p1.sum()\n",
    "        self.strategy_p2 /= self.strategy_p2.sum()\n",
    "        \n",
    "        # Enregistrer\n",
    "        self.history['exploitability'].append(\n",
    "            self.game.exploitability(self.strategy_p1, self.strategy_p2)\n",
    "        )\n",
    "        self.history['strategy_p1'].append(self.strategy_p1.copy())\n",
    "        self.history['strategy_p2'].append(self.strategy_p2.copy())\n",
    "    \n",
    "    def train(self, iterations: int):\n",
    "        \"\"\"Entraine pendant n iterations.\"\"\"\n",
    "        for _ in range(iterations):\n",
    "            self.step()\n",
    "\n",
    "\n",
    "# Demonstration : self-play naif sur RPS\n",
    "print(\"Self-Play Naif sur Rock-Paper-Scissors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "rps = create_rps()\n",
    "sp_naive = NaiveSelfPlay(rps, learning_rate=0.3)\n",
    "sp_naive.train(200)\n",
    "\n",
    "print(f\"\\nStrategie finale P1: {sp_naive.strategy_p1}\")\n",
    "print(f\"Strategie finale P2: {sp_naive.strategy_p2}\")\n",
    "print(f\"Exploitabilite finale: {sp_naive.history['exploitability'][-1]:.4f}\")\n",
    "\n",
    "# Visualisation des cycles\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Exploitabilite\n",
    "ax1 = axes[0]\n",
    "ax1.plot(sp_naive.history['exploitability'], linewidth=1)\n",
    "ax1.axhline(y=0, color='r', linestyle='--', label='Nash')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Exploitabilite')\n",
    "ax1.set_title('Exploitabilite - Self-Play Naif')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Evolution des strategies\n",
    "ax2 = axes[1]\n",
    "strategies = np.array(sp_naive.history['strategy_p1'])\n",
    "for i, label in enumerate(['Rock', 'Paper', 'Scissors']):\n",
    "    ax2.plot(strategies[:, i], label=label)\n",
    "ax2.axhline(y=1/3, color='gray', linestyle='--', alpha=0.5, label='Nash (1/3)')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Probabilite')\n",
    "ax2.set_title('Strategies P1 - Cycles evidents')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('naive_selfplay_rps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n-> Le self-play naif oscille sans converger vers Nash!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fictitious Play\n",
    "\n",
    "### 3.1 Principe (Brown, 1951)\n",
    "\n",
    "Chaque joueur :\n",
    "1. Maintient une estimation de la strategie adverse (moyenne empirique)\n",
    "2. Joue la meilleure reponse a cette estimation\n",
    "\n",
    "### 3.2 Convergence\n",
    "\n",
    "- Converge vers Nash dans les jeux a somme nulle\n",
    "- Converge dans les jeux a 2 joueurs avec somme constante\n",
    "- Peut ne pas converger dans les jeux generaux\n",
    "\n",
    "### 3.3 Avantages\n",
    "\n",
    "- Simple a implementer\n",
    "- Garanties theoriques dans certaines classes de jeux\n",
    "- Base pour des methodes plus sophistiquees (NFSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FictitiousPlay:\n",
    "    \"\"\"\n",
    "    Fictitious Play : joue la meilleure reponse a la\n",
    "    strategie moyenne historique de l'adversaire.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, game: MatrixGame):\n",
    "        self.game = game\n",
    "        \n",
    "        # Compteurs d'actions jouees\n",
    "        self.counts_p1 = np.ones(game.n_actions_p1)  # Laplace smoothing\n",
    "        self.counts_p2 = np.ones(game.n_actions_p2)\n",
    "        \n",
    "        self.history = {\n",
    "            'exploitability': [],\n",
    "            'avg_strategy_p1': [],\n",
    "            'avg_strategy_p2': []\n",
    "        }\n",
    "    \n",
    "    def get_average_strategy(self, player: int) -> np.ndarray:\n",
    "        \"\"\"Retourne la strategie moyenne (frequence empirique).\"\"\"\n",
    "        if player == 0:\n",
    "            return self.counts_p1 / self.counts_p1.sum()\n",
    "        else:\n",
    "            return self.counts_p2 / self.counts_p2.sum()\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Une iteration de Fictitious Play.\"\"\"\n",
    "        # Strategies moyennes adverses\n",
    "        avg_p1 = self.get_average_strategy(0)\n",
    "        avg_p2 = self.get_average_strategy(1)\n",
    "        \n",
    "        # Meilleures reponses aux moyennes\n",
    "        br_p1 = self.game.best_response(avg_p2, 0)\n",
    "        br_p2 = self.game.best_response(avg_p1, 1)\n",
    "        \n",
    "        # Action jouee = argmax de la meilleure reponse\n",
    "        action_p1 = np.argmax(br_p1)\n",
    "        action_p2 = np.argmax(br_p2)\n",
    "        \n",
    "        # Mettre a jour les compteurs\n",
    "        self.counts_p1[action_p1] += 1\n",
    "        self.counts_p2[action_p2] += 1\n",
    "        \n",
    "        # Enregistrer\n",
    "        new_avg_p1 = self.get_average_strategy(0)\n",
    "        new_avg_p2 = self.get_average_strategy(1)\n",
    "        \n",
    "        self.history['exploitability'].append(\n",
    "            self.game.exploitability(new_avg_p1, new_avg_p2)\n",
    "        )\n",
    "        self.history['avg_strategy_p1'].append(new_avg_p1.copy())\n",
    "        self.history['avg_strategy_p2'].append(new_avg_p2.copy())\n",
    "    \n",
    "    def train(self, iterations: int):\n",
    "        for _ in range(iterations):\n",
    "            self.step()\n",
    "\n",
    "\n",
    "# Comparaison avec Self-Play naif\n",
    "print(\"Fictitious Play sur Rock-Paper-Scissors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fp = FictitiousPlay(rps)\n",
    "fp.train(500)\n",
    "\n",
    "print(f\"\\nStrategie moyenne P1: {fp.get_average_strategy(0)}\")\n",
    "print(f\"Strategie moyenne P2: {fp.get_average_strategy(1)}\")\n",
    "print(f\"Exploitabilite finale: {fp.history['exploitability'][-1]:.6f}\")\n",
    "\n",
    "# Comparaison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Exploitabilite comparee\n",
    "ax1 = axes[0]\n",
    "ax1.plot(sp_naive.history['exploitability'][:500], label='Self-Play Naif', alpha=0.7)\n",
    "ax1.plot(fp.history['exploitability'], label='Fictitious Play', alpha=0.7)\n",
    "ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Exploitabilite')\n",
    "ax1.set_title('Convergence : Self-Play vs Fictitious Play')\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Evolution des strategies FP\n",
    "ax2 = axes[1]\n",
    "strategies = np.array(fp.history['avg_strategy_p1'])\n",
    "for i, label in enumerate(['Rock', 'Paper', 'Scissors']):\n",
    "    ax2.plot(strategies[:, i], label=label)\n",
    "ax2.axhline(y=1/3, color='gray', linestyle='--', alpha=0.5, label='Nash')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Probabilite')\n",
    "ax2.set_title('Strategies Fictitious Play - Convergence')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fictitious_play_rps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n-> Fictitious Play converge vers Nash!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Fictitious Self-Play (NFSP)\n",
    "\n",
    "### 4.1 Motivation (Heinrich & Silver, 2016)\n",
    "\n",
    "Fictitious Play necessite de stocker tout l'historique. NFSP utilise des reseaux de neurones pour :\n",
    "1. **Approximer la meilleure reponse** (RL network)\n",
    "2. **Approximer la strategie moyenne** (Supervised Learning network)\n",
    "\n",
    "### 4.2 Architecture\n",
    "\n",
    "```\n",
    "+-------------------+     +-------------------+\n",
    "|   RL Network      |     |   SL Network      |\n",
    "|   (Best Response) |     |   (Avg Strategy)  |\n",
    "+--------+----------+     +--------+----------+\n",
    "         |                         |\n",
    "         v                         v\n",
    "    Q(s, a)                    pi(a|s)\n",
    "         |                         |\n",
    "         +------------+------------+\n",
    "                      |\n",
    "                      v\n",
    "              epsilon-greedy:\n",
    "              eta * pi + (1-eta) * BR(Q)\n",
    "```\n",
    "\n",
    "### 4.3 Resultats\n",
    "\n",
    "- Premier algorithme RL a approximer l'equilibre de Nash au Heads-Up Limit Hold'em\n",
    "- Base pour des approches plus avancees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedNFSP:\n",
    "    \"\"\"\n",
    "    Version simplifiee de NFSP pour jeux matriciels.\n",
    "    \n",
    "    Utilise des tables au lieu de reseaux de neurones.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, game: MatrixGame, \n",
    "                 eta: float = 0.1,        # Prob d'utiliser avg strategy\n",
    "                 rl_lr: float = 0.01,     # Learning rate RL\n",
    "                 sl_lr: float = 0.01):    # Learning rate SL\n",
    "        self.game = game\n",
    "        self.eta = eta\n",
    "        self.rl_lr = rl_lr\n",
    "        self.sl_lr = sl_lr\n",
    "        \n",
    "        # Q-values pour best response (RL)\n",
    "        self.Q_p1 = np.zeros(game.n_actions_p1)\n",
    "        self.Q_p2 = np.zeros(game.n_actions_p2)\n",
    "        \n",
    "        # Strategie moyenne (SL)\n",
    "        self.avg_strategy_p1 = np.ones(game.n_actions_p1) / game.n_actions_p1\n",
    "        self.avg_strategy_p2 = np.ones(game.n_actions_p2) / game.n_actions_p2\n",
    "        \n",
    "        self.history = {\n",
    "            'exploitability': [],\n",
    "            'avg_strategy_p1': [],\n",
    "            'avg_strategy_p2': []\n",
    "        }\n",
    "    \n",
    "    def get_action_p1(self, use_avg: bool) -> int:\n",
    "        \"\"\"Selectionne une action pour P1.\"\"\"\n",
    "        if use_avg:\n",
    "            return np.random.choice(self.game.n_actions_p1, p=self.avg_strategy_p1)\n",
    "        else:\n",
    "            return np.argmax(self.Q_p1)  # Best response\n",
    "    \n",
    "    def get_action_p2(self, use_avg: bool) -> int:\n",
    "        \"\"\"Selectionne une action pour P2.\"\"\"\n",
    "        if use_avg:\n",
    "            return np.random.choice(self.game.n_actions_p2, p=self.avg_strategy_p2)\n",
    "        else:\n",
    "            return np.argmin(self.Q_p2)  # Min car P2 minimise\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Une iteration de NFSP.\"\"\"\n",
    "        # Decider si on utilise avg strategy ou best response\n",
    "        use_avg_p1 = np.random.random() < self.eta\n",
    "        use_avg_p2 = np.random.random() < self.eta\n",
    "        \n",
    "        # Jouer\n",
    "        action_p1 = self.get_action_p1(use_avg_p1)\n",
    "        action_p2 = self.get_action_p2(use_avg_p2)\n",
    "        \n",
    "        # Obtenir les payoffs\n",
    "        payoff_p1, payoff_p2 = self.game.get_payoff(action_p1, action_p2)\n",
    "        \n",
    "        # Mise a jour Q-values (RL - contre la strategie moyenne adverse)\n",
    "        # Pour P1 : Q[a] = E[payoff | a, opponent plays avg]\n",
    "        expected_payoffs_p1 = self.game.A @ self.avg_strategy_p2\n",
    "        self.Q_p1 = (1 - self.rl_lr) * self.Q_p1 + self.rl_lr * expected_payoffs_p1\n",
    "        \n",
    "        expected_payoffs_p2 = -self.game.A.T @ self.avg_strategy_p1  # Negatif car somme nulle\n",
    "        self.Q_p2 = (1 - self.rl_lr) * self.Q_p2 + self.rl_lr * expected_payoffs_p2\n",
    "        \n",
    "        # Mise a jour strategie moyenne (SL - supervised vers best response)\n",
    "        br_p1 = np.zeros(self.game.n_actions_p1)\n",
    "        br_p1[np.argmax(self.Q_p1)] = 1.0\n",
    "        self.avg_strategy_p1 = (1 - self.sl_lr) * self.avg_strategy_p1 + self.sl_lr * br_p1\n",
    "        self.avg_strategy_p1 /= self.avg_strategy_p1.sum()\n",
    "        \n",
    "        br_p2 = np.zeros(self.game.n_actions_p2)\n",
    "        br_p2[np.argmin(self.Q_p2)] = 1.0\n",
    "        self.avg_strategy_p2 = (1 - self.sl_lr) * self.avg_strategy_p2 + self.sl_lr * br_p2\n",
    "        self.avg_strategy_p2 /= self.avg_strategy_p2.sum()\n",
    "        \n",
    "        # Enregistrer\n",
    "        self.history['exploitability'].append(\n",
    "            self.game.exploitability(self.avg_strategy_p1, self.avg_strategy_p2)\n",
    "        )\n",
    "        self.history['avg_strategy_p1'].append(self.avg_strategy_p1.copy())\n",
    "        self.history['avg_strategy_p2'].append(self.avg_strategy_p2.copy())\n",
    "    \n",
    "    def train(self, iterations: int):\n",
    "        for _ in range(iterations):\n",
    "            self.step()\n",
    "\n",
    "\n",
    "# Test NFSP\n",
    "print(\"Neural Fictitious Self-Play (simplifie) sur RPS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "nfsp = SimplifiedNFSP(rps, eta=0.1, rl_lr=0.05, sl_lr=0.01)\n",
    "nfsp.train(1000)\n",
    "\n",
    "print(f\"\\nStrategie moyenne P1: {nfsp.avg_strategy_p1}\")\n",
    "print(f\"Strategie moyenne P2: {nfsp.avg_strategy_p2}\")\n",
    "print(f\"Exploitabilite finale: {nfsp.history['exploitability'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Policy-Space Response Oracles (PSRO)\n",
    "\n",
    "### 5.1 Motivation (Lanctot et al., 2017)\n",
    "\n",
    "Au lieu d'une seule strategie, maintenir une **population** de strategies et calculer un equilibre sur cette population.\n",
    "\n",
    "### 5.2 Algorithme\n",
    "\n",
    "1. Initialiser avec une population de strategies $\\Pi = \\{\\pi_1\\}$\n",
    "2. Repeter :\n",
    "   a. Calculer le meta-jeu (payoffs entre toutes les strategies)\n",
    "   b. Trouver l'equilibre du meta-jeu (meta-Nash)\n",
    "   c. Entrainer une meilleure reponse a la distribution meta-Nash\n",
    "   d. Ajouter la nouvelle strategie a la population\n",
    "\n",
    "### 5.3 Variantes\n",
    "\n",
    "- **Double Oracle** : PSRO avec best response exacte\n",
    "- **Alpha-PSRO** : meilleure reponse approximative (RL)\n",
    "- **DCH** : meta-solver utilisant Hedge au lieu de Nash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSRO:\n",
    "    \"\"\"\n",
    "    Policy-Space Response Oracles pour jeux matriciels.\n",
    "    \n",
    "    Maintient une population de strategies et calcule des\n",
    "    equilibres sur le meta-jeu.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, game: MatrixGame):\n",
    "        self.game = game\n",
    "        \n",
    "        # Population de strategies (chaque ligne est une strategie mixte)\n",
    "        # Initialiser avec strategie uniforme\n",
    "        self.population_p1 = [np.ones(game.n_actions_p1) / game.n_actions_p1]\n",
    "        self.population_p2 = [np.ones(game.n_actions_p2) / game.n_actions_p2]\n",
    "        \n",
    "        # Meta-game payoff matrix\n",
    "        self.meta_game = np.zeros((1, 1))\n",
    "        self._update_meta_game()\n",
    "        \n",
    "        self.history = {\n",
    "            'population_size': [],\n",
    "            'exploitability': [],\n",
    "            'meta_nash_p1': [],\n",
    "            'meta_nash_p2': []\n",
    "        }\n",
    "    \n",
    "    def _update_meta_game(self):\n",
    "        \"\"\"Recalcule la matrice du meta-jeu.\"\"\"\n",
    "        n1 = len(self.population_p1)\n",
    "        n2 = len(self.population_p2)\n",
    "        self.meta_game = np.zeros((n1, n2))\n",
    "        \n",
    "        for i, pi1 in enumerate(self.population_p1):\n",
    "            for j, pi2 in enumerate(self.population_p2):\n",
    "                payoff, _ = self.game.expected_payoff(pi1, pi2)\n",
    "                self.meta_game[i, j] = payoff\n",
    "    \n",
    "    def _solve_meta_nash(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Resout l'equilibre de Nash du meta-jeu.\n",
    "        \n",
    "        Utilise une approximation par support enumeration simplifie.\n",
    "        \"\"\"\n",
    "        n1 = len(self.population_p1)\n",
    "        n2 = len(self.population_p2)\n",
    "        \n",
    "        # Approximation simple : fictitious play sur le meta-jeu\n",
    "        meta_game_obj = MatrixGame(self.meta_game, \"meta\")\n",
    "        fp = FictitiousPlay(meta_game_obj)\n",
    "        fp.train(100)\n",
    "        \n",
    "        return fp.get_average_strategy(0), fp.get_average_strategy(1)\n",
    "    \n",
    "    def _compute_best_response(self, meta_nash: np.ndarray, \n",
    "                               player: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calcule la meilleure reponse a la distribution meta-Nash.\n",
    "        \"\"\"\n",
    "        if player == 0:  # P1\n",
    "            # Strategie adverse = melange des strategies de population P2\n",
    "            opponent_strategy = np.zeros(self.game.n_actions_p2)\n",
    "            for j, weight in enumerate(meta_nash):\n",
    "                opponent_strategy += weight * self.population_p2[j]\n",
    "            \n",
    "            return self.game.best_response(opponent_strategy, 0)\n",
    "        else:  # P2\n",
    "            opponent_strategy = np.zeros(self.game.n_actions_p1)\n",
    "            for i, weight in enumerate(meta_nash):\n",
    "                opponent_strategy += weight * self.population_p1[i]\n",
    "            \n",
    "            return self.game.best_response(opponent_strategy, 1)\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Une iteration de PSRO.\"\"\"\n",
    "        # Resoudre le meta-jeu\n",
    "        meta_nash_p1, meta_nash_p2 = self._solve_meta_nash()\n",
    "        \n",
    "        # Calculer les meilleures reponses\n",
    "        br_p1 = self._compute_best_response(meta_nash_p2, 0)\n",
    "        br_p2 = self._compute_best_response(meta_nash_p1, 1)\n",
    "        \n",
    "        # Ajouter a la population (si nouvelle)\n",
    "        if not any(np.allclose(br_p1, pi) for pi in self.population_p1):\n",
    "            self.population_p1.append(br_p1)\n",
    "        if not any(np.allclose(br_p2, pi) for pi in self.population_p2):\n",
    "            self.population_p2.append(br_p2)\n",
    "        \n",
    "        # Mettre a jour le meta-jeu\n",
    "        self._update_meta_game()\n",
    "        \n",
    "        # Calculer l'exploitabilite de la solution courante\n",
    "        final_strategy_p1 = np.zeros(self.game.n_actions_p1)\n",
    "        final_strategy_p2 = np.zeros(self.game.n_actions_p2)\n",
    "        \n",
    "        meta_nash_p1, meta_nash_p2 = self._solve_meta_nash()\n",
    "        \n",
    "        for i, weight in enumerate(meta_nash_p1):\n",
    "            final_strategy_p1 += weight * self.population_p1[i]\n",
    "        for j, weight in enumerate(meta_nash_p2):\n",
    "            final_strategy_p2 += weight * self.population_p2[j]\n",
    "        \n",
    "        # Enregistrer\n",
    "        self.history['population_size'].append(\n",
    "            (len(self.population_p1), len(self.population_p2))\n",
    "        )\n",
    "        self.history['exploitability'].append(\n",
    "            self.game.exploitability(final_strategy_p1, final_strategy_p2)\n",
    "        )\n",
    "        self.history['meta_nash_p1'].append(meta_nash_p1)\n",
    "        self.history['meta_nash_p2'].append(meta_nash_p2)\n",
    "    \n",
    "    def train(self, iterations: int):\n",
    "        for _ in range(iterations):\n",
    "            self.step()\n",
    "    \n",
    "    def get_final_strategy(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Retourne les strategies finales.\"\"\"\n",
    "        meta_nash_p1, meta_nash_p2 = self._solve_meta_nash()\n",
    "        \n",
    "        final_p1 = np.zeros(self.game.n_actions_p1)\n",
    "        final_p2 = np.zeros(self.game.n_actions_p2)\n",
    "        \n",
    "        for i, weight in enumerate(meta_nash_p1):\n",
    "            final_p1 += weight * self.population_p1[i]\n",
    "        for j, weight in enumerate(meta_nash_p2):\n",
    "            final_p2 += weight * self.population_p2[j]\n",
    "        \n",
    "        return final_p1, final_p2\n",
    "\n",
    "\n",
    "# Test PSRO\n",
    "print(\"PSRO sur Rock-Paper-Scissors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "psro = PSRO(rps)\n",
    "psro.train(10)\n",
    "\n",
    "final_p1, final_p2 = psro.get_final_strategy()\n",
    "print(f\"\\nTaille population: P1={len(psro.population_p1)}, P2={len(psro.population_p2)}\")\n",
    "print(f\"Strategie finale P1: {final_p1}\")\n",
    "print(f\"Strategie finale P2: {final_p2}\")\n",
    "print(f\"Exploitabilite finale: {psro.history['exploitability'][-1]:.6f}\")\n",
    "\n",
    "print(\"\\nPopulation P1:\")\n",
    "for i, pi in enumerate(psro.population_p1):\n",
    "    print(f\"  {i}: {pi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison finale de toutes les methodes\n",
    "print(\"Comparaison des methodes MARL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Reinitialiser et entrainer\n",
    "n_iter = 300\n",
    "\n",
    "sp_naive = NaiveSelfPlay(rps, learning_rate=0.3)\n",
    "sp_naive.train(n_iter)\n",
    "\n",
    "fp = FictitiousPlay(rps)\n",
    "fp.train(n_iter)\n",
    "\n",
    "nfsp = SimplifiedNFSP(rps, eta=0.1, rl_lr=0.05, sl_lr=0.01)\n",
    "nfsp.train(n_iter)\n",
    "\n",
    "psro = PSRO(rps)\n",
    "psro.train(15)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Exploitabilite\n",
    "ax1 = axes[0]\n",
    "ax1.plot(sp_naive.history['exploitability'], label='Self-Play Naif', alpha=0.7)\n",
    "ax1.plot(fp.history['exploitability'], label='Fictitious Play', alpha=0.7)\n",
    "ax1.plot(nfsp.history['exploitability'], label='NFSP', alpha=0.7)\n",
    "\n",
    "# PSRO a moins d'iterations, interpoler\n",
    "psro_x = np.linspace(0, n_iter, len(psro.history['exploitability']))\n",
    "ax1.plot(psro_x, psro.history['exploitability'], 'o-', label='PSRO', alpha=0.7, markersize=4)\n",
    "\n",
    "ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='Nash')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Exploitabilite')\n",
    "ax1.set_title('Convergence des algorithmes MARL')\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Strategies finales\n",
    "ax2 = axes[1]\n",
    "methods = ['Self-Play', 'Fictitious', 'NFSP', 'PSRO', 'Nash']\n",
    "strategies = [\n",
    "    sp_naive.strategy_p1,\n",
    "    fp.get_average_strategy(0),\n",
    "    nfsp.avg_strategy_p1,\n",
    "    psro.get_final_strategy()[0],\n",
    "    np.array([1/3, 1/3, 1/3])\n",
    "]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.25\n",
    "colors = ['red', 'green', 'blue']\n",
    "labels = ['Rock', 'Paper', 'Scissors']\n",
    "\n",
    "for i, (color, label) in enumerate(zip(colors, labels)):\n",
    "    values = [s[i] for s in strategies]\n",
    "    ax2.bar(x + (i - 1) * width, values, width, label=label, color=color, alpha=0.7)\n",
    "\n",
    "ax2.axhline(y=1/3, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(methods)\n",
    "ax2.set_ylabel('Probabilite')\n",
    "ax2.set_title('Strategies finales P1')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('marl_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Tableau recapitulatif\n",
    "print(\"\\nRecapitulatif:\")\n",
    "print(f\"{'Methode':<15} {'Exploitabilite':<15} {'Converge vers Nash?'}\")\n",
    "print(\"-\"*45)\n",
    "print(f\"{'Self-Play Naif':<15} {sp_naive.history['exploitability'][-1]:<15.4f} {'Non (cycles)'}\")\n",
    "print(f\"{'Fictitious Play':<15} {fp.history['exploitability'][-1]:<15.4f} {'Oui'}\")\n",
    "print(f\"{'NFSP':<15} {nfsp.history['exploitability'][-1]:<15.4f} {'Oui (approx)'}\")\n",
    "print(f\"{'PSRO':<15} {psro.history['exploitability'][-1]:<15.4f} {'Oui'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. OpenSpiel : MARL a Grande Echelle\n",
    "\n",
    "OpenSpiel fournit des implementations optimisees de ces algorithmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENSPIEL_AVAILABLE:\n",
    "    from open_spiel.python.algorithms import fictitious_play\n",
    "    from open_spiel.python.algorithms import exploitability as expl_module\n",
    "    from open_spiel.python import rl_environment\n",
    "\n",
    "    # Charger Kuhn Poker\n",
    "    game = pyspiel.load_game(\"kuhn_poker\")\n",
    "\n",
    "    print(\"Fictitious Play (OpenSpiel) sur Kuhn Poker\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # XFP = extensive-form fictitious play\n",
    "    xfp_solver = fictitious_play.XFPSolver(game)\n",
    "\n",
    "    exploitabilities = []\n",
    "    iterations = [0, 10, 50, 100, 200, 500, 1000]\n",
    "\n",
    "    current_iter = 0\n",
    "    for target in iterations:\n",
    "        while current_iter < target:\n",
    "            xfp_solver.iteration()\n",
    "            current_iter += 1\n",
    "\n",
    "        # L'API XFPSolver n'a pas de methode exploitability() directe\n",
    "        # On utilise le module exploitability avec average_policy()\n",
    "        avg_policy = xfp_solver.average_policy()\n",
    "        conv = expl_module.exploitability(game, avg_policy)\n",
    "        exploitabilities.append(conv)\n",
    "        print(f\"  Iter {target:>4}: exploitability = {conv:.6f}\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(iterations, exploitabilities, 'bo-', markersize=8)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Exploitabilite')\n",
    "    plt.title('XFP sur Kuhn Poker (OpenSpiel)')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xfp_kuhn_openspiel.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"OpenSpiel requis pour cette section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. AlphaZero : Self-Play + MCTS\n",
    "\n",
    "### 7.1 Architecture\n",
    "\n",
    "AlphaZero combine :\n",
    "1. **Reseau de neurones** $f_\\theta(s) \\to (p, v)$ : politique + valeur\n",
    "2. **Monte Carlo Tree Search (MCTS)** : recherche guidee par le reseau\n",
    "3. **Self-play** : generation de donnees d'entrainement\n",
    "\n",
    "### 7.2 Boucle d'entrainement\n",
    "\n",
    "```\n",
    "for iteration in range(N):\n",
    "    # 1. Self-play avec MCTS\n",
    "    games = []\n",
    "    for _ in range(M):\n",
    "        game = play_game_with_mcts(network)\n",
    "        games.append(game)\n",
    "    \n",
    "    # 2. Entrainer le reseau\n",
    "    for (state, mcts_policy, outcome) in sample_from_games(games):\n",
    "        loss = cross_entropy(network.policy(state), mcts_policy) + \n",
    "               mse(network.value(state), outcome)\n",
    "        network.update(loss)\n",
    "```\n",
    "\n",
    "### 7.3 Resultats\n",
    "\n",
    "- **AlphaGo Zero** : a battu AlphaGo (qui avait battu Lee Sedol) en partant de zero\n",
    "- **AlphaZero** : Generalisation a echecs et shogi, surpassant Stockfish/Elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema conceptuel d'AlphaZero\n",
    "print(\"Architecture AlphaZero (conceptuel)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "alphazero_diagram = \"\"\"\n",
    "+------------------+\n",
    "|   Self-Play      |<---------------------------+\n",
    "|   (MCTS guided   |                            |\n",
    "|    by network)   |                            |\n",
    "+--------+---------+                            |\n",
    "         |                                      |\n",
    "         | (s, pi_mcts, z)                      |\n",
    "         v                                      |\n",
    "+--------+---------+                            |\n",
    "|  Replay Buffer   |                            |\n",
    "+--------+---------+                            |\n",
    "         |                                      |\n",
    "         v                                      |\n",
    "+--------+---------+     +------------------+   |\n",
    "|   Neural Net     |     |      MCTS        |   |\n",
    "|   f(s) -> (p,v)  |---->|   (search tree)  |---+\n",
    "+------------------+     +------------------+\n",
    "\n",
    "Loss = CrossEntropy(p, pi_mcts) + MSE(v, z) + L2(weights)\n",
    "\"\"\"\n",
    "\n",
    "print(alphazero_diagram)\n",
    "\n",
    "print(\"\\nElements cles:\")\n",
    "print(\"  1. Reseau unique pour politique ET valeur\")\n",
    "print(\"  2. MCTS utilise le reseau pour guider la recherche\")\n",
    "print(\"  3. Self-play genere les donnees d'entrainement\")\n",
    "print(\"  4. Pas de connaissances a priori (tabula rasa)\")\n",
    "\n",
    "print(\"\\nAm√©liorations successives:\")\n",
    "print(\"  - AlphaGo (2016): supervision + RL + MCTS\")\n",
    "print(\"  - AlphaGo Zero (2017): self-play pur, un seul reseau\")\n",
    "print(\"  - AlphaZero (2018): generalisation multi-jeux\")\n",
    "print(\"  - MuZero (2020): modele appris de la dynamique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercices\n",
    "\n",
    "### Exercice 1 : Biased Rock-Paper-Scissors\n",
    "\n",
    "Testez les algorithmes sur un RPS biaise ou Rock bat Paper avec proba 0.6.\n",
    "\n",
    "### Exercice 2 : Matching Pennies\n",
    "\n",
    "Appliquez PSRO a Matching Pennies et analysez la population.\n",
    "\n",
    "### Exercice 3 : Independent Q-Learning\n",
    "\n",
    "Implementez Q-learning independant et montrez la non-stationnarite.\n",
    "\n",
    "### Exercice 4 : Win Rate tracking\n",
    "\n",
    "Modifiez NFSP pour tracker le win rate contre differents adversaires fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espace pour les exercices\n",
    "\n",
    "# Exercice 2 : PSRO sur Matching Pennies\n",
    "print(\"Exercice 2 : PSRO sur Matching Pennies\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "mp = create_matching_pennies()\n",
    "psro_mp = PSRO(mp)\n",
    "psro_mp.train(8)\n",
    "\n",
    "final_p1, final_p2 = psro_mp.get_final_strategy()\n",
    "print(f\"\\nStrategie finale P1: {final_p1}\")\n",
    "print(f\"Strategie finale P2: {final_p2}\")\n",
    "print(f\"Equilibre theorique: [0.5, 0.5]\")\n",
    "print(f\"Exploitabilite: {psro_mp.history['exploitability'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resume et Points Cles\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "1. **Defis MARL** : non-stationnarite, credit assignment, equilibres multiples\n",
    "2. **Self-play naif** : cycles, ne converge pas forcement\n",
    "3. **Fictitious Play** : converge dans les jeux a somme nulle\n",
    "4. **NFSP** : approximation neurale de Fictitious Play\n",
    "5. **PSRO** : population de strategies + meta-equilibre\n",
    "6. **AlphaZero** : MCTS + reseaux de neurones + self-play\n",
    "\n",
    "### Comparaison des methodes\n",
    "\n",
    "| Methode | Convergence | Complexite | Usage |\n",
    "|---------|-------------|------------|-------|\n",
    "| Self-play naif | Non garanti | Faible | Baseline |\n",
    "| Fictitious Play | Somme nulle | Faible | Theorie |\n",
    "| NFSP | Approximative | Moyenne | Poker |\n",
    "| PSRO | Oui | Elevee | General |\n",
    "| AlphaZero | Empirique | Tres elevee | Jeux parfaits |\n",
    "\n",
    "### Applications recentes\n",
    "\n",
    "- **Poker** : Libratus, Pluribus (CFR + NFSP)\n",
    "- **Go/Echecs** : AlphaZero, KataGo\n",
    "- **Strategos** : Diplomacy (Meta AI, 2022)\n",
    "- **Starcraft** : AlphaStar (DeepMind, 2019)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebooks suivants** : Serie Lean 4 pour les formalisations (Notebooks 16-19)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GameTheory WSL + OpenSpiel)",
   "language": "python",
   "name": "gametheory-wsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}