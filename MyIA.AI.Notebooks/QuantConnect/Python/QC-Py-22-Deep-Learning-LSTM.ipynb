{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC-Py-22 - Modern Time Series Deep Learning (SOTA 2024-2026)\n",
    "\n",
    "> **Architectures PyTorch SOTA pour la prediction de series temporelles financieres**\n",
    "> Duree: 100 minutes | Niveau: Avance | Python + PyTorch + QuantConnect\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous serez capable de :\n",
    "\n",
    "1. Comprendre l'**evolution des architectures** time series (LSTM -> Transformers -> SSMs)\n",
    "2. Maitriser **PyTorch** pour les series temporelles financieres\n",
    "3. Implementer **DLinear** comme baseline efficace (AAAI 2023)\n",
    "4. Utiliser **PatchTST** avec tokenization par patches (ICLR 2023)\n",
    "5. Appliquer **iTransformer** avec attention inversee (ICLR 2024 Spotlight)\n",
    "6. Experimenter **TimeMixer** sans attention (ICLR 2024)\n",
    "7. **Comparer** les architectures sur donnees financieres\n",
    "8. **Integrer** les modeles dans QuantConnect (ObjectStore, inference CPU)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Notebooks QC-Py-01 a 21 completes\n",
    "- Comprehension de base des reseaux de neurones\n",
    "- Familiarite avec PyTorch (tenseurs, modules)\n",
    "- numpy, pandas, sklearn\n",
    "\n",
    "## Structure du Notebook\n",
    "\n",
    "| Partie | Sujet | Duree |\n",
    "|--------|-------|-------|\n",
    "| 1 | Evolution des Architectures (2015-2026) | 15 min |\n",
    "| 2 | Setup PyTorch et Donnees | 15 min |\n",
    "| 3 | DLinear - Baseline MLP (AAAI 2023) | 15 min |\n",
    "| 4 | PatchTST - Patch-based Transformer (ICLR 2023) | 20 min |\n",
    "| 5 | iTransformer - Inverted Attention (ICLR 2024) | 15 min |\n",
    "| 6 | TimeMixer - MLP Multiscale (ICLR 2024) | 10 min |\n",
    "| 7 | Comparaison et Benchmarks | 15 min |\n",
    "| 8 | Integration QuantConnect | 15 min |\n",
    "\n",
    "---\n",
    "\n",
    "## References SOTA\n",
    "\n",
    "| Architecture | Paper | Conference | Code |\n",
    "|--------------|-------|------------|------|\n",
    "| **DLinear** | Are Transformers Effective for Time Series? | AAAI 2023 | [cure-lab/LTSF-Linear](https://github.com/cure-lab/LTSF-Linear) |\n",
    "| **PatchTST** | A Time Series is Worth 64 Words | ICLR 2023 | [yuqinie98/PatchTST](https://github.com/yuqinie98/PatchTST) |\n",
    "| **iTransformer** | Inverted Transformers Are Effective | ICLR 2024 Spotlight | [thuml/iTransformer](https://github.com/thuml/iTransformer) |\n",
    "| **TimeMixer** | TimeMixer: Decomposable Multiscale Mixing | ICLR 2024 | [kwuking/TimeMixer](https://github.com/kwuking/TimeMixer) |\n",
    "| **Time-Series-Library** | Framework unifie 20+ modeles | Tsinghua | [thuml/Time-Series-Library](https://github.com/thuml/Time-Series-Library) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 1 : Evolution des Architectures (2015-2026)\n",
    "\n",
    "### Timeline des Architectures Time Series\n",
    "\n",
    "```\n",
    "2015-2017: RNN/LSTM Era\n",
    "    - LSTM, GRU dominant\n",
    "    - Vanishing gradient problem\n",
    "    - Sequential processing (slow)\n",
    "\n",
    "2017-2022: Transformer Era\n",
    "    - Attention mechanisms\n",
    "    - Parallel processing\n",
    "    - O(n^2) complexity problem\n",
    "\n",
    "2023: \"Are Transformers Effective?\" Moment\n",
    "    - DLinear surpasse les Transformers complexes!\n",
    "    - Remise en question des architectures\n",
    "    - Focus sur simplicite et efficacite\n",
    "\n",
    "2023-2024: Patch-based & Inverted Attention\n",
    "    - PatchTST: tokenization intelligente\n",
    "    - iTransformer: attention sur variables\n",
    "    - TimeMixer: MLP multiscale\n",
    "\n",
    "2024-2026: State Space Models (Mamba)\n",
    "    - Complexite O(n) au lieu de O(n^2)\n",
    "    - Voir notebook QC-Py-23\n",
    "```\n",
    "\n",
    "### Le Paradoxe DLinear (AAAI 2023)\n",
    "\n",
    "Le paper \"Are Transformers Effective for Time Series Forecasting?\" a demontre que:\n",
    "\n",
    "| Modele | MSE (ETTh1) | Complexite | Parametres |\n",
    "|--------|-------------|------------|------------|\n",
    "| Informer | 0.865 | O(n log n) | 11M |\n",
    "| Autoformer | 0.449 | O(n^2) | 10M |\n",
    "| FEDformer | 0.376 | O(n) | 8M |\n",
    "| **DLinear** | **0.375** | **O(1)** | **~10K** |\n",
    "\n",
    "**Conclusion**: La simplicite peut battre la complexite!\n",
    "\n",
    "### Pourquoi les Transformers classiques echouent?\n",
    "\n",
    "1. **Permutation invariance**: L'attention standard ignore l'ordre temporel\n",
    "2. **Point-wise attention**: Chaque timestep = 1 token (trop granulaire)\n",
    "3. **Overfitting**: Trop de parametres pour les series financieres\n",
    "4. **Computational cost**: O(n^2) prohibitif pour longues sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions SOTA 2024-2026\n",
    "\n",
    "| Architecture | Innovation | Avantage |\n",
    "|--------------|------------|----------|\n",
    "| **DLinear** | Decomposition + Linear | Ultra-simple, baseline forte |\n",
    "| **PatchTST** | Patches au lieu de points | Capture patterns locaux |\n",
    "| **iTransformer** | Attention sur variables | Capture correlations inter-series |\n",
    "| **TimeMixer** | Mixing multiscale sans attention | Efficace, pas d'attention |\n",
    "| **Mamba/SSMs** | State Space Models | O(n), long context (voir QC-Py-23) |\n",
    "\n",
    "### Architecture Comparison\n",
    "\n",
    "```\n",
    "LSTM (2015):          Transformer (2017):       PatchTST (2023):\n",
    "x1 -> h1 -> ...       [x1,x2,...,xn]           [patch1, patch2, ...]\n",
    "Sequential            Full Attention O(n^2)    Patch Attention\n",
    "\n",
    "iTransformer (2024):  TimeMixer (2024):        Mamba (2024):\n",
    "[var1,var2,...,varm]  Multiscale MLP           State Space\n",
    "Variable Attention    No Attention             O(n) Selective\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 2 : Setup PyTorch et Donnees (15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports standards\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imports de base reussis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Configuration device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Reproductibilite\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn pour preprocessing et metriques\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "print(\"Sklearn importe avec succes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation de donnees financieres simulees\n",
    "\n",
    "def generate_financial_data(n_days=1000, n_features=7, seed=42):\n",
    "    \"\"\"\n",
    "    Genere des donnees financieres multi-variees simulees.\n",
    "    \n",
    "    Features:\n",
    "    - close: Prix de cloture\n",
    "    - volume: Volume normalise\n",
    "    - returns: Rendements journaliers\n",
    "    - sma_20: SMA 20 jours\n",
    "    - sma_50: SMA 50 jours\n",
    "    - rsi: RSI 14 jours\n",
    "    - volatility: Volatilite 20 jours\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame avec features, target = close\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Dates\n",
    "    dates = pd.date_range(start='2019-01-01', periods=n_days, freq='B')\n",
    "    \n",
    "    # Prix avec tendance + cycles + bruit\n",
    "    trend = np.linspace(100, 180, n_days)\n",
    "    cycle1 = 15 * np.sin(np.linspace(0, 10 * np.pi, n_days))\n",
    "    cycle2 = 8 * np.sin(np.linspace(0, 40 * np.pi, n_days))\n",
    "    noise = np.cumsum(np.random.randn(n_days) * 0.7)\n",
    "    \n",
    "    close = trend + cycle1 + cycle2 + noise\n",
    "    close = np.maximum(close, 50)\n",
    "    \n",
    "    # Rendements\n",
    "    returns = np.diff(close, prepend=close[0]) / np.maximum(close, 1)\n",
    "    \n",
    "    # Volume (correle negativement avec le prix pour simuler)\n",
    "    volume = 1_000_000 * (1 + np.random.exponential(0.3, n_days))\n",
    "    volume = volume * (1 - 0.3 * (close - close.mean()) / close.std())\n",
    "    \n",
    "    # SMA\n",
    "    sma_20 = pd.Series(close).rolling(20).mean().fillna(method='bfill').values\n",
    "    sma_50 = pd.Series(close).rolling(50).mean().fillna(method='bfill').values\n",
    "    \n",
    "    # RSI\n",
    "    delta = pd.Series(close).diff()\n",
    "    gain = delta.clip(lower=0).rolling(14).mean()\n",
    "    loss = (-delta.clip(upper=0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    rsi = (100 - (100 / (1 + rs))).fillna(50).values\n",
    "    \n",
    "    # Volatilite\n",
    "    volatility = pd.Series(returns).rolling(20).std().fillna(method='bfill').values * np.sqrt(252)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'close': close,\n",
    "        'volume': volume,\n",
    "        'returns': returns,\n",
    "        'sma_20': sma_20,\n",
    "        'sma_50': sma_50,\n",
    "        'rsi': rsi,\n",
    "        'volatility': volatility\n",
    "    }, index=dates)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generer les donnees\n",
    "df = generate_financial_data(n_days=1000)\n",
    "\n",
    "print(f\"Donnees generees: {len(df)} jours\")\n",
    "print(f\"Features: {list(df.columns)}\")\n",
    "print(f\"Periode: {df.index[0].date()} a {df.index[-1].date()}\")\n",
    "print(f\"\\nApercu:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des donnees\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Prix et SMAs\n",
    "ax1 = axes[0]\n",
    "ax1.plot(df.index, df['close'], 'b-', linewidth=1.5, label='Close', alpha=0.8)\n",
    "ax1.plot(df.index, df['sma_20'], 'orange', linewidth=1, label='SMA 20', alpha=0.7)\n",
    "ax1.plot(df.index, df['sma_50'], 'green', linewidth=1, label='SMA 50', alpha=0.7)\n",
    "ax1.set_ylabel('Prix')\n",
    "ax1.set_title('Prix et Moyennes Mobiles', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# RSI\n",
    "ax2 = axes[1]\n",
    "ax2.plot(df.index, df['rsi'], 'purple', linewidth=1)\n",
    "ax2.axhline(70, color='red', linestyle='--', alpha=0.5)\n",
    "ax2.axhline(30, color='green', linestyle='--', alpha=0.5)\n",
    "ax2.fill_between(df.index, 30, 70, alpha=0.1, color='gray')\n",
    "ax2.set_ylabel('RSI')\n",
    "ax2.set_title('Relative Strength Index', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Volatilite\n",
    "ax3 = axes[2]\n",
    "ax3.fill_between(df.index, 0, df['volatility'] * 100, alpha=0.5, color='steelblue')\n",
    "ax3.set_ylabel('Volatilite Annualisee (%)')\n",
    "ax3.set_xlabel('Date')\n",
    "ax3.set_title('Volatilite Realisee (20 jours)', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset PyTorch pour Time Series\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset PyTorch pour time series forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.array\n",
    "        Donnees normalisees [n_samples, n_features]\n",
    "    seq_len : int\n",
    "        Longueur de la sequence d'entree (lookback)\n",
    "    pred_len : int\n",
    "        Longueur de la prediction (forecast horizon)\n",
    "    target_idx : int\n",
    "        Index de la feature cible (default: 0 = close)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, seq_len=96, pred_len=24, target_idx=0):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.target_idx = target_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - self.pred_len + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: [seq_len, n_features]\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        \n",
    "        # Target: [pred_len] (uniquement la feature cible)\n",
    "        y = self.data[idx + self.seq_len:idx + self.seq_len + self.pred_len, self.target_idx]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "print(\"TimeSeriesDataset defini\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  dataset = TimeSeriesDataset(data, seq_len=96, pred_len=24)\")\n",
    "print(\"  x, y = dataset[0]\")\n",
    "print(\"  -> x shape: [seq_len, n_features]\")\n",
    "print(\"  -> y shape: [pred_len]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparer les donnees\n",
    "\n",
    "# Parametres\n",
    "SEQ_LEN = 96      # ~4 mois de trading days\n",
    "PRED_LEN = 24     # ~1 mois de prediction\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df.values)\n",
    "\n",
    "print(f\"Donnees normalisees: {data_scaled.shape}\")\n",
    "\n",
    "# Split Train/Val/Test (70/15/15)\n",
    "n = len(data_scaled)\n",
    "train_end = int(n * 0.7)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "train_data = data_scaled[:train_end]\n",
    "val_data = data_scaled[train_end:val_end]\n",
    "test_data = data_scaled[val_end:]\n",
    "\n",
    "print(f\"\\nSplit:\")\n",
    "print(f\"  Train: {len(train_data)} samples\")\n",
    "print(f\"  Val:   {len(val_data)} samples\")\n",
    "print(f\"  Test:  {len(test_data)} samples\")\n",
    "\n",
    "# Creer les datasets\n",
    "train_dataset = TimeSeriesDataset(train_data, seq_len=SEQ_LEN, pred_len=PRED_LEN)\n",
    "val_dataset = TimeSeriesDataset(val_data, seq_len=SEQ_LEN, pred_len=PRED_LEN)\n",
    "test_dataset = TimeSeriesDataset(test_data, seq_len=SEQ_LEN, pred_len=PRED_LEN)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataLoaders crees:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")\n",
    "\n",
    "# Verifier les shapes\n",
    "x_sample, y_sample = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  x shape: {x_sample.shape}  [batch, seq_len, features]\")\n",
    "print(f\"  y shape: {y_sample.shape}  [batch, pred_len]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 3 : DLinear - Baseline MLP (AAAI 2023)\n",
    "\n",
    "### Concept\n",
    "\n",
    "DLinear decompose la serie en **tendance** + **saisonnalite** puis applique des couches lineaires separees:\n",
    "\n",
    "```\n",
    "Input [B, L, C]\n",
    "      |\n",
    "      v\n",
    "+-------------+\n",
    "| Decompose   | -> Trend + Seasonal\n",
    "+-------------+\n",
    "      |    \\\n",
    "      v     v\n",
    "  Linear  Linear\n",
    "      |     |\n",
    "      +--+--+\n",
    "         |\n",
    "         v\n",
    "   Output [B, H, C]\n",
    "```\n",
    "\n",
    "### Pourquoi ca marche?\n",
    "\n",
    "1. **Decomposition**: Separe les patterns long-terme (trend) et court-terme (seasonal)\n",
    "2. **Linearite**: Les series financieres ont souvent des relations quasi-lineaires\n",
    "3. **Simplicite**: Moins de parametres = moins d'overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation DLinear\n",
    "\n",
    "class MovingAvg(nn.Module):\n",
    "    \"\"\"Moyenne mobile pour decomposition.\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, L, C]\n",
    "        # Padding pour garder la meme longueur\n",
    "        front = x[:, :1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        \n",
    "        # AvgPool attend [B, C, L]\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeriesDecomposition(nn.Module):\n",
    "    \"\"\"Decomposition en Trend + Seasonal.\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Trend = moyenne mobile\n",
    "        trend = self.moving_avg(x)\n",
    "        # Seasonal = residuel\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend\n",
    "\n",
    "\n",
    "class DLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    DLinear: Decomposition + Linear\n",
    "    \n",
    "    Paper: \"Are Transformers Effective for Time Series Forecasting?\" (AAAI 2023)\n",
    "    Code: https://github.com/cure-lab/LTSF-Linear\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seq_len : int\n",
    "        Longueur de la sequence d'entree\n",
    "    pred_len : int\n",
    "        Longueur de la prediction\n",
    "    enc_in : int\n",
    "        Nombre de features d'entree\n",
    "    individual : bool\n",
    "        True pour un modele par feature (recommande)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual=True):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in = enc_in\n",
    "        self.individual = individual\n",
    "        \n",
    "        # Decomposition\n",
    "        kernel_size = 25  # Fenetre pour moyenne mobile\n",
    "        self.decomposition = SeriesDecomposition(kernel_size)\n",
    "        \n",
    "        if individual:\n",
    "            # Un modele lineaire par feature\n",
    "            self.Linear_Seasonal = nn.ModuleList([\n",
    "                nn.Linear(seq_len, pred_len) for _ in range(enc_in)\n",
    "            ])\n",
    "            self.Linear_Trend = nn.ModuleList([\n",
    "                nn.Linear(seq_len, pred_len) for _ in range(enc_in)\n",
    "            ])\n",
    "        else:\n",
    "            # Un seul modele partage\n",
    "            self.Linear_Seasonal = nn.Linear(seq_len, pred_len)\n",
    "            self.Linear_Trend = nn.Linear(seq_len, pred_len)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, L, C]\n",
    "        seasonal, trend = self.decomposition(x)\n",
    "        \n",
    "        # Permute pour Linear: [B, C, L]\n",
    "        seasonal = seasonal.permute(0, 2, 1)\n",
    "        trend = trend.permute(0, 2, 1)\n",
    "        \n",
    "        if self.individual:\n",
    "            seasonal_output = torch.zeros(\n",
    "                [x.size(0), self.enc_in, self.pred_len], \n",
    "                device=x.device\n",
    "            )\n",
    "            trend_output = torch.zeros(\n",
    "                [x.size(0), self.enc_in, self.pred_len], \n",
    "                device=x.device\n",
    "            )\n",
    "            \n",
    "            for i in range(self.enc_in):\n",
    "                seasonal_output[:, i, :] = self.Linear_Seasonal[i](seasonal[:, i, :])\n",
    "                trend_output[:, i, :] = self.Linear_Trend[i](trend[:, i, :])\n",
    "        else:\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal)\n",
    "            trend_output = self.Linear_Trend(trend)\n",
    "        \n",
    "        # Combiner et permuter: [B, H, C]\n",
    "        output = seasonal_output + trend_output\n",
    "        output = output.permute(0, 2, 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Instancier le modele\n",
    "n_features = df.shape[1]\n",
    "model_dlinear = DLinear(\n",
    "    seq_len=SEQ_LEN,\n",
    "    pred_len=PRED_LEN,\n",
    "    enc_in=n_features,\n",
    "    individual=True\n",
    ").to(device)\n",
    "\n",
    "# Compter les parametres\n",
    "n_params = sum(p.numel() for p in model_dlinear.parameters())\n",
    "\n",
    "print(\"DLinear Model:\")\n",
    "print(f\"  Input:  [batch, {SEQ_LEN}, {n_features}]\")\n",
    "print(f\"  Output: [batch, {PRED_LEN}, {n_features}]\")\n",
    "print(f\"  Parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions d'entrainement et evaluation\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Entraine le modele pour une epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        output = model(x)  # [B, H, C]\n",
    "        \n",
    "        # On predit uniquement la premiere feature (close)\n",
    "        pred = output[:, :, 0]  # [B, H]\n",
    "        \n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evalue le modele.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            output = model(x)\n",
    "            pred = output[:, :, 0]\n",
    "            \n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds.append(pred.cpu().numpy())\n",
    "            targets.append(y.cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    \n",
    "    # Metriques\n",
    "    mse = mean_squared_error(targets.flatten(), preds.flatten())\n",
    "    mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
    "    \n",
    "    return total_loss / len(loader), mse, mae, preds, targets\n",
    "\n",
    "\n",
    "print(\"Fonctions d'entrainement definies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainer DLinear\n",
    "\n",
    "# Hyperparametres\n",
    "EPOCHS = 30\n",
    "LR = 0.001\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_dlinear.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTRAINEMENT DLINEAR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model_dlinear, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_mse, val_mae, _, _ = evaluate(model_dlinear, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model_dlinear.state_dict(), 'dlinear_best.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(f\"\\nMeilleur Val Loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation sur test set\n",
    "\n",
    "model_dlinear.load_state_dict(torch.load('dlinear_best.pt'))\n",
    "test_loss, test_mse, test_mae, preds_dlinear, targets_dlinear = evaluate(\n",
    "    model_dlinear, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DLINEAR - RESULTATS TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  MSE:  {test_mse:.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(test_mse):.6f}\")\n",
    "print(f\"  MAE:  {test_mae:.6f}\")\n",
    "\n",
    "# Direction accuracy\n",
    "dir_true = np.sign(np.diff(targets_dlinear.flatten()))\n",
    "dir_pred = np.sign(np.diff(preds_dlinear.flatten()))\n",
    "dir_acc = np.mean(dir_true == dir_pred)\n",
    "print(f\"  Direction Accuracy: {dir_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 4 : PatchTST - Patch-based Transformer (ICLR 2023)\n",
    "\n",
    "### Concept\n",
    "\n",
    "PatchTST traite la serie comme une **sequence de patches** au lieu de points individuels:\n",
    "\n",
    "```\n",
    "Input:  [x1, x2, x3, x4, x5, x6, x7, x8, ...]\n",
    "                 |\n",
    "                 v (Patching)\n",
    "Patches: [patch1, patch2, patch3, ...]\n",
    "         [x1-x4]  [x5-x8]  [...]\n",
    "                 |\n",
    "                 v\n",
    "Transformer Encoder\n",
    "                 |\n",
    "                 v\n",
    "Linear Head -> Prediction\n",
    "```\n",
    "\n",
    "### Avantages\n",
    "\n",
    "1. **Reduction de sequence**: n/patch_len tokens au lieu de n\n",
    "2. **Semantic locale**: Chaque patch capture un pattern local\n",
    "3. **Moins de compute**: O((n/p)^2) au lieu de O(n^2)\n",
    "4. **Meilleure generalisation**: Vision-like tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation PatchTST simplifiee\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Embedding de patches pour time series.\"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len, patch_len, stride, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.n_patches = (seq_len - patch_len) // stride + 1\n",
    "        \n",
    "        # Projection lineaire du patch vers d_model\n",
    "        self.projection = nn.Linear(patch_len, d_model)\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.n_patches, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, L, C] -> extraire patches\n",
    "        B, L, C = x.shape\n",
    "        \n",
    "        # Unfold pour extraire les patches\n",
    "        # [B, L, C] -> [B, C, L] pour unfold\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Extraire patches: [B, C, n_patches, patch_len]\n",
    "        patches = x.unfold(dimension=2, size=self.patch_len, step=self.stride)\n",
    "        \n",
    "        # Reshape: [B, C, n_patches, patch_len] -> [B*C, n_patches, patch_len]\n",
    "        B, C, N, P = patches.shape\n",
    "        patches = patches.reshape(B * C, N, P)\n",
    "        \n",
    "        # Projection: [B*C, n_patches, d_model]\n",
    "        embeddings = self.projection(patches)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        embeddings = embeddings + self.pos_embedding\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings, C  # Retourne aussi n_channels\n",
    "\n",
    "\n",
    "class PatchTST(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchTST: A Time Series is Worth 64 Words\n",
    "    \n",
    "    Paper: ICLR 2023\n",
    "    Code: https://github.com/yuqinie98/PatchTST\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seq_len : int\n",
    "        Longueur de la sequence d'entree\n",
    "    pred_len : int\n",
    "        Longueur de la prediction\n",
    "    enc_in : int\n",
    "        Nombre de features\n",
    "    d_model : int\n",
    "        Dimension du modele\n",
    "    n_heads : int\n",
    "        Nombre de heads d'attention\n",
    "    n_layers : int\n",
    "        Nombre de couches Transformer\n",
    "    patch_len : int\n",
    "        Longueur d'un patch\n",
    "    stride : int\n",
    "        Pas entre patches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len, enc_in, d_model=64, n_heads=4, \n",
    "                 n_layers=2, patch_len=16, stride=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in = enc_in\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            seq_len, patch_len, stride, d_model, dropout\n",
    "        )\n",
    "        n_patches = self.patch_embedding.n_patches\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Prediction head (flatten + linear)\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.head = nn.Linear(n_patches * d_model, pred_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [B, L, C]\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding: [B*C, n_patches, d_model]\n",
    "        embeddings, n_channels = self.patch_embedding(x)\n",
    "        \n",
    "        # Transformer: [B*C, n_patches, d_model]\n",
    "        encoded = self.transformer_encoder(embeddings)\n",
    "        \n",
    "        # Flatten: [B*C, n_patches * d_model]\n",
    "        flat = self.flatten(encoded)\n",
    "        \n",
    "        # Prediction: [B*C, pred_len]\n",
    "        pred = self.head(flat)\n",
    "        \n",
    "        # Reshape: [B, C, pred_len] -> [B, pred_len, C]\n",
    "        pred = pred.reshape(B, n_channels, self.pred_len)\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "# Instancier PatchTST\n",
    "model_patchtst = PatchTST(\n",
    "    seq_len=SEQ_LEN,\n",
    "    pred_len=PRED_LEN,\n",
    "    enc_in=n_features,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    patch_len=16,\n",
    "    stride=8\n",
    ").to(device)\n",
    "\n",
    "n_params_patchtst = sum(p.numel() for p in model_patchtst.parameters())\n",
    "\n",
    "print(\"PatchTST Model:\")\n",
    "print(f\"  Input:  [batch, {SEQ_LEN}, {n_features}]\")\n",
    "print(f\"  Output: [batch, {PRED_LEN}, {n_features}]\")\n",
    "print(f\"  Parameters: {n_params_patchtst:,}\")\n",
    "print(f\"  Patches: {model_patchtst.patch_embedding.n_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainer PatchTST\n",
    "\n",
    "optimizer_patchtst = optim.Adam(model_patchtst.parameters(), lr=LR)\n",
    "scheduler_patchtst = optim.lr_scheduler.ReduceLROnPlateau(optimizer_patchtst, patience=5, factor=0.5)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTRAINEMENT PATCHTST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_loss_patchtst = float('inf')\n",
    "train_losses_patchtst, val_losses_patchtst = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model_patchtst, train_loader, criterion, optimizer_patchtst, device)\n",
    "    val_loss, val_mse, val_mae, _, _ = evaluate(model_patchtst, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses_patchtst.append(train_loss)\n",
    "    val_losses_patchtst.append(val_loss)\n",
    "    \n",
    "    scheduler_patchtst.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss_patchtst:\n",
    "        best_val_loss_patchtst = val_loss\n",
    "        torch.save(model_patchtst.state_dict(), 'patchtst_best.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(f\"\\nMeilleur Val Loss: {best_val_loss_patchtst:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation PatchTST\n",
    "\n",
    "model_patchtst.load_state_dict(torch.load('patchtst_best.pt'))\n",
    "test_loss_patchtst, test_mse_patchtst, test_mae_patchtst, preds_patchtst, targets_patchtst = evaluate(\n",
    "    model_patchtst, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PATCHTST - RESULTATS TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  MSE:  {test_mse_patchtst:.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(test_mse_patchtst):.6f}\")\n",
    "print(f\"  MAE:  {test_mae_patchtst:.6f}\")\n",
    "\n",
    "dir_true_patchtst = np.sign(np.diff(targets_patchtst.flatten()))\n",
    "dir_pred_patchtst = np.sign(np.diff(preds_patchtst.flatten()))\n",
    "dir_acc_patchtst = np.mean(dir_true_patchtst == dir_pred_patchtst)\n",
    "print(f\"  Direction Accuracy: {dir_acc_patchtst:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 5 : iTransformer - Inverted Attention (ICLR 2024 Spotlight)\n",
    "\n",
    "### Concept\n",
    "\n",
    "iTransformer inverse l'approche: au lieu d'attention sur les timesteps, on fait attention sur les **variables**:\n",
    "\n",
    "```\n",
    "Standard Transformer:    iTransformer:\n",
    "                         \n",
    "Variables  Variables     Variables  Variables\n",
    "    |          |             |          |\n",
    "    v          v             v          v\n",
    "[t1, t2, t3, t4]         [t1-t4]    [t1-t4]   <- Each var = 1 token\n",
    "    |                        |          |\n",
    "    v                        +----+-----+\n",
    "Attention(time)                   |\n",
    "                                  v\n",
    "                          Attention(vars)\n",
    "```\n",
    "\n",
    "### Avantages\n",
    "\n",
    "1. **Correlations inter-variables**: Capture les relations entre features\n",
    "2. **Embedding riche**: Chaque variable embede sa serie complete\n",
    "3. **Scalabilite**: O(n_vars^2) au lieu de O(n_time^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation iTransformer simplifiee\n",
    "\n",
    "class iTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    iTransformer: Inverted Transformers Are Effective for Time Series Forecasting\n",
    "    \n",
    "    Paper: ICLR 2024 Spotlight\n",
    "    Code: https://github.com/thuml/iTransformer\n",
    "    \n",
    "    Key insight: Treat each variable as a token, apply attention across variables.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seq_len : int\n",
    "        Longueur de la sequence d'entree\n",
    "    pred_len : int\n",
    "        Longueur de la prediction\n",
    "    enc_in : int\n",
    "        Nombre de variables (features)\n",
    "    d_model : int\n",
    "        Dimension du modele\n",
    "    n_heads : int\n",
    "        Nombre de heads d'attention\n",
    "    n_layers : int\n",
    "        Nombre de couches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len, enc_in, d_model=128, n_heads=4,\n",
    "                 n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in = enc_in\n",
    "        \n",
    "        # Embedding: projette chaque variable (sequence entiere) vers d_model\n",
    "        self.embedding = nn.Linear(seq_len, d_model)\n",
    "        \n",
    "        # Transformer encoder (attention sur les variables)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Projection vers prediction\n",
    "        self.projection = nn.Linear(d_model, pred_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [B, L, C] -> invert to [B, C, L]\n",
    "        x = x.permute(0, 2, 1)  # [B, C, L]\n",
    "        \n",
    "        # Embed each variable: [B, C, d_model]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer attention across variables: [B, C, d_model]\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # Project to prediction: [B, C, pred_len]\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # Permute back: [B, pred_len, C]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Instancier iTransformer\n",
    "model_itransformer = iTransformer(\n",
    "    seq_len=SEQ_LEN,\n",
    "    pred_len=PRED_LEN,\n",
    "    enc_in=n_features,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=2\n",
    ").to(device)\n",
    "\n",
    "n_params_itrans = sum(p.numel() for p in model_itransformer.parameters())\n",
    "\n",
    "print(\"iTransformer Model:\")\n",
    "print(f\"  Input:  [batch, {SEQ_LEN}, {n_features}]\")\n",
    "print(f\"  Output: [batch, {PRED_LEN}, {n_features}]\")\n",
    "print(f\"  Parameters: {n_params_itrans:,}\")\n",
    "print(f\"  Attention: across {n_features} variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainer iTransformer\n",
    "\n",
    "optimizer_itrans = optim.Adam(model_itransformer.parameters(), lr=LR)\n",
    "scheduler_itrans = optim.lr_scheduler.ReduceLROnPlateau(optimizer_itrans, patience=5, factor=0.5)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTRAINEMENT iTRANSFORMER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_loss_itrans = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model_itransformer, train_loader, criterion, optimizer_itrans, device)\n",
    "    val_loss, val_mse, val_mae, _, _ = evaluate(model_itransformer, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler_itrans.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss_itrans:\n",
    "        best_val_loss_itrans = val_loss\n",
    "        torch.save(model_itransformer.state_dict(), 'itransformer_best.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(f\"\\nMeilleur Val Loss: {best_val_loss_itrans:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation iTransformer\n",
    "\n",
    "model_itransformer.load_state_dict(torch.load('itransformer_best.pt'))\n",
    "test_loss_itrans, test_mse_itrans, test_mae_itrans, preds_itrans, targets_itrans = evaluate(\n",
    "    model_itransformer, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"iTRANSFORMER - RESULTATS TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  MSE:  {test_mse_itrans:.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(test_mse_itrans):.6f}\")\n",
    "print(f\"  MAE:  {test_mae_itrans:.6f}\")\n",
    "\n",
    "dir_true_itrans = np.sign(np.diff(targets_itrans.flatten()))\n",
    "dir_pred_itrans = np.sign(np.diff(preds_itrans.flatten()))\n",
    "dir_acc_itrans = np.mean(dir_true_itrans == dir_pred_itrans)\n",
    "print(f\"  Direction Accuracy: {dir_acc_itrans:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 6 : TimeMixer - MLP Multiscale (ICLR 2024)\n",
    "\n",
    "### Concept\n",
    "\n",
    "TimeMixer n'utilise **pas d'attention** mais un **mixing MLP multiscale**:\n",
    "\n",
    "```\n",
    "Input [B, L, C]\n",
    "      |\n",
    "      v\n",
    "+-----------------+\n",
    "| Multi-scale     |\n",
    "| Decomposition   | -> [scale1, scale2, scale3, ...]\n",
    "+-----------------+\n",
    "      |\n",
    "      v\n",
    "+-----------------+\n",
    "| Past-Decomp     |  MLP mixing across scales\n",
    "| Mixing          |\n",
    "+-----------------+\n",
    "      |\n",
    "      v\n",
    "+-----------------+\n",
    "| Future-Multipredictor |\n",
    "| Mixing          |\n",
    "+-----------------+\n",
    "      |\n",
    "      v\n",
    "Output [B, H, C]\n",
    "```\n",
    "\n",
    "### Avantages\n",
    "\n",
    "1. **Pas d'attention**: Plus simple, plus rapide\n",
    "2. **Multiscale**: Capture patterns a differentes echelles\n",
    "3. **Efficace**: SOTA sur plusieurs benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation TimeMixer simplifiee\n",
    "\n",
    "class TimeMixer(nn.Module):\n",
    "    \"\"\"\n",
    "    TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting\n",
    "    \n",
    "    Paper: ICLR 2024\n",
    "    Code: https://github.com/kwuking/TimeMixer\n",
    "    \n",
    "    Simplified version focusing on multiscale mixing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seq_len : int\n",
    "        Longueur de la sequence d'entree\n",
    "    pred_len : int\n",
    "        Longueur de la prediction\n",
    "    enc_in : int\n",
    "        Nombre de features\n",
    "    d_model : int\n",
    "        Dimension du modele\n",
    "    n_scales : int\n",
    "        Nombre d'echelles pour decomposition\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len, enc_in, d_model=64, n_scales=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in = enc_in\n",
    "        self.n_scales = n_scales\n",
    "        \n",
    "        # Downsampling pour chaque echelle\n",
    "        self.downsamples = nn.ModuleList([\n",
    "            nn.AvgPool1d(kernel_size=2**i, stride=2**i) if i > 0 else nn.Identity()\n",
    "            for i in range(n_scales)\n",
    "        ])\n",
    "        \n",
    "        # Calcul des longueurs a chaque echelle\n",
    "        self.scale_lens = [seq_len // (2**i) for i in range(n_scales)]\n",
    "        \n",
    "        # Mixing layers (MLP pour chaque echelle)\n",
    "        self.mixing_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(sl, d_model),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model, d_model)\n",
    "            )\n",
    "            for sl in self.scale_lens\n",
    "        ])\n",
    "        \n",
    "        # Scale aggregation\n",
    "        self.scale_weights = nn.Parameter(torch.ones(n_scales) / n_scales)\n",
    "        \n",
    "        # Prediction head\n",
    "        self.head = nn.Linear(d_model, pred_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [B, L, C]\n",
    "        B, L, C = x.shape\n",
    "        \n",
    "        # Process each variable independently\n",
    "        x = x.permute(0, 2, 1)  # [B, C, L]\n",
    "        \n",
    "        # Multiscale representations\n",
    "        scale_outputs = []\n",
    "        \n",
    "        for i, (downsample, mixing) in enumerate(zip(self.downsamples, self.mixing_layers)):\n",
    "            # Downsample: [B, C, L_i]\n",
    "            x_scale = downsample(x)\n",
    "            \n",
    "            # Mix: [B, C, d_model]\n",
    "            x_mixed = mixing(x_scale)\n",
    "            \n",
    "            scale_outputs.append(x_mixed)\n",
    "        \n",
    "        # Weighted aggregation: [B, C, d_model]\n",
    "        weights = torch.softmax(self.scale_weights, dim=0)\n",
    "        aggregated = sum(w * out for w, out in zip(weights, scale_outputs))\n",
    "        \n",
    "        # Prediction: [B, C, pred_len]\n",
    "        pred = self.head(aggregated)\n",
    "        \n",
    "        # Output: [B, pred_len, C]\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "# Instancier TimeMixer\n",
    "model_timemixer = TimeMixer(\n",
    "    seq_len=SEQ_LEN,\n",
    "    pred_len=PRED_LEN,\n",
    "    enc_in=n_features,\n",
    "    d_model=64,\n",
    "    n_scales=3\n",
    ").to(device)\n",
    "\n",
    "n_params_mixer = sum(p.numel() for p in model_timemixer.parameters())\n",
    "\n",
    "print(\"TimeMixer Model:\")\n",
    "print(f\"  Input:  [batch, {SEQ_LEN}, {n_features}]\")\n",
    "print(f\"  Output: [batch, {PRED_LEN}, {n_features}]\")\n",
    "print(f\"  Parameters: {n_params_mixer:,}\")\n",
    "print(f\"  Scales: {model_timemixer.scale_lens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainer TimeMixer\n",
    "\n",
    "optimizer_mixer = optim.Adam(model_timemixer.parameters(), lr=LR)\n",
    "scheduler_mixer = optim.lr_scheduler.ReduceLROnPlateau(optimizer_mixer, patience=5, factor=0.5)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTRAINEMENT TIMEMIXER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_loss_mixer = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model_timemixer, train_loader, criterion, optimizer_mixer, device)\n",
    "    val_loss, val_mse, val_mae, _, _ = evaluate(model_timemixer, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler_mixer.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss_mixer:\n",
    "        best_val_loss_mixer = val_loss\n",
    "        torch.save(model_timemixer.state_dict(), 'timemixer_best.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(f\"\\nMeilleur Val Loss: {best_val_loss_mixer:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation TimeMixer\n",
    "\n",
    "model_timemixer.load_state_dict(torch.load('timemixer_best.pt'))\n",
    "test_loss_mixer, test_mse_mixer, test_mae_mixer, preds_mixer, targets_mixer = evaluate(\n",
    "    model_timemixer, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TIMEMIXER - RESULTATS TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  MSE:  {test_mse_mixer:.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(test_mse_mixer):.6f}\")\n",
    "print(f\"  MAE:  {test_mae_mixer:.6f}\")\n",
    "\n",
    "dir_true_mixer = np.sign(np.diff(targets_mixer.flatten()))\n",
    "dir_pred_mixer = np.sign(np.diff(preds_mixer.flatten()))\n",
    "dir_acc_mixer = np.mean(dir_true_mixer == dir_pred_mixer)\n",
    "print(f\"  Direction Accuracy: {dir_acc_mixer:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 7 : Comparaison et Benchmarks (15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['DLinear', 'PatchTST', 'iTransformer', 'TimeMixer'],\n",
    "    'MSE': [test_mse, test_mse_patchtst, test_mse_itrans, test_mse_mixer],\n",
    "    'RMSE': [np.sqrt(test_mse), np.sqrt(test_mse_patchtst), np.sqrt(test_mse_itrans), np.sqrt(test_mse_mixer)],\n",
    "    'MAE': [test_mae, test_mae_patchtst, test_mae_itrans, test_mae_mixer],\n",
    "    'Direction Acc': [dir_acc, dir_acc_patchtst, dir_acc_itrans, dir_acc_mixer],\n",
    "    'Parameters': [n_params, n_params_patchtst, n_params_itrans, n_params_mixer]\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARAISON DES ARCHITECTURES SOTA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset: {len(df)} jours, {n_features} features\")\n",
    "print(f\"Sequence: {SEQ_LEN} -> Prediction: {PRED_LEN}\")\n",
    "print(f\"\\n{results.to_string(index=False)}\")\n",
    "\n",
    "# Meilleur modele\n",
    "best_idx = results['MSE'].idxmin()\n",
    "print(f\"\\nMeilleur modele (MSE): {results.loc[best_idx, 'Model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# MSE comparison\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['steelblue', 'coral', 'seagreen', 'mediumpurple']\n",
    "ax1.bar(results['Model'], results['MSE'], color=colors, edgecolor='black')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.set_title('MSE par Modele', fontsize=12, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Direction Accuracy\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(results['Model'], results['Direction Acc'] * 100, color=colors, edgecolor='black')\n",
    "ax2.set_ylabel('Direction Accuracy (%)')\n",
    "ax2.set_title('Direction Accuracy par Modele', fontsize=12, fontweight='bold')\n",
    "ax2.axhline(50, color='red', linestyle='--', alpha=0.5, label='Random (50%)')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Parameters (log scale)\n",
    "ax3 = axes[1, 0]\n",
    "ax3.bar(results['Model'], results['Parameters'], color=colors, edgecolor='black')\n",
    "ax3.set_ylabel('Parametres')\n",
    "ax3.set_title('Nombre de Parametres', fontsize=12, fontweight='bold')\n",
    "ax3.set_yscale('log')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Efficiency: MSE vs Parameters\n",
    "ax4 = axes[1, 1]\n",
    "for i, row in results.iterrows():\n",
    "    ax4.scatter(row['Parameters'], row['MSE'], s=200, c=colors[i], \n",
    "                edgecolors='black', label=row['Model'], zorder=5)\n",
    "ax4.set_xlabel('Parametres')\n",
    "ax4.set_ylabel('MSE')\n",
    "ax4.set_title('Efficacite: MSE vs Parametres', fontsize=12, fontweight='bold')\n",
    "ax4.set_xscale('log')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des predictions\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "models_preds = [\n",
    "    ('DLinear', preds_dlinear, targets_dlinear),\n",
    "    ('PatchTST', preds_patchtst, targets_patchtst),\n",
    "    ('iTransformer', preds_itrans, targets_itrans),\n",
    "    ('TimeMixer', preds_mixer, targets_mixer)\n",
    "]\n",
    "\n",
    "for ax, (name, preds, targets) in zip(axes.flatten(), models_preds):\n",
    "    # Prendre un echantillon pour visualisation\n",
    "    n_show = min(200, len(preds.flatten()))\n",
    "    \n",
    "    ax.plot(range(n_show), targets.flatten()[:n_show], 'b-', \n",
    "            linewidth=1, label='Reel', alpha=0.7)\n",
    "    ax.plot(range(n_show), preds.flatten()[:n_show], 'r-', \n",
    "            linewidth=1, label='Predit', alpha=0.7)\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Prix (normalise)')\n",
    "    ax.set_title(f'{name}: Predictions vs Reel', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommandations\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RECOMMANDATIONS POUR LE TRADING ALGORITHMIQUE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "recommendations = [\n",
    "    (\"Baseline simple\", \"DLinear\", \"Ultra-leger, rapide, souvent suffisant\"),\n",
    "    (\"Patterns locaux\", \"PatchTST\", \"Capture les motifs a court terme\"),\n",
    "    (\"Multi-assets\", \"iTransformer\", \"Correlations entre actifs\"),\n",
    "    (\"Multiscale\", \"TimeMixer\", \"Patterns a differentes echelles\"),\n",
    "    (\"Long sequences\", \"Mamba (QC-Py-23)\", \"O(n) pour >1000 timesteps\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Cas d\\'usage':<20} {'Modele':<15} {'Raison'}\")\n",
    "print(\"-\" * 70)\n",
    "for use_case, model, reason in recommendations:\n",
    "    print(f\"{use_case:<20} {model:<15} {reason}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONTRAINTES QUANTCONNECT\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. ObjectStore: Max ~9 MB par fichier\n",
    "   -> Utiliser torch.save(state_dict) uniquement\n",
    "   -> DLinear (~40 KB) et PatchTST small (~2 MB) OK\n",
    "\n",
    "2. CPU-only (free tier): Pas de GPU\n",
    "   -> Inference rapide requise (<100ms)\n",
    "   -> Modeles legers recommandes\n",
    "\n",
    "3. Retrain: Hors plateforme (GPU local) puis upload\n",
    "   -> Sauvegarder state_dict localement\n",
    "   -> Charger dans ObjectStore via API\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 8 : Integration QuantConnect (15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern de sauvegarde/chargement pour ObjectStore\n",
    "\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "def save_model_for_qc(model, scaler, model_name='dlinear'):\n",
    "    \"\"\"\n",
    "    Prepare le modele pour QuantConnect ObjectStore.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict avec bytes du modele et du scaler\n",
    "    \"\"\"\n",
    "    # Sauvegarder state_dict dans un buffer\n",
    "    model_buffer = io.BytesIO()\n",
    "    torch.save(model.state_dict(), model_buffer)\n",
    "    model_bytes = model_buffer.getvalue()\n",
    "    \n",
    "    # Sauvegarder le scaler\n",
    "    scaler_bytes = pickle.dumps(scaler)\n",
    "    \n",
    "    # Taille totale\n",
    "    total_size = len(model_bytes) + len(scaler_bytes)\n",
    "    \n",
    "    print(f\"Modele '{model_name}' prepare pour QC:\")\n",
    "    print(f\"  state_dict: {len(model_bytes):,} bytes ({len(model_bytes)/1024:.1f} KB)\")\n",
    "    print(f\"  scaler:     {len(scaler_bytes):,} bytes\")\n",
    "    print(f\"  Total:      {total_size:,} bytes ({total_size/1024:.1f} KB)\")\n",
    "    \n",
    "    if total_size > 9 * 1024 * 1024:  # 9 MB limit\n",
    "        print(f\"  WARNING: Depasse la limite ObjectStore (~9 MB)!\")\n",
    "    else:\n",
    "        print(f\"  OK pour ObjectStore\")\n",
    "    \n",
    "    return {\n",
    "        'model_bytes': model_bytes,\n",
    "        'scaler_bytes': scaler_bytes,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "\n",
    "# Test avec DLinear (le plus leger)\n",
    "saved_dlinear = save_model_for_qc(model_dlinear, scaler, 'dlinear')\n",
    "\n",
    "print(\"\\n\")\n",
    "saved_patchtst = save_model_for_qc(model_patchtst, scaler, 'patchtst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code complet pour QuantConnect\n",
    "\n",
    "qc_code = '''\n",
    "# === INTEGRATION QUANTCONNECT - PYTORCH SOTA MODELS ===\n",
    "\n",
    "from AlgorithmImports import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import io\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# === DLinear Model Definition ===\n",
    "\n",
    "class MovingAvg(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        front = x[:, :1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "class SeriesDecomposition(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        trend = self.moving_avg(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend\n",
    "\n",
    "\n",
    "class DLinear(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual=True):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in = enc_in\n",
    "        self.individual = individual\n",
    "        \n",
    "        kernel_size = 25\n",
    "        self.decomposition = SeriesDecomposition(kernel_size)\n",
    "        \n",
    "        if individual:\n",
    "            self.Linear_Seasonal = nn.ModuleList([\n",
    "                nn.Linear(seq_len, pred_len) for _ in range(enc_in)\n",
    "            ])\n",
    "            self.Linear_Trend = nn.ModuleList([\n",
    "                nn.Linear(seq_len, pred_len) for _ in range(enc_in)\n",
    "            ])\n",
    "        else:\n",
    "            self.Linear_Seasonal = nn.Linear(seq_len, pred_len)\n",
    "            self.Linear_Trend = nn.Linear(seq_len, pred_len)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seasonal, trend = self.decomposition(x)\n",
    "        seasonal = seasonal.permute(0, 2, 1)\n",
    "        trend = trend.permute(0, 2, 1)\n",
    "        \n",
    "        if self.individual:\n",
    "            seasonal_output = torch.zeros(\n",
    "                [x.size(0), self.enc_in, self.pred_len], device=x.device\n",
    "            )\n",
    "            trend_output = torch.zeros(\n",
    "                [x.size(0), self.enc_in, self.pred_len], device=x.device\n",
    "            )\n",
    "            for i in range(self.enc_in):\n",
    "                seasonal_output[:, i, :] = self.Linear_Seasonal[i](seasonal[:, i, :])\n",
    "                trend_output[:, i, :] = self.Linear_Trend[i](trend[:, i, :])\n",
    "        else:\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal)\n",
    "            trend_output = self.Linear_Trend(trend)\n",
    "        \n",
    "        output = seasonal_output + trend_output\n",
    "        return output.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "# === Alpha Model ===\n",
    "\n",
    "class DLinearAlphaModel(AlphaModel):\n",
    "    \"\"\"\n",
    "    Alpha Model utilisant DLinear (SOTA 2023) pour prediction.\n",
    "    \n",
    "    Architecture legere, compatible ObjectStore (<100 KB).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len=96, pred_len=24, n_features=7,\n",
    "                 model_key=\"models/dlinear\", prediction_threshold=0.005):\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.n_features = n_features\n",
    "        self.model_key = model_key\n",
    "        self.prediction_threshold = prediction_threshold\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.symbol_data = {}\n",
    "        self.device = torch.device(\"cpu\")\n",
    "    \n",
    "    def Update(self, algorithm, data):\n",
    "        insights = []\n",
    "        \n",
    "        # Charger le modele si pas encore fait\n",
    "        if self.model is None:\n",
    "            self._load_model(algorithm)\n",
    "            if self.model is None:\n",
    "                return insights\n",
    "        \n",
    "        for symbol, sd in self.symbol_data.items():\n",
    "            if not data.ContainsKey(symbol):\n",
    "                continue\n",
    "            \n",
    "            # Mettre a jour les donnees\n",
    "            sd.Update(data[symbol])\n",
    "            \n",
    "            if len(sd.features) < self.seq_len:\n",
    "                continue\n",
    "            \n",
    "            # Preparer l\\'input\n",
    "            features = np.array(list(sd.features))\n",
    "            features_scaled = self.scaler.transform(features)\n",
    "            \n",
    "            # Tensor: [1, seq_len, n_features]\n",
    "            x = torch.FloatTensor(features_scaled).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Prediction\n",
    "            with torch.no_grad():\n",
    "                pred = self.model(x)  # [1, pred_len, n_features]\n",
    "            \n",
    "            # Extraire prediction du prix (feature 0)\n",
    "            pred_price = pred[0, :, 0].mean().item()  # Moyenne des predictions\n",
    "            current_price = features_scaled[-1, 0]\n",
    "            \n",
    "            predicted_return = pred_price - current_price\n",
    "            \n",
    "            # Signal\n",
    "            if abs(predicted_return) >= self.prediction_threshold:\n",
    "                direction = InsightDirection.Up if predicted_return > 0 else InsightDirection.Down\n",
    "                confidence = min(abs(predicted_return) / 0.05, 1.0)\n",
    "                \n",
    "                insight = Insight.Price(\n",
    "                    symbol,\n",
    "                    timedelta(days=self.pred_len),\n",
    "                    direction,\n",
    "                    magnitude=abs(predicted_return),\n",
    "                    confidence=confidence\n",
    "                )\n",
    "                insights.append(insight)\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def OnSecuritiesChanged(self, algorithm, changes):\n",
    "        for security in changes.AddedSecurities:\n",
    "            symbol = security.Symbol\n",
    "            if symbol not in self.symbol_data:\n",
    "                self.symbol_data[symbol] = DLinearSymbolData(\n",
    "                    algorithm, symbol, self.seq_len\n",
    "                )\n",
    "        \n",
    "        for security in changes.RemovedSecurities:\n",
    "            symbol = security.Symbol\n",
    "            if symbol in self.symbol_data:\n",
    "                del self.symbol_data[symbol]\n",
    "    \n",
    "    def _load_model(self, algorithm):\n",
    "        \"\"\"Charge le modele depuis ObjectStore.\"\"\"\n",
    "        if not algorithm.ObjectStore.ContainsKey(self.model_key):\n",
    "            algorithm.Debug(f\"Modele non trouve: {self.model_key}\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Charger state_dict\n",
    "            model_bytes = algorithm.ObjectStore.ReadBytes(self.model_key)\n",
    "            state_dict = torch.load(io.BytesIO(bytes(model_bytes)), map_location=self.device)\n",
    "            \n",
    "            # Instancier le modele\n",
    "            self.model = DLinear(\n",
    "                seq_len=self.seq_len,\n",
    "                pred_len=self.pred_len,\n",
    "                enc_in=self.n_features,\n",
    "                individual=True\n",
    "            ).to(self.device)\n",
    "            self.model.load_state_dict(state_dict)\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Charger scaler\n",
    "            scaler_bytes = algorithm.ObjectStore.ReadBytes(self.model_key + \"_scaler\")\n",
    "            self.scaler = pickle.loads(bytes(scaler_bytes))\n",
    "            \n",
    "            algorithm.Debug(f\"Modele DLinear charge depuis {self.model_key}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            algorithm.Debug(f\"Erreur chargement modele: {e}\")\n",
    "\n",
    "\n",
    "class DLinearSymbolData:\n",
    "    \"\"\"Stocke les features par symbole.\"\"\"\n",
    "    \n",
    "    def __init__(self, algorithm, symbol, seq_len):\n",
    "        self.symbol = symbol\n",
    "        self.seq_len = seq_len\n",
    "        self.features = deque(maxlen=seq_len)\n",
    "        \n",
    "        # Indicateurs\n",
    "        self.sma_20 = algorithm.SMA(symbol, 20)\n",
    "        self.sma_50 = algorithm.SMA(symbol, 50)\n",
    "        self.rsi = algorithm.RSI(symbol, 14)\n",
    "        \n",
    "        self.last_close = None\n",
    "    \n",
    "    def Update(self, bar):\n",
    "        \"\"\"Met a jour les features avec la nouvelle barre.\"\"\"\n",
    "        if not self.sma_20.IsReady:\n",
    "            return\n",
    "        \n",
    "        # Calculer les features\n",
    "        close = float(bar.Close)\n",
    "        volume = float(bar.Volume)\n",
    "        returns = (close / self.last_close - 1) if self.last_close else 0\n",
    "        sma_20 = float(self.sma_20.Current.Value)\n",
    "        sma_50 = float(self.sma_50.Current.Value) if self.sma_50.IsReady else sma_20\n",
    "        rsi = float(self.rsi.Current.Value) if self.rsi.IsReady else 50\n",
    "        volatility = abs(returns) * np.sqrt(252)  # Approximation\n",
    "        \n",
    "        feature_vector = [close, volume, returns, sma_20, sma_50, rsi, volatility]\n",
    "        self.features.append(feature_vector)\n",
    "        \n",
    "        self.last_close = close\n",
    "\n",
    "\n",
    "# === Strategie Complete ===\n",
    "\n",
    "class DLinearTradingStrategy(QCAlgorithm):\n",
    "    \"\"\"\n",
    "    Strategie utilisant DLinear (SOTA 2023) pour prediction.\n",
    "    \n",
    "    - Modele: DLinear (decomposition + linear)\n",
    "    - Input: 96 jours, 7 features\n",
    "    - Output: 24 jours de prediction\n",
    "    - Signal: Moyenne des predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def Initialize(self):\n",
    "        self.SetStartDate(2022, 1, 1)\n",
    "        self.SetEndDate(2023, 12, 31)\n",
    "        self.SetCash(100000)\n",
    "        \n",
    "        # Univers\n",
    "        tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\"]\n",
    "        for ticker in tickers:\n",
    "            self.AddEquity(ticker, Resolution.Daily)\n",
    "        \n",
    "        # Alpha Model\n",
    "        self.SetAlpha(DLinearAlphaModel(\n",
    "            seq_len=96,\n",
    "            pred_len=24,\n",
    "            n_features=7,\n",
    "            model_key=\"models/dlinear\",\n",
    "            prediction_threshold=0.005\n",
    "        ))\n",
    "        \n",
    "        # Portfolio Construction\n",
    "        self.SetPortfolioConstruction(EqualWeightingPortfolioConstructionModel())\n",
    "        \n",
    "        # Execution\n",
    "        self.SetExecution(ImmediateExecutionModel())\n",
    "        \n",
    "        # Risk Management\n",
    "        self.SetRiskManagement(MaximumDrawdownPercentPerSecurity(0.10))\n",
    "        \n",
    "        self.SetWarmup(100)\n",
    "        self.Debug(\"DLinear Trading Strategy initialized\")\n",
    "'''\n",
    "\n",
    "print(\"Code QuantConnect:\")\n",
    "print(\"=\"*60)\n",
    "print(\"- DLinear model definition\")\n",
    "print(\"- DLinearAlphaModel (charge depuis ObjectStore)\")\n",
    "print(\"- DLinearSymbolData (features avec indicateurs)\")\n",
    "print(\"- DLinearTradingStrategy (strategie complete)\")\n",
    "print(\"\\nTaille du code: ~200 lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion et Prochaines Etapes\n",
    "\n",
    "### Recapitulatif\n",
    "\n",
    "| Partie | Sujet | Points Cles |\n",
    "|--------|-------|-------------|\n",
    "| 1 | Evolution 2015-2026 | LSTM -> Transformers -> DLinear -> SSMs |\n",
    "| 2 | Setup PyTorch | Device, DataLoader, TimeSeriesDataset |\n",
    "| 3 | DLinear | Decomposition + Linear, baseline forte |\n",
    "| 4 | PatchTST | Tokenization par patches, ICLR 2023 |\n",
    "| 5 | iTransformer | Attention sur variables, ICLR 2024 |\n",
    "| 6 | TimeMixer | MLP multiscale, pas d'attention |\n",
    "| 7 | Comparaison | MSE, Direction Accuracy, Parametres |\n",
    "| 8 | Integration QC | ObjectStore, Alpha Model |\n",
    "\n",
    "### Points Cles SOTA 2024-2026\n",
    "\n",
    "| Concept | Insight |\n",
    "|---------|--------|\n",
    "| **Simplicite** | DLinear (MLP) bat souvent les Transformers complexes |\n",
    "| **Patches** | Tokenization intelligente > point-wise attention |\n",
    "| **Variables** | iTransformer: attention sur correlations inter-series |\n",
    "| **Multiscale** | TimeMixer: patterns a differentes echelles |\n",
    "| **Efficiency** | Moins de parametres = meilleure generalisation |\n",
    "\n",
    "### Limitations et Avertissements\n",
    "\n",
    "| Limitation | Description | Mitigation |\n",
    "|------------|-------------|------------|\n",
    "| **Regime changes** | Modeles degradent lors de crises | Retrain frequent, ensembling |\n",
    "| **Overfitting** | Series financieres bruitees | Regularisation, early stopping |\n",
    "| **Inference time** | Important pour HFT | Modeles legers (DLinear) |\n",
    "| **Data leakage** | Walk-forward validation essentielle | Train/Val/Test temporel strict |\n",
    "\n",
    "### Prochaines Etapes\n",
    "\n",
    "| Notebook | Contenu |\n",
    "|----------|--------|\n",
    "| **QC-Py-23** | State Space Models (Mamba) - O(n) pour longues sequences |\n",
    "| **QC-Py-24** | Generative Anomaly Detection (VAE-Transformer + HMM) |\n",
    "| **QC-Py-25** | Reinforcement Learning (PPO/DQN) |\n",
    "\n",
    "### Ressources SOTA\n",
    "\n",
    "- [Time-Series-Library](https://github.com/thuml/Time-Series-Library) - Framework unifie Tsinghua (20+ modeles)\n",
    "- [PatchTST Paper](https://arxiv.org/abs/2211.14730) - ICLR 2023\n",
    "- [iTransformer Paper](https://arxiv.org/abs/2310.06625) - ICLR 2024 Spotlight\n",
    "- [TimeMixer Paper](https://arxiv.org/abs/2405.14616) - ICLR 2024\n",
    "- [Are Transformers Effective?](https://arxiv.org/abs/2205.13504) - AAAI 2023\n",
    "- [Awesome Time Series](https://github.com/qingsongedu/time-series-transformers-review) - Survey\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook complete. Les architectures SOTA 2024-2026 offrent un excellent compromis entre performance et efficacite. DLinear reste une baseline remarquablement forte, tandis que PatchTST et iTransformer apportent des innovations significatives.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
