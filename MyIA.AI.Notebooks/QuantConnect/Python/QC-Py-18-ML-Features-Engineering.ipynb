{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC-Py-18 - Feature Engineering pour Machine Learning Trading\n",
    "\n",
    "> **Construire des features predictives pour les modeles ML**\n",
    "> Duree: 90 minutes | Niveau: Intermediaire-Avance | Python + QuantConnect\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous serez capable de :\n",
    "\n",
    "1. Comprendre l'importance du **Feature Engineering** en trading quantitatif\n",
    "2. Creer des **features techniques** basees sur les prix et volumes\n",
    "3. Construire des **features basees sur indicateurs** (RSI, MACD, Bollinger...)\n",
    "4. Extraire des **features fondamentales** depuis QuantConnect\n",
    "5. Maitriser le **labeling** pour classification et regression\n",
    "6. Appliquer des techniques de **feature selection** et importance\n",
    "7. Implementer un **preprocessing pipeline** robuste\n",
    "8. Construire un **pipeline complet** de preparation des donnees ML\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Notebooks QC-Py-01 a 15 completes\n",
    "- Comprehension des indicateurs techniques (QC-Py-11)\n",
    "- Notions de base en Machine Learning (classification, regression)\n",
    "- Familiarite avec pandas, numpy, sklearn\n",
    "\n",
    "## Structure du Notebook\n",
    "\n",
    "1. Introduction au Feature Engineering (15 min)\n",
    "2. Features Techniques: Price-Based (10 min)\n",
    "3. Features Techniques: Indicator-Based (15 min)\n",
    "4. Features Fondamentales (20 min)\n",
    "5. Labeling pour Classification et Regression (20 min)\n",
    "6. Feature Selection et Importance (20 min)\n",
    "7. Preprocessing et Normalization (15 min)\n",
    "8. Pipeline Complet de Feature Engineering (20 min)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Introduction au Feature Engineering (15 min)\n",
    "\n",
    "### Pourquoi le Feature Engineering est-il crucial?\n",
    "\n",
    "En Machine Learning, la qualite des predictions depend directement de la qualite des features. C'est le principe **\"Garbage In, Garbage Out\"** :\n",
    "\n",
    "```\n",
    "Donnees Brutes     Features Bien          Modele ML        Predictions\n",
    "(OHLCV)      -->   Construites      -->   (RF, XGB)   -->  Profitables\n",
    "                        |\n",
    "                        v\n",
    "               Signal + Information\n",
    "```\n",
    "\n",
    "### Features vs Donnees Brutes\n",
    "\n",
    "| Donnees Brutes | Features Derivees |\n",
    "|----------------|-------------------|\n",
    "| Prix de cloture | Rendements sur N periodes |\n",
    "| Volume | Ratio volume/moyenne |\n",
    "| High/Low | Range (High-Low)/Close |\n",
    "| Historique | Momentum, Volatilite, Tendance |\n",
    "\n",
    "### Importance pour l'Interpretabilite\n",
    "\n",
    "Des features bien construites permettent :\n",
    "\n",
    "1. **Comprendre** ce que le modele apprend\n",
    "2. **Debugger** les predictions erronees\n",
    "3. **Communiquer** la logique aux stakeholders\n",
    "4. **Verifier** que le modele n'utilise pas de lookahead bias\n",
    "\n",
    "### Feature Selection vs Feature Extraction\n",
    "\n",
    "| Approche | Description | Techniques |\n",
    "|----------|-------------|------------|\n",
    "| **Selection** | Choisir les meilleures features existantes | Correlation, Importance, RFE |\n",
    "| **Extraction** | Creer de nouvelles features combinees | PCA, Auto-encoders |\n",
    "\n",
    "### Categories de Features en Trading\n",
    "\n",
    "```\n",
    "Features\n",
    "   |\n",
    "   +-- Techniques\n",
    "   |      +-- Price-based (returns, volatility, range)\n",
    "   |      +-- Indicator-based (RSI, MACD, BB)\n",
    "   |      +-- Volume-based (OBV, volume ratio)\n",
    "   |\n",
    "   +-- Fondamentales\n",
    "   |      +-- Valuation (P/E, P/B, P/S)\n",
    "   |      +-- Profitability (ROE, ROA, Margin)\n",
    "   |      +-- Growth (Revenue, EPS)\n",
    "   |\n",
    "   +-- Alternatives\n",
    "          +-- Sentiment (news, social)\n",
    "          +-- Macro (rates, indices)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Import bibliotheque partagee\n",
    "import sys\n",
    "sys.path.insert(0, '../shared')\n",
    "\n",
    "try:\n",
    "    from features import calculate_returns, add_technical_features, create_labels, walk_forward_split\n",
    "    print(\"Import depuis shared/features.py reussi\")\n",
    "except ImportError as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Les fonctions seront definies dans ce notebook.\")\n",
    "\n",
    "print(\"\\nImports reussis!\")\n",
    "print(\"Ce notebook couvre le Feature Engineering pour ML Trading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creer des donnees de demonstration (simulees)\n",
    "# En production, ces donnees viendraient de QuantConnect\n",
    "\n",
    "def generate_sample_data(n_days=500, seed=42):\n",
    "    \"\"\"\n",
    "    Genere des donnees OHLCV simulees pour demonstration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_days : int\n",
    "        Nombre de jours de donnees\n",
    "    seed : int\n",
    "        Graine aleatoire pour reproductibilite\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame avec colonnes OHLCV\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Dates\n",
    "    dates = pd.date_range(start='2022-01-01', periods=n_days, freq='B')  # Business days\n",
    "    \n",
    "    # Prix de depart\n",
    "    start_price = 100.0\n",
    "    \n",
    "    # Generer rendements avec tendance et volatilite variables\n",
    "    daily_drift = 0.0003  # ~7.5% annuel\n",
    "    daily_volatility = 0.015  # ~24% annuel\n",
    "    \n",
    "    returns = np.random.normal(daily_drift, daily_volatility, n_days)\n",
    "    \n",
    "    # Ajouter des regimes (periodes de haute/basse volatilite)\n",
    "    regime = np.sin(np.linspace(0, 4*np.pi, n_days)) * 0.005\n",
    "    returns = returns + regime\n",
    "    \n",
    "    # Calculer les prix de cloture\n",
    "    close = start_price * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    # Generer OHLV a partir de close\n",
    "    intraday_vol = 0.01\n",
    "    high = close * (1 + np.abs(np.random.normal(0, intraday_vol, n_days)))\n",
    "    low = close * (1 - np.abs(np.random.normal(0, intraday_vol, n_days)))\n",
    "    open_price = close * (1 + np.random.normal(0, intraday_vol/2, n_days))\n",
    "    \n",
    "    # Volume (correle negativement avec les rendements)\n",
    "    base_volume = 1_000_000\n",
    "    volume = base_volume * (1 + np.random.exponential(0.5, n_days))\n",
    "    volume = volume * (1 + np.abs(returns) * 10)  # Volume augmente avec volatilite\n",
    "    \n",
    "    # Creer DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'open': open_price,\n",
    "        'high': high,\n",
    "        'low': low,\n",
    "        'close': close,\n",
    "        'volume': volume.astype(int)\n",
    "    }, index=dates)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generer donnees\n",
    "df = generate_sample_data(n_days=500)\n",
    "\n",
    "print(\"Donnees de demonstration generees:\")\n",
    "print(f\"  Periode: {df.index[0].date()} a {df.index[-1].date()}\")\n",
    "print(f\"  Nombre de jours: {len(df)}\")\n",
    "print(f\"\\nApercu:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des donnees brutes\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Prix\n",
    "axes[0].plot(df.index, df['close'], label='Close', color='blue', linewidth=1)\n",
    "axes[0].fill_between(df.index, df['low'], df['high'], alpha=0.2, color='blue', label='High-Low Range')\n",
    "axes[0].set_ylabel('Prix ($)')\n",
    "axes[0].set_title('Donnees OHLCV Brutes')\n",
    "axes[0].legend()\n",
    "\n",
    "# Volume\n",
    "axes[1].bar(df.index, df['volume'], color='gray', alpha=0.6)\n",
    "axes[1].set_ylabel('Volume')\n",
    "axes[1].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStatistiques descriptives:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 2 : Features Techniques - Price-Based (10 min)\n",
    "\n",
    "### Features derivees du prix\n",
    "\n",
    "Les features basees sur le prix capturent :\n",
    "\n",
    "| Feature | Description | Formule |\n",
    "|---------|-------------|--------|\n",
    "| **Rendements** | Variation relative du prix | `(P_t - P_{t-n}) / P_{t-n}` |\n",
    "| **Volatilite** | Ecart-type des rendements | `std(returns, window)` |\n",
    "| **Range** | Amplitude intraday | `(High - Low) / Close` |\n",
    "| **Distance from extremes** | Position vs high/low recents | `(High_max - Close) / Close` |\n",
    "\n",
    "### Pourquoi les rendements plutot que les prix?\n",
    "\n",
    "1. **Stationnarite** : Les prix sont non-stationnaires, les rendements le sont (approximativement)\n",
    "2. **Comparabilite** : Permet de comparer des actifs a prix differents\n",
    "3. **Interpretabilite** : Un rendement de 2% est directement comprehensible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price_features(df, return_periods=[1, 5, 10, 20], volatility_window=20):\n",
    "    \"\"\"\n",
    "    Calcule les features basees sur le prix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame avec colonnes OHLCV\n",
    "    return_periods : list\n",
    "        Periodes pour calculer les rendements (ex: [1, 5, 20] jours)\n",
    "    volatility_window : int\n",
    "        Fenetre pour la volatilite\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame avec features ajoutees\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # === RETURNS ===\n",
    "    # Rendements sur differentes periodes\n",
    "    for period in return_periods:\n",
    "        result[f'return_{period}d'] = result['close'].pct_change(period)\n",
    "    \n",
    "    # Log-returns (plus adaptes pour les modeles)\n",
    "    result['log_return_1d'] = np.log(result['close'] / result['close'].shift(1))\n",
    "    \n",
    "    # === VOLATILITY ===\n",
    "    # Volatilite historique (ecart-type des rendements)\n",
    "    result[f'volatility_{volatility_window}d'] = result['return_1d'].rolling(volatility_window).std()\n",
    "    \n",
    "    # Volatilite annualisee\n",
    "    result[f'volatility_annualized'] = result[f'volatility_{volatility_window}d'] * np.sqrt(252)\n",
    "    \n",
    "    # === RANGE (Amplitude) ===\n",
    "    # Range intraday normalise\n",
    "    result['range'] = (result['high'] - result['low']) / result['close']\n",
    "    \n",
    "    # True Range (inclut les gaps overnight)\n",
    "    result['true_range'] = np.maximum(\n",
    "        result['high'] - result['low'],\n",
    "        np.maximum(\n",
    "            np.abs(result['high'] - result['close'].shift(1)),\n",
    "            np.abs(result['low'] - result['close'].shift(1))\n",
    "        )\n",
    "    ) / result['close']\n",
    "    \n",
    "    # Average True Range normalise\n",
    "    result['atr_norm'] = result['true_range'].rolling(14).mean()\n",
    "    \n",
    "    # === DISTANCE FROM EXTREMES ===\n",
    "    # Distance par rapport au plus haut des 20 derniers jours\n",
    "    result['dist_from_high_20d'] = (result['high'].rolling(20).max() - result['close']) / result['close']\n",
    "    \n",
    "    # Distance par rapport au plus bas des 20 derniers jours\n",
    "    result['dist_from_low_20d'] = (result['close'] - result['low'].rolling(20).min()) / result['close']\n",
    "    \n",
    "    # Position dans le range 20 jours (0 = au plus bas, 1 = au plus haut)\n",
    "    high_20d = result['high'].rolling(20).max()\n",
    "    low_20d = result['low'].rolling(20).min()\n",
    "    result['position_in_range_20d'] = (result['close'] - low_20d) / (high_20d - low_20d)\n",
    "    \n",
    "    # === PRICE MOMENTUM ===\n",
    "    # Ratio prix actuel / prix moyen\n",
    "    result['price_to_sma_20'] = result['close'] / result['close'].rolling(20).mean()\n",
    "    result['price_to_sma_50'] = result['close'] / result['close'].rolling(50).mean()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Appliquer\n",
    "df_features = calculate_price_features(df)\n",
    "\n",
    "print(\"Features Price-Based ajoutees:\")\n",
    "price_features = [col for col in df_features.columns if col not in ['open', 'high', 'low', 'close', 'volume']]\n",
    "for f in price_features:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\nNombre de features: {len(price_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des features price-based\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Rendements multi-periodes\n",
    "ax1 = axes[0]\n",
    "for period in [1, 5, 20]:\n",
    "    ax1.plot(df_features.index, df_features[f'return_{period}d'] * 100, \n",
    "             label=f'Return {period}D', alpha=0.7)\n",
    "ax1.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "ax1.set_ylabel('Rendement (%)')\n",
    "ax1.set_title('Rendements sur differentes periodes')\n",
    "ax1.legend()\n",
    "\n",
    "# Volatilite\n",
    "ax2 = axes[1]\n",
    "ax2.plot(df_features.index, df_features['volatility_annualized'] * 100, \n",
    "         color='red', label='Volatilite Annualisee')\n",
    "ax2.fill_between(df_features.index, 0, df_features['volatility_annualized'] * 100, \n",
    "                 alpha=0.3, color='red')\n",
    "ax2.set_ylabel('Volatilite (%)')\n",
    "ax2.set_title('Volatilite Historique (rolling 20D, annualisee)')\n",
    "ax2.legend()\n",
    "\n",
    "# Position dans le range\n",
    "ax3 = axes[2]\n",
    "ax3.plot(df_features.index, df_features['position_in_range_20d'], color='purple')\n",
    "ax3.axhline(y=0.5, color='black', linestyle='--', linewidth=0.5)\n",
    "ax3.axhline(y=0.2, color='green', linestyle=':', linewidth=1, label='Zone Oversold')\n",
    "ax3.axhline(y=0.8, color='red', linestyle=':', linewidth=1, label='Zone Overbought')\n",
    "ax3.fill_between(df_features.index, 0, 0.2, alpha=0.1, color='green')\n",
    "ax3.fill_between(df_features.index, 0.8, 1, alpha=0.1, color='red')\n",
    "ax3.set_ylabel('Position (0-1)')\n",
    "ax3.set_xlabel('Date')\n",
    "ax3.set_title('Position dans le Range 20D (0=Low, 1=High)')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 3 : Features Techniques - Indicator-Based (15 min)\n",
    "\n",
    "### Indicateurs techniques comme features\n",
    "\n",
    "Les indicateurs techniques populaires capturent differents aspects du marche :\n",
    "\n",
    "| Indicateur | Type | Ce qu'il capture |\n",
    "|------------|------|------------------|\n",
    "| **RSI** | Momentum | Conditions surachat/survente |\n",
    "| **MACD** | Trend/Momentum | Direction et force de la tendance |\n",
    "| **Bollinger Bands** | Volatilite | Position relative et volatilite |\n",
    "| **Moving Averages** | Trend | Tendance lissee |\n",
    "| **ADX** | Trend Strength | Force de la tendance (pas direction) |\n",
    "\n",
    "### Normalisation des indicateurs\n",
    "\n",
    "Pour le ML, il est important de normaliser :\n",
    "\n",
    "- **RSI** : Deja normalise [0, 100]\n",
    "- **MACD** : Diviser par le prix ou utiliser le ratio\n",
    "- **BB** : Utiliser %B (position normalisee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_indicator_features(df):\n",
    "    \"\"\"\n",
    "    Calcule les features basees sur indicateurs techniques.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame avec colonnes OHLCV\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame avec features indicateurs ajoutees\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    close = result['close']\n",
    "    high = result['high']\n",
    "    low = result['low']\n",
    "    \n",
    "    # === MOVING AVERAGES ===\n",
    "    result['sma_20'] = close.rolling(20).mean()\n",
    "    result['sma_50'] = close.rolling(50).mean()\n",
    "    result['ema_12'] = close.ewm(span=12, adjust=False).mean()\n",
    "    result['ema_26'] = close.ewm(span=26, adjust=False).mean()\n",
    "    \n",
    "    # Ratios de MA (features normalisees)\n",
    "    result['ma_ratio_20_50'] = result['sma_20'] / result['sma_50']\n",
    "    result['price_to_ema_12'] = close / result['ema_12']\n",
    "    \n",
    "    # Cross signals (1 si fast > slow, 0 sinon)\n",
    "    result['ma_cross_20_50'] = (result['sma_20'] > result['sma_50']).astype(int)\n",
    "    \n",
    "    # === RSI (Relative Strength Index) ===\n",
    "    delta = close.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = (-delta).clip(lower=0)\n",
    "    \n",
    "    avg_gain = gain.rolling(14).mean()\n",
    "    avg_loss = loss.rolling(14).mean()\n",
    "    \n",
    "    rs = avg_gain / avg_loss\n",
    "    result['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # RSI normalise [-1, 1] pour ML\n",
    "    result['rsi_normalized'] = (result['rsi_14'] - 50) / 50\n",
    "    \n",
    "    # === MACD (Moving Average Convergence Divergence) ===\n",
    "    result['macd'] = result['ema_12'] - result['ema_26']\n",
    "    result['macd_signal'] = result['macd'].ewm(span=9, adjust=False).mean()\n",
    "    result['macd_hist'] = result['macd'] - result['macd_signal']\n",
    "    \n",
    "    # MACD normalise par prix (pour comparabilite)\n",
    "    result['macd_norm'] = result['macd'] / close\n",
    "    result['macd_hist_norm'] = result['macd_hist'] / close\n",
    "    \n",
    "    # === BOLLINGER BANDS ===\n",
    "    bb_period = 20\n",
    "    bb_std = 2\n",
    "    \n",
    "    bb_middle = close.rolling(bb_period).mean()\n",
    "    bb_std_val = close.rolling(bb_period).std()\n",
    "    bb_upper = bb_middle + (bb_std * bb_std_val)\n",
    "    bb_lower = bb_middle - (bb_std * bb_std_val)\n",
    "    \n",
    "    # %B: Position dans les bandes (0 = lower, 1 = upper)\n",
    "    result['bb_percent_b'] = (close - bb_lower) / (bb_upper - bb_lower)\n",
    "    \n",
    "    # Bandwidth: Largeur des bandes (mesure de volatilite)\n",
    "    result['bb_bandwidth'] = (bb_upper - bb_lower) / bb_middle\n",
    "    \n",
    "    # Distance au middle band\n",
    "    result['bb_dist_to_middle'] = (close - bb_middle) / (bb_std_val * bb_std)\n",
    "    \n",
    "    # === STOCHASTIC OSCILLATOR ===\n",
    "    stoch_period = 14\n",
    "    stoch_smooth = 3\n",
    "    \n",
    "    lowest_low = low.rolling(stoch_period).min()\n",
    "    highest_high = high.rolling(stoch_period).max()\n",
    "    \n",
    "    # %K: Position relative dans le range\n",
    "    result['stoch_k'] = 100 * (close - lowest_low) / (highest_high - lowest_low)\n",
    "    # %D: Signal line (moyenne de %K)\n",
    "    result['stoch_d'] = result['stoch_k'].rolling(stoch_smooth).mean()\n",
    "    \n",
    "    # Stochastic normalise [-1, 1]\n",
    "    result['stoch_normalized'] = (result['stoch_k'] - 50) / 50\n",
    "    \n",
    "    # === ADX (Average Directional Index) ===\n",
    "    # Mesure la FORCE de la tendance (pas la direction)\n",
    "    adx_period = 14\n",
    "    \n",
    "    # Directional Movement\n",
    "    plus_dm = high.diff()\n",
    "    minus_dm = -low.diff()\n",
    "    \n",
    "    plus_dm = plus_dm.where((plus_dm > minus_dm) & (plus_dm > 0), 0)\n",
    "    minus_dm = minus_dm.where((minus_dm > plus_dm) & (minus_dm > 0), 0)\n",
    "    \n",
    "    # True Range pour ADX\n",
    "    tr = np.maximum(\n",
    "        high - low,\n",
    "        np.maximum(\n",
    "            np.abs(high - close.shift(1)),\n",
    "            np.abs(low - close.shift(1))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Smoothed\n",
    "    atr = tr.rolling(adx_period).mean()\n",
    "    plus_di = 100 * (plus_dm.rolling(adx_period).mean() / atr)\n",
    "    minus_di = 100 * (minus_dm.rolling(adx_period).mean() / atr)\n",
    "    \n",
    "    # ADX\n",
    "    dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di)\n",
    "    result['adx'] = dx.rolling(adx_period).mean()\n",
    "    \n",
    "    # DI difference (direction)\n",
    "    result['di_diff'] = plus_di - minus_di\n",
    "    \n",
    "    # === CCI (Commodity Channel Index) ===\n",
    "    cci_period = 20\n",
    "    typical_price = (high + low + close) / 3\n",
    "    sma_tp = typical_price.rolling(cci_period).mean()\n",
    "    mean_deviation = (typical_price - sma_tp).abs().rolling(cci_period).mean()\n",
    "    result['cci'] = (typical_price - sma_tp) / (0.015 * mean_deviation)\n",
    "    \n",
    "    # CCI normalise (typiquement entre -200 et +200)\n",
    "    result['cci_normalized'] = result['cci'] / 200\n",
    "    result['cci_normalized'] = result['cci_normalized'].clip(-1, 1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Appliquer\n",
    "df_features = calculate_indicator_features(df_features)\n",
    "\n",
    "print(\"Features Indicator-Based ajoutees:\")\n",
    "indicator_features = ['sma_20', 'sma_50', 'ma_ratio_20_50', 'rsi_14', 'rsi_normalized',\n",
    "                      'macd', 'macd_hist', 'macd_norm', 'bb_percent_b', 'bb_bandwidth',\n",
    "                      'stoch_k', 'stoch_normalized', 'adx', 'di_diff', 'cci_normalized']\n",
    "for f in indicator_features:\n",
    "    if f in df_features.columns:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\nTotal features: {len([c for c in df_features.columns if c not in ['open', 'high', 'low', 'close', 'volume']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des features indicator-based\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Prix avec MAs\n",
    "ax1 = axes[0]\n",
    "ax1.plot(df_features.index, df_features['close'], label='Close', linewidth=1)\n",
    "ax1.plot(df_features.index, df_features['sma_20'], label='SMA 20', linewidth=1)\n",
    "ax1.plot(df_features.index, df_features['sma_50'], label='SMA 50', linewidth=1)\n",
    "ax1.set_ylabel('Prix ($)')\n",
    "ax1.set_title('Prix et Moving Averages')\n",
    "ax1.legend()\n",
    "\n",
    "# RSI\n",
    "ax2 = axes[1]\n",
    "ax2.plot(df_features.index, df_features['rsi_14'], color='purple')\n",
    "ax2.axhline(y=70, color='red', linestyle='--', label='Overbought (70)')\n",
    "ax2.axhline(y=30, color='green', linestyle='--', label='Oversold (30)')\n",
    "ax2.axhline(y=50, color='gray', linestyle=':')\n",
    "ax2.fill_between(df_features.index, 70, 100, alpha=0.1, color='red')\n",
    "ax2.fill_between(df_features.index, 0, 30, alpha=0.1, color='green')\n",
    "ax2.set_ylabel('RSI')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.set_title('RSI (14)')\n",
    "ax2.legend()\n",
    "\n",
    "# MACD\n",
    "ax3 = axes[2]\n",
    "ax3.plot(df_features.index, df_features['macd'], label='MACD', color='blue')\n",
    "ax3.plot(df_features.index, df_features['macd_signal'], label='Signal', color='orange')\n",
    "colors = ['green' if x > 0 else 'red' for x in df_features['macd_hist']]\n",
    "ax3.bar(df_features.index, df_features['macd_hist'], color=colors, alpha=0.5, label='Histogram')\n",
    "ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax3.set_ylabel('MACD')\n",
    "ax3.set_title('MACD (12, 26, 9)')\n",
    "ax3.legend()\n",
    "\n",
    "# Bollinger %B\n",
    "ax4 = axes[3]\n",
    "ax4.plot(df_features.index, df_features['bb_percent_b'], color='teal')\n",
    "ax4.axhline(y=1, color='red', linestyle='--', label='Upper Band')\n",
    "ax4.axhline(y=0, color='green', linestyle='--', label='Lower Band')\n",
    "ax4.axhline(y=0.5, color='gray', linestyle=':')\n",
    "ax4.fill_between(df_features.index, 1, df_features['bb_percent_b'].max(), \n",
    "                 where=df_features['bb_percent_b'] > 1, alpha=0.2, color='red')\n",
    "ax4.fill_between(df_features.index, df_features['bb_percent_b'].min(), 0, \n",
    "                 where=df_features['bb_percent_b'] < 0, alpha=0.2, color='green')\n",
    "ax4.set_ylabel('%B')\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_title('Bollinger Bands %B (position normalisee)')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 4 : Features Fondamentales (20 min)\n",
    "\n",
    "### Donnees fondamentales dans QuantConnect\n",
    "\n",
    "QuantConnect fournit des donnees fondamentales via `MorningstarData`. Ces features capturent la qualite et la valorisation des entreprises.\n",
    "\n",
    "### Categories de features fondamentales\n",
    "\n",
    "| Categorie | Features | Description |\n",
    "|-----------|----------|-------------|\n",
    "| **Valuation** | P/E, P/B, P/S, EV/EBITDA | L'action est-elle chere ou bon marche? |\n",
    "| **Profitability** | ROE, ROA, Margin | L'entreprise est-elle rentable? |\n",
    "| **Growth** | Revenue Growth, EPS Growth | L'entreprise croit-elle? |\n",
    "| **Debt** | D/E, Interest Coverage | Le bilan est-il sain? |\n",
    "| **Size** | Market Cap, Enterprise Value | Quelle est la taille? |\n",
    "\n",
    "### Utilisation dans QuantConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de code QuantConnect pour features fondamentales\n",
    "# A copier dans l'IDE QuantConnect\n",
    "\n",
    "qc_fundamental_code = '''\n",
    "def calculate_fundamental_features(self, fundamentals):\n",
    "    \"\"\"\n",
    "    Extrait les features fondamentales depuis QuantConnect.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fundamentals : QuantConnect.Data.Fundamental.Fundamental\n",
    "        Objet fondamental pour un actif\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionnaire de features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # === VALUATION RATIOS ===\n",
    "    if fundamentals.ValuationRatios is not None:\n",
    "        vr = fundamentals.ValuationRatios\n",
    "        \n",
    "        # Price to Earnings\n",
    "        features['pe_ratio'] = vr.PERatio if hasattr(vr, 'PERatio') else np.nan\n",
    "        \n",
    "        # Price to Book\n",
    "        features['pb_ratio'] = vr.PBRatio if hasattr(vr, 'PBRatio') else np.nan\n",
    "        \n",
    "        # Price to Sales\n",
    "        features['ps_ratio'] = vr.PSRatio if hasattr(vr, 'PSRatio') else np.nan\n",
    "        \n",
    "        # Price to Cash Flow\n",
    "        features['pcf_ratio'] = vr.PCFRatio if hasattr(vr, 'PCFRatio') else np.nan\n",
    "        \n",
    "        # Dividend Yield\n",
    "        features['dividend_yield'] = vr.DividendYield if hasattr(vr, 'DividendYield') else 0\n",
    "        \n",
    "        # Earnings Yield (inverse of P/E)\n",
    "        features['earnings_yield'] = vr.EarningYield if hasattr(vr, 'EarningYield') else np.nan\n",
    "    \n",
    "    # === PROFITABILITY ===\n",
    "    if fundamentals.OperationRatios is not None:\n",
    "        op = fundamentals.OperationRatios\n",
    "        \n",
    "        # Return on Equity\n",
    "        if hasattr(op, 'ROE') and op.ROE is not None:\n",
    "            features['roe'] = op.ROE.Value if hasattr(op.ROE, 'Value') else op.ROE\n",
    "        else:\n",
    "            features['roe'] = np.nan\n",
    "        \n",
    "        # Return on Assets\n",
    "        if hasattr(op, 'ROA') and op.ROA is not None:\n",
    "            features['roa'] = op.ROA.Value if hasattr(op.ROA, 'Value') else op.ROA\n",
    "        else:\n",
    "            features['roa'] = np.nan\n",
    "        \n",
    "        # Return on Invested Capital\n",
    "        if hasattr(op, 'ROIC') and op.ROIC is not None:\n",
    "            features['roic'] = op.ROIC.Value if hasattr(op.ROIC, 'Value') else op.ROIC\n",
    "        else:\n",
    "            features['roic'] = np.nan\n",
    "        \n",
    "        # Profit Margins\n",
    "        if hasattr(op, 'NetMargin') and op.NetMargin is not None:\n",
    "            features['net_margin'] = op.NetMargin.Value if hasattr(op.NetMargin, 'Value') else op.NetMargin\n",
    "        else:\n",
    "            features['net_margin'] = np.nan\n",
    "        \n",
    "        if hasattr(op, 'GrossMargin') and op.GrossMargin is not None:\n",
    "            features['gross_margin'] = op.GrossMargin.Value if hasattr(op.GrossMargin, 'Value') else op.GrossMargin\n",
    "        else:\n",
    "            features['gross_margin'] = np.nan\n",
    "        \n",
    "        # Asset Turnover\n",
    "        if hasattr(op, 'AssetsTurnover') and op.AssetsTurnover is not None:\n",
    "            features['asset_turnover'] = op.AssetsTurnover.Value if hasattr(op.AssetsTurnover, 'Value') else op.AssetsTurnover\n",
    "        else:\n",
    "            features['asset_turnover'] = np.nan\n",
    "    \n",
    "    # === GROWTH ===\n",
    "    if fundamentals.OperationRatios is not None:\n",
    "        op = fundamentals.OperationRatios\n",
    "        \n",
    "        # Revenue Growth\n",
    "        if hasattr(op, 'RevenueGrowth') and op.RevenueGrowth is not None:\n",
    "            if hasattr(op.RevenueGrowth, 'ThreeMonths'):\n",
    "                features['revenue_growth_3m'] = op.RevenueGrowth.ThreeMonths\n",
    "            if hasattr(op.RevenueGrowth, 'OneYear'):\n",
    "                features['revenue_growth_1y'] = op.RevenueGrowth.OneYear\n",
    "    \n",
    "    if fundamentals.EarningRatios is not None:\n",
    "        er = fundamentals.EarningRatios\n",
    "        \n",
    "        # EPS Growth\n",
    "        if hasattr(er, 'DilutedEPSGrowth') and er.DilutedEPSGrowth is not None:\n",
    "            features['eps_growth'] = er.DilutedEPSGrowth\n",
    "    \n",
    "    # === DEBT / LEVERAGE ===\n",
    "    if fundamentals.OperationRatios is not None:\n",
    "        op = fundamentals.OperationRatios\n",
    "        \n",
    "        # Debt to Equity\n",
    "        if hasattr(op, 'DebttoEquityRatio') and op.DebttoEquityRatio is not None:\n",
    "            features['debt_to_equity'] = op.DebttoEquityRatio\n",
    "        else:\n",
    "            features['debt_to_equity'] = np.nan\n",
    "        \n",
    "        # Current Ratio\n",
    "        if hasattr(op, 'CurrentRatio') and op.CurrentRatio is not None:\n",
    "            features['current_ratio'] = op.CurrentRatio.Value if hasattr(op.CurrentRatio, 'Value') else op.CurrentRatio\n",
    "        else:\n",
    "            features['current_ratio'] = np.nan\n",
    "    \n",
    "    # === SIZE ===\n",
    "    # Market Cap\n",
    "    if hasattr(fundamentals, 'MarketCap'):\n",
    "        features['market_cap'] = fundamentals.MarketCap\n",
    "        features['log_market_cap'] = np.log(fundamentals.MarketCap) if fundamentals.MarketCap > 0 else np.nan\n",
    "    \n",
    "    return features\n",
    "'''\n",
    "\n",
    "print(\"Code QuantConnect pour features fondamentales:\")\n",
    "print(qc_fundamental_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuler des donnees fondamentales pour demonstration\n",
    "\n",
    "def generate_fundamental_data(n_stocks=50, seed=42):\n",
    "    \"\"\"\n",
    "    Genere des donnees fondamentales simulees.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame avec features fondamentales\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Tickers simulees\n",
    "    tickers = [f'STOCK_{i:03d}' for i in range(n_stocks)]\n",
    "    \n",
    "    # Generer features avec distributions realistes\n",
    "    data = {\n",
    "        'ticker': tickers,\n",
    "        \n",
    "        # Valuation (log-normal)\n",
    "        'pe_ratio': np.exp(np.random.normal(2.7, 0.6, n_stocks)),  # Mean ~15\n",
    "        'pb_ratio': np.exp(np.random.normal(0.7, 0.5, n_stocks)),  # Mean ~2\n",
    "        'ps_ratio': np.exp(np.random.normal(0.5, 0.7, n_stocks)),  # Mean ~1.6\n",
    "        'dividend_yield': np.abs(np.random.normal(0.02, 0.015, n_stocks)),\n",
    "        \n",
    "        # Profitability (normal avec bounds)\n",
    "        'roe': np.clip(np.random.normal(0.15, 0.1, n_stocks), -0.3, 0.5),\n",
    "        'roa': np.clip(np.random.normal(0.08, 0.06, n_stocks), -0.2, 0.3),\n",
    "        'net_margin': np.clip(np.random.normal(0.10, 0.08, n_stocks), -0.2, 0.4),\n",
    "        'gross_margin': np.clip(np.random.normal(0.35, 0.15, n_stocks), 0.1, 0.8),\n",
    "        \n",
    "        # Growth (normal)\n",
    "        'revenue_growth': np.random.normal(0.08, 0.15, n_stocks),\n",
    "        'eps_growth': np.random.normal(0.10, 0.25, n_stocks),\n",
    "        \n",
    "        # Debt (positive)\n",
    "        'debt_to_equity': np.abs(np.random.normal(0.8, 0.5, n_stocks)),\n",
    "        'current_ratio': np.abs(np.random.normal(1.5, 0.5, n_stocks)),\n",
    "        \n",
    "        # Size (log-normal)\n",
    "        'market_cap': np.exp(np.random.normal(23, 2, n_stocks)),  # En dollars\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['log_market_cap'] = np.log(df['market_cap'])\n",
    "    \n",
    "    # Ajouter earnings_yield (inverse de P/E)\n",
    "    df['earnings_yield'] = 1 / df['pe_ratio']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generer\n",
    "df_fundamentals = generate_fundamental_data(n_stocks=50)\n",
    "\n",
    "print(\"Donnees fondamentales simulees:\")\n",
    "print(f\"  Nombre d'actions: {len(df_fundamentals)}\")\n",
    "print(f\"\\nApercu:\")\n",
    "print(df_fundamentals.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des features fondamentales\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# P/E Ratio distribution\n",
    "axes[0, 0].hist(df_fundamentals['pe_ratio'], bins=20, color='steelblue', edgecolor='white')\n",
    "axes[0, 0].axvline(df_fundamentals['pe_ratio'].median(), color='red', linestyle='--', label=f\"Median: {df_fundamentals['pe_ratio'].median():.1f}\")\n",
    "axes[0, 0].set_xlabel('P/E Ratio')\n",
    "axes[0, 0].set_title('Distribution du P/E Ratio')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# ROE distribution\n",
    "axes[0, 1].hist(df_fundamentals['roe'] * 100, bins=20, color='forestgreen', edgecolor='white')\n",
    "axes[0, 1].axvline(df_fundamentals['roe'].median() * 100, color='red', linestyle='--', label=f\"Median: {df_fundamentals['roe'].median()*100:.1f}%\")\n",
    "axes[0, 1].set_xlabel('ROE (%)')\n",
    "axes[0, 1].set_title('Distribution du ROE')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Debt to Equity\n",
    "axes[0, 2].hist(df_fundamentals['debt_to_equity'], bins=20, color='coral', edgecolor='white')\n",
    "axes[0, 2].axvline(1.0, color='black', linestyle='--', label='D/E = 1.0')\n",
    "axes[0, 2].set_xlabel('Debt/Equity')\n",
    "axes[0, 2].set_title('Distribution du Debt/Equity')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Scatter P/E vs ROE (Value vs Quality)\n",
    "scatter = axes[1, 0].scatter(df_fundamentals['pe_ratio'], df_fundamentals['roe'] * 100,\n",
    "                              c=df_fundamentals['log_market_cap'], cmap='viridis', alpha=0.6)\n",
    "axes[1, 0].set_xlabel('P/E Ratio')\n",
    "axes[1, 0].set_ylabel('ROE (%)')\n",
    "axes[1, 0].set_title('P/E vs ROE (couleur = Market Cap)')\n",
    "plt.colorbar(scatter, ax=axes[1, 0], label='Log Market Cap')\n",
    "\n",
    "# Growth vs Margin\n",
    "scatter2 = axes[1, 1].scatter(df_fundamentals['revenue_growth'] * 100, df_fundamentals['net_margin'] * 100,\n",
    "                               c=df_fundamentals['pe_ratio'], cmap='coolwarm', alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Revenue Growth (%)')\n",
    "axes[1, 1].set_ylabel('Net Margin (%)')\n",
    "axes[1, 1].set_title('Growth vs Margin (couleur = P/E)')\n",
    "axes[1, 1].axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.colorbar(scatter2, ax=axes[1, 1], label='P/E Ratio')\n",
    "\n",
    "# Correlation heatmap\n",
    "fundamental_cols = ['pe_ratio', 'pb_ratio', 'roe', 'roa', 'net_margin', \n",
    "                    'revenue_growth', 'debt_to_equity', 'log_market_cap']\n",
    "corr = df_fundamentals[fundamental_cols].corr()\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0, ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Correlation entre Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 5 : Labeling pour Classification et Regression (20 min)\n",
    "\n",
    "### Qu'est-ce que le labeling?\n",
    "\n",
    "Le **labeling** est le processus de creation des **targets** (y) a partir des donnees de prix futurs. C'est une etape critique car elle definit ce que le modele va apprendre a predire.\n",
    "\n",
    "### Types de labels\n",
    "\n",
    "| Type | Description | Utilisation |\n",
    "|------|-------------|-------------|\n",
    "| **Binary** | Up (1) / Down (0) | Classification simple |\n",
    "| **Ternary** | Up (1) / Flat (0) / Down (-1) | Classification avec neutre |\n",
    "| **Multi-class** | Quantiles (Q1, Q2, Q3, Q4) | Classification fine |\n",
    "| **Regression** | Rendement continu | Prediction du rendement exact |\n",
    "\n",
    "### Horizon de prediction\n",
    "\n",
    "```\n",
    "t=0 (maintenant)                    t=horizon (futur)\n",
    "    |                                    |\n",
    "    v                                    v\n",
    "[Features calculees]  ---> [Modele] ---> [Label = future return]\n",
    "```\n",
    "\n",
    "**Attention au lookahead bias!** Le label utilise des donnees futures, donc ne jamais l'inclure dans les features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_labels(df, horizon=5, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Cree des labels pour classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame avec colonne 'close'\n",
    "    horizon : int\n",
    "        Nombre de jours pour le rendement futur\n",
    "    threshold : float\n",
    "        Seuil pour separer up/down (0 = simple direction)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame avec labels ajoutes\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Rendement futur sur l'horizon\n",
    "    result['future_return'] = result['close'].shift(-horizon) / result['close'] - 1\n",
    "    \n",
    "    # === BINARY LABEL ===\n",
    "    # 1 si rendement > threshold, 0 sinon\n",
    "    result['label_binary'] = (result['future_return'] > threshold).astype(int)\n",
    "    \n",
    "    # === TERNARY LABEL ===\n",
    "    # 1 (up), 0 (flat), -1 (down)\n",
    "    result['label_ternary'] = 0\n",
    "    result.loc[result['future_return'] > threshold, 'label_ternary'] = 1\n",
    "    result.loc[result['future_return'] < -threshold, 'label_ternary'] = -1\n",
    "    \n",
    "    # === QUANTILE LABEL ===\n",
    "    # Diviser en quartiles (0, 1, 2, 3)\n",
    "    result['label_quantile'] = pd.qcut(\n",
    "        result['future_return'].dropna(), \n",
    "        q=4, \n",
    "        labels=[0, 1, 2, 3],\n",
    "        duplicates='drop'\n",
    "    ).reindex(result.index)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def create_regression_labels(df, horizon=5):\n",
    "    \"\"\"\n",
    "    Cree des labels pour regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame avec colonne 'close'\n",
    "    horizon : int\n",
    "        Nombre de jours pour la prediction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame avec labels regression ajoutes\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Rendement futur (target principal)\n",
    "    result['target_return'] = result['close'].shift(-horizon) / result['close'] - 1\n",
    "    \n",
    "    # Log-return (souvent mieux pour ML)\n",
    "    result['target_log_return'] = np.log(result['close'].shift(-horizon) / result['close'])\n",
    "    \n",
    "    # Volatilite future (pour prediction de risque)\n",
    "    result['target_volatility'] = result['return_1d'].shift(-horizon).rolling(horizon).std()\n",
    "    \n",
    "    # Sharpe-like ratio (rendement / volatilite)\n",
    "    result['target_sharpe'] = result['target_return'] / (result['target_volatility'] + 1e-8)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Appliquer les deux types de labeling\n",
    "df_labeled = create_classification_labels(df_features, horizon=5, threshold=0.01)\n",
    "df_labeled = create_regression_labels(df_labeled, horizon=5)\n",
    "\n",
    "print(\"Labels crees:\")\n",
    "print(\"\\nClassification:\")\n",
    "print(f\"  - label_binary: {df_labeled['label_binary'].value_counts().to_dict()}\")\n",
    "print(f\"  - label_ternary: {df_labeled['label_ternary'].value_counts().to_dict()}\")\n",
    "print(\"\\nRegression:\")\n",
    "print(f\"  - target_return: mean={df_labeled['target_return'].mean():.4f}, std={df_labeled['target_return'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des labels\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Distribution du rendement futur\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(df_labeled['future_return'].dropna() * 100, bins=50, color='steelblue', edgecolor='white')\n",
    "ax1.axvline(0, color='red', linestyle='--', label='Zero')\n",
    "ax1.axvline(1, color='green', linestyle=':', label='+1% threshold')\n",
    "ax1.axvline(-1, color='orange', linestyle=':', label='-1% threshold')\n",
    "ax1.set_xlabel('Future Return (%)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title(f'Distribution du Rendement Futur ({5}D)')\n",
    "ax1.legend()\n",
    "\n",
    "# Distribution des labels binaires\n",
    "ax2 = axes[0, 1]\n",
    "counts = df_labeled['label_binary'].value_counts().sort_index()\n",
    "bars = ax2.bar(['Down (0)', 'Up (1)'], counts.values, color=['red', 'green'])\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Distribution des Labels Binaires')\n",
    "for bar, count in zip(bars, counts.values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "             f'{count}\\n({count/counts.sum()*100:.1f}%)', ha='center')\n",
    "\n",
    "# Distribution des labels ternaires\n",
    "ax3 = axes[1, 0]\n",
    "counts_ternary = df_labeled['label_ternary'].value_counts().sort_index()\n",
    "bars = ax3.bar(['Down (-1)', 'Flat (0)', 'Up (1)'], counts_ternary.values, \n",
    "               color=['red', 'gray', 'green'])\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Distribution des Labels Ternaires (threshold=1%)')\n",
    "for bar, count in zip(bars, counts_ternary.values):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "             f'{count}\\n({count/counts_ternary.sum()*100:.1f}%)', ha='center')\n",
    "\n",
    "# Rendement futur vs indicateurs\n",
    "ax4 = axes[1, 1]\n",
    "valid_data = df_labeled.dropna(subset=['future_return', 'rsi_14'])\n",
    "scatter = ax4.scatter(valid_data['rsi_14'], valid_data['future_return'] * 100,\n",
    "                      c=valid_data['label_binary'], cmap='RdYlGn', alpha=0.5)\n",
    "ax4.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax4.axvline(30, color='blue', linestyle='--', alpha=0.5, label='RSI Oversold')\n",
    "ax4.axvline(70, color='blue', linestyle='--', alpha=0.5, label='RSI Overbought')\n",
    "ax4.set_xlabel('RSI (14)')\n",
    "ax4.set_ylabel('Future Return (%)')\n",
    "ax4.set_title('RSI vs Rendement Futur')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methodes de labeling avancees\n",
    "\n",
    "def create_triple_barrier_labels(df, horizon=5, profit_take=0.02, stop_loss=0.02):\n",
    "    \"\"\"\n",
    "    Methode Triple Barrier de Lopez de Prado.\n",
    "    \n",
    "    Le label est determine par quelle barriere est touchee en premier:\n",
    "    - Upper barrier (profit take): label = 1\n",
    "    - Lower barrier (stop loss): label = -1\n",
    "    - Time barrier (horizon): label = sign(return)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame avec colonnes OHLCV\n",
    "    horizon : int\n",
    "        Nombre de jours maximum\n",
    "    profit_take : float\n",
    "        Seuil de profit (ex: 0.02 = 2%)\n",
    "    stop_loss : float\n",
    "        Seuil de perte (ex: 0.02 = 2%)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Labels (-1, 0, 1)\n",
    "    \"\"\"\n",
    "    labels = pd.Series(index=df.index, dtype=float)\n",
    "    \n",
    "    for i in range(len(df) - horizon):\n",
    "        entry_price = df['close'].iloc[i]\n",
    "        upper_barrier = entry_price * (1 + profit_take)\n",
    "        lower_barrier = entry_price * (1 - stop_loss)\n",
    "        \n",
    "        # Regarder les jours suivants\n",
    "        for j in range(1, horizon + 1):\n",
    "            if i + j >= len(df):\n",
    "                break\n",
    "            \n",
    "            high = df['high'].iloc[i + j]\n",
    "            low = df['low'].iloc[i + j]\n",
    "            close = df['close'].iloc[i + j]\n",
    "            \n",
    "            # Upper barrier touchee?\n",
    "            if high >= upper_barrier:\n",
    "                labels.iloc[i] = 1\n",
    "                break\n",
    "            \n",
    "            # Lower barrier touchee?\n",
    "            if low <= lower_barrier:\n",
    "                labels.iloc[i] = -1\n",
    "                break\n",
    "            \n",
    "            # Time barrier (dernier jour)\n",
    "            if j == horizon:\n",
    "                ret = (close - entry_price) / entry_price\n",
    "                if abs(ret) < 0.005:  # < 0.5% = flat\n",
    "                    labels.iloc[i] = 0\n",
    "                else:\n",
    "                    labels.iloc[i] = 1 if ret > 0 else -1\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Appliquer Triple Barrier\n",
    "df_labeled['label_triple_barrier'] = create_triple_barrier_labels(\n",
    "    df_labeled, horizon=5, profit_take=0.02, stop_loss=0.02\n",
    ")\n",
    "\n",
    "print(\"Triple Barrier Labels:\")\n",
    "print(df_labeled['label_triple_barrier'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nAvantages du Triple Barrier:\")\n",
    "print(\"  - Plus realiste (simule take profit / stop loss)\")\n",
    "print(\"  - Capture le timing du mouvement\")\n",
    "print(\"  - Moins de labels neutres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 6 : Feature Selection et Importance (20 min)\n",
    "\n",
    "### Pourquoi selectionner les features?\n",
    "\n",
    "| Raison | Explication |\n",
    "|--------|-------------|\n",
    "| **Overfitting** | Trop de features = modele memorise le bruit |\n",
    "| **Curse of dimensionality** | Performance degrade avec dimensions |\n",
    "| **Interpretabilite** | Moins de features = plus comprehensible |\n",
    "| **Vitesse** | Moins de features = entrainement plus rapide |\n",
    "\n",
    "### Techniques de selection\n",
    "\n",
    "```\n",
    "Feature Selection\n",
    "       |\n",
    "       +-- Filter Methods (rapide, independant du modele)\n",
    "       |      +-- Correlation avec target\n",
    "       |      +-- Variance threshold\n",
    "       |      +-- Mutual Information\n",
    "       |\n",
    "       +-- Wrapper Methods (lent, modele-dependant)\n",
    "       |      +-- Forward selection\n",
    "       |      +-- Backward elimination\n",
    "       |      +-- RFE (Recursive Feature Elimination)\n",
    "       |\n",
    "       +-- Embedded Methods (integre dans le modele)\n",
    "              +-- L1 Regularization (Lasso)\n",
    "              +-- Tree-based importance\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "\n",
    "def calculate_feature_importance(X, y, method='random_forest'):\n",
    "    \"\"\"\n",
    "    Calcule l'importance des features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Features\n",
    "    y : pd.Series\n",
    "        Labels\n",
    "    method : str\n",
    "        'random_forest', 'mutual_info', ou 'correlation'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame avec features et leur importance\n",
    "    \"\"\"\n",
    "    if method == 'random_forest':\n",
    "        # Tree-based importance\n",
    "        model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        model.fit(X, y)\n",
    "        importance = model.feature_importances_\n",
    "        \n",
    "    elif method == 'mutual_info':\n",
    "        # Information mutuelle (capture non-linearites)\n",
    "        importance = mutual_info_classif(X, y, random_state=42)\n",
    "        \n",
    "    elif method == 'correlation':\n",
    "        # Correlation absolue avec target\n",
    "        importance = X.apply(lambda col: abs(col.corr(y))).values\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Normaliser (0-1)\n",
    "    importance_df['importance_normalized'] = (\n",
    "        importance_df['importance'] / importance_df['importance'].max()\n",
    "    )\n",
    "    \n",
    "    return importance_df.reset_index(drop=True)\n",
    "\n",
    "# Preparer les donnees pour feature selection\n",
    "feature_cols = [col for col in df_labeled.columns \n",
    "                if col not in ['open', 'high', 'low', 'close', 'volume',\n",
    "                               'future_return', 'label_binary', 'label_ternary',\n",
    "                               'label_quantile', 'target_return', 'target_log_return',\n",
    "                               'target_volatility', 'target_sharpe', 'label_triple_barrier']]\n",
    "\n",
    "# Supprimer les lignes avec NaN\n",
    "df_clean = df_labeled.dropna(subset=feature_cols + ['label_binary'])\n",
    "\n",
    "X = df_clean[feature_cols]\n",
    "y = df_clean['label_binary']\n",
    "\n",
    "print(f\"Nombre de features: {len(feature_cols)}\")\n",
    "print(f\"Nombre d'echantillons: {len(df_clean)}\")\n",
    "\n",
    "# Calculer importance avec Random Forest\n",
    "importance_rf = calculate_feature_importance(X, y, method='random_forest')\n",
    "\n",
    "print(\"\\nTop 15 Features (Random Forest Importance):\")\n",
    "print(importance_rf.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_features(df, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Supprime les features fortement correlees.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame de features\n",
    "    threshold : float\n",
    "        Seuil de correlation (ex: 0.95 = 95%)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (DataFrame filtre, liste des features supprimees)\n",
    "    \"\"\"\n",
    "    # Matrice de correlation\n",
    "    corr_matrix = df.corr().abs()\n",
    "    \n",
    "    # Triangle superieur (eviter doublons)\n",
    "    upper = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Features a supprimer (correlation > threshold)\n",
    "    to_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
    "    \n",
    "    # Supprimer\n",
    "    df_filtered = df.drop(columns=to_drop)\n",
    "    \n",
    "    return df_filtered, to_drop\n",
    "\n",
    "# Appliquer\n",
    "X_filtered, dropped_features = remove_correlated_features(X, threshold=0.90)\n",
    "\n",
    "print(f\"Features supprimees (correlation > 90%):\")\n",
    "for f in dropped_features:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\nFeatures restantes: {len(X_filtered.columns)} (vs {len(X.columns)} avant)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'importance des features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Barplot importance\n",
    "ax1 = axes[0]\n",
    "top_features = importance_rf.head(20)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
    "bars = ax1.barh(range(len(top_features)), top_features['importance_normalized'], color=colors)\n",
    "ax1.set_yticks(range(len(top_features)))\n",
    "ax1.set_yticklabels(top_features['feature'])\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Importance Normalisee')\n",
    "ax1.set_title('Top 20 Features par Importance (Random Forest)')\n",
    "\n",
    "# Heatmap correlation des top features\n",
    "ax2 = axes[1]\n",
    "top_feature_names = importance_rf.head(10)['feature'].tolist()\n",
    "corr_top = X[top_feature_names].corr()\n",
    "sns.heatmap(corr_top, annot=True, fmt='.2f', cmap='RdBu_r', center=0, ax=ax2)\n",
    "ax2.set_title('Correlation entre Top 10 Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 7 : Preprocessing et Normalization (15 min)\n",
    "\n",
    "### Pourquoi normaliser?\n",
    "\n",
    "| Raison | Explication |\n",
    "|--------|-------------|\n",
    "| **Echelle** | Features avec grandes valeurs dominent |\n",
    "| **Gradient** | Convergence plus rapide (neural networks) |\n",
    "| **Distance** | K-NN, SVM sensibles a l'echelle |\n",
    "| **Regularisation** | L1/L2 penalisent differemment selon echelle |\n",
    "\n",
    "### Methodes de normalisation\n",
    "\n",
    "| Methode | Formule | Quand utiliser |\n",
    "|---------|---------|----------------|\n",
    "| **StandardScaler** | `(x - mean) / std` | Distribution ~normale |\n",
    "| **MinMaxScaler** | `(x - min) / (max - min)` | Range [0, 1] souhaite |\n",
    "| **RobustScaler** | `(x - median) / IQR` | Donnees avec outliers |\n",
    "\n",
    "### Rolling Normalization pour eviter lookahead\n",
    "\n",
    "En trading, on ne peut pas utiliser mean/std globaux (lookahead bias). On utilise une **normalisation roulante** :\n",
    "\n",
    "```python\n",
    "# WRONG (lookahead bias)\n",
    "scaler.fit(all_data)\n",
    "\n",
    "# CORRECT (rolling)\n",
    "rolling_mean = data.rolling(window).mean()\n",
    "rolling_std = data.rolling(window).std()\n",
    "normalized = (data - rolling_mean) / rolling_std\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "def preprocess_features_sklearn(X_train, X_test, method='standard'):\n",
    "    \"\"\"\n",
    "    Preprocessing avec scikit-learn (pour split train/test).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pd.DataFrame\n",
    "        Features d'entrainement\n",
    "    X_test : pd.DataFrame\n",
    "        Features de test\n",
    "    method : str\n",
    "        'standard', 'minmax', ou 'robust'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (X_train_scaled, X_test_scaled, scaler)\n",
    "    \"\"\"\n",
    "    if method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # Fit sur train, transform sur les deux\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train),\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    \n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "\n",
    "def rolling_normalize(df, window=252):\n",
    "    \"\"\"\n",
    "    Normalisation roulante pour eviter lookahead bias.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Features a normaliser\n",
    "    window : int\n",
    "        Fenetre pour calcul mean/std (ex: 252 = 1 an)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Features normalisees (Z-score roulant)\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        rolling_mean = df[col].rolling(window).mean()\n",
    "        rolling_std = df[col].rolling(window).std()\n",
    "        \n",
    "        # Z-score avec rolling stats\n",
    "        result[col] = (df[col] - rolling_mean) / (rolling_std + 1e-8)\n",
    "        \n",
    "        # Clip outliers extremes\n",
    "        result[col] = result[col].clip(-3, 3)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def percentile_rank(df, window=252):\n",
    "    \"\"\"\n",
    "    Rang percentile roulant (robuste aux outliers).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Features\n",
    "    window : int\n",
    "        Fenetre\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Rangs percentiles [0, 1]\n",
    "    \"\"\"\n",
    "    def rolling_percentile(series, window):\n",
    "        def rank_pct(x):\n",
    "            return pd.Series(x).rank(pct=True).iloc[-1]\n",
    "        return series.rolling(window).apply(rank_pct, raw=True)\n",
    "    \n",
    "    result = pd.DataFrame(index=df.index)\n",
    "    for col in df.columns:\n",
    "        result[col] = rolling_percentile(df[col], window)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Demonstration\n",
    "print(\"Methodes de normalisation:\")\n",
    "print(\"\\n1. StandardScaler (sklearn):\")\n",
    "print(\"   - Fit sur train, transform sur test\")\n",
    "print(\"   - Resultat: mean=0, std=1\")\n",
    "\n",
    "print(\"\\n2. Rolling Normalize:\")\n",
    "print(\"   - Pas de lookahead bias\")\n",
    "print(\"   - Utilise mean/std des N derniers jours\")\n",
    "\n",
    "print(\"\\n3. Percentile Rank:\")\n",
    "print(\"   - Robuste aux outliers\")\n",
    "print(\"   - Resultat: [0, 1] = percentile actuel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer rolling normalization\n",
    "X_rolling_norm = rolling_normalize(X_filtered, window=60)\n",
    "\n",
    "# Visualiser l'effet\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Avant normalisation\n",
    "feature_example = 'rsi_14'\n",
    "if feature_example in X_filtered.columns:\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(X_filtered.index, X_filtered[feature_example])\n",
    "    ax1.set_title(f'{feature_example} - Avant Normalisation')\n",
    "    ax1.set_ylabel('Valeur brute')\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(X_filtered[feature_example].dropna(), bins=50, color='steelblue', edgecolor='white')\n",
    "    ax2.set_title(f'{feature_example} - Distribution (brute)')\n",
    "\n",
    "# Apres normalisation\n",
    "if feature_example in X_rolling_norm.columns:\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(X_rolling_norm.index, X_rolling_norm[feature_example])\n",
    "    ax3.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax3.axhline(2, color='orange', linestyle=':', alpha=0.5)\n",
    "    ax3.axhline(-2, color='orange', linestyle=':', alpha=0.5)\n",
    "    ax3.set_title(f'{feature_example} - Apres Rolling Normalization')\n",
    "    ax3.set_ylabel('Z-score')\n",
    "    ax3.set_xlabel('Date')\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.hist(X_rolling_norm[feature_example].dropna(), bins=50, color='forestgreen', edgecolor='white')\n",
    "    ax4.axvline(0, color='red', linestyle='--')\n",
    "    ax4.set_title(f'{feature_example} - Distribution (normalisee)')\n",
    "    ax4.set_xlabel('Z-score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStatistiques avant/apres:\")\n",
    "print(f\"  Avant: mean={X_filtered[feature_example].mean():.2f}, std={X_filtered[feature_example].std():.2f}\")\n",
    "print(f\"  Apres: mean={X_rolling_norm[feature_example].mean():.2f}, std={X_rolling_norm[feature_example].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 8 : Pipeline Complet de Feature Engineering (20 min)\n",
    "\n",
    "### Architecture du pipeline\n",
    "\n",
    "```\n",
    "Donnees OHLCV\n",
    "     |\n",
    "     v\n",
    "+--------------------+\n",
    "| 1. Price Features  |  (returns, volatility, range)\n",
    "+--------------------+\n",
    "     |\n",
    "     v\n",
    "+--------------------+\n",
    "| 2. Indicator Feat. |  (RSI, MACD, BB, ADX)\n",
    "+--------------------+\n",
    "     |\n",
    "     v\n",
    "+--------------------+\n",
    "| 3. Create Labels   |  (future returns, classification)\n",
    "+--------------------+\n",
    "     |\n",
    "     v\n",
    "+--------------------+\n",
    "| 4. Train/Test Split|  (temporal, walk-forward)\n",
    "+--------------------+\n",
    "     |\n",
    "     v\n",
    "+--------------------+\n",
    "| 5. Normalization   |  (rolling z-score)\n",
    "+--------------------+\n",
    "     |\n",
    "     v\n",
    "+--------------------+\n",
    "| 6. Feature Select. |  (importance, correlation)\n",
    "+--------------------+\n",
    "     |\n",
    "     v\n",
    "X_train, X_test, y_train, y_test\n",
    "      (Pret pour ML)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineeringPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline complet de Feature Engineering pour ML Trading.\n",
    "    \n",
    "    Etapes:\n",
    "    1. Calcul des features techniques\n",
    "    2. Creation des labels\n",
    "    3. Train/Test split temporel\n",
    "    4. Normalisation\n",
    "    5. Selection des features\n",
    "    \n",
    "    Usage:\n",
    "        pipeline = FeatureEngineeringPipeline(horizon=5, train_ratio=0.7)\n",
    "        X_train, X_test, y_train, y_test = pipeline.fit_transform(df_ohlcv)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 horizon=5,\n",
    "                 train_ratio=0.7,\n",
    "                 label_type='binary',\n",
    "                 label_threshold=0.0,\n",
    "                 norm_window=60,\n",
    "                 n_features=20,\n",
    "                 correlation_threshold=0.90):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        horizon : int\n",
    "            Horizon de prediction (jours)\n",
    "        train_ratio : float\n",
    "            Ratio train/test\n",
    "        label_type : str\n",
    "            'binary', 'ternary', ou 'regression'\n",
    "        label_threshold : float\n",
    "            Seuil pour labels (classification)\n",
    "        norm_window : int\n",
    "            Fenetre pour rolling normalization\n",
    "        n_features : int\n",
    "            Nombre de features a garder\n",
    "        correlation_threshold : float\n",
    "            Seuil pour supprimer features correlees\n",
    "        \"\"\"\n",
    "        self.horizon = horizon\n",
    "        self.train_ratio = train_ratio\n",
    "        self.label_type = label_type\n",
    "        self.label_threshold = label_threshold\n",
    "        self.norm_window = norm_window\n",
    "        self.n_features = n_features\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        \n",
    "        # Stockage apres fit\n",
    "        self.feature_names_ = None\n",
    "        self.selected_features_ = None\n",
    "        self.feature_importance_ = None\n",
    "        self.scaler_ = None\n",
    "    \n",
    "    def _calculate_features(self, df):\n",
    "        \"\"\"Calcule toutes les features techniques.\"\"\"\n",
    "        result = df.copy()\n",
    "        \n",
    "        # Price features\n",
    "        result = calculate_price_features(result)\n",
    "        \n",
    "        # Indicator features\n",
    "        result = calculate_indicator_features(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _create_labels(self, df):\n",
    "        \"\"\"Cree les labels selon le type.\"\"\"\n",
    "        # Rendement futur\n",
    "        df['future_return'] = df['close'].shift(-self.horizon) / df['close'] - 1\n",
    "        \n",
    "        if self.label_type == 'binary':\n",
    "            df['label'] = (df['future_return'] > self.label_threshold).astype(int)\n",
    "        elif self.label_type == 'ternary':\n",
    "            df['label'] = 0\n",
    "            df.loc[df['future_return'] > self.label_threshold, 'label'] = 1\n",
    "            df.loc[df['future_return'] < -self.label_threshold, 'label'] = -1\n",
    "        elif self.label_type == 'regression':\n",
    "            df['label'] = df['future_return']\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _temporal_split(self, df):\n",
    "        \"\"\"Split temporel train/test.\"\"\"\n",
    "        split_idx = int(len(df) * self.train_ratio)\n",
    "        train = df.iloc[:split_idx]\n",
    "        test = df.iloc[split_idx:]\n",
    "        return train, test\n",
    "    \n",
    "    def _normalize(self, X_train, X_test):\n",
    "        \"\"\"Normalise les features.\"\"\"\n",
    "        self.scaler_ = StandardScaler()\n",
    "        \n",
    "        X_train_norm = pd.DataFrame(\n",
    "            self.scaler_.fit_transform(X_train),\n",
    "            columns=X_train.columns,\n",
    "            index=X_train.index\n",
    "        )\n",
    "        \n",
    "        X_test_norm = pd.DataFrame(\n",
    "            self.scaler_.transform(X_test),\n",
    "            columns=X_test.columns,\n",
    "            index=X_test.index\n",
    "        )\n",
    "        \n",
    "        return X_train_norm, X_test_norm\n",
    "    \n",
    "    def _select_features(self, X_train, y_train):\n",
    "        \"\"\"Selectionne les meilleures features.\"\"\"\n",
    "        # 1. Supprimer features tres correlees\n",
    "        X_filtered, dropped = remove_correlated_features(X_train, self.correlation_threshold)\n",
    "        \n",
    "        # 2. Calculer importance\n",
    "        self.feature_importance_ = calculate_feature_importance(\n",
    "            X_filtered, y_train, method='random_forest'\n",
    "        )\n",
    "        \n",
    "        # 3. Garder top N features\n",
    "        self.selected_features_ = self.feature_importance_.head(self.n_features)['feature'].tolist()\n",
    "        \n",
    "        return self.selected_features_\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"\n",
    "        Execute le pipeline complet.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Donnees OHLCV brutes\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple : (X_train, X_test, y_train, y_test)\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"FEATURE ENGINEERING PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. Calculer features\n",
    "        print(\"\\n[1/5] Calcul des features techniques...\")\n",
    "        df_features = self._calculate_features(df)\n",
    "        \n",
    "        # 2. Creer labels\n",
    "        print(f\"[2/5] Creation des labels ({self.label_type}, horizon={self.horizon}D)...\")\n",
    "        df_labeled = self._create_labels(df_features)\n",
    "        \n",
    "        # Identifier colonnes features vs non-features\n",
    "        non_feature_cols = ['open', 'high', 'low', 'close', 'volume', \n",
    "                           'future_return', 'label']\n",
    "        self.feature_names_ = [c for c in df_labeled.columns if c not in non_feature_cols]\n",
    "        print(f\"    {len(self.feature_names_)} features calculees\")\n",
    "        \n",
    "        # Supprimer NaN\n",
    "        df_clean = df_labeled.dropna()\n",
    "        print(f\"    {len(df_clean)} echantillons valides (apres drop NaN)\")\n",
    "        \n",
    "        # 3. Split temporel\n",
    "        print(f\"[3/5] Split temporel (train={self.train_ratio*100:.0f}%)...\")\n",
    "        train_df, test_df = self._temporal_split(df_clean)\n",
    "        \n",
    "        X_train = train_df[self.feature_names_]\n",
    "        y_train = train_df['label']\n",
    "        X_test = test_df[self.feature_names_]\n",
    "        y_test = test_df['label']\n",
    "        \n",
    "        print(f\"    Train: {len(X_train)} samples ({train_df.index[0].date()} - {train_df.index[-1].date()})\")\n",
    "        print(f\"    Test:  {len(X_test)} samples ({test_df.index[0].date()} - {test_df.index[-1].date()})\")\n",
    "        \n",
    "        # 4. Selection features\n",
    "        print(f\"[4/5] Selection des {self.n_features} meilleures features...\")\n",
    "        selected = self._select_features(X_train, y_train)\n",
    "        \n",
    "        X_train = X_train[selected]\n",
    "        X_test = X_test[selected]\n",
    "        \n",
    "        # 5. Normalisation\n",
    "        print(\"[5/5] Normalisation (StandardScaler)...\")\n",
    "        X_train, X_test = self._normalize(X_train, X_test)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nResultat:\")\n",
    "        print(f\"  X_train: {X_train.shape}\")\n",
    "        print(f\"  X_test:  {X_test.shape}\")\n",
    "        print(f\"  y_train: {len(y_train)} (distribution: {dict(y_train.value_counts())})\")\n",
    "        print(f\"  y_test:  {len(y_test)} (distribution: {dict(y_test.value_counts())})\")\n",
    "        \n",
    "        print(f\"\\nTop 10 Features selectionnees:\")\n",
    "        for i, feat in enumerate(selected[:10]):\n",
    "            imp = self.feature_importance_[self.feature_importance_['feature'] == feat]['importance'].values[0]\n",
    "            print(f\"  {i+1}. {feat} (importance: {imp:.4f})\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def transform(self, df_new):\n",
    "        \"\"\"\n",
    "        Transforme de nouvelles donnees (utilise les parametres du fit).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df_new : pd.DataFrame\n",
    "            Nouvelles donnees OHLCV\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Features transformees\n",
    "        \"\"\"\n",
    "        if self.selected_features_ is None:\n",
    "            raise ValueError(\"Pipeline not fitted. Call fit_transform first.\")\n",
    "        \n",
    "        # Calculer features\n",
    "        df_features = self._calculate_features(df_new)\n",
    "        df_clean = df_features.dropna()\n",
    "        \n",
    "        # Selectionner et normaliser\n",
    "        X = df_clean[self.selected_features_]\n",
    "        X_norm = pd.DataFrame(\n",
    "            self.scaler_.transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executer le pipeline complet\n",
    "pipeline = FeatureEngineeringPipeline(\n",
    "    horizon=5,                    # Predire direction sur 5 jours\n",
    "    train_ratio=0.7,              # 70% train, 30% test\n",
    "    label_type='binary',          # Classification binaire\n",
    "    label_threshold=0.0,          # Seuil = 0 (up/down simple)\n",
    "    n_features=15,                # Garder 15 features\n",
    "    correlation_threshold=0.90    # Supprimer si correlation > 90%\n",
    ")\n",
    "\n",
    "# Transformer les donnees\n",
    "X_train, X_test, y_train, y_test = pipeline.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification rapide avec un modele ML simple\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Entrainer un Random Forest\n",
    "print(\"\\nValidation avec Random Forest...\")\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"\\nAccuracy Train: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"Accuracy Test:  {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Down', 'Up']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter les donnees pour les notebooks ML suivants\n",
    "print(\"\\nPipeline pret pour export:\")\n",
    "print(\"\\nDonnees disponibles:\")\n",
    "print(f\"  - X_train: DataFrame {X_train.shape}\")\n",
    "print(f\"  - X_test:  DataFrame {X_test.shape}\")\n",
    "print(f\"  - y_train: Series {len(y_train)}\")\n",
    "print(f\"  - y_test:  Series {len(y_test)}\")\n",
    "print(f\"  - pipeline: FeatureEngineeringPipeline (pour transform nouvelles donnees)\")\n",
    "\n",
    "print(\"\\nFeatures selectionnees:\")\n",
    "print(f\"  {pipeline.selected_features_}\")\n",
    "\n",
    "# Code pour sauvegarder (optionnel)\n",
    "save_code = '''\n",
    "# Sauvegarder les donnees preparees\n",
    "import joblib\n",
    "\n",
    "# Sauvegarder le pipeline\n",
    "joblib.dump(pipeline, 'feature_pipeline.pkl')\n",
    "\n",
    "# Sauvegarder les donnees\n",
    "X_train.to_parquet('X_train.parquet')\n",
    "X_test.to_parquet('X_test.parquet')\n",
    "y_train.to_frame().to_parquet('y_train.parquet')\n",
    "y_test.to_frame().to_parquet('y_test.parquet')\n",
    "\n",
    "# Charger plus tard\n",
    "pipeline = joblib.load('feature_pipeline.pkl')\n",
    "X_train = pd.read_parquet('X_train.parquet')\n",
    "'''\n",
    "\n",
    "print(\"\\nCode pour sauvegarder/charger:\")\n",
    "print(save_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion et Prochaines Etapes\n",
    "\n",
    "### Recapitulatif\n",
    "\n",
    "Dans ce notebook, nous avons couvert :\n",
    "\n",
    "1. **Introduction au Feature Engineering** :\n",
    "   - Importance du \"Garbage In, Garbage Out\"\n",
    "   - Categories de features (techniques, fondamentales, alternatives)\n",
    "\n",
    "2. **Features Techniques (Price-Based)** :\n",
    "   - Rendements multi-periodes\n",
    "   - Volatilite historique\n",
    "   - Range et distance from extremes\n",
    "\n",
    "3. **Features Techniques (Indicator-Based)** :\n",
    "   - RSI, MACD, Bollinger Bands\n",
    "   - ADX, Stochastic, CCI\n",
    "   - Normalisation des indicateurs pour ML\n",
    "\n",
    "4. **Features Fondamentales** :\n",
    "   - Valuation (P/E, P/B, P/S)\n",
    "   - Profitability (ROE, ROA, Margins)\n",
    "   - Growth et Debt ratios\n",
    "\n",
    "5. **Labeling** :\n",
    "   - Classification (binary, ternary, quantile)\n",
    "   - Regression (future returns)\n",
    "   - Triple Barrier method\n",
    "\n",
    "6. **Feature Selection** :\n",
    "   - Random Forest importance\n",
    "   - Correlation filtering\n",
    "   - Mutual information\n",
    "\n",
    "7. **Preprocessing** :\n",
    "   - StandardScaler, RobustScaler\n",
    "   - Rolling normalization (anti-lookahead)\n",
    "\n",
    "8. **Pipeline Complet** :\n",
    "   - Classe `FeatureEngineeringPipeline`\n",
    "   - Train/Test split temporel\n",
    "   - Export pour ML\n",
    "\n",
    "### Points Cles a Retenir\n",
    "\n",
    "| Concept | Point Cle |\n",
    "|---------|----------|\n",
    "| **Lookahead Bias** | Ne jamais utiliser de donnees futures dans les features |\n",
    "| **Normalisation** | Rolling stats pour eviter biais, pas mean/std global |\n",
    "| **Selection** | Supprimer features correlees avant importance |\n",
    "| **Labeling** | Horizon et threshold impactent fortement les resultats |\n",
    "| **Split** | Toujours temporel en finance (pas random) |\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Les features techniques seules sont souvent insuffisantes\n",
    "- Le marche evolue, les features perdent leur pouvoir predictif (regime change)\n",
    "- Overfitting facile avec trop de features\n",
    "- Les donnees simulees ne capturent pas toute la complexite reelle\n",
    "\n",
    "### Prochaines Etapes\n",
    "\n",
    "| Notebook | Contenu |\n",
    "|----------|--------|\n",
    "| **QC-Py-19** | Classification Models (Random Forest, XGBoost) |\n",
    "| **QC-Py-20** | Regression Models et prediction de rendements |\n",
    "| **QC-Py-21** | Deep Learning (LSTM, Transformers) |\n",
    "| **QC-Py-22** | Reinforcement Learning pour trading |\n",
    "\n",
    "### Ressources Complementaires\n",
    "\n",
    "- **shared/features.py** - Bibliotheque de fonctions de feature engineering\n",
    "- **shared/ml_utils.py** - Utilitaires ML (train, evaluate, save)\n",
    "- [Advances in Financial ML](https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089) - Lopez de Prado\n",
    "- [QuantConnect Data Documentation](https://www.quantconnect.com/docs/v2/writing-algorithms/datasets)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook complete. Pret pour QC-Py-19 (Classification Models).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
