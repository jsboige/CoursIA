{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC-Py-19 - Machine Learning Classification pour Direction Prediction\n",
    "\n",
    "> **Predire la direction du marche avec Random Forest et XGBoost**\n",
    "> Duree: 75 minutes | Niveau: Intermediaire-Avance | Python + QuantConnect\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous serez capable de :\n",
    "\n",
    "1. Comprendre pourquoi le **ML est adapte** a la prediction de direction vs regles fixes\n",
    "2. Implementer un **Random Forest Classifier** pour predire Up/Down\n",
    "3. Utiliser **XGBoost** avec early stopping et hyperparametres optimises\n",
    "4. Appliquer **TimeSeriesSplit** pour validation sans lookahead bias\n",
    "5. Calculer les **metriques de classification** adaptees au trading\n",
    "6. Implementer **Walk-Forward Validation** avec retrain periodique\n",
    "7. Persister les modeles avec **ObjectStore** de QuantConnect\n",
    "8. Construire un **ML Alpha Model** complet pour la production\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "- Notebook QC-Py-18 (Feature Engineering) complete\n",
    "- Comprehension des concepts ML de base (classification, train/test split)\n",
    "- Familiarite avec pandas, numpy, sklearn\n",
    "\n",
    "## Structure du Notebook\n",
    "\n",
    "1. Introduction au ML pour Trading (15 min)\n",
    "2. Random Forest Classifier (25 min)\n",
    "3. XGBoost Classifier (25 min)\n",
    "4. Validation et Metriques (20 min)\n",
    "5. Integration QuantConnect (20 min)\n",
    "6. Strategie Complete (20 min)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports standards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imports de base reussis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports ML\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "print(\"Imports sklearn reussis\")\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(f\"XGBoost importe avec succes (version {xgb.__version__})\")\n",
    "except ImportError:\n",
    "    print(\"XGBoost non installe. Executez: pip install xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 1 : Introduction au ML pour Trading (15 min)\n",
    "\n",
    "### 1.1 Classification vs Regles Fixes\n",
    "\n",
    "| Approche | Avantages | Inconvenients |\n",
    "|----------|-----------|---------------|\n",
    "| **Regles Fixes** | Interpretable, rapide, pas d'entrainement | Rigide, ne s'adapte pas, arbitraire |\n",
    "| **ML Classification** | Apprend des patterns complexes, s'adapte | Black box, overfitting, besoin de donnees |\n",
    "\n",
    "### Pourquoi ML pour predire la direction?\n",
    "\n",
    "```\n",
    "Regles Fixes:                     ML Classification:\n",
    "                                  \n",
    "if RSI < 30:                      features = [RSI, MACD, Volume, ...]\n",
    "    BUY                           model.fit(features, direction)\n",
    "elif RSI > 70:                    prediction = model.predict(new_features)\n",
    "    SELL                          \n",
    "                                  -> Capture interactions complexes\n",
    "-> Seuils arbitraires             -> Poids appris automatiquement\n",
    "-> Ignore autres facteurs         -> Combine multiple signaux\n",
    "```\n",
    "\n",
    "### Challenges specifiques au Trading\n",
    "\n",
    "| Challenge | Description | Solution |\n",
    "|-----------|-------------|----------|\n",
    "| **Non-stationnarite** | Les marches changent | Retrain periodique |\n",
    "| **Regime Changes** | Bull/Bear/Sideways | Features de regime, detection |\n",
    "| **Overfitting** | Memorise le passe | Regularisation, validation robuste |\n",
    "| **Class Imbalance** | Plus de Up que Down (ou inverse) | Class weights, resampling |\n",
    "| **Lookahead Bias** | Utiliser le futur | TimeSeriesSplit strict |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generer des donnees de demonstration\n",
    "# (En production, utiliser le pipeline de QC-Py-18)\n",
    "\n",
    "def generate_sample_data(n_days=500, seed=42):\n",
    "    \"\"\"\n",
    "    Genere des donnees OHLCV simulees avec features et labels.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    dates = pd.date_range(start='2022-01-01', periods=n_days, freq='B')\n",
    "    \n",
    "    # Prix avec tendance et cycles\n",
    "    trend = np.linspace(100, 130, n_days)\n",
    "    cycle = 15 * np.sin(np.linspace(0, 6 * np.pi, n_days))\n",
    "    noise = np.cumsum(np.random.randn(n_days) * 0.5)\n",
    "    close = trend + cycle + noise\n",
    "    \n",
    "    # OHLV\n",
    "    high = close * (1 + np.abs(np.random.normal(0, 0.01, n_days)))\n",
    "    low = close * (1 - np.abs(np.random.normal(0, 0.01, n_days)))\n",
    "    open_price = close * (1 + np.random.normal(0, 0.005, n_days))\n",
    "    volume = 1_000_000 * (1 + np.random.exponential(0.5, n_days))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'open': open_price,\n",
    "        'high': high,\n",
    "        'low': low,\n",
    "        'close': close,\n",
    "        'volume': volume.astype(int)\n",
    "    }, index=dates)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_features(df):\n",
    "    \"\"\"\n",
    "    Calcule les features techniques (simplifie du QC-Py-18).\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    close = result['close']\n",
    "    \n",
    "    # Returns\n",
    "    for period in [1, 5, 10, 20]:\n",
    "        result[f'return_{period}d'] = close.pct_change(period)\n",
    "    \n",
    "    # Volatility\n",
    "    result['volatility_20d'] = result['return_1d'].rolling(20).std()\n",
    "    \n",
    "    # SMA ratios\n",
    "    result['sma_20'] = close.rolling(20).mean()\n",
    "    result['sma_50'] = close.rolling(50).mean()\n",
    "    result['price_to_sma_20'] = close / result['sma_20']\n",
    "    result['ma_ratio_20_50'] = result['sma_20'] / result['sma_50']\n",
    "    \n",
    "    # RSI\n",
    "    delta = close.diff()\n",
    "    gain = delta.clip(lower=0).rolling(14).mean()\n",
    "    loss = (-delta.clip(upper=0)).rolling(14).mean()\n",
    "    rs = gain / loss\n",
    "    result['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "    result['rsi_normalized'] = (result['rsi_14'] - 50) / 50\n",
    "    \n",
    "    # MACD\n",
    "    ema_12 = close.ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = close.ewm(span=26, adjust=False).mean()\n",
    "    result['macd'] = ema_12 - ema_26\n",
    "    result['macd_signal'] = result['macd'].ewm(span=9, adjust=False).mean()\n",
    "    result['macd_hist'] = result['macd'] - result['macd_signal']\n",
    "    result['macd_norm'] = result['macd'] / close\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    bb_middle = close.rolling(20).mean()\n",
    "    bb_std = close.rolling(20).std()\n",
    "    bb_upper = bb_middle + 2 * bb_std\n",
    "    bb_lower = bb_middle - 2 * bb_std\n",
    "    result['bb_percent_b'] = (close - bb_lower) / (bb_upper - bb_lower)\n",
    "    result['bb_bandwidth'] = (bb_upper - bb_lower) / bb_middle\n",
    "    \n",
    "    # Volume features\n",
    "    result['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n",
    "    \n",
    "    # Range\n",
    "    result['range'] = (df['high'] - df['low']) / close\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def create_labels(df, horizon=5, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Cree les labels de classification.\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Future return\n",
    "    result['future_return'] = result['close'].shift(-horizon) / result['close'] - 1\n",
    "    \n",
    "    # Binary label: 1 = Up, 0 = Down\n",
    "    result['label'] = (result['future_return'] > threshold).astype(int)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Generer et preparer les donnees\n",
    "df = generate_sample_data(n_days=600)\n",
    "df = calculate_features(df)\n",
    "df = create_labels(df, horizon=5)\n",
    "\n",
    "print(f\"Donnees generees: {len(df)} jours\")\n",
    "print(f\"Periode: {df.index[0].date()} a {df.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparer X et y\n",
    "\n",
    "# Colonnes features (exclure OHLCV et labels)\n",
    "exclude_cols = ['open', 'high', 'low', 'close', 'volume', \n",
    "                'future_return', 'label', 'sma_20', 'sma_50', 'macd', 'macd_signal']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Supprimer NaN\n",
    "df_clean = df.dropna()\n",
    "\n",
    "X = df_clean[feature_cols]\n",
    "y = df_clean['label']\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"  {feature_cols}\")\n",
    "print(f\"\\nEchantillons: {len(X)}\")\n",
    "print(f\"\\nDistribution des labels:\")\n",
    "print(f\"  Down (0): {(y == 0).sum()} ({(y == 0).mean()*100:.1f}%)\")\n",
    "print(f\"  Up (1):   {(y == 1).sum()} ({(y == 1).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split temporel Train/Test\n",
    "train_ratio = 0.7\n",
    "split_idx = int(len(X) * train_ratio)\n",
    "\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_test = X.iloc[split_idx:]\n",
    "y_train = y.iloc[:split_idx]\n",
    "y_test = y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train: {len(X_train)} echantillons ({X_train.index[0].date()} - {X_train.index[-1].date()})\")\n",
    "print(f\"Test:  {len(X_test)} echantillons ({X_test.index[0].date()} - {X_test.index[-1].date()})\")\n",
    "\n",
    "# Normalisation (fit sur train, transform sur les deux)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"\\nNormalisation appliquee (StandardScaler)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 2 : Random Forest Classifier (25 min)\n",
    "\n",
    "### 2.1 Pourquoi Random Forest?\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **Robuste** | Resistant a l'overfitting grace a l'ensemble |\n",
    "| **Non-lineaire** | Capture relations complexes |\n",
    "| **Feature Importance** | Interpretabilite des features |\n",
    "| **Peu d'hyperparametres** | Facile a tuner |\n",
    "| **Pas de normalisation** | Fonctionne sur donnees brutes |\n",
    "\n",
    "### Architecture Random Forest\n",
    "\n",
    "```\n",
    "            Donnees\n",
    "               |\n",
    "    +----------+----------+\n",
    "    |          |          |\n",
    " Arbre 1   Arbre 2  ...  Arbre N\n",
    " (sample1) (sample2)     (sampleN)\n",
    "    |          |          |\n",
    " pred_1     pred_2      pred_N\n",
    "    |          |          |\n",
    "    +----------+----------+\n",
    "               |\n",
    "           VOTE MAJORITAIRE\n",
    "               |\n",
    "           Prediction Finale\n",
    "```\n",
    "\n",
    "### Hyperparametres importants\n",
    "\n",
    "| Parametre | Description | Valeur typique |\n",
    "|-----------|-------------|----------------|\n",
    "| `n_estimators` | Nombre d'arbres | 100-500 |\n",
    "| `max_depth` | Profondeur max par arbre | 5-15 |\n",
    "| `min_samples_leaf` | Echantillons min par feuille | 5-20 |\n",
    "| `max_features` | Features par split | 'sqrt' ou 0.3-0.7 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,       # 100 arbres\n",
    "    max_depth=5,            # Limiter profondeur (anti-overfitting)\n",
    "    min_samples_leaf=10,    # Min 10 echantillons par feuille\n",
    "    max_features='sqrt',    # sqrt(n_features) par split\n",
    "    random_state=42,\n",
    "    n_jobs=-1,              # Utiliser tous les CPU\n",
    "    class_weight='balanced' # Gerer desequilibre de classes\n",
    ")\n",
    "\n",
    "print(\"Random Forest Classifier configure:\")\n",
    "print(f\"  n_estimators: {rf_model.n_estimators}\")\n",
    "print(f\"  max_depth: {rf_model.max_depth}\")\n",
    "print(f\"  min_samples_leaf: {rf_model.min_samples_leaf}\")\n",
    "print(f\"  max_features: {rf_model.max_features}\")\n",
    "print(f\"  class_weight: {rf_model.class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement avec TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(\"TimeSeriesSplit Cross-Validation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cv_scores = []\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_scaled), 1):\n",
    "    # Split\n",
    "    X_cv_train = X_train_scaled.iloc[train_idx]\n",
    "    X_cv_val = X_train_scaled.iloc[val_idx]\n",
    "    y_cv_train = y_train.iloc[train_idx]\n",
    "    y_cv_val = y_train.iloc[val_idx]\n",
    "    \n",
    "    # Train\n",
    "    rf_model.fit(X_cv_train, y_cv_train)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = rf_model.predict(X_cv_val)\n",
    "    proba = rf_model.predict_proba(X_cv_val)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_cv_val, predictions)\n",
    "    prec = precision_score(y_cv_val, predictions, zero_division=0)\n",
    "    rec = recall_score(y_cv_val, predictions, zero_division=0)\n",
    "    f1 = f1_score(y_cv_val, predictions, zero_division=0)\n",
    "    \n",
    "    cv_scores.append(acc)\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'train_size': len(train_idx),\n",
    "        'val_size': len(val_idx),\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nFold {fold}:\")\n",
    "    print(f\"  Train: {len(train_idx)} | Val: {len(val_idx)}\")\n",
    "    print(f\"  Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainer le modele final sur tout le train set\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions sur test\n",
    "rf_predictions = rf_model.predict(X_test_scaled)\n",
    "rf_proba = rf_model.predict_proba(X_test_scaled)\n",
    "\n",
    "print(\"Random Forest - Resultats sur Test Set:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, rf_predictions):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, rf_predictions):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, rf_predictions):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, rf_predictions):.4f}\")\n",
    "\n",
    "# AUC si applicable\n",
    "try:\n",
    "    auc = roc_auc_score(y_test, rf_proba[:, 1])\n",
    "    print(f\"ROC AUC:   {auc:.4f}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Random Forest):\")\n",
    "print(\"=\"*50)\n",
    "for i, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"  {row['feature']:25s} {row['importance']:.4f}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "top_features = feature_importance.head(12)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
    "ax.barh(range(len(top_features)), top_features['importance'], color=colors)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Random Forest - Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 3 : XGBoost Classifier (25 min)\n",
    "\n",
    "### 3.1 Pourquoi XGBoost?\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **Performance** | Souvent meilleur que Random Forest |\n",
    "| **Regularisation** | L1/L2 integree (anti-overfitting) |\n",
    "| **Early Stopping** | Arrete avant overfitting |\n",
    "| **Gestion des NaN** | Native (pas besoin d'imputation) |\n",
    "| **Vitesse** | Optimise pour la performance |\n",
    "\n",
    "### Gradient Boosting vs Random Forest\n",
    "\n",
    "```\n",
    "Random Forest:           XGBoost:\n",
    "                         \n",
    " Arbre1  Arbre2  ArbreN   Arbre1 --> Arbre2 --> Arbre3 --> ArbreN\n",
    "   |       |       |         |         |         |          |\n",
    "   +-------+-------+         |      corrige    corrige    corrige\n",
    "           |                 |      erreurs   erreurs    erreurs\n",
    "        MOYENNE              |      de 1      de 2       de N-1\n",
    "                             +-------+--------+-----------+\n",
    "                                            SOMME\n",
    "```\n",
    "\n",
    "### Hyperparametres XGBoost\n",
    "\n",
    "| Parametre | Description | Valeur typique |\n",
    "|-----------|-------------|----------------|\n",
    "| `n_estimators` | Nombre d'iterations (arbres) | 100-1000 |\n",
    "| `max_depth` | Profondeur max | 3-6 |\n",
    "| `learning_rate` | Taux d'apprentissage | 0.01-0.3 |\n",
    "| `subsample` | Ratio d'echantillons par arbre | 0.7-0.9 |\n",
    "| `colsample_bytree` | Ratio de features par arbre | 0.7-0.9 |\n",
    "| `reg_alpha` | Regularisation L1 | 0-1 |\n",
    "| `reg_lambda` | Regularisation L2 | 1-10 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Classifier\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,          # Plus d'arbres (early stopping arretera)\n",
    "    max_depth=4,               # Moins profond que RF (boosting compense)\n",
    "    learning_rate=0.1,         # Taux d'apprentissage\n",
    "    subsample=0.8,             # 80% des echantillons par arbre\n",
    "    colsample_bytree=0.8,      # 80% des features par arbre\n",
    "    reg_alpha=0.1,             # Regularisation L1\n",
    "    reg_lambda=1.0,            # Regularisation L2\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"XGBoost Classifier configure:\")\n",
    "print(f\"  n_estimators: {xgb_model.n_estimators}\")\n",
    "print(f\"  max_depth: {xgb_model.max_depth}\")\n",
    "print(f\"  learning_rate: {xgb_model.learning_rate}\")\n",
    "print(f\"  subsample: {xgb_model.subsample}\")\n",
    "print(f\"  colsample_bytree: {xgb_model.colsample_bytree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement avec Early Stopping\n",
    "\n",
    "# Split train en train/validation pour early stopping\n",
    "val_ratio = 0.2\n",
    "val_split_idx = int(len(X_train_scaled) * (1 - val_ratio))\n",
    "\n",
    "X_train_xgb = X_train_scaled.iloc[:val_split_idx]\n",
    "X_val_xgb = X_train_scaled.iloc[val_split_idx:]\n",
    "y_train_xgb = y_train.iloc[:val_split_idx]\n",
    "y_val_xgb = y_train.iloc[val_split_idx:]\n",
    "\n",
    "print(f\"Train XGBoost: {len(X_train_xgb)} echantillons\")\n",
    "print(f\"Validation:    {len(X_val_xgb)} echantillons\")\n",
    "\n",
    "# Entrainer avec early stopping\n",
    "xgb_model.fit(\n",
    "    X_train_xgb, y_train_xgb,\n",
    "    eval_set=[(X_val_xgb, y_val_xgb)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nEntrainement termine.\")\n",
    "print(f\"Meilleur nombre d'iterations: {xgb_model.best_iteration if hasattr(xgb_model, 'best_iteration') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions XGBoost sur test\n",
    "\n",
    "xgb_predictions = xgb_model.predict(X_test_scaled)\n",
    "xgb_proba = xgb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "print(\"XGBoost - Resultats sur Test Set:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, xgb_predictions):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, xgb_predictions):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, xgb_predictions):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, xgb_predictions):.4f}\")\n",
    "\n",
    "try:\n",
    "    auc = roc_auc_score(y_test, xgb_proba[:, 1])\n",
    "    print(f\"ROC AUC:   {auc:.4f}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Feature Importance\n",
    "\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (XGBoost):\")\n",
    "print(\"=\"*50)\n",
    "for i, row in xgb_importance.head(10).iterrows():\n",
    "    print(f\"  {row['feature']:25s} {row['importance']:.4f}\")\n",
    "\n",
    "# Comparaison RF vs XGBoost\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# RF\n",
    "ax1 = axes[0]\n",
    "top_rf = feature_importance.head(10)\n",
    "ax1.barh(range(len(top_rf)), top_rf['importance'], color='steelblue')\n",
    "ax1.set_yticks(range(len(top_rf)))\n",
    "ax1.set_yticklabels(top_rf['feature'])\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Importance')\n",
    "ax1.set_title('Random Forest', fontsize=14, fontweight='bold')\n",
    "\n",
    "# XGBoost\n",
    "ax2 = axes[1]\n",
    "top_xgb = xgb_importance.head(10)\n",
    "ax2.barh(range(len(top_xgb)), top_xgb['importance'], color='coral')\n",
    "ax2.set_yticks(range(len(top_xgb)))\n",
    "ax2.set_yticklabels(top_xgb['feature'])\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_title('XGBoost', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Comparaison Feature Importance', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des modeles\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Random Forest': [\n",
    "        accuracy_score(y_test, rf_predictions),\n",
    "        precision_score(y_test, rf_predictions),\n",
    "        recall_score(y_test, rf_predictions),\n",
    "        f1_score(y_test, rf_predictions),\n",
    "        roc_auc_score(y_test, rf_proba[:, 1]) if len(np.unique(y_test)) > 1 else 0\n",
    "    ],\n",
    "    'XGBoost': [\n",
    "        accuracy_score(y_test, xgb_predictions),\n",
    "        precision_score(y_test, xgb_predictions),\n",
    "        recall_score(y_test, xgb_predictions),\n",
    "        f1_score(y_test, xgb_predictions),\n",
    "        roc_auc_score(y_test, xgb_proba[:, 1]) if len(np.unique(y_test)) > 1 else 0\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Comparaison Random Forest vs XGBoost:\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison['Random Forest'], width, label='Random Forest', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, comparison['XGBoost'], width, label='XGBoost', color='coral')\n",
    "\n",
    "ax.set_xlabel('Metric', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Comparaison des Modeles', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Metric'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 4 : Validation et Metriques (20 min)\n",
    "\n",
    "### 4.1 Metriques de Classification pour le Trading\n",
    "\n",
    "| Metrique | Formule | Interpretation Trading |\n",
    "|----------|---------|------------------------|\n",
    "| **Accuracy** | (TP+TN) / Total | % de predictions correctes |\n",
    "| **Precision** | TP / (TP+FP) | % de trades gagnants parmi les signaux BUY |\n",
    "| **Recall** | TP / (TP+FN) | % de jours Up detectes |\n",
    "| **F1 Score** | 2 * (P*R)/(P+R) | Balance precision/recall |\n",
    "\n",
    "### Importance de la Precision en Trading\n",
    "\n",
    "```\n",
    "                        Actual\n",
    "                    Up      Down\n",
    "              +--------+--------+\n",
    "Predicted Up  |   TP   |   FP   |  <-- Precision = TP / (TP + FP)\n",
    "              +--------+--------+      (% de signaux BUY profitables)\n",
    "Predicted Down|   FN   |   TN   |\n",
    "              +--------+--------+\n",
    "\n",
    "Pour un trader:\n",
    "- FP (Faux Positif) = Achat suivi de baisse = PERTE\n",
    "- Une Precision de 55% peut etre profitable si gain > perte\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metriques detaillees\n",
    "\n",
    "def detailed_classification_metrics(y_true, y_pred, y_proba=None, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calcule et affiche les metriques detaillees de classification.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} - Metriques Detaillees\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Metriques de base\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    print(f\"\\nMetriques de Base:\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}  (% trades gagnants parmi les BUY)\")\n",
    "    print(f\"  Recall:    {rec:.4f}  (% jours Up detectes)\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\nMatrice de Confusion:\")\n",
    "    print(f\"                  Predicted\")\n",
    "    print(f\"                Down    Up\")\n",
    "    print(f\"  Actual Down   {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
    "    print(f\"  Actual Up     {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
    "    \n",
    "    # Interpretation trading\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\nInterpretation Trading:\")\n",
    "    print(f\"  True Positives (TP):  {tp:4d} - Signaux BUY corrects\")\n",
    "    print(f\"  False Positives (FP): {fp:4d} - Signaux BUY errones (pertes)\")\n",
    "    print(f\"  True Negatives (TN):  {tn:4d} - Signaux SELL corrects\")\n",
    "    print(f\"  False Negatives (FN): {fn:4d} - Opportunites manquees\")\n",
    "    \n",
    "    # ROC AUC\n",
    "    if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "        auc = roc_auc_score(y_true, y_proba[:, 1])\n",
    "        print(f\"\\n  ROC AUC: {auc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Appliquer aux deux modeles\n",
    "rf_metrics = detailed_classification_metrics(y_test, rf_predictions, rf_proba, \"Random Forest\")\n",
    "xgb_metrics = detailed_classification_metrics(y_test, xgb_predictions, xgb_proba, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des matrices de confusion\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Random Forest\n",
    "ax1 = axes[0]\n",
    "cm_rf = confusion_matrix(y_test, rf_predictions)\n",
    "im1 = ax1.imshow(cm_rf, cmap='Blues')\n",
    "ax1.set_xticks([0, 1])\n",
    "ax1.set_yticks([0, 1])\n",
    "ax1.set_xticklabels(['Down (0)', 'Up (1)'])\n",
    "ax1.set_yticklabels(['Down (0)', 'Up (1)'])\n",
    "ax1.set_xlabel('Predicted', fontsize=12)\n",
    "ax1.set_ylabel('Actual', fontsize=12)\n",
    "ax1.set_title('Random Forest', fontsize=14, fontweight='bold')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax1.text(j, i, cm_rf[i, j], ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# XGBoost\n",
    "ax2 = axes[1]\n",
    "cm_xgb = confusion_matrix(y_test, xgb_predictions)\n",
    "im2 = ax2.imshow(cm_xgb, cmap='Oranges')\n",
    "ax2.set_xticks([0, 1])\n",
    "ax2.set_yticks([0, 1])\n",
    "ax2.set_xticklabels(['Down (0)', 'Up (1)'])\n",
    "ax2.set_yticklabels(['Down (0)', 'Up (1)'])\n",
    "ax2.set_xlabel('Predicted', fontsize=12)\n",
    "ax2.set_ylabel('Actual', fontsize=12)\n",
    "ax2.set_title('XGBoost', fontsize=14, fontweight='bold')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax2.text(j, i, cm_xgb[i, j], ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Matrices de Confusion', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Random Forest ROC\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_proba[:, 1])\n",
    "auc_rf = roc_auc_score(y_test, rf_proba[:, 1])\n",
    "ax.plot(fpr_rf, tpr_rf, 'b-', linewidth=2, label=f'Random Forest (AUC = {auc_rf:.3f})')\n",
    "\n",
    "# XGBoost ROC\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_proba[:, 1])\n",
    "auc_xgb = roc_auc_score(y_test, xgb_proba[:, 1])\n",
    "ax.plot(fpr_xgb, tpr_xgb, 'r-', linewidth=2, label=f'XGBoost (AUC = {auc_xgb:.3f})')\n",
    "\n",
    "# Ligne aleatoire\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.500)')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('Courbes ROC', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Walk-Forward Validation\n",
    "\n",
    "La Walk-Forward Validation simule le trading reel en retrainant periodiquement le modele.\n",
    "\n",
    "```\n",
    "Expanding Window:                    Sliding Window:\n",
    "\n",
    "|=====Train=====|Test|               |=====Train=====|Test|\n",
    "|=======Train========|Test|          |     |====Train====|Test|\n",
    "|=========Train==========|Test|      |          |===Train===|Test|\n",
    "\n",
    "-> Train grandit                     -> Train a taille fixe\n",
    "-> Plus de donnees                   -> Plus recent = plus pertinent\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(X, y, model_class, model_params,\n",
    "                            train_window=252, test_window=21,\n",
    "                            expanding=True):\n",
    "    \"\"\"\n",
    "    Walk-Forward Validation avec retrain periodique.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Features\n",
    "    y : pd.Series\n",
    "        Labels\n",
    "    model_class : class\n",
    "        Classe du modele (ex: RandomForestClassifier)\n",
    "    model_params : dict\n",
    "        Parametres du modele\n",
    "    train_window : int\n",
    "        Taille de la fenetre d'entrainement (jours)\n",
    "    test_window : int\n",
    "        Taille de la fenetre de test (jours)\n",
    "    expanding : bool\n",
    "        True = Expanding window, False = Sliding window\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Resultats de la validation\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'predictions': [],\n",
    "        'actuals': [],\n",
    "        'probas': [],\n",
    "        'dates': [],\n",
    "        'fold_metrics': []\n",
    "    }\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    current_idx = train_window\n",
    "    fold = 0\n",
    "    \n",
    "    while current_idx + test_window <= n_samples:\n",
    "        fold += 1\n",
    "        \n",
    "        # Define train/test indices\n",
    "        if expanding:\n",
    "            train_start = 0\n",
    "        else:\n",
    "            train_start = current_idx - train_window\n",
    "        \n",
    "        train_end = current_idx\n",
    "        test_start = current_idx\n",
    "        test_end = min(current_idx + test_window, n_samples)\n",
    "        \n",
    "        # Split data\n",
    "        X_train_wf = X.iloc[train_start:train_end]\n",
    "        y_train_wf = y.iloc[train_start:train_end]\n",
    "        X_test_wf = X.iloc[test_start:test_end]\n",
    "        y_test_wf = y.iloc[test_start:test_end]\n",
    "        \n",
    "        # Scale\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_wf)\n",
    "        X_test_scaled = scaler.transform(X_test_wf)\n",
    "        \n",
    "        # Train model\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train_scaled, y_train_wf)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(X_test_scaled)\n",
    "        proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Store results\n",
    "        results['predictions'].extend(preds)\n",
    "        results['actuals'].extend(y_test_wf.values)\n",
    "        results['probas'].extend(proba)\n",
    "        results['dates'].extend(X_test_wf.index.tolist())\n",
    "        \n",
    "        # Fold metrics\n",
    "        acc = accuracy_score(y_test_wf, preds)\n",
    "        results['fold_metrics'].append({\n",
    "            'fold': fold,\n",
    "            'train_size': len(X_train_wf),\n",
    "            'test_size': len(X_test_wf),\n",
    "            'accuracy': acc\n",
    "        })\n",
    "        \n",
    "        # Move to next window\n",
    "        current_idx += test_window\n",
    "    \n",
    "    # Overall metrics\n",
    "    results['overall_accuracy'] = accuracy_score(results['actuals'], results['predictions'])\n",
    "    results['overall_precision'] = precision_score(results['actuals'], results['predictions'])\n",
    "    results['overall_f1'] = f1_score(results['actuals'], results['predictions'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Executer Walk-Forward Validation\n",
    "print(\"Walk-Forward Validation (Monthly Retrain):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "wf_results = walk_forward_validation(\n",
    "    X, y,\n",
    "    RandomForestClassifier,\n",
    "    {'n_estimators': 100, 'max_depth': 5, 'random_state': 42, 'n_jobs': -1},\n",
    "    train_window=252,   # 1 an de train\n",
    "    test_window=21,     # 1 mois de test\n",
    "    expanding=True\n",
    ")\n",
    "\n",
    "print(f\"\\nNombre de folds: {len(wf_results['fold_metrics'])}\")\n",
    "print(f\"\\nMetriques par fold:\")\n",
    "for fm in wf_results['fold_metrics'][:5]:\n",
    "    print(f\"  Fold {fm['fold']}: Train={fm['train_size']}, Test={fm['test_size']}, Acc={fm['accuracy']:.4f}\")\n",
    "if len(wf_results['fold_metrics']) > 5:\n",
    "    print(f\"  ...\")\n",
    "\n",
    "print(f\"\\nResultats Globaux:\")\n",
    "print(f\"  Accuracy:  {wf_results['overall_accuracy']:.4f}\")\n",
    "print(f\"  Precision: {wf_results['overall_precision']:.4f}\")\n",
    "print(f\"  F1 Score:  {wf_results['overall_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation Walk-Forward\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Accuracy par fold\n",
    "ax1 = axes[0]\n",
    "folds = [fm['fold'] for fm in wf_results['fold_metrics']]\n",
    "accs = [fm['accuracy'] for fm in wf_results['fold_metrics']]\n",
    "ax1.bar(folds, accs, color='steelblue', edgecolor='black')\n",
    "ax1.axhline(y=wf_results['overall_accuracy'], color='red', linestyle='--', \n",
    "            label=f\"Mean Accuracy: {wf_results['overall_accuracy']:.3f}\")\n",
    "ax1.axhline(y=0.5, color='gray', linestyle=':', label='Random Baseline')\n",
    "ax1.set_xlabel('Fold', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Walk-Forward Validation - Accuracy par Fold', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Predictions cumulees\n",
    "ax2 = axes[1]\n",
    "dates = wf_results['dates']\n",
    "preds = wf_results['predictions']\n",
    "actuals = wf_results['actuals']\n",
    "\n",
    "# Calculer accuracy cumulee\n",
    "cumulative_correct = np.cumsum([1 if p == a else 0 for p, a in zip(preds, actuals)])\n",
    "cumulative_total = np.arange(1, len(preds) + 1)\n",
    "cumulative_accuracy = cumulative_correct / cumulative_total\n",
    "\n",
    "ax2.plot(dates, cumulative_accuracy, 'b-', linewidth=2)\n",
    "ax2.axhline(y=0.5, color='gray', linestyle=':', label='Random Baseline')\n",
    "ax2.fill_between(dates, 0.5, cumulative_accuracy, alpha=0.3, color='steelblue')\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Accuracy', fontsize=12)\n",
    "ax2.set_title('Accuracy Cumulee au Fil du Temps', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 5 : Integration QuantConnect (20 min)\n",
    "\n",
    "### 5.1 ObjectStore pour Persistence des Modeles\n",
    "\n",
    "QuantConnect fournit **ObjectStore** pour persister des objets entre les executions:\n",
    "\n",
    "- Sauvegarder un modele entraine\n",
    "- Charger un modele pour predictions\n",
    "- Partager entre Research et Algorithm\n",
    "\n",
    "```python\n",
    "# Sauvegarder\n",
    "model_bytes = pickle.dumps(model)\n",
    "self.ObjectStore.SaveBytes(\"model/rf_classifier\", model_bytes)\n",
    "\n",
    "# Charger\n",
    "model_bytes = self.ObjectStore.ReadBytes(\"model/rf_classifier\")\n",
    "model = pickle.loads(model_bytes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code QuantConnect pour ObjectStore\n",
    "\n",
    "objectstore_code = '''\n",
    "from AlgorithmImports import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class MLModelPersistence(QCAlgorithm):\n",
    "    \"\"\"\n",
    "    Demonstration de la persistence des modeles ML avec ObjectStore.\n",
    "    \"\"\"\n",
    "    \n",
    "    def Initialize(self):\n",
    "        self.SetStartDate(2024, 1, 1)\n",
    "        self.SetCash(100000)\n",
    "        \n",
    "        self.symbol = self.AddEquity(\"SPY\", Resolution.Daily).Symbol\n",
    "        \n",
    "        # Cle pour le modele dans ObjectStore\n",
    "        self.model_key = \"model/rf_direction_classifier\"\n",
    "        self.scaler_key = \"model/scaler\"\n",
    "        \n",
    "        # Charger ou entrainer le modele\n",
    "        self.model = self.LoadOrTrainModel()\n",
    "    \n",
    "    def LoadOrTrainModel(self):\n",
    "        \"\"\"\n",
    "        Charge le modele depuis ObjectStore ou entraine un nouveau.\n",
    "        \"\"\"\n",
    "        if self.ObjectStore.ContainsKey(self.model_key):\n",
    "            self.Debug(\"Chargement du modele depuis ObjectStore...\")\n",
    "            \n",
    "            # Charger le modele\n",
    "            model_bytes = self.ObjectStore.ReadBytes(self.model_key)\n",
    "            model = pickle.loads(model_bytes)\n",
    "            \n",
    "            # Charger le scaler\n",
    "            scaler_bytes = self.ObjectStore.ReadBytes(self.scaler_key)\n",
    "            self.scaler = pickle.loads(scaler_bytes)\n",
    "            \n",
    "            self.Debug(\"Modele charge avec succes!\")\n",
    "            return model\n",
    "        else:\n",
    "            self.Debug(\"Entrainement d\\'un nouveau modele...\")\n",
    "            return self.TrainAndSaveModel()\n",
    "    \n",
    "    def TrainAndSaveModel(self):\n",
    "        \"\"\"\n",
    "        Entraine un nouveau modele et le sauvegarde.\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        \n",
    "        # Recuperer les donnees historiques\n",
    "        history = self.History(self.symbol, 500, Resolution.Daily)\n",
    "        \n",
    "        if history.empty:\n",
    "            self.Debug(\"Pas assez de donnees historiques\")\n",
    "            return None\n",
    "        \n",
    "        # Calculer features (simplifie)\n",
    "        df = history[\\'close\\'].unstack(level=0)\n",
    "        df.columns = [\\'close\\']\n",
    "        \n",
    "        df[\\'return_1d\\'] = df[\\'close\\'].pct_change()\n",
    "        df[\\'return_5d\\'] = df[\\'close\\'].pct_change(5)\n",
    "        df[\\'volatility\\'] = df[\\'return_1d\\'].rolling(20).std()\n",
    "        df[\\'sma_ratio\\'] = df[\\'close\\'] / df[\\'close\\'].rolling(20).mean()\n",
    "        \n",
    "        # Label: direction future sur 5 jours\n",
    "        df[\\'future_return\\'] = df[\\'close\\'].shift(-5) / df[\\'close\\'] - 1\n",
    "        df[\\'label\\'] = (df[\\'future_return\\'] > 0).astype(int)\n",
    "        \n",
    "        # Preparer X, y\n",
    "        feature_cols = [\\'return_1d\\', \\'return_5d\\', \\'volatility\\', \\'sma_ratio\\']\n",
    "        df_clean = df.dropna()\n",
    "        \n",
    "        X = df_clean[feature_cols]\n",
    "        y = df_clean[\\'label\\']\n",
    "        \n",
    "        # Scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Entrainer\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_scaled, y)\n",
    "        \n",
    "        # Sauvegarder dans ObjectStore\n",
    "        model_bytes = pickle.dumps(model)\n",
    "        self.ObjectStore.SaveBytes(self.model_key, model_bytes)\n",
    "        \n",
    "        scaler_bytes = pickle.dumps(self.scaler)\n",
    "        self.ObjectStore.SaveBytes(self.scaler_key, scaler_bytes)\n",
    "        \n",
    "        self.Debug(f\"Modele entraine et sauvegarde. Accuracy: {model.score(X_scaled, y):.4f}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def DeleteModel(self):\n",
    "        \"\"\"\n",
    "        Supprime le modele de ObjectStore (pour forcer retrain).\n",
    "        \"\"\"\n",
    "        if self.ObjectStore.ContainsKey(self.model_key):\n",
    "            self.ObjectStore.Delete(self.model_key)\n",
    "            self.ObjectStore.Delete(self.scaler_key)\n",
    "            self.Debug(\"Modele supprime de ObjectStore\")\n",
    "'''\n",
    "\n",
    "print(\"ObjectStore pour Persistence des Modeles:\")\n",
    "print(objectstore_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ML Classification Alpha Model\n",
    "\n",
    "Creeons un Alpha Model qui utilise un classificateur ML pour generer des Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Classification Alpha Model\n",
    "\n",
    "ml_alpha_code = '''\n",
    "from AlgorithmImports import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import timedelta\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class MLClassificationAlphaModel(AlphaModel):\n",
    "    \"\"\"\n",
    "    Alpha Model utilisant un classificateur ML pour predire la direction.\n",
    "    \n",
    "    Features:\n",
    "    - Rendements multi-periodes\n",
    "    - Volatilite\n",
    "    - Ratio prix/SMA\n",
    "    - RSI normalise\n",
    "    \n",
    "    Prediction:\n",
    "    - Direction sur 5 jours (Up/Down)\n",
    "    - Confiance = probabilite du modele\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model_key: str = \"model/rf_classifier\",\n",
    "                 lookback: int = 252,\n",
    "                 retrain_frequency: int = 21,\n",
    "                 prediction_horizon: int = 5,\n",
    "                 probability_threshold: float = 0.6):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_key : str\n",
    "            Cle ObjectStore pour le modele\n",
    "        lookback : int\n",
    "            Jours d\\'historique pour entrainement\n",
    "        retrain_frequency : int\n",
    "            Frequence de retrain (jours)\n",
    "        prediction_horizon : int\n",
    "            Horizon de prediction (jours)\n",
    "        probability_threshold : float\n",
    "            Seuil de probabilite pour generer un Insight\n",
    "        \"\"\"\n",
    "        self.model_key = model_key\n",
    "        self.lookback = lookback\n",
    "        self.retrain_frequency = retrain_frequency\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.probability_threshold = probability_threshold\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.last_train_time = None\n",
    "        self.symbol_data = {}\n",
    "    \n",
    "    def Update(self, algorithm, data):\n",
    "        \"\"\"\n",
    "        Genere des Insights bases sur les predictions ML.\n",
    "        \"\"\"\n",
    "        insights = []\n",
    "        \n",
    "        # Verifier si retrain necessaire\n",
    "        if self._should_retrain(algorithm):\n",
    "            self._train_model(algorithm)\n",
    "        \n",
    "        if self.model is None:\n",
    "            return insights\n",
    "        \n",
    "        # Generer predictions pour chaque symbole\n",
    "        for symbol, sd in self.symbol_data.items():\n",
    "            if not data.ContainsKey(symbol):\n",
    "                continue\n",
    "            \n",
    "            # Extraire features\n",
    "            features = sd.ExtractFeatures(algorithm, symbol)\n",
    "            if features is None:\n",
    "                continue\n",
    "            \n",
    "            # Scaler et predire\n",
    "            features_scaled = self.scaler.transform([features])\n",
    "            prediction = self.model.predict(features_scaled)[0]\n",
    "            proba = self.model.predict_proba(features_scaled)[0]\n",
    "            \n",
    "            # Probabilite de la classe predite\n",
    "            confidence = max(proba)\n",
    "            \n",
    "            # Generer Insight si confiance suffisante\n",
    "            if confidence >= self.probability_threshold:\n",
    "                direction = InsightDirection.Up if prediction == 1 else InsightDirection.Down\n",
    "                \n",
    "                insights.append(Insight.Price(\n",
    "                    symbol,\n",
    "                    timedelta(days=self.prediction_horizon),\n",
    "                    direction,\n",
    "                    magnitude=confidence - 0.5,  # Excess confidence\n",
    "                    confidence=confidence\n",
    "                ))\n",
    "                \n",
    "                algorithm.Debug(f\"{algorithm.Time}: {symbol.Value} -> {direction}, Conf={confidence:.3f}\")\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def OnSecuritiesChanged(self, algorithm, changes):\n",
    "        \"\"\"\n",
    "        Gere les ajouts/suppressions de securities.\n",
    "        \"\"\"\n",
    "        for security in changes.AddedSecurities:\n",
    "            symbol = security.Symbol\n",
    "            if symbol not in self.symbol_data:\n",
    "                self.symbol_data[symbol] = MLSymbolData(algorithm, symbol, self.lookback)\n",
    "        \n",
    "        for security in changes.RemovedSecurities:\n",
    "            symbol = security.Symbol\n",
    "            if symbol in self.symbol_data:\n",
    "                del self.symbol_data[symbol]\n",
    "    \n",
    "    def _should_retrain(self, algorithm):\n",
    "        \"\"\"\n",
    "        Determine si le modele doit etre retraine.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            return True\n",
    "        \n",
    "        if self.last_train_time is None:\n",
    "            return True\n",
    "        \n",
    "        days_since_train = (algorithm.Time - self.last_train_time).days\n",
    "        return days_since_train >= self.retrain_frequency\n",
    "    \n",
    "    def _train_model(self, algorithm):\n",
    "        \"\"\"\n",
    "        Entraine le modele sur les donnees recentes.\n",
    "        \"\"\"\n",
    "        algorithm.Debug(f\"{algorithm.Time}: Entrainement du modele ML...\")\n",
    "        \n",
    "        # Collecter les donnees de tous les symboles\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for symbol, sd in self.symbol_data.items():\n",
    "            X, y = sd.GetTrainingData(algorithm, symbol, self.lookback, self.prediction_horizon)\n",
    "            if X is not None and len(X) > 50:\n",
    "                all_features.extend(X)\n",
    "                all_labels.extend(y)\n",
    "        \n",
    "        if len(all_features) < 100:\n",
    "            algorithm.Debug(\"Pas assez de donnees pour entrainer\")\n",
    "            return\n",
    "        \n",
    "        # Convertir en arrays\n",
    "        X = np.array(all_features)\n",
    "        y = np.array(all_labels)\n",
    "        \n",
    "        # Scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Entrainer\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.model.fit(X_scaled, y)\n",
    "        \n",
    "        self.last_train_time = algorithm.Time\n",
    "        \n",
    "        # Log performance\n",
    "        train_acc = self.model.score(X_scaled, y)\n",
    "        algorithm.Debug(f\"Modele entraine. Train Accuracy: {train_acc:.4f}\")\n",
    "        \n",
    "        # Sauvegarder dans ObjectStore\n",
    "        self._save_model(algorithm)\n",
    "    \n",
    "    def _save_model(self, algorithm):\n",
    "        \"\"\"\n",
    "        Sauvegarde le modele dans ObjectStore.\n",
    "        \"\"\"\n",
    "        model_bytes = pickle.dumps(self.model)\n",
    "        algorithm.ObjectStore.SaveBytes(self.model_key, model_bytes)\n",
    "        \n",
    "        scaler_bytes = pickle.dumps(self.scaler)\n",
    "        algorithm.ObjectStore.SaveBytes(self.model_key + \"_scaler\", scaler_bytes)\n",
    "    \n",
    "    def _load_model(self, algorithm):\n",
    "        \"\"\"\n",
    "        Charge le modele depuis ObjectStore.\n",
    "        \"\"\"\n",
    "        if algorithm.ObjectStore.ContainsKey(self.model_key):\n",
    "            model_bytes = algorithm.ObjectStore.ReadBytes(self.model_key)\n",
    "            self.model = pickle.loads(model_bytes)\n",
    "            \n",
    "            scaler_bytes = algorithm.ObjectStore.ReadBytes(self.model_key + \"_scaler\")\n",
    "            self.scaler = pickle.loads(scaler_bytes)\n",
    "            \n",
    "            algorithm.Debug(\"Modele charge depuis ObjectStore\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class MLSymbolData:\n",
    "    \"\"\"\n",
    "    Donnees par symbole pour le ML Alpha Model.\n",
    "    Calcule et stocke les features techniques.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, algorithm, symbol, lookback):\n",
    "        self.symbol = symbol\n",
    "        self.lookback = lookback\n",
    "        \n",
    "        # Indicateurs\n",
    "        self.sma_20 = algorithm.SMA(symbol, 20, Resolution.Daily)\n",
    "        self.rsi_14 = algorithm.RSI(symbol, 14, Resolution.Daily)\n",
    "        \n",
    "        # Historique des prix\n",
    "        self.price_history = deque(maxlen=lookback)\n",
    "        \n",
    "        # Warmup\n",
    "        history = algorithm.History(symbol, lookback, Resolution.Daily)\n",
    "        if not history.empty:\n",
    "            for bar in history.itertuples():\n",
    "                self.price_history.append(bar.close)\n",
    "                self.sma_20.Update(bar.Index[1], bar.close)\n",
    "                self.rsi_14.Update(bar.Index[1], bar.close)\n",
    "    \n",
    "    def ExtractFeatures(self, algorithm, symbol):\n",
    "        \"\"\"\n",
    "        Extrait les features pour une prediction.\n",
    "        \"\"\"\n",
    "        if not self.sma_20.IsReady or not self.rsi_14.IsReady:\n",
    "            return None\n",
    "        \n",
    "        if len(self.price_history) < 20:\n",
    "            return None\n",
    "        \n",
    "        prices = list(self.price_history)\n",
    "        current_price = prices[-1]\n",
    "        \n",
    "        # Features\n",
    "        return_1d = (prices[-1] - prices[-2]) / prices[-2] if len(prices) >= 2 else 0\n",
    "        return_5d = (prices[-1] - prices[-6]) / prices[-6] if len(prices) >= 6 else 0\n",
    "        return_10d = (prices[-1] - prices[-11]) / prices[-11] if len(prices) >= 11 else 0\n",
    "        return_20d = (prices[-1] - prices[-21]) / prices[-21] if len(prices) >= 21 else 0\n",
    "        \n",
    "        # Volatility\n",
    "        returns = [(prices[i] - prices[i-1]) / prices[i-1] for i in range(1, len(prices))]\n",
    "        volatility = np.std(returns[-20:]) if len(returns) >= 20 else 0\n",
    "        \n",
    "        # SMA ratio\n",
    "        sma_ratio = current_price / self.sma_20.Current.Value\n",
    "        \n",
    "        # RSI normalise\n",
    "        rsi_norm = (self.rsi_14.Current.Value - 50) / 50\n",
    "        \n",
    "        return [return_1d, return_5d, return_10d, return_20d, volatility, sma_ratio, rsi_norm]\n",
    "    \n",
    "    def GetTrainingData(self, algorithm, symbol, lookback, horizon):\n",
    "        \"\"\"\n",
    "        Recupere les donnees d\\'entrainement.\n",
    "        \"\"\"\n",
    "        history = algorithm.History(symbol, lookback + horizon + 50, Resolution.Daily)\n",
    "        \n",
    "        if history.empty or len(history) < lookback:\n",
    "            return None, None\n",
    "        \n",
    "        # Convertir en DataFrame\n",
    "        df = history[\\'close\\'].unstack(level=0)\n",
    "        df.columns = [\\'close\\']\n",
    "        \n",
    "        # Features\n",
    "        df[\\'return_1d\\'] = df[\\'close\\'].pct_change(1)\n",
    "        df[\\'return_5d\\'] = df[\\'close\\'].pct_change(5)\n",
    "        df[\\'return_10d\\'] = df[\\'close\\'].pct_change(10)\n",
    "        df[\\'return_20d\\'] = df[\\'close\\'].pct_change(20)\n",
    "        df[\\'volatility\\'] = df[\\'return_1d\\'].rolling(20).std()\n",
    "        df[\\'sma_ratio\\'] = df[\\'close\\'] / df[\\'close\\'].rolling(20).mean()\n",
    "        \n",
    "        # RSI\n",
    "        delta = df[\\'close\\'].diff()\n",
    "        gain = delta.clip(lower=0).rolling(14).mean()\n",
    "        loss = (-delta.clip(upper=0)).rolling(14).mean()\n",
    "        rs = gain / loss\n",
    "        df[\\'rsi_norm\\'] = ((100 - (100 / (1 + rs))) - 50) / 50\n",
    "        \n",
    "        # Label\n",
    "        df[\\'future_return\\'] = df[\\'close\\'].shift(-horizon) / df[\\'close\\'] - 1\n",
    "        df[\\'label\\'] = (df[\\'future_return\\'] > 0).astype(int)\n",
    "        \n",
    "        # Clean\n",
    "        feature_cols = [\\'return_1d\\', \\'return_5d\\', \\'return_10d\\', \\'return_20d\\',\n",
    "                        \\'volatility\\', \\'sma_ratio\\', \\'rsi_norm\\']\n",
    "        df_clean = df.dropna()\n",
    "        \n",
    "        X = df_clean[feature_cols].values.tolist()\n",
    "        y = df_clean[\\'label\\'].values.tolist()\n",
    "        \n",
    "        return X, y\n",
    "'''\n",
    "\n",
    "print(\"MLClassificationAlphaModel:\")\n",
    "print(ml_alpha_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 6 : Strategie Complete (20 min)\n",
    "\n",
    "### 6.1 Architecture de la Strategie\n",
    "\n",
    "```\n",
    "            Donnees OHLCV\n",
    "                  |\n",
    "                  v\n",
    "    +---------------------------+\n",
    "    |    Feature Engineering    |\n",
    "    | (returns, vol, RSI, SMA)  |\n",
    "    +---------------------------+\n",
    "                  |\n",
    "                  v\n",
    "    +---------------------------+\n",
    "    |     Random Forest         |\n",
    "    |     Classifier            |\n",
    "    | (retrain mensuel)         |\n",
    "    +---------------------------+\n",
    "                  |\n",
    "                  v\n",
    "    +---------------------------+\n",
    "    |     Probability           |\n",
    "    |     Threshold (>0.6)      |\n",
    "    +---------------------------+\n",
    "                  |\n",
    "                  v\n",
    "    +---------------------------+\n",
    "    |     Position Sizing       |\n",
    "    |     (par confidence)      |\n",
    "    +---------------------------+\n",
    "                  |\n",
    "                  v\n",
    "           Execution\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategie complete ML Direction Prediction\n",
    "\n",
    "complete_strategy_code = '''\n",
    "from AlgorithmImports import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class MLDirectionPredictionStrategy(QCAlgorithm):\n",
    "    \"\"\"\n",
    "    Strategie de trading basee sur ML Classification.\n",
    "    \n",
    "    - Modele: Random Forest Classifier\n",
    "    - Features: Returns, Volatility, SMA, RSI\n",
    "    - Prediction: Direction sur 5 jours\n",
    "    - Retrain: Mensuel\n",
    "    - Position sizing: Proportionnel a la confiance\n",
    "    \"\"\"\n",
    "    \n",
    "    def Initialize(self):\n",
    "        # === CONFIGURATION ===\n",
    "        self.SetStartDate(2020, 1, 1)\n",
    "        self.SetEndDate(2024, 1, 1)\n",
    "        self.SetCash(100000)\n",
    "        \n",
    "        # Parametres\n",
    "        self.lookback = 252              # 1 an d\\'historique\n",
    "        self.retrain_days = 21           # Retrain mensuel\n",
    "        self.prediction_horizon = 5      # Prediction sur 5 jours\n",
    "        self.probability_threshold = 0.6 # Seuil de confiance\n",
    "        self.max_position = 0.2          # Max 20% par position\n",
    "        \n",
    "        # Univers\n",
    "        self.tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\"]\n",
    "        self.symbols = {}\n",
    "        self.indicators = {}\n",
    "        \n",
    "        for ticker in self.tickers:\n",
    "            equity = self.AddEquity(ticker, Resolution.Daily)\n",
    "            symbol = equity.Symbol\n",
    "            self.symbols[ticker] = symbol\n",
    "            \n",
    "            self.indicators[symbol] = {\n",
    "                \"sma_20\": self.SMA(symbol, 20, Resolution.Daily),\n",
    "                \"rsi_14\": self.RSI(symbol, 14, Resolution.Daily)\n",
    "            }\n",
    "        \n",
    "        # Modele\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.last_train = None\n",
    "        \n",
    "        # Schedule retrain mensuel\n",
    "        self.Schedule.On(\n",
    "            self.DateRules.MonthStart(),\n",
    "            self.TimeRules.AfterMarketOpen(\"SPY\", 30),\n",
    "            self.TrainModel\n",
    "        )\n",
    "        \n",
    "        # Warmup\n",
    "        self.SetWarmup(self.lookback)\n",
    "        \n",
    "        self.Log(\"ML Direction Prediction Strategy initialized\")\n",
    "    \n",
    "    def TrainModel(self):\n",
    "        \"\"\"\n",
    "        Entraine le modele sur les donnees recentes.\n",
    "        \"\"\"\n",
    "        self.Debug(f\"{self.Time}: Entrainement du modele...\")\n",
    "        \n",
    "        all_X = []\n",
    "        all_y = []\n",
    "        \n",
    "        for ticker, symbol in self.symbols.items():\n",
    "            # Historique\n",
    "            history = self.History(symbol, self.lookback + 50, Resolution.Daily)\n",
    "            if history.empty or len(history) < self.lookback:\n",
    "                continue\n",
    "            \n",
    "            # Convertir\n",
    "            df = history[\\'close\\'].unstack(level=0)\n",
    "            df.columns = [\\'close\\']\n",
    "            \n",
    "            # Features\n",
    "            df[\\'return_1d\\'] = df[\\'close\\'].pct_change(1)\n",
    "            df[\\'return_5d\\'] = df[\\'close\\'].pct_change(5)\n",
    "            df[\\'return_20d\\'] = df[\\'close\\'].pct_change(20)\n",
    "            df[\\'volatility\\'] = df[\\'return_1d\\'].rolling(20).std()\n",
    "            df[\\'sma_ratio\\'] = df[\\'close\\'] / df[\\'close\\'].rolling(20).mean()\n",
    "            \n",
    "            # RSI\n",
    "            delta = df[\\'close\\'].diff()\n",
    "            gain = delta.clip(lower=0).rolling(14).mean()\n",
    "            loss = (-delta.clip(upper=0)).rolling(14).mean()\n",
    "            rs = gain / loss\n",
    "            df[\\'rsi_norm\\'] = ((100 - (100 / (1 + rs))) - 50) / 50\n",
    "            \n",
    "            # Label\n",
    "            df[\\'future_return\\'] = df[\\'close\\'].shift(-self.prediction_horizon) / df[\\'close\\'] - 1\n",
    "            df[\\'label\\'] = (df[\\'future_return\\'] > 0).astype(int)\n",
    "            \n",
    "            # Clean et collecter\n",
    "            feature_cols = [\\'return_1d\\', \\'return_5d\\', \\'return_20d\\', \\'volatility\\', \\'sma_ratio\\', \\'rsi_norm\\']\n",
    "            df_clean = df.dropna()\n",
    "            \n",
    "            all_X.extend(df_clean[feature_cols].values.tolist())\n",
    "            all_y.extend(df_clean[\\'label\\'].values.tolist())\n",
    "        \n",
    "        if len(all_X) < 100:\n",
    "            self.Debug(\"Pas assez de donnees\")\n",
    "            return\n",
    "        \n",
    "        # Entrainer\n",
    "        X = np.array(all_X)\n",
    "        y = np.array(all_y)\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.model.fit(X_scaled, y)\n",
    "        \n",
    "        self.last_train = self.Time\n",
    "        self.Debug(f\"Modele entraine. Samples: {len(X)}, Accuracy: {self.model.score(X_scaled, y):.4f}\")\n",
    "    \n",
    "    def OnData(self, data):\n",
    "        \"\"\"\n",
    "        Execute la strategie.\n",
    "        \"\"\"\n",
    "        if self.IsWarmingUp:\n",
    "            return\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.TrainModel()\n",
    "            if self.model is None:\n",
    "                return\n",
    "        \n",
    "        for ticker, symbol in self.symbols.items():\n",
    "            if not data.ContainsKey(symbol):\n",
    "                continue\n",
    "            \n",
    "            # Verifier indicateurs\n",
    "            ind = self.indicators[symbol]\n",
    "            if not ind[\"sma_20\"].IsReady or not ind[\"rsi_14\"].IsReady:\n",
    "                continue\n",
    "            \n",
    "            # Extraire features\n",
    "            features = self.ExtractFeatures(symbol)\n",
    "            if features is None:\n",
    "                continue\n",
    "            \n",
    "            # Predire\n",
    "            features_scaled = self.scaler.transform([features])\n",
    "            prediction = self.model.predict(features_scaled)[0]\n",
    "            proba = self.model.predict_proba(features_scaled)[0]\n",
    "            confidence = max(proba)\n",
    "            \n",
    "            # Trading logic\n",
    "            if confidence >= self.probability_threshold:\n",
    "                if prediction == 1:  # Prediction UP\n",
    "                    # Position proportionnelle a la confiance\n",
    "                    target_weight = self.max_position * (confidence - 0.5) * 2\n",
    "                    current_weight = self.Portfolio[symbol].HoldingsValue / self.Portfolio.TotalPortfolioValue\n",
    "                    \n",
    "                    if target_weight > current_weight + 0.02:  # Seuil de changement\n",
    "                        self.SetHoldings(symbol, target_weight)\n",
    "                        self.Log(f\"{self.Time.date()}: BUY {ticker}, Conf={confidence:.3f}, Weight={target_weight:.2%}\")\n",
    "                \n",
    "                elif prediction == 0:  # Prediction DOWN\n",
    "                    if self.Portfolio[symbol].Invested:\n",
    "                        self.Liquidate(symbol)\n",
    "                        self.Log(f\"{self.Time.date()}: SELL {ticker}, Conf={confidence:.3f}\")\n",
    "    \n",
    "    def ExtractFeatures(self, symbol):\n",
    "        \"\"\"\n",
    "        Extrait les features pour un symbole.\n",
    "        \"\"\"\n",
    "        history = self.History(symbol, 25, Resolution.Daily)\n",
    "        if history.empty or len(history) < 21:\n",
    "            return None\n",
    "        \n",
    "        prices = history[\\'close\\'].values\n",
    "        \n",
    "        return_1d = (prices[-1] - prices[-2]) / prices[-2]\n",
    "        return_5d = (prices[-1] - prices[-6]) / prices[-6]\n",
    "        return_20d = (prices[-1] - prices[-21]) / prices[-21]\n",
    "        \n",
    "        returns = [(prices[i] - prices[i-1]) / prices[i-1] for i in range(1, len(prices))]\n",
    "        volatility = np.std(returns[-20:])\n",
    "        \n",
    "        sma_ratio = prices[-1] / self.indicators[symbol][\"sma_20\"].Current.Value\n",
    "        rsi_norm = (self.indicators[symbol][\"rsi_14\"].Current.Value - 50) / 50\n",
    "        \n",
    "        return [return_1d, return_5d, return_20d, volatility, sma_ratio, rsi_norm]\n",
    "    \n",
    "    def OnEndOfAlgorithm(self):\n",
    "        \"\"\"\n",
    "        Resume final.\n",
    "        \"\"\"\n",
    "        self.Log(\"=\"*60)\n",
    "        self.Log(\"ML DIRECTION PREDICTION STRATEGY - SUMMARY\")\n",
    "        self.Log(\"=\"*60)\n",
    "        self.Log(f\"Final Value: ${self.Portfolio.TotalPortfolioValue:,.2f}\")\n",
    "        total_return = (self.Portfolio.TotalPortfolioValue - 100000) / 100000\n",
    "        self.Log(f\"Total Return: {total_return:.2%}\")\n",
    "        self.Log(\"=\"*60)\n",
    "'''\n",
    "\n",
    "print(\"MLDirectionPredictionStrategy Complete:\")\n",
    "print(complete_strategy_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume de la strategie\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RESUME: ML Direction Prediction Strategy\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "strategy_components = {\n",
    "    'Feature Engineering': {\n",
    "        'Returns': '1D, 5D, 20D',\n",
    "        'Volatility': 'Rolling 20D std',\n",
    "        'SMA Ratio': 'Price / SMA(20)',\n",
    "        'RSI': 'Normalized [-1, 1]'\n",
    "    },\n",
    "    'Modele': {\n",
    "        'Type': 'Random Forest Classifier',\n",
    "        'Arbres': '100',\n",
    "        'Profondeur': '5',\n",
    "        'Min samples leaf': '10'\n",
    "    },\n",
    "    'Validation': {\n",
    "        'Methode': 'TimeSeriesSplit + Walk-Forward',\n",
    "        'Retrain': 'Mensuel',\n",
    "        'Lookback': '252 jours (1 an)'\n",
    "    },\n",
    "    'Trading': {\n",
    "        'Horizon': '5 jours',\n",
    "        'Seuil probabilite': '0.6 (60%)',\n",
    "        'Position sizing': 'Proportionnel a confiance',\n",
    "        'Max position': '20%'\n",
    "    },\n",
    "    'Signaux': {\n",
    "        'BUY': 'Prediction=UP AND Confiance >= 0.6',\n",
    "        'SELL': 'Prediction=DOWN AND Confiance >= 0.6'\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, details in strategy_components.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependances Python\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEPENDANCES PYTHON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dependencies = [\n",
    "    {\n",
    "        'package': 'scikit-learn',\n",
    "        'install': 'pip install scikit-learn',\n",
    "        'usage': 'RandomForestClassifier, StandardScaler, metriques',\n",
    "        'note': 'Inclus dans QuantConnect'\n",
    "    },\n",
    "    {\n",
    "        'package': 'xgboost',\n",
    "        'install': 'pip install xgboost',\n",
    "        'usage': 'XGBClassifier avec early stopping',\n",
    "        'note': 'Inclus dans QuantConnect'\n",
    "    },\n",
    "    {\n",
    "        'package': 'numpy',\n",
    "        'install': 'pip install numpy',\n",
    "        'usage': 'Operations numeriques',\n",
    "        'note': 'Inclus dans QuantConnect'\n",
    "    },\n",
    "    {\n",
    "        'package': 'pandas',\n",
    "        'install': 'pip install pandas',\n",
    "        'usage': 'Manipulation de donnees',\n",
    "        'note': 'Inclus dans QuantConnect'\n",
    "    }\n",
    "]\n",
    "\n",
    "for dep in dependencies:\n",
    "    print(f\"\\n{dep['package']}:\")\n",
    "    print(f\"  Installation: {dep['install']}\")\n",
    "    print(f\"  Usage: {dep['usage']}\")\n",
    "    print(f\"  Note: {dep['note']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion et Prochaines Etapes\n",
    "\n",
    "### Recapitulatif\n",
    "\n",
    "Dans ce notebook, nous avons couvert:\n",
    "\n",
    "1. **Introduction ML Trading** : Pourquoi ML vs regles fixes, challenges specifiques\n",
    "\n",
    "2. **Random Forest** : Configuration, entrainement, feature importance\n",
    "\n",
    "3. **XGBoost** : Early stopping, regularisation, comparaison avec RF\n",
    "\n",
    "4. **Validation** : TimeSeriesSplit, Walk-Forward, metriques trading\n",
    "\n",
    "5. **Integration QC** : ObjectStore, ML Alpha Model\n",
    "\n",
    "6. **Strategie Complete** : Pipeline end-to-end avec retrain periodique\n",
    "\n",
    "### Points Cles a Retenir\n",
    "\n",
    "| Concept | Point Cle |\n",
    "|---------|----------|\n",
    "| **TimeSeriesSplit** | Toujours train avant test (pas de shuffle) |\n",
    "| **Retrain** | Mensuel minimum pour s'adapter aux regimes |\n",
    "| **Precision** | Plus importante que accuracy en trading |\n",
    "| **Probabilite** | Utiliser proba pour confidence, pas juste la classe |\n",
    "| **Feature Importance** | Comprendre ce que le modele apprend |\n",
    "| **Regularisation** | Limiter profondeur, min_samples_leaf |\n",
    "\n",
    "### Limitations et Avertissements\n",
    "\n",
    "| Limitation | Description |\n",
    "|------------|-------------|\n",
    "| **Overfitting** | Modeles complexes memorisent le bruit |\n",
    "| **Regime Change** | Performances degradent apres changement de regime |\n",
    "| **Latence** | Retrain et inference prennent du temps |\n",
    "| **Data Quality** | Les features dependent de donnees propres |\n",
    "| **Black Box** | Difficile d'expliquer pourquoi un trade |\n",
    "\n",
    "### Ameliorations Possibles\n",
    "\n",
    "1. **Ensemble** : Combiner RF + XGBoost + autres modeles\n",
    "2. **Features** : Ajouter fondamentaux, sentiment, alternatifs\n",
    "3. **Hyperparameter Tuning** : GridSearch, Optuna\n",
    "4. **Calibration** : Isotonic/Platt pour probabilites calibrees\n",
    "5. **Online Learning** : Mise a jour incrementale\n",
    "\n",
    "### Prochaines Etapes\n",
    "\n",
    "| Notebook | Contenu |\n",
    "|----------|--------|\n",
    "| **QC-Py-20** | ML Regression (predire le rendement exact) |\n",
    "| **QC-Py-21** | Deep Learning (LSTM, Transformers) |\n",
    "| **QC-Py-22** | Reinforcement Learning pour trading |\n",
    "\n",
    "### Ressources Complementaires\n",
    "\n",
    "- [QuantConnect ML Documentation](https://www.quantconnect.com/docs/v2/writing-algorithms/machine-learning)\n",
    "- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
    "- [Advances in Financial ML](https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089) - Lopez de Prado\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook complete. La classification ML est un outil puissant mais doit etre utilisee avec rigueur: validation robuste, retrain regulier, et gestion du risque appropriee.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
