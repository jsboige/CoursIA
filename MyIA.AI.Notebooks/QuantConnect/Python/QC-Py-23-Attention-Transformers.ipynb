{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC-Py-23 - State Space Models (Mamba) pour Trading\n",
    "\n",
    "> **Au-delà des Transformers : Complexité O(n) pour séries temporelles longues**\n",
    "> Durée : 100 minutes | Niveau : Avancé | Python + PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "\n",
    "À la fin de ce notebook, vous serez capable de :\n",
    "\n",
    "1. Comprendre le **problème de l'attention quadratique O(n²)** des Transformers\n",
    "2. Maîtriser la théorie des **State Space Models (S4 → Mamba)**\n",
    "3. Implémenter un **Mamba Block** pour time series en PyTorch\n",
    "4. Construire un modèle **SST Hybrid** (Mamba + Transformer)\n",
    "5. Appliquer **CMDMamba** optimisé pour la finance (2025)\n",
    "6. Comparer **LSTM vs Transformer vs Mamba** sur données financières\n",
    "7. Intégrer dans une **stratégie QuantConnect** avec ObjectStore\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "- Notebook QC-Py-22 complété (PyTorch, architectures modernes)\n",
    "- Compréhension des Transformers et de l'attention\n",
    "- Familiarité avec les RNN/LSTM\n",
    "- Notions d'équations différentielles (optionnel, mais utile)\n",
    "\n",
    "## Structure du Notebook\n",
    "\n",
    "| Partie | Sujet | Durée |\n",
    "|--------|-------|-------|\n",
    "| 1 | Le problème de l'attention O(n²) | 10 min |\n",
    "| 2 | Théorie des State Space Models | 20 min |\n",
    "| 3 | De S4 à Mamba : Selective State Spaces | 20 min |\n",
    "| 4 | Implémentation Mamba en PyTorch | 20 min |\n",
    "| 5 | SST Hybrid : Mamba + Transformer | 15 min |\n",
    "| 6 | Comparaison LSTM vs Transformer vs Mamba | 10 min |\n",
    "| 7 | Intégration QuantConnect | 15 min |\n",
    "\n",
    "## Références SOTA 2024-2026\n",
    "\n",
    "| Paper | Venue | Contribution |\n",
    "|-------|-------|-------------|\n",
    "| **Mamba** | arXiv 2312.00752 | Selective State Spaces, hardware-aware |\n",
    "| **S4** | ICLR 2022 | Structured State Space Sequences |\n",
    "| **SST** | CIKM 2024 | Hybrid Mamba-Transformer |\n",
    "| **CMDMamba** | Frontiers AI 2025 | Dual-layer Mamba pour finance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 1 : Le Problème de l'Attention O(n²) (10 min)\n",
    "\n",
    "### Pourquoi les Transformers ont une limite ?\n",
    "\n",
    "L'attention des Transformers calcule une matrice **Q × K^T** de taille **(n × n)** où n est la longueur de la séquence.\n",
    "\n",
    "| Longueur séquence | Taille matrice | Mémoire (float32) | Temps relatif |\n",
    "|------------------|----------------|-------------------|---------------|\n",
    "| 60 jours | 3,600 | 14 KB | 1x |\n",
    "| 252 jours (1 an) | 63,504 | 254 KB | 18x |\n",
    "| 504 jours (2 ans) | 254,016 | 1 MB | 70x |\n",
    "| 1,260 jours (5 ans) | 1,587,600 | 6.3 MB | 440x |\n",
    "\n",
    "### Impact pour le trading\n",
    "\n",
    "- **Patterns saisonniers** : Nécessitent ~252 jours minimum\n",
    "- **Cycles économiques** : Nécessitent 3-5 ans de données\n",
    "- **Régimes de marché** : Changent sur des années\n",
    "\n",
    "**Conclusion** : Les Transformers standard ne scalent pas pour les longues séries financières.\n",
    "\n",
    "### Alternatives explorées\n",
    "\n",
    "| Approche | Complexité | Limitation |\n",
    "|----------|------------|------------|\n",
    "| **Sparse Attention** (Longformer) | O(n√n) | Patterns fixes, perte d'info |\n",
    "| **Linear Attention** (Performer) | O(n) | Approximation, moins précis |\n",
    "| **Chunked Attention** | O(n×k) | Perte des très longues dépendances |\n",
    "| **State Space Models** | O(n) | ✅ Exact, scalable, hardware-friendly |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Math\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"\\nCe notebook est optimisé CPU-first.\")\n",
    "print(\"Les SSMs sont particulièrement efficaces sur CPU grâce à leur complexité O(n).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_complexity_comparison():\n",
    "    \"\"\"\n",
    "    Visualise la différence de complexité entre Transformer et SSM.\n",
    "    \"\"\"\n",
    "    seq_lengths = np.array([60, 120, 252, 504, 756, 1008, 1260])\n",
    "    \n",
    "    # Complexités (normalisées)\n",
    "    transformer_complexity = seq_lengths ** 2  # O(n²)\n",
    "    ssm_complexity = seq_lengths  # O(n)\n",
    "    lstm_complexity = seq_lengths  # O(n) mais séquentiel\n",
    "    \n",
    "    # Normalisation\n",
    "    transformer_norm = transformer_complexity / transformer_complexity[0]\n",
    "    ssm_norm = ssm_complexity / ssm_complexity[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Complexité théorique\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(seq_lengths, transformer_norm, 'o-', label='Transformer O(n²)', \n",
    "             color='coral', linewidth=2, markersize=8)\n",
    "    ax1.plot(seq_lengths, ssm_norm, 's-', label='SSM/Mamba O(n)', \n",
    "             color='steelblue', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Longueur de séquence (jours)', fontsize=11)\n",
    "    ax1.set_ylabel('Complexité relative (base: 60 jours)', fontsize=11)\n",
    "    ax1.set_title('Complexité Computationnelle', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Annotations\n",
    "    ax1.annotate('1 an\\n(252j)', xy=(252, 17.64), xytext=(300, 50),\n",
    "                arrowprops=dict(arrowstyle='->', color='gray'),\n",
    "                fontsize=9, color='gray')\n",
    "    ax1.annotate('5 ans\\n(1260j)', xy=(1260, 441), xytext=(1100, 150),\n",
    "                arrowprops=dict(arrowstyle='->', color='gray'),\n",
    "                fontsize=9, color='gray')\n",
    "    \n",
    "    # Mémoire GPU estimée\n",
    "    ax2 = axes[1]\n",
    "    # Attention matrix: n² × 4 bytes × batch_size × num_heads\n",
    "    batch_size = 32\n",
    "    num_heads = 4\n",
    "    memory_transformer_mb = (seq_lengths ** 2 * 4 * batch_size * num_heads) / (1024 ** 2)\n",
    "    memory_ssm_mb = (seq_lengths * 64 * 4 * batch_size) / (1024 ** 2)  # state_size=64\n",
    "    \n",
    "    ax2.bar(np.arange(len(seq_lengths)) - 0.2, memory_transformer_mb, 0.4, \n",
    "           label='Transformer', color='coral', alpha=0.8)\n",
    "    ax2.bar(np.arange(len(seq_lengths)) + 0.2, memory_ssm_mb, 0.4, \n",
    "           label='SSM/Mamba', color='steelblue', alpha=0.8)\n",
    "    ax2.set_xticks(np.arange(len(seq_lengths)))\n",
    "    ax2.set_xticklabels([f'{l}j' for l in seq_lengths])\n",
    "    ax2.set_xlabel('Longueur de séquence', fontsize=11)\n",
    "    ax2.set_ylabel('Mémoire estimée (MB)', fontsize=11)\n",
    "    ax2.set_title('Consommation Mémoire (batch=32)', fontsize=13, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nConclusion:\")\n",
    "    print(f\"  - À 1260 jours (5 ans), Transformer: {transformer_norm[-1]:.0f}x plus lent\")\n",
    "    print(f\"  - SSM/Mamba: scalabilité linéaire, même performance relative\")\n",
    "    print(f\"  - Mémoire Transformer pour 5 ans: {memory_transformer_mb[-1]:.1f} MB\")\n",
    "    print(f\"  - Mémoire SSM pour 5 ans: {memory_ssm_mb[-1]:.1f} MB\")\n",
    "\n",
    "visualize_complexity_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 2 : Théorie des State Space Models (20 min)\n",
    "\n",
    "### Qu'est-ce qu'un State Space Model ?\n",
    "\n",
    "Un SSM est défini par un système d'équations différentielles linéaires :\n",
    "\n",
    "$$\\frac{dh(t)}{dt} = Ah(t) + Bx(t)$$\n",
    "$$y(t) = Ch(t) + Dx(t)$$\n",
    "\n",
    "Où :\n",
    "- **x(t)** : Input (signal d'entrée, ex: prix)\n",
    "- **h(t)** : État caché (mémoire interne)\n",
    "- **y(t)** : Output (prédiction)\n",
    "- **A, B, C, D** : Matrices de paramètres apprises\n",
    "\n",
    "### Discrétisation pour séries temporelles\n",
    "\n",
    "Pour traiter des données discrètes (jours, heures), on discrétise avec un pas Δ :\n",
    "\n",
    "$$\\bar{A} = e^{\\Delta A}$$\n",
    "$$\\bar{B} = (\\Delta A)^{-1}(e^{\\Delta A} - I) \\cdot \\Delta B$$\n",
    "\n",
    "Ce qui donne la récurrence :\n",
    "\n",
    "$$h_k = \\bar{A} h_{k-1} + \\bar{B} x_k$$\n",
    "$$y_k = C h_k$$\n",
    "\n",
    "### Avantage : Calcul parallèle via convolution\n",
    "\n",
    "La séquence entière peut être calculée comme une **convolution** :\n",
    "\n",
    "$$y = x * \\bar{K}$$\n",
    "\n",
    "où $\\bar{K} = (C\\bar{B}, C\\bar{A}\\bar{B}, C\\bar{A}^2\\bar{B}, ...)$ est le kernel SSM.\n",
    "\n",
    "**C'est la clé de la scalabilité O(n)** : on peut utiliser FFT pour calculer la convolution en O(n log n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_ssm_concept():\n",
    "    \"\"\"\n",
    "    Démontre visuellement le concept de State Space Model.\n",
    "    \"\"\"\n",
    "    # Paramètres simples pour illustration\n",
    "    seq_len = 100\n",
    "    state_dim = 4\n",
    "    \n",
    "    # Matrices (simplifiées)\n",
    "    np.random.seed(42)\n",
    "    A = np.eye(state_dim) * 0.95  # Matrice d'état (stable)\n",
    "    A[0, 1] = 0.1\n",
    "    A[1, 0] = -0.1\n",
    "    B = np.random.randn(state_dim, 1) * 0.3\n",
    "    C = np.random.randn(1, state_dim) * 0.5\n",
    "    \n",
    "    # Signal d'entrée (prix simulé)\n",
    "    t = np.linspace(0, 10, seq_len)\n",
    "    x = np.sin(t) + 0.5 * np.sin(3*t) + np.random.randn(seq_len) * 0.2\n",
    "    \n",
    "    # Simulation récurrente\n",
    "    h = np.zeros((seq_len, state_dim))\n",
    "    y = np.zeros(seq_len)\n",
    "    \n",
    "    for k in range(1, seq_len):\n",
    "        h[k] = A @ h[k-1] + B.flatten() * x[k]\n",
    "        y[k] = C @ h[k]\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "    \n",
    "    # Input\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(t, x, color='steelblue', alpha=0.8)\n",
    "    ax1.set_xlabel('Temps')\n",
    "    ax1.set_ylabel('Valeur')\n",
    "    ax1.set_title('Input x(t) : Signal d\\'entrée', fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # États cachés\n",
    "    ax2 = axes[0, 1]\n",
    "    for i in range(state_dim):\n",
    "        ax2.plot(t, h[:, i], label=f'h{i+1}', alpha=0.7)\n",
    "    ax2.set_xlabel('Temps')\n",
    "    ax2.set_ylabel('État')\n",
    "    ax2.set_title('États cachés h(t) : Mémoire interne', fontweight='bold')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Output\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(t, y, color='coral', alpha=0.8)\n",
    "    ax3.set_xlabel('Temps')\n",
    "    ax3.set_ylabel('Output')\n",
    "    ax3.set_title('Output y(t) : Prédiction', fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Comparaison input/output\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.plot(t, x / np.abs(x).max(), label='Input (norm)', color='steelblue', alpha=0.6)\n",
    "    ax4.plot(t, y / np.abs(y).max(), label='Output (norm)', color='coral', alpha=0.8)\n",
    "    ax4.set_xlabel('Temps')\n",
    "    ax4.set_ylabel('Valeur normalisée')\n",
    "    ax4.set_title('Input vs Output : Transformation SSM', fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterprétation:\")\n",
    "    print(\"  - Le SSM transforme l'input via son état caché\")\n",
    "    print(\"  - Chaque état h_i capture un aspect différent du signal\")\n",
    "    print(\"  - L'output est une combinaison linéaire des états\")\n",
    "    print(\"  - La matrice A contrôle la 'mémoire' (decay)\")\n",
    "\n",
    "demonstrate_ssm_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4 : Structured State Space Sequence (ICLR 2022)\n",
    "\n",
    "Le papier S4 de Gu et al. a résolu les problèmes de stabilité numérique des SSM avec :\n",
    "\n",
    "1. **HiPPO Initialization** : Matrice A spéciale qui préserve l'historique\n",
    "2. **NPLR Representation** : Normal Plus Low-Rank pour calcul efficace\n",
    "3. **Cauchy Kernel** : Permet FFT efficace\n",
    "\n",
    "$$A = -\\frac{1}{2} + \\text{low-rank}$$\n",
    "\n",
    "**Résultat** : Premier SSM compétitif avec les Transformers sur Long Range Arena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified S4 Layer for educational purposes.\n",
    "    \n",
    "    Based on: \"Efficiently Modeling Long Sequences with Structured State Spaces\"\n",
    "    Gu et al., ICLR 2022\n",
    "    \n",
    "    Note: This is a simplified implementation. For production, use:\n",
    "    - https://github.com/state-spaces/s4\n",
    "    - https://github.com/HazyResearch/safari\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_state: int = 64, dropout: float = 0.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        d_model : int\n",
    "            Model dimension (input/output size)\n",
    "        d_state : int\n",
    "            State dimension (N in the paper)\n",
    "        dropout : float\n",
    "            Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        \n",
    "        # Learnable SSM parameters\n",
    "        # A: state transition matrix (d_state, d_state)\n",
    "        # Using HiPPO-inspired initialization\n",
    "        A = torch.arange(1, d_state + 1, dtype=torch.float32)\n",
    "        A = -0.5 + 1j * math.pi * A  # Complex for oscillatory behavior\n",
    "        self.register_buffer('A', torch.view_as_real(A.unsqueeze(0).expand(d_model, -1)))\n",
    "        \n",
    "        # B: input matrix (d_model, d_state)\n",
    "        self.B = nn.Parameter(torch.randn(d_model, d_state) * 0.02)\n",
    "        \n",
    "        # C: output matrix (d_model, d_state)\n",
    "        self.C = nn.Parameter(torch.randn(d_model, d_state) * 0.02)\n",
    "        \n",
    "        # Delta: discretization step (learned per channel)\n",
    "        self.log_delta = nn.Parameter(torch.randn(d_model) * 0.1 - 4.0)\n",
    "        \n",
    "        # D: skip connection\n",
    "        self.D = nn.Parameter(torch.ones(d_model))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def discretize(self):\n",
    "        \"\"\"\n",
    "        Discretize continuous SSM parameters.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        A_bar, B_bar : discretized matrices\n",
    "        \"\"\"\n",
    "        delta = torch.exp(self.log_delta)  # (d_model,)\n",
    "        \n",
    "        # Get complex A\n",
    "        A = torch.view_as_complex(self.A)  # (d_model, d_state)\n",
    "        \n",
    "        # Discretization using ZOH (Zero-Order Hold)\n",
    "        # A_bar = exp(delta * A)\n",
    "        A_bar = torch.exp(delta.unsqueeze(-1) * A)  # (d_model, d_state)\n",
    "        \n",
    "        # B_bar = (A^-1)(exp(delta*A) - I) * delta * B\n",
    "        # Simplified: B_bar ≈ delta * B for small delta\n",
    "        B_bar = delta.unsqueeze(-1) * self.B  # (d_model, d_state)\n",
    "        \n",
    "        return A_bar, B_bar\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass using recurrent formulation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : tensor\n",
    "            Input of shape (batch, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tensor : Output of shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Discretize\n",
    "        A_bar, B_bar = self.discretize()\n",
    "        \n",
    "        # Initialize state\n",
    "        h = torch.zeros(batch, d_model, self.d_state, dtype=torch.complex64, device=x.device)\n",
    "        \n",
    "        # Recurrent computation\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]  # (batch, d_model)\n",
    "            \n",
    "            # State update: h_t = A_bar * h_{t-1} + B_bar * x_t\n",
    "            h = A_bar.unsqueeze(0) * h + B_bar.unsqueeze(0) * x_t.unsqueeze(-1).to(torch.complex64)\n",
    "            \n",
    "            # Output: y_t = Re(C * h_t) + D * x_t\n",
    "            y_t = torch.real(torch.sum(self.C.unsqueeze(0) * h, dim=-1)) + self.D * x_t\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        y = torch.stack(outputs, dim=1)  # (batch, seq_len, d_model)\n",
    "        y = self.dropout(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "# Test S4 Layer\n",
    "print(\"Test de S4Layer:\")\n",
    "s4_layer = S4Layer(d_model=32, d_state=16)\n",
    "test_input = torch.randn(2, 60, 32)  # (batch, seq_len, d_model)\n",
    "output = s4_layer(test_input)\n",
    "\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in s4_layer.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 3 : De S4 à Mamba - Selective State Spaces (20 min)\n",
    "\n",
    "### Limitation de S4 : Matrices fixes\n",
    "\n",
    "Dans S4, les matrices A, B, C sont **indépendantes de l'input**. Cela pose problème pour :\n",
    "\n",
    "- **Sélectivité** : Impossible de \"ignorer\" certaines entrées\n",
    "- **Context-awareness** : Pas d'adaptation au contenu\n",
    "\n",
    "### Mamba : Selective State Spaces\n",
    "\n",
    "L'innovation clé de Mamba (Gu & Dao, 2023) est de rendre B, C, Δ **dépendants de l'input** :\n",
    "\n",
    "$$B_t = \\text{Linear}_B(x_t)$$\n",
    "$$C_t = \\text{Linear}_C(x_t)$$\n",
    "$$\\Delta_t = \\text{softplus}(\\text{Linear}_\\Delta(x_t))$$\n",
    "\n",
    "### Pourquoi \"Selective\" ?\n",
    "\n",
    "Le modèle peut maintenant **sélectionner** quelles informations mémoriser :\n",
    "\n",
    "| Input | Δ petit | Δ grand |\n",
    "|-------|---------|--------|\n",
    "| Information importante | ✓ Mémorise | - |\n",
    "| Bruit/padding | - | ✓ \"Oublie\" |\n",
    "\n",
    "C'est similaire au **forget gate** des LSTM, mais plus efficace.\n",
    "\n",
    "### Architecture hardware-aware\n",
    "\n",
    "Mamba utilise des optimisations GPU spécifiques :\n",
    "- **Parallel scan** au lieu de récurrence séquentielle\n",
    "- **Kernel fusion** pour réduire les accès mémoire\n",
    "- **Recomputation** dans le backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mamba Block - Selective State Space Model.\n",
    "    \n",
    "    Based on: \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\"\n",
    "    Gu & Dao, arXiv 2312.00752\n",
    "    \n",
    "    This is a simplified PyTorch implementation for educational purposes.\n",
    "    For production, use: https://github.com/state-spaces/mamba\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, \n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "        expand: int = 2,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        d_model : int\n",
    "            Model dimension\n",
    "        d_state : int\n",
    "            SSM state dimension (N)\n",
    "        d_conv : int\n",
    "            Local convolution width\n",
    "        expand : int\n",
    "            Inner dimension expansion factor\n",
    "        dropout : float\n",
    "            Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.d_inner = d_model * expand\n",
    "        \n",
    "        # Input projection (to 2x for gating)\n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n",
    "        \n",
    "        # 1D Convolution for local context\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            kernel_size=d_conv,\n",
    "            padding=d_conv - 1,\n",
    "            groups=self.d_inner  # Depthwise\n",
    "        )\n",
    "        \n",
    "        # SSM parameters (input-dependent)\n",
    "        # x_proj projects to (delta, B, C)\n",
    "        self.x_proj = nn.Linear(self.d_inner, d_state * 2 + 1, bias=False)\n",
    "        \n",
    "        # Delta (discretization step) projection\n",
    "        self.dt_proj = nn.Linear(1, self.d_inner, bias=True)\n",
    "        \n",
    "        # A parameter (log scale for stability)\n",
    "        A = torch.arange(1, d_state + 1, dtype=torch.float32)\n",
    "        self.A_log = nn.Parameter(torch.log(A.repeat(self.d_inner, 1)))\n",
    "        \n",
    "        # D skip connection\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def ssm_step(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        h: torch.Tensor,\n",
    "        delta: torch.Tensor,\n",
    "        A: torch.Tensor,\n",
    "        B: torch.Tensor,\n",
    "        C: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Single SSM step with selective parameters.\n",
    "        \n",
    "        h_t = exp(delta * A) * h_{t-1} + delta * B * x_t\n",
    "        y_t = C * h_t\n",
    "        \"\"\"\n",
    "        # Discretize A\n",
    "        A_bar = torch.exp(delta.unsqueeze(-1) * A)  # (batch, d_inner, d_state)\n",
    "        \n",
    "        # Discretize B\n",
    "        B_bar = delta.unsqueeze(-1) * B  # (batch, d_inner, d_state)\n",
    "        \n",
    "        # State update\n",
    "        h = A_bar * h + B_bar * x.unsqueeze(-1)\n",
    "        \n",
    "        # Output\n",
    "        y = torch.sum(C * h, dim=-1)  # (batch, d_inner)\n",
    "        \n",
    "        return y, h\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : tensor\n",
    "            Input of shape (batch, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tensor : Output of shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        # Input projection with gating\n",
    "        xz = self.in_proj(x)  # (batch, seq_len, d_inner * 2)\n",
    "        x_main, z = xz.chunk(2, dim=-1)  # Each: (batch, seq_len, d_inner)\n",
    "        \n",
    "        # 1D convolution for local context\n",
    "        x_conv = x_main.transpose(1, 2)  # (batch, d_inner, seq_len)\n",
    "        x_conv = self.conv1d(x_conv)[:, :, :seq_len]  # Causal\n",
    "        x_conv = x_conv.transpose(1, 2)  # (batch, seq_len, d_inner)\n",
    "        x_main = F.silu(x_conv)\n",
    "        \n",
    "        # Project to SSM parameters\n",
    "        x_ssm = self.x_proj(x_main)  # (batch, seq_len, d_state*2 + 1)\n",
    "        \n",
    "        # Split into delta, B, C\n",
    "        delta_raw = x_ssm[:, :, :1]  # (batch, seq_len, 1)\n",
    "        B = x_ssm[:, :, 1:1+self.d_state]  # (batch, seq_len, d_state)\n",
    "        C = x_ssm[:, :, 1+self.d_state:]  # (batch, seq_len, d_state)\n",
    "        \n",
    "        # Delta projection + softplus\n",
    "        delta = F.softplus(self.dt_proj(delta_raw))  # (batch, seq_len, d_inner)\n",
    "        \n",
    "        # A (negative for stability)\n",
    "        A = -torch.exp(self.A_log)  # (d_inner, d_state)\n",
    "        \n",
    "        # Initialize state\n",
    "        h = torch.zeros(batch, self.d_inner, self.d_state, device=x.device)\n",
    "        \n",
    "        # Recurrent SSM (simplified, not optimized)\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x_main[:, t, :]  # (batch, d_inner)\n",
    "            delta_t = delta[:, t, :]  # (batch, d_inner)\n",
    "            B_t = B[:, t, :].unsqueeze(1).expand(-1, self.d_inner, -1)  # (batch, d_inner, d_state)\n",
    "            C_t = C[:, t, :].unsqueeze(1).expand(-1, self.d_inner, -1)  # (batch, d_inner, d_state)\n",
    "            \n",
    "            y_t, h = self.ssm_step(x_t, h, delta_t, A, B_t, C_t)\n",
    "            \n",
    "            # Skip connection\n",
    "            y_t = y_t + self.D * x_t\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        y = torch.stack(outputs, dim=1)  # (batch, seq_len, d_inner)\n",
    "        \n",
    "        # Gating with z\n",
    "        y = y * F.silu(z)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.out_proj(y)\n",
    "        y = self.dropout(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "# Test Mamba Block\n",
    "print(\"Test de MambaBlock:\")\n",
    "mamba_block = MambaBlock(d_model=32, d_state=16, d_conv=4, expand=2)\n",
    "test_input = torch.randn(2, 60, 32)\n",
    "output = mamba_block(test_input)\n",
    "\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in mamba_block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 4 : Implémentation Mamba Complète pour Trading (20 min)\n",
    "\n",
    "### Architecture du modèle\n",
    "\n",
    "```\n",
    "Input (batch, seq_len, n_features)\n",
    "           |\n",
    "     [Input Projection]\n",
    "           |\n",
    "     [Mamba Block 1]\n",
    "           |\n",
    "     [Mamba Block 2]\n",
    "           |\n",
    "     [Global Pooling]\n",
    "           |\n",
    "     [Classification Head]\n",
    "           |\n",
    "Output (batch, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaForTrading(nn.Module):\n",
    "    \"\"\"\n",
    "    Mamba model for financial time series classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input projection\n",
    "    - N x Mamba blocks with residual connections\n",
    "    - Global average pooling\n",
    "    - Classification head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        d_model: int = 32,\n",
    "        d_state: int = 16,\n",
    "        n_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        n_classes: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        # Mamba blocks\n",
    "        self.mamba_blocks = nn.ModuleList([\n",
    "            MambaBlock(\n",
    "                d_model=d_model,\n",
    "                d_state=d_state,\n",
    "                d_conv=4,\n",
    "                expand=2,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer norms for residual\n",
    "        self.norms = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : tensor\n",
    "            Input of shape (batch, seq_len, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tensor : Logits of shape (batch, n_classes)\n",
    "        \"\"\"\n",
    "        # Project to d_model\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Mamba blocks with residual\n",
    "        for mamba, norm in zip(self.mamba_blocks, self.norms):\n",
    "            x = x + mamba(norm(x))\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)  # (batch, d_model)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# Test complet\n",
    "print(\"Test de MambaForTrading:\")\n",
    "model = MambaForTrading(\n",
    "    n_features=5,\n",
    "    d_model=32,\n",
    "    d_state=16,\n",
    "    n_layers=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "test_input = torch.randn(4, 60, 5)  # (batch, seq_len, features)\n",
    "output = model(test_input)\n",
    "\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Comparer avec un Transformer équivalent\n",
    "print(f\"\\n  Note: Un Transformer équivalent aurait ~2x plus de paramètres\")\n",
    "print(f\"  et une complexité O(n²) au lieu de O(n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer des données de trading synthétiques\n",
    "def generate_trading_data(n_samples=2000, seq_length=60, n_features=5):\n",
    "    \"\"\"\n",
    "    Génère des données synthétiques de trading.\n",
    "    \n",
    "    Features:\n",
    "    - returns: Rendements journaliers\n",
    "    - volume_norm: Volume normalisé\n",
    "    - volatility: Volatilité glissante\n",
    "    - rsi_norm: RSI normalisé\n",
    "    - momentum: Momentum sur 10 jours\n",
    "    \n",
    "    Target: Direction du rendement futur (1: up, 0: down)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Tendance aléatoire\n",
    "        trend = np.random.choice([-1, 1]) * np.random.uniform(0.0003, 0.001)\n",
    "        \n",
    "        # Générer les rendements avec tendance + bruit\n",
    "        returns = trend + np.random.randn(seq_length + 1) * 0.015\n",
    "        \n",
    "        # Calculer les features\n",
    "        volume = np.abs(returns[:-1]) * 100 + np.random.uniform(50, 150, seq_length)\n",
    "        volume_norm = (volume - volume.mean()) / (volume.std() + 1e-8)\n",
    "        \n",
    "        volatility = pd.Series(returns[:-1]).rolling(5, min_periods=1).std().values\n",
    "        vol_norm = (volatility - volatility.mean()) / (volatility.std() + 1e-8)\n",
    "        \n",
    "        # RSI simplifié\n",
    "        gains = np.maximum(returns[:-1], 0)\n",
    "        losses = -np.minimum(returns[:-1], 0)\n",
    "        avg_gain = pd.Series(gains).rolling(7, min_periods=1).mean().values\n",
    "        avg_loss = pd.Series(losses).rolling(7, min_periods=1).mean().values\n",
    "        rsi = 100 - (100 / (1 + avg_gain / (avg_loss + 1e-8)))\n",
    "        rsi_norm = (rsi - 50) / 50\n",
    "        \n",
    "        # Momentum\n",
    "        prices = np.exp(np.cumsum(returns))\n",
    "        momentum = (prices[:-1] / np.roll(prices[:-1], 10) - 1)\n",
    "        momentum[:10] = 0\n",
    "        mom_norm = (momentum - momentum.mean()) / (momentum.std() + 1e-8)\n",
    "        \n",
    "        # Stack features\n",
    "        features = np.stack([returns[:-1], volume_norm, vol_norm, rsi_norm, mom_norm], axis=1)\n",
    "        X.append(features)\n",
    "        \n",
    "        # Target: direction du dernier rendement\n",
    "        y.append(1 if returns[-1] > 0 else 0)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Générer les données\n",
    "print(\"Génération des données...\")\n",
    "X, y = generate_trading_data(n_samples=3000, seq_length=60, n_features=5)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Distribution: {y.mean():.1%} up, {1-y.mean():.1%} down\")\n",
    "\n",
    "# Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# Convertir en tensors\n",
    "X_train_t = torch.FloatTensor(X_train)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_val_t = torch.FloatTensor(X_val)\n",
    "y_val_t = torch.FloatTensor(y_val)\n",
    "X_test_t = torch.FloatTensor(X_test)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mamba_model(model, train_loader, val_loader, epochs=20, lr=0.001):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle Mamba.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch).squeeze(-1)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * len(y_batch)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            train_correct += (preds == y_batch).sum().item()\n",
    "            train_total += len(y_batch)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                logits = model(X_batch).squeeze(-1)\n",
    "                loss = criterion(logits, y_batch)\n",
    "                \n",
    "                val_loss += loss.item() * len(y_batch)\n",
    "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "                val_total += len(y_batch)\n",
    "        \n",
    "        # Metrics\n",
    "        train_loss /= train_total\n",
    "        val_loss /= val_total\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                  f\"Train Loss={train_loss:.4f}, Acc={train_acc:.2%} | \"\n",
    "                  f\"Val Loss={val_loss:.4f}, Acc={val_acc:.2%}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Créer et entraîner le modèle Mamba\n",
    "print(\"Entraînement du modèle Mamba...\")\n",
    "print(\"(Architecture légère pour CPU)\\n\")\n",
    "\n",
    "mamba_model = MambaForTrading(\n",
    "    n_features=5,\n",
    "    d_model=32,\n",
    "    d_state=16,\n",
    "    n_layers=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "mamba_model, history = train_mamba_model(\n",
    "    mamba_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    epochs=20,\n",
    "    lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'entraînement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Loss\n",
    "ax1 = axes[0]\n",
    "ax1.plot(history['train_loss'], label='Train', color='steelblue')\n",
    "ax1.plot(history['val_loss'], label='Validation', color='coral')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2 = axes[1]\n",
    "ax2.plot(history['train_acc'], label='Train', color='steelblue')\n",
    "ax2.plot(history['val_acc'], label='Validation', color='coral')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training Accuracy', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Évaluation sur le test set\n",
    "mamba_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = mamba_model(X_test_t.to(device)).squeeze(-1).cpu()\n",
    "    y_pred = (torch.sigmoid(logits) > 0.5).numpy().astype(int)\n",
    "\n",
    "print(\"\\nÉvaluation sur le test set:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, y_pred):.2%}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Down', 'Up']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 5 : SST Hybrid - Mamba + Transformer (15 min)\n",
    "\n",
    "### Pourquoi un hybride ?\n",
    "\n",
    "| Modèle | Forces | Faiblesses |\n",
    "|--------|--------|------------|\n",
    "| **Mamba** | Long-range, O(n), efficace | Moins bon sur patterns locaux |\n",
    "| **Transformer** | Patterns locaux, interprétable | O(n²), memory-intensive |\n",
    "| **SST Hybrid** | ✅ Best of both worlds | Complexité d'implémentation |\n",
    "\n",
    "### Architecture SST (CIKM 2024)\n",
    "\n",
    "Le papier \"SST: State-space models with self-attention for long sequence modeling\" propose :\n",
    "\n",
    "```\n",
    "Input\n",
    "  |\n",
    "  +---> [Mamba Block] ---> Long-range features\n",
    "  |                              |\n",
    "  +---> [Attention Block] ---> Local features\n",
    "                                 |\n",
    "                            [Fusion]\n",
    "                                 |\n",
    "                            Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Local (windowed) attention for short-range patterns.\n",
    "    \n",
    "    Only attends to nearby positions within a window.\n",
    "    Complexity: O(n * window_size) instead of O(n²)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int = 4, window_size: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.window_size = window_size\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        # QKV projection\n",
    "        qkv = self.qkv(x)  # (batch, seq_len, 3 * d_model)\n",
    "        qkv = qkv.reshape(batch, seq_len, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, n_heads, seq_len, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Create local attention mask\n",
    "        mask = torch.ones(seq_len, seq_len, device=x.device)\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - self.window_size // 2)\n",
    "            end = min(seq_len, i + self.window_size // 2 + 1)\n",
    "            mask[i, :start] = 0\n",
    "            mask[i, end:] = 0\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0) == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention\n",
    "        out = torch.matmul(attn, v)  # (batch, n_heads, seq_len, head_dim)\n",
    "        out = out.transpose(1, 2).reshape(batch, seq_len, self.d_model)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class SSTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    SST Block: Combines Mamba (long-range) with Local Attention (short-range).\n",
    "    \n",
    "    Based on: \"SST: State-space models with self-attention for long sequence modeling\"\n",
    "    CIKM 2024\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_state: int = 16,\n",
    "        n_heads: int = 4,\n",
    "        window_size: int = 8,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Mamba for long-range\n",
    "        self.mamba = MambaBlock(d_model=d_model, d_state=d_state, dropout=dropout)\n",
    "        \n",
    "        # Local attention for short-range\n",
    "        self.local_attn = LocalAttention(\n",
    "            d_model=d_model, \n",
    "            n_heads=n_heads, \n",
    "            window_size=window_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Fusion\n",
    "        self.gate = nn.Linear(d_model * 2, d_model)\n",
    "        \n",
    "        # Norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm_out = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Long-range path (Mamba)\n",
    "        long_range = self.mamba(self.norm1(x))\n",
    "        \n",
    "        # Short-range path (Local Attention)\n",
    "        short_range = self.local_attn(self.norm2(x))\n",
    "        \n",
    "        # Fusion with learned gating\n",
    "        concat = torch.cat([long_range, short_range], dim=-1)\n",
    "        gate = torch.sigmoid(self.gate(concat))\n",
    "        fused = gate * long_range + (1 - gate) * short_range\n",
    "        \n",
    "        # Residual\n",
    "        x = x + fused\n",
    "        \n",
    "        # FFN\n",
    "        x = x + self.ffn(self.norm_out(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SSTForTrading(nn.Module):\n",
    "    \"\"\"\n",
    "    SST model for trading: Mamba + Local Attention hybrid.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        d_model: int = 32,\n",
    "        d_state: int = 16,\n",
    "        n_layers: int = 2,\n",
    "        n_heads: int = 4,\n",
    "        window_size: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        n_classes: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        # SST blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SSTBlock(\n",
    "                d_model=d_model,\n",
    "                d_state=d_state,\n",
    "                n_heads=n_heads,\n",
    "                window_size=window_size,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = x.mean(dim=1)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "# Test SST\n",
    "print(\"Test de SSTForTrading:\")\n",
    "sst_model = SSTForTrading(\n",
    "    n_features=5,\n",
    "    d_model=32,\n",
    "    n_layers=2,\n",
    "    n_heads=4,\n",
    "    window_size=8\n",
    ")\n",
    "\n",
    "test_input = torch.randn(4, 60, 5)\n",
    "output = sst_model(test_input)\n",
    "\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in sst_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 6 : Comparaison LSTM vs Transformer vs Mamba (10 min)\n",
    "\n",
    "### Benchmark sur les mêmes données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseline(nn.Module):\n",
    "    \"\"\"LSTM baseline pour comparaison.\"\"\"\n",
    "    def __init__(self, n_features, hidden_size=32, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.head = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.head(h_n[-1])\n",
    "\n",
    "\n",
    "class TransformerBaseline(nn.Module):\n",
    "    \"\"\"Transformer baseline pour comparaison.\"\"\"\n",
    "    def __init__(self, n_features, d_model=32, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(n_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "def benchmark_models(models_dict, train_loader, val_loader, X_test, y_test, epochs=15):\n",
    "    \"\"\"\n",
    "    Compare plusieurs modèles sur les mêmes données.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {name}...\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(X_batch).squeeze(-1)\n",
    "                loss = criterion(logits, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_d = torch.FloatTensor(X_test).to(device)\n",
    "            logits = model(X_test_d).squeeze(-1).cpu()\n",
    "            y_pred = (torch.sigmoid(logits) > 0.5).numpy().astype(int)\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Parameters': sum(p.numel() for p in model.parameters()),\n",
    "            'Train Time (s)': train_time,\n",
    "            'Test Accuracy': acc\n",
    "        })\n",
    "        \n",
    "        print(f\"  Time: {train_time:.1f}s, Accuracy: {acc:.2%}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Créer les modèles\n",
    "models = {\n",
    "    'LSTM': LSTMBaseline(n_features=5, hidden_size=32, n_layers=2),\n",
    "    'Transformer': TransformerBaseline(n_features=5, d_model=32, n_heads=4, n_layers=2),\n",
    "    'Mamba': MambaForTrading(n_features=5, d_model=32, d_state=16, n_layers=2),\n",
    "    'SST Hybrid': SSTForTrading(n_features=5, d_model=32, n_layers=2, n_heads=4)\n",
    "}\n",
    "\n",
    "# Benchmark\n",
    "print(\"Benchmark des architectures...\")\n",
    "results_df = benchmark_models(models, train_loader, val_loader, X_test, y_test, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les résultats\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RÉSULTATS DU BENCHMARK\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Parameters\n",
    "ax1 = axes[0]\n",
    "colors = ['steelblue', 'coral', 'seagreen', 'orchid']\n",
    "ax1.bar(results_df['Model'], results_df['Parameters'], color=colors)\n",
    "ax1.set_ylabel('Nombre de paramètres')\n",
    "ax1.set_title('Taille du modèle', fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=15)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Train time\n",
    "ax2 = axes[1]\n",
    "ax2.bar(results_df['Model'], results_df['Train Time (s)'], color=colors)\n",
    "ax2.set_ylabel('Temps (secondes)')\n",
    "ax2.set_title('Temps d\\'entraînement', fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=15)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Accuracy\n",
    "ax3 = axes[2]\n",
    "ax3.bar(results_df['Model'], results_df['Test Accuracy'] * 100, color=colors)\n",
    "ax3.set_ylabel('Accuracy (%)')\n",
    "ax3.set_title('Performance (Test Set)', fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=15)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "ax3.set_ylim(45, 60)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnalyse:\")\n",
    "print(\"  - Mamba offre un bon compromis paramètres/performance\")\n",
    "print(\"  - SST Hybrid capture à la fois long-range et short-range patterns\")\n",
    "print(\"  - Sur des séquences plus longues (>256), Mamba surpasserait le Transformer\")\n",
    "print(\"  - LSTM reste compétitif sur séquences courtes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 7 : Intégration QuantConnect (15 min)\n",
    "\n",
    "### Stratégie avec Mamba\n",
    "\n",
    "Architecture de déploiement :\n",
    "\n",
    "```\n",
    "LOCAL (GPU/CPU puissant)\n",
    "├── Entraîner MambaForTrading\n",
    "├── Sauvegarder state_dict (<9MB)\n",
    "└── Upload vers ObjectStore\n",
    "\n",
    "QUANTCONNECT CLOUD (CPU)\n",
    "├── Charger depuis ObjectStore\n",
    "├── Recréer architecture identique\n",
    "├── Inférence quotidienne (~50ms)\n",
    "└── Générer signaux de trading\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle pour QuantConnect\n",
    "import io\n",
    "\n",
    "# Sauvegarder state_dict\n",
    "buffer = io.BytesIO()\n",
    "torch.save(mamba_model.state_dict(), buffer)\n",
    "model_size = buffer.tell()\n",
    "\n",
    "print(f\"Taille du modèle: {model_size / 1024:.1f} KB\")\n",
    "print(f\"Compatible ObjectStore: {'Oui' if model_size < 9 * 1024 * 1024 else 'Non (>9MB)'}\")\n",
    "\n",
    "# Sauvegarder localement pour test\n",
    "torch.save(mamba_model.state_dict(), 'mamba_trading_model.pt')\n",
    "print(\"\\nModèle sauvegardé: mamba_trading_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code QuantConnect pour Mamba Trading Strategy\n",
    "qc_mamba_strategy = '''\n",
    "from AlgorithmImports import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "import io\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAMBA MODEL DEFINITION (simplified for QC)\n",
    "# ============================================\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    \"\"\"Simplified Mamba Block for QuantConnect.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_state: int = 16, d_conv: int = 4, expand: int = 2):\n",
    "        super().__init__()\n",
    "        self.d_inner = d_model * expand\n",
    "        self.d_state = d_state\n",
    "        \n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n",
    "        self.conv1d = nn.Conv1d(self.d_inner, self.d_inner, d_conv, \n",
    "                               padding=d_conv-1, groups=self.d_inner)\n",
    "        self.x_proj = nn.Linear(self.d_inner, d_state * 2 + 1, bias=False)\n",
    "        self.dt_proj = nn.Linear(1, self.d_inner, bias=True)\n",
    "        self.A_log = nn.Parameter(torch.log(torch.arange(1, d_state+1).repeat(self.d_inner, 1)))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch, seq_len, _ = x.shape\n",
    "        xz = self.in_proj(x)\n",
    "        x_main, z = xz.chunk(2, dim=-1)\n",
    "        \n",
    "        x_conv = self.conv1d(x_main.transpose(1, 2))[:, :, :seq_len].transpose(1, 2)\n",
    "        x_main = F.silu(x_conv)\n",
    "        \n",
    "        x_ssm = self.x_proj(x_main)\n",
    "        delta_raw = x_ssm[:, :, :1]\n",
    "        B = x_ssm[:, :, 1:1+self.d_state]\n",
    "        C = x_ssm[:, :, 1+self.d_state:]\n",
    "        delta = F.softplus(self.dt_proj(delta_raw))\n",
    "        A = -torch.exp(self.A_log)\n",
    "        \n",
    "        h = torch.zeros(batch, self.d_inner, self.d_state, device=x.device)\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            x_t = x_main[:, t, :]\n",
    "            delta_t = delta[:, t, :]\n",
    "            B_t = B[:, t, :].unsqueeze(1).expand(-1, self.d_inner, -1)\n",
    "            C_t = C[:, t, :].unsqueeze(1).expand(-1, self.d_inner, -1)\n",
    "            \n",
    "            A_bar = torch.exp(delta_t.unsqueeze(-1) * A)\n",
    "            B_bar = delta_t.unsqueeze(-1) * B_t\n",
    "            h = A_bar * h + B_bar * x_t.unsqueeze(-1)\n",
    "            y_t = torch.sum(C_t * h, dim=-1) + self.D * x_t\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        y = torch.stack(outputs, dim=1)\n",
    "        y = y * F.silu(z)\n",
    "        return self.out_proj(y)\n",
    "\n",
    "\n",
    "class MambaForTrading(nn.Module):\n",
    "    \"\"\"Mamba model for trading signals.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_features=5, d_model=32, d_state=16, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(n_features, d_model)\n",
    "        self.mamba_blocks = nn.ModuleList([\n",
    "            MambaBlock(d_model, d_state) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layers)])\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        for mamba, norm in zip(self.mamba_blocks, self.norms):\n",
    "            x = x + mamba(norm(x))\n",
    "        x = x.mean(dim=1)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# QUANTCONNECT ALPHA MODEL\n",
    "# ============================================\n",
    "\n",
    "class MambaAlphaModel(AlphaModel):\n",
    "    \"\"\"\n",
    "    Alpha Model using Mamba SSM for direction prediction.\n",
    "    \n",
    "    Features:\n",
    "    - O(n) complexity for long sequences\n",
    "    - Selective state space for filtering noise\n",
    "    - CPU-optimized architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, algorithm, model_key=\"models/mamba_trading\"):\n",
    "        self.algorithm = algorithm\n",
    "        self.model_key = model_key\n",
    "        self.lookback = 60\n",
    "        self.n_features = 5\n",
    "        self.model = None\n",
    "        self.symbols_data = {}\n",
    "        \n",
    "        # Load model\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load Mamba model from ObjectStore.\"\"\"\n",
    "        try:\n",
    "            if self.algorithm.ObjectStore.ContainsKey(self.model_key):\n",
    "                model_bytes = self.algorithm.ObjectStore.ReadBytes(self.model_key)\n",
    "                buffer = io.BytesIO(model_bytes)\n",
    "                \n",
    "                self.model = MambaForTrading(\n",
    "                    n_features=self.n_features,\n",
    "                    d_model=32,\n",
    "                    d_state=16,\n",
    "                    n_layers=2\n",
    "                )\n",
    "                self.model.load_state_dict(torch.load(buffer, map_location=\"cpu\"))\n",
    "                self.model.eval()\n",
    "                \n",
    "                self.algorithm.Debug(\"Mamba model loaded from ObjectStore\")\n",
    "            else:\n",
    "                self.algorithm.Debug(f\"Model not found: {self.model_key}\")\n",
    "        except Exception as e:\n",
    "            self.algorithm.Debug(f\"Error loading model: {e}\")\n",
    "    \n",
    "    def _prepare_features(self, symbol) -> np.ndarray:\n",
    "        \"\"\"Prepare features for a symbol.\"\"\"\n",
    "        history = self.algorithm.History(symbol, self.lookback + 5, Resolution.Daily)\n",
    "        \n",
    "        if history.empty or len(history) < self.lookback:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            close = history[\"close\"].values\n",
    "            volume = history[\"volume\"].values\n",
    "            \n",
    "            # Returns\n",
    "            returns = np.diff(close) / close[:-1]\n",
    "            \n",
    "            # Volume normalized\n",
    "            vol_norm = (volume - volume.mean()) / (volume.std() + 1e-8)\n",
    "            \n",
    "            # Volatility\n",
    "            volatility = np.array([returns[max(0,i-4):i+1].std() \n",
    "                                  for i in range(len(returns))])\n",
    "            vol_std_norm = (volatility - volatility.mean()) / (volatility.std() + 1e-8)\n",
    "            \n",
    "            # RSI\n",
    "            gains = np.maximum(returns, 0)\n",
    "            losses = -np.minimum(returns, 0)\n",
    "            avg_gain = np.convolve(gains, np.ones(7)/7, mode=\"valid\")\n",
    "            avg_loss = np.convolve(losses, np.ones(7)/7, mode=\"valid\")\n",
    "            rsi = 100 - (100 / (1 + avg_gain / (avg_loss + 1e-8)))\n",
    "            rsi_norm = np.concatenate([[0]*6, (rsi - 50) / 50])\n",
    "            \n",
    "            # Momentum\n",
    "            prices = np.exp(np.cumsum(np.concatenate([[0], returns])))\n",
    "            momentum = prices[10:] / prices[:-10] - 1\n",
    "            mom_norm = np.concatenate([[0]*10, (momentum - momentum.mean()) / (momentum.std() + 1e-8)])\n",
    "            \n",
    "            # Stack (last lookback days)\n",
    "            features = np.stack([\n",
    "                np.concatenate([[0], returns])[-self.lookback:],\n",
    "                vol_norm[-self.lookback:],\n",
    "                np.concatenate([[0], vol_std_norm])[-self.lookback:],\n",
    "                rsi_norm[-self.lookback:],\n",
    "                mom_norm[-self.lookback:]\n",
    "            ], axis=1)\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.algorithm.Debug(f\"Feature error for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def Update(self, algorithm, data):\n",
    "        \"\"\"Generate insights.\"\"\"\n",
    "        insights = []\n",
    "        \n",
    "        if self.model is None:\n",
    "            return insights\n",
    "        \n",
    "        for symbol in algorithm.ActiveSecurities.Keys:\n",
    "            if not data.ContainsKey(symbol):\n",
    "                continue\n",
    "            \n",
    "            features = self._prepare_features(symbol)\n",
    "            \n",
    "            if features is None:\n",
    "                continue\n",
    "            \n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                x = torch.FloatTensor(features).unsqueeze(0)\n",
    "                logit = self.model(x).item()\n",
    "                prob = 1 / (1 + np.exp(-logit))\n",
    "            \n",
    "            # Generate insight if confident\n",
    "            if prob > 0.55:\n",
    "                insights.append(Insight.Price(\n",
    "                    symbol, timedelta(days=5), InsightDirection.Up,\n",
    "                    magnitude=prob - 0.5, confidence=prob\n",
    "                ))\n",
    "            elif prob < 0.45:\n",
    "                insights.append(Insight.Price(\n",
    "                    symbol, timedelta(days=5), InsightDirection.Down,\n",
    "                    magnitude=0.5 - prob, confidence=1 - prob\n",
    "                ))\n",
    "        \n",
    "        return insights\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN ALGORITHM\n",
    "# ============================================\n",
    "\n",
    "class MambaTradingAlgorithm(QCAlgorithm):\n",
    "    \"\"\"\n",
    "    Trading algorithm using Mamba State Space Model.\n",
    "    \n",
    "    - Uses Mamba SSM with O(n) complexity\n",
    "    - 60-day lookback with 5 features\n",
    "    - Equal-weight top positions by confidence\n",
    "    \"\"\"\n",
    "    \n",
    "    def Initialize(self):\n",
    "        self.SetStartDate(2020, 1, 1)\n",
    "        self.SetEndDate(2023, 12, 31)\n",
    "        self.SetCash(100000)\n",
    "        \n",
    "        # Universe\n",
    "        self.UniverseSettings.Resolution = Resolution.Daily\n",
    "        self.AddUniverse(self.CoarseFilter)\n",
    "        \n",
    "        # Framework\n",
    "        self.SetAlpha(MambaAlphaModel(self))\n",
    "        self.SetPortfolioConstruction(EqualWeightingPortfolioConstructionModel())\n",
    "        self.SetExecution(ImmediateExecutionModel())\n",
    "        self.SetRiskManagement(MaximumDrawdownPercentPerSecurity(0.05))\n",
    "    \n",
    "    def CoarseFilter(self, coarse):\n",
    "        filtered = [x for x in coarse\n",
    "                   if x.HasFundamentalData\n",
    "                   and x.Price > 10\n",
    "                   and x.DollarVolume > 10000000]\n",
    "        sorted_by_volume = sorted(filtered, key=lambda x: x.DollarVolume, reverse=True)\n",
    "        return [x.Symbol for x in sorted_by_volume[:50]]\n",
    "    \n",
    "    def OnEndOfAlgorithm(self):\n",
    "        self.Debug(\"=\" * 60)\n",
    "        self.Debug(\"MAMBA SSM TRADING - FINAL SUMMARY\")\n",
    "        self.Debug(\"=\" * 60)\n",
    "        self.Debug(f\"Final Value: ${self.Portfolio.TotalPortfolioValue:,.2f}\")\n",
    "        total_return = (self.Portfolio.TotalPortfolioValue / 100000 - 1) * 100\n",
    "        self.Debug(f\"Total Return: {total_return:.2f}%\")\n",
    "'''\n",
    "\n",
    "print(\"MambaTradingAlgorithm défini pour QuantConnect\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ARCHITECTURE MAMBA\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Modèle:\n",
    "  - d_model: 32 (CPU-optimized)\n",
    "  - d_state: 16 (state dimension)\n",
    "  - n_layers: 2\n",
    "  - Complexité: O(n) vs O(n²) pour Transformers\n",
    "\n",
    "Features (5):\n",
    "  1. returns: Rendements journaliers\n",
    "  2. volume_norm: Volume normalisé\n",
    "  3. volatility: Volatilité glissante\n",
    "  4. rsi_norm: RSI normalisé  \n",
    "  5. momentum: Momentum 10 jours\n",
    "\n",
    "Avantages vs Transformer:\n",
    "  - Scalabilité linéaire pour longues séquences\n",
    "  - Sélectivité (ignore le bruit)\n",
    "  - Moins de mémoire GPU/CPU\n",
    "  - Comparable en performance sur séquences courtes\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion et Prochaines Étapes\n",
    "\n",
    "### Récapitulatif\n",
    "\n",
    "| Concept | Description | Avantage Trading |\n",
    "|---------|-------------|------------------|\n",
    "| **State Space Models** | Modèles continus discrétisés | Fondations mathématiques solides |\n",
    "| **S4** | SSM structuré avec HiPPO | Premier SSM compétitif |\n",
    "| **Mamba** | SSM sélectif (input-dependent) | Filtre le bruit, adapte la mémoire |\n",
    "| **SST Hybrid** | Mamba + Attention locale | Best of both worlds |\n",
    "| **Complexité O(n)** | Scalabilité linéaire | 5+ ans de données possibles |\n",
    "\n",
    "### Quand utiliser Mamba vs Transformer ?\n",
    "\n",
    "| Scénario | Recommandation |\n",
    "|----------|----------------|\n",
    "| Séquences courtes (<100) | Transformer (plus mature) |\n",
    "| Séquences longues (>256) | **Mamba** (scalabilité) |\n",
    "| Contraintes mémoire | **Mamba** (O(n) mémoire) |\n",
    "| Besoin d'interprétabilité | Transformer (attention weights) |\n",
    "| Données bruitées | **Mamba** (sélectivité) |\n",
    "| Multi-scale patterns | SST Hybrid |\n",
    "\n",
    "### Ressources\n",
    "\n",
    "- [Mamba Paper](https://arxiv.org/abs/2312.00752) - Gu & Dao\n",
    "- [state-spaces/mamba](https://github.com/state-spaces/mamba) - Implémentation officielle\n",
    "- [S4 Paper](https://arxiv.org/abs/2111.00396) - Gu et al., ICLR 2022\n",
    "- [Awesome State Space Models](https://github.com/radarFudan/Awesome-state-space-models)\n",
    "- [Mamba Paper List](https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List)\n",
    "\n",
    "### Prochaines étapes\n",
    "\n",
    "- **QC-Py-24** : Modèles génératifs (VAE-Transformer) pour détection d'anomalies\n",
    "- **QC-Py-25** : Reinforcement Learning pour trading adaptatif\n",
    "- **QC-Py-26** : LLMs pour signaux de trading\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook complété. Vous maîtrisez maintenant les State Space Models (Mamba) pour le trading algorithmique à complexité O(n).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
