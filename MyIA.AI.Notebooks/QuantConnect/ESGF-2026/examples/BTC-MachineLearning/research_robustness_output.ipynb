{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235cc0c4",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [1]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6554f1",
   "metadata": {
    "papermill": {
     "duration": 0.003985,
     "end_time": "2026-02-17T09:32:49.652416",
     "exception": false,
     "start_time": "2026-02-17T09:32:49.648431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Research: ML Strategy Robustness Analysis\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Valider la robustesse de la stratégie Machine Learning pour BTC en explorant:\n",
    "\n",
    "1. **Impact d'une période d'entraînement étendue**: Entraîner sur 2017-2022 (5 ans) vs 2021-2022 (2 ans)\n",
    "2. **Stabilité des features par régime**: Quelles features sont importantes en marché haussier vs baissier?\n",
    "3. **Fréquence optimale de retraining**: Comparer 30, 60, 90 jours\n",
    "4. **Walk-forward validation**: Le test ultime pour éviter l'overfitting\n",
    "5. **Comparaison au buy-and-hold**: Le ML bat-il vraiment le HODL?\n",
    "\n",
    "## Défis du ML en Trading\n",
    "\n",
    "Le Machine Learning appliqué au trading crypto pose des défis majeurs:\n",
    "\n",
    "### 1. Overfitting (surajustement)\n",
    "- Les modèles peuvent \"mémoriser\" les patterns du passé sans vraie généralisation\n",
    "- Symptôme: excellente performance sur données d'entraînement, mauvaise sur données test\n",
    "- Solution: Walk-forward validation, régularisation, limitation de la complexité du modèle\n",
    "\n",
    "### 2. Data Leakage (fuite de données)\n",
    "- Utiliser involontairement des informations futures pour entraîner le modèle\n",
    "- Exemples: features calculées sur toute la série temporelle, normalisation globale\n",
    "- Solution: Séparation stricte train/test, calcul rolling des features\n",
    "\n",
    "### 3. Regime Changes (changements de régime)\n",
    "- Les crypto alternent entre marchés haussiers/baissiers avec des caractéristiques différentes\n",
    "- Un modèle entraîné sur un bull market peut échouer en bear market\n",
    "- Solution: Retraining fréquent, features adaptatives, walk-forward\n",
    "\n",
    "### 4. Stationnarité\n",
    "- Les distributions de prix changent dans le temps (non-stationnaires)\n",
    "- Les relations entre features et labels ne sont pas constantes\n",
    "- Solution: Features différenciées (returns), fenêtres glissantes\n",
    "\n",
    "## Pourquoi le Walk-Forward est le Gold Standard\n",
    "\n",
    "La **walk-forward validation** est la méthode la plus robuste pour tester une stratégie ML:\n",
    "\n",
    "```\n",
    "Train [========] Test [==] Retrain [========] Test [==] Retrain [...]\n",
    "  2017-2018         2019      2018-2019         2020      2019-2020\n",
    "```\n",
    "\n",
    "**Avantages**:\n",
    "- Simule exactement le processus de production (entraîner sur le passé, prédire le futur)\n",
    "- Teste la capacité du modèle à s'adapter aux changements de régime\n",
    "- Évalue la stabilité des performances dans le temps\n",
    "- Évite le biais de \"cherry-picking\" une période de test favorable\n",
    "\n",
    "**Paramètres clés**:\n",
    "- **Train period**: Combien de données historiques pour entraîner (e.g., 730 jours)\n",
    "- **Test period**: Durée de prédiction out-of-sample (e.g., 90 jours)\n",
    "- **Retrain interval**: Fréquence de mise à jour du modèle (e.g., 30, 60, 90 jours)\n",
    "\n",
    "## Hypothèses à tester\n",
    "\n",
    "1. **H1**: Entraîner sur 2017-2022 (5 ans, incluant bull + bears) améliore le Sharpe vs 2021-2022 (2 ans)\n",
    "2. **H2**: Les features importantes changent selon le régime (RSI en bear, momentum en bull)\n",
    "3. **H3**: Un retraining tous les 30 jours offre le meilleur trade-off adaptation/stabilité\n",
    "4. **H4**: La stratégie ML bat le buy-and-hold après coûts de transaction\n",
    "5. **H5**: Le walk-forward montre des performances cohérentes (pas de lucky period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b20458",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56d5ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T09:32:49.659943Z",
     "iopub.status.busy": "2026-02-17T09:32:49.659635Z",
     "iopub.status.idle": "2026-02-17T09:32:49.999956Z",
     "shell.execute_reply": "2026-02-17T09:32:49.999236Z"
    },
    "papermill": {
     "duration": 0.346102,
     "end_time": "2026-02-17T09:32:50.001321",
     "exception": true,
     "start_time": "2026-02-17T09:32:49.655219",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'AlgorithmImports'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 2: Setup et chargement des données\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAlgorithmImports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'AlgorithmImports'"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup et chargement des données\n",
    "from AlgorithmImports import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "qb = QuantBook()\n",
    "print(\"QuantBook initialized\")\n",
    "\n",
    "# Chargement BTC depuis 2017 (maximiser la fenêtre)\n",
    "btc = qb.AddCrypto(\"BTCUSDT\", Resolution.Daily, Market.Binance).Symbol\n",
    "start_date = datetime(2017, 1, 1)\n",
    "end_date = datetime.now()\n",
    "\n",
    "print(f\"Loading BTC data from {start_date} to {end_date}...\")\n",
    "history = qb.History(btc, start_date, end_date, Resolution.Daily)\n",
    "\n",
    "# Conversion en DataFrame simple\n",
    "if isinstance(history.index, pd.MultiIndex):\n",
    "    df = history.loc[btc].copy()\n",
    "else:\n",
    "    df = history.copy()\n",
    "\n",
    "print(f\"\\nBTC data loaded:\")\n",
    "print(f\"  - Total bars: {len(df)}\")\n",
    "print(f\"  - Date range: {df.index[0]} to {df.index[-1]}\")\n",
    "print(f\"  - Price range: ${df['close'].min():.2f} to ${df['close'].max():.2f}\")\n",
    "print(f\"  - Total return: {(df['close'].iloc[-1] / df['close'].iloc[0] - 1) * 100:.1f}%\")\n",
    "\n",
    "# Affichage du prix BTC sur toute la période\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df.index, df['close'], linewidth=1.5, label='BTC Price')\n",
    "plt.title('Bitcoin Price History (2017-Present)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USDT)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Marqueurs des régimes\n",
    "plt.axvspan(datetime(2017, 1, 1), datetime(2018, 12, 31), alpha=0.1, color='red', label='2017-2018 Bull+Bear')\n",
    "plt.axvspan(datetime(2019, 1, 1), datetime(2020, 12, 31), alpha=0.1, color='orange', label='2019-2020 Recovery+Bull')\n",
    "plt.axvspan(datetime(2021, 1, 1), datetime(2022, 12, 31), alpha=0.1, color='blue', label='2021-2022 Bull+Bear')\n",
    "plt.axvspan(datetime(2023, 1, 1), datetime.now(), alpha=0.1, color='green', label='2023+ Recovery')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nData ready for feature engineering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e36d1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3: Feature Engineering - Calcul des 9 features ML\n",
    "\n",
    "def compute_features(df_input):\n",
    "    \"\"\"\n",
    "    Calcule les 9 features utilisées par la stratégie:\n",
    "    1. SMA_ratio (close / SMA(20))\n",
    "    2. RSI_14\n",
    "    3. DailyReturn (pct_change)\n",
    "    4-7. EMA ratios (10, 20, 50, 200) / close\n",
    "    8. ADX (approximation via True Range)\n",
    "    9. ATR / close (normalized)\n",
    "    10. Label: next-day direction (1=up, 0=down)\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. SMA ratio\n",
    "    features['sma_ratio'] = df['close'] / df['close'].rolling(20).mean()\n",
    "    \n",
    "    # 2. RSI (14-period Wilder's RSI)\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(14).mean()\n",
    "    avg_loss = loss.rolling(14).mean()\n",
    "    rs = avg_gain / avg_loss.replace(0, 1e-10)\n",
    "    features['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # 3. Daily return\n",
    "    features['daily_return'] = df['close'].pct_change()\n",
    "    \n",
    "    # 4-7. EMA ratios (normalized by current price)\n",
    "    for period in [10, 20, 50, 200]:\n",
    "        ema = df['close'].ewm(span=period, adjust=False).mean()\n",
    "        features[f'ema_{period}'] = ema / df['close']\n",
    "    \n",
    "    # 8. ADX approximation (simplified True Range)\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = (df['high'] - df['close'].shift()).abs()\n",
    "    low_close = (df['low'] - df['close'].shift()).abs()\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    features['adx'] = true_range.rolling(14).mean()\n",
    "    \n",
    "    # 9. ATR normalized\n",
    "    features['atr'] = true_range.rolling(14).mean() / df['close']\n",
    "    \n",
    "    # LABEL: Next-day direction (1 = price up, 0 = price down)\n",
    "    # CRITICAL: Use shift(-1) pour éviter data leakage\n",
    "    features['label'] = (df['close'].shift(-1) > df['close']).astype(int)\n",
    "    \n",
    "    # Drop NaN (premiers jours + dernier jour sans label)\n",
    "    features_clean = features.dropna()\n",
    "    \n",
    "    return features_clean\n",
    "\n",
    "# Calcul des features sur toute la période\n",
    "print(\"Computing features...\")\n",
    "features_df = compute_features(df)\n",
    "\n",
    "print(f\"\\nFeatures computed:\")\n",
    "print(f\"  - Total samples: {len(features_df)}\")\n",
    "print(f\"  - Features: {[c for c in features_df.columns if c != 'label']}\")\n",
    "print(f\"  - Label distribution: {features_df['label'].value_counts().to_dict()}\")\n",
    "print(f\"  - Date range: {features_df.index[0]} to {features_df.index[-1]}\")\n",
    "\n",
    "# Vérification data leakage: le label ne doit PAS être prédictible par les features du même jour\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "feature_cols = [c for c in features_df.columns if c != 'label']\n",
    "X_leakage_test = features_df[feature_cols].iloc[:1000].values\n",
    "y_leakage_test = features_df['label'].iloc[:1000].values\n",
    "rf_leakage = RandomForestClassifier(n_estimators=50, max_depth=3, random_state=42)\n",
    "rf_leakage.fit(X_leakage_test, y_leakage_test)\n",
    "leakage_score = rf_leakage.score(X_leakage_test, y_leakage_test)\n",
    "\n",
    "print(f\"\\nData leakage check (training accuracy on same period):\")\n",
    "print(f\"  - Score: {leakage_score:.3f}\")\n",
    "if leakage_score > 0.65:\n",
    "    print(\"  - ⚠️ WARNING: Possible data leakage (score too high)\")\n",
    "else:\n",
    "    print(\"  - ✓ OK: No obvious data leakage (score reasonable for noisy financial data)\")\n",
    "\n",
    "print(\"\\nFeatures ready for ML training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce79c39",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4: Hypothèse 1 - Impact de la période d'entraînement étendue\n",
    "# Comparer: 2021-2022 (2 ans) vs 2017-2022 (5 ans)\n",
    "# Test sur: 2023-2025\n",
    "\n",
    "def train_and_evaluate(features, train_start, train_end, test_start, test_end, label=\"Experiment\"):\n",
    "    \"\"\"\n",
    "    Entraîne un RandomForest sur [train_start, train_end] et teste sur [test_start, test_end].\n",
    "    Retourne les métriques de performance.\n",
    "    \"\"\"\n",
    "    # Séparation train/test\n",
    "    train_df = features.loc[train_start:train_end]\n",
    "    test_df = features.loc[test_start:test_end]\n",
    "    \n",
    "    if len(train_df) < 100 or len(test_df) < 30:\n",
    "        print(f\"Insufficient data for {label}\")\n",
    "        return None\n",
    "    \n",
    "    feature_cols = [c for c in features.columns if c != 'label']\n",
    "    X_train = train_df[feature_cols].values\n",
    "    y_train = train_df['label'].values\n",
    "    X_test = test_df[feature_cols].values\n",
    "    y_test = test_df['label'].values\n",
    "    \n",
    "    # Entraînement\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]  # Proba de hausse\n",
    "    \n",
    "    # Métriques classification\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Simulation trading simple (long si proba > 0.6)\n",
    "    signals = (y_proba > 0.6).astype(float) * y_proba  # Position sizing probabiliste\n",
    "    daily_returns = test_df['daily_return'].values\n",
    "    \n",
    "    # Décalage signal pour éviter lookahead\n",
    "    strategy_returns = daily_returns[1:] * signals[:-1]\n",
    "    \n",
    "    # Métriques trading\n",
    "    mean_ret = strategy_returns.mean() * 365  # Annualisé\n",
    "    std_ret = strategy_returns.std() * np.sqrt(365)\n",
    "    sharpe = mean_ret / std_ret if std_ret > 0 else 0\n",
    "    \n",
    "    # Cumulative returns\n",
    "    cumulative = (1 + strategy_returns).cumprod()\n",
    "    max_dd = (cumulative / cumulative.cummax() - 1).min()\n",
    "    \n",
    "    # Buy-and-hold benchmark\n",
    "    bnh_returns = daily_returns[1:]\n",
    "    bnh_cumulative = (1 + bnh_returns).cumprod()\n",
    "    bnh_total = bnh_cumulative[-1] - 1\n",
    "    strategy_total = cumulative[-1] - 1\n",
    "    \n",
    "    return {\n",
    "        'label': label,\n",
    "        'train_samples': len(train_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'accuracy': accuracy,\n",
    "        'sharpe': sharpe,\n",
    "        'annual_return': mean_ret,\n",
    "        'volatility': std_ret,\n",
    "        'max_drawdown': max_dd,\n",
    "        'total_return': strategy_total,\n",
    "        'bnh_total_return': bnh_total,\n",
    "        'excess_return': strategy_total - bnh_total,\n",
    "        'model': model,\n",
    "        'feature_importance': dict(zip(feature_cols, model.feature_importances_))\n",
    "    }\n",
    "\n",
    "# Expérience 1: Training 2021-2022 (période actuelle de la stratégie)\n",
    "print(\"Experiment 1: Training on 2021-2022 (2 years)\")\n",
    "result_2y = train_and_evaluate(\n",
    "    features_df,\n",
    "    train_start=\"2021-01-01\",\n",
    "    train_end=\"2022-12-31\",\n",
    "    test_start=\"2023-01-01\",\n",
    "    test_end=features_df.index[-1],\n",
    "    label=\"2Y Training (2021-2022)\"\n",
    ")\n",
    "\n",
    "# Expérience 2: Training 2017-2022 (période étendue)\n",
    "print(\"\\nExperiment 2: Training on 2017-2022 (5 years)\")\n",
    "result_5y = train_and_evaluate(\n",
    "    features_df,\n",
    "    train_start=\"2017-01-01\",\n",
    "    train_end=\"2022-12-31\",\n",
    "    test_start=\"2023-01-01\",\n",
    "    test_end=features_df.index[-1],\n",
    "    label=\"5Y Training (2017-2022)\"\n",
    ")\n",
    "\n",
    "# Comparaison\n",
    "comparison = pd.DataFrame([result_2y, result_5y]).set_index('label')\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 1: Extended Training Period Impact\")\n",
    "print(\"=\"*80)\n",
    "print(comparison[['train_samples', 'accuracy', 'sharpe', 'annual_return', 'max_drawdown', 'total_return', 'excess_return']])\n",
    "\n",
    "# Verdict\n",
    "print(\"\\nVERDICT:\")\n",
    "if result_5y['sharpe'] > result_2y['sharpe']:\n",
    "    improvement = (result_5y['sharpe'] - result_2y['sharpe']) / result_2y['sharpe'] * 100\n",
    "    print(f\"✓ H1 CONFIRMED: 5Y training improves Sharpe by {improvement:.1f}%\")\n",
    "    print(f\"  Sharpe: {result_2y['sharpe']:.3f} → {result_5y['sharpe']:.3f}\")\n",
    "else:\n",
    "    print(f\"✗ H1 REJECTED: 2Y training performs better\")\n",
    "    print(f\"  Sharpe: {result_2y['sharpe']:.3f} (2Y) vs {result_5y['sharpe']:.3f} (5Y)\")\n",
    "\n",
    "print(f\"\\nExcess return vs Buy-and-Hold:\")\n",
    "print(f\"  2Y: {result_2y['excess_return']*100:+.1f}%\")\n",
    "print(f\"  5Y: {result_5y['excess_return']*100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac938745",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5: Hypothèse 2 - Feature Importance par Régime\n",
    "# Analyser quelles features sont importantes en bull vs bear market\n",
    "\n",
    "def train_by_regime(features, start, end, regime_name):\n",
    "    \"\"\"\n",
    "    Entraîne sur une période spécifique et retourne les feature importances.\n",
    "    \"\"\"\n",
    "    regime_df = features.loc[start:end]\n",
    "    \n",
    "    if len(regime_df) < 100:\n",
    "        return None\n",
    "    \n",
    "    feature_cols = [c for c in features.columns if c != 'label']\n",
    "    X = regime_df[feature_cols].values\n",
    "    y = regime_df['label'].values\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    importance_dict = dict(zip(feature_cols, model.feature_importances_))\n",
    "    \n",
    "    # Calcul du return total sur la période pour caractériser le régime\n",
    "    total_return = (df.loc[start:end, 'close'].iloc[-1] / df.loc[start:end, 'close'].iloc[0] - 1) * 100\n",
    "    \n",
    "    return {\n",
    "        'regime': regime_name,\n",
    "        'period': f\"{start} to {end}\",\n",
    "        'samples': len(regime_df),\n",
    "        'total_return': total_return,\n",
    "        'importance': importance_dict\n",
    "    }\n",
    "\n",
    "# Définition des régimes\n",
    "regimes = [\n",
    "    (\"2017-01-01\", \"2017-12-31\", \"2017 Bull\"),\n",
    "    (\"2018-01-01\", \"2018-12-31\", \"2018 Bear\"),\n",
    "    (\"2019-01-01\", \"2020-03-31\", \"2019-Q1'20 Recovery\"),\n",
    "    (\"2020-04-01\", \"2021-11-30\", \"2020-2021 Bull\"),\n",
    "    (\"2021-12-01\", \"2022-12-31\", \"2022 Bear\"),\n",
    "    (\"2023-01-01\", features_df.index[-1].strftime(\"%Y-%m-%d\"), \"2023+ Recovery\")\n",
    "]\n",
    "\n",
    "regime_results = []\n",
    "for start, end, name in regimes:\n",
    "    result = train_by_regime(features_df, start, end, name)\n",
    "    if result:\n",
    "        regime_results.append(result)\n",
    "        print(f\"Regime: {name:20s} | Return: {result['total_return']:+6.1f}% | Samples: {result['samples']}\")\n",
    "\n",
    "# Création d'une matrice d'importance par régime\n",
    "importance_matrix = pd.DataFrame([r['importance'] for r in regime_results],\n",
    "                                  index=[r['regime'] for r in regime_results])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 2: Feature Importance Stability Across Regimes\")\n",
    "print(\"=\"*80)\n",
    "print(importance_matrix.round(3))\n",
    "\n",
    "# Visualisation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(importance_matrix.T, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Importance'})\n",
    "plt.title('Feature Importance by Market Regime', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Market Regime')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse de stabilité\n",
    "importance_std = importance_matrix.std(axis=0).sort_values(ascending=False)\n",
    "print(\"\\nFeature Stability (std dev across regimes):\")\n",
    "print(importance_std)\n",
    "\n",
    "print(\"\\nVERDICT:\")\n",
    "most_stable = importance_std.idxmin()\n",
    "least_stable = importance_std.idxmax()\n",
    "print(f\"  Most stable feature: {most_stable} (std={importance_std[most_stable]:.4f})\")\n",
    "print(f\"  Least stable feature: {least_stable} (std={importance_std[least_stable]:.4f})\")\n",
    "\n",
    "# Top features par régime\n",
    "print(\"\\nTop feature by regime:\")\n",
    "for regime in importance_matrix.index:\n",
    "    top_feature = importance_matrix.loc[regime].idxmax()\n",
    "    top_value = importance_matrix.loc[regime, top_feature]\n",
    "    print(f\"  {regime:20s}: {top_feature:15s} ({top_value:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede014f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6: Walk-Forward Validation - Le Gold Standard\n",
    "# Train 2 ans, test 3 mois, roll forward\n",
    "\n",
    "def walk_forward_validation(features, train_days=730, test_days=90, retrain_interval=60):\n",
    "    \"\"\"\n",
    "    Implémente la walk-forward validation.\n",
    "    \n",
    "    Paramètres:\n",
    "    - train_days: Nombre de jours pour l'entraînement\n",
    "    - test_days: Nombre de jours pour le test\n",
    "    - retrain_interval: Fréquence de retraining (jours)\n",
    "    \n",
    "    Retourne les résultats de chaque fenêtre.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    feature_cols = [c for c in features.columns if c != 'label']\n",
    "    \n",
    "    idx = 0\n",
    "    window_num = 0\n",
    "    \n",
    "    while idx + train_days + test_days <= len(features):\n",
    "        window_num += 1\n",
    "        \n",
    "        # Séparation train/test\n",
    "        train_slice = features.iloc[idx:idx+train_days]\n",
    "        test_slice = features.iloc[idx+train_days:idx+train_days+test_days]\n",
    "        \n",
    "        X_train = train_slice[feature_cols].values\n",
    "        y_train = train_slice['label'].values\n",
    "        X_test = test_slice[feature_cols].values\n",
    "        y_test = test_slice['label'].values\n",
    "        \n",
    "        # Entraînement\n",
    "        model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Prédictions\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_proba > 0.5).astype(int)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Simulation trading\n",
    "        signals = (y_proba > 0.6).astype(float) * y_proba\n",
    "        returns = test_slice['daily_return'].values\n",
    "        \n",
    "        # Éviter lookahead\n",
    "        strategy_returns = returns[1:] * signals[:-1]\n",
    "        \n",
    "        # Métriques\n",
    "        mean_ret = strategy_returns.mean() * 365\n",
    "        std_ret = strategy_returns.std() * np.sqrt(365)\n",
    "        sharpe = mean_ret / std_ret if std_ret > 0 else 0\n",
    "        \n",
    "        cumulative = (1 + strategy_returns).cumprod()\n",
    "        total_return = cumulative[-1] - 1 if len(cumulative) > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'window': window_num,\n",
    "            'train_start': train_slice.index[0],\n",
    "            'train_end': train_slice.index[-1],\n",
    "            'test_start': test_slice.index[0],\n",
    "            'test_end': test_slice.index[-1],\n",
    "            'accuracy': accuracy,\n",
    "            'sharpe': sharpe,\n",
    "            'total_return': total_return,\n",
    "            'n_trades': (signals > 0).sum()\n",
    "        })\n",
    "        \n",
    "        print(f\"Window {window_num}: Test {test_slice.index[0].strftime('%Y-%m-%d')} → {test_slice.index[-1].strftime('%Y-%m-%d')} | \"\n",
    "              f\"Sharpe: {sharpe:.3f} | Accuracy: {accuracy:.2%} | Return: {total_return*100:+.1f}%\")\n",
    "        \n",
    "        # Roll forward\n",
    "        idx += retrain_interval\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"WALK-FORWARD VALIDATION (Train: 730d, Test: 90d, Retrain: 60d)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "wf_results = walk_forward_validation(features_df, train_days=730, test_days=90, retrain_interval=60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WALK-FORWARD SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total windows: {len(wf_results)}\")\n",
    "print(f\"Average Sharpe: {wf_results['sharpe'].mean():.3f} ± {wf_results['sharpe'].std():.3f}\")\n",
    "print(f\"Median Sharpe: {wf_results['sharpe'].median():.3f}\")\n",
    "print(f\"Best Sharpe: {wf_results['sharpe'].max():.3f}\")\n",
    "print(f\"Worst Sharpe: {wf_results['sharpe'].min():.3f}\")\n",
    "print(f\"Positive Sharpe windows: {(wf_results['sharpe'] > 0).sum()} / {len(wf_results)}\")\n",
    "print(f\"Average accuracy: {wf_results['accuracy'].mean():.2%}\")\n",
    "print(f\"Average return per window: {wf_results['total_return'].mean()*100:+.2f}%\")\n",
    "\n",
    "# Visualisation de la stabilité\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Sharpe par fenêtre\n",
    "axes[0].plot(wf_results['window'], wf_results['sharpe'], marker='o', linewidth=2, markersize=6)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(y=wf_results['sharpe'].mean(), color='green', linestyle='--', alpha=0.5, label=f\"Mean: {wf_results['sharpe'].mean():.3f}\")\n",
    "axes[0].set_title('Sharpe Ratio by Walk-Forward Window', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Window Number')\n",
    "axes[0].set_ylabel('Sharpe Ratio')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Returns cumulatifs\n",
    "cumulative_returns = (1 + wf_results['total_return']).cumprod() - 1\n",
    "axes[1].plot(wf_results['window'], cumulative_returns * 100, marker='o', linewidth=2, markersize=6, color='green')\n",
    "axes[1].set_title('Cumulative Returns Across Walk-Forward Windows', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Window Number')\n",
    "axes[1].set_ylabel('Cumulative Return (%)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVERDICT:\")\n",
    "if wf_results['sharpe'].mean() > 0.3 and (wf_results['sharpe'] > 0).sum() / len(wf_results) > 0.6:\n",
    "    print(\"✓ H5 CONFIRMED: Walk-forward shows consistent positive performance\")\n",
    "    print(f\"  {(wf_results['sharpe'] > 0).sum()}/{len(wf_results)} windows with positive Sharpe\")\n",
    "else:\n",
    "    print(\"✗ H5 REJECTED: Performance not consistent across walk-forward windows\")\n",
    "    print(f\"  Only {(wf_results['sharpe'] > 0).sum()}/{len(wf_results)} windows with positive Sharpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6fd0a3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 7: Hypothèse 3 - Fréquence Optimale de Retraining\n",
    "# Comparer 30, 60, 90 jours de retraining via walk-forward\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HYPOTHESIS 3: Optimal Retraining Frequency\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "retrain_intervals = [30, 60, 90]\n",
    "retrain_comparison = []\n",
    "\n",
    "for interval in retrain_intervals:\n",
    "    print(f\"\\nTesting retraining interval: {interval} days\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    wf = walk_forward_validation(features_df, train_days=730, test_days=90, retrain_interval=interval)\n",
    "    \n",
    "    retrain_comparison.append({\n",
    "        'interval': interval,\n",
    "        'n_windows': len(wf),\n",
    "        'mean_sharpe': wf['sharpe'].mean(),\n",
    "        'median_sharpe': wf['sharpe'].median(),\n",
    "        'std_sharpe': wf['sharpe'].std(),\n",
    "        'positive_pct': (wf['sharpe'] > 0).sum() / len(wf) * 100,\n",
    "        'mean_accuracy': wf['accuracy'].mean(),\n",
    "        'mean_return': wf['total_return'].mean()\n",
    "    })\n",
    "\n",
    "retrain_df = pd.DataFrame(retrain_comparison).set_index('interval')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RETRAINING FREQUENCY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(retrain_df)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Mean Sharpe\n",
    "axes[0].bar(retrain_df.index, retrain_df['mean_sharpe'], color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Mean Sharpe by Retraining Interval', fontweight='bold')\n",
    "axes[0].set_xlabel('Retraining Interval (days)')\n",
    "axes[0].set_ylabel('Mean Sharpe Ratio')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Sharpe Stability (std)\n",
    "axes[1].bar(retrain_df.index, retrain_df['std_sharpe'], color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "axes[1].set_title('Sharpe Stability (Lower is Better)', fontweight='bold')\n",
    "axes[1].set_xlabel('Retraining Interval (days)')\n",
    "axes[1].set_ylabel('Sharpe Std Dev')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Positive Windows %\n",
    "axes[2].bar(retrain_df.index, retrain_df['positive_pct'], color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "axes[2].set_title('% of Positive Sharpe Windows', fontweight='bold')\n",
    "axes[2].set_xlabel('Retraining Interval (days)')\n",
    "axes[2].set_ylabel('Positive Windows (%)')\n",
    "axes[2].axhline(y=50, color='red', linestyle='--', alpha=0.5, label='50% baseline')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verdict\n",
    "best_interval = retrain_df['mean_sharpe'].idxmax()\n",
    "best_sharpe = retrain_df.loc[best_interval, 'mean_sharpe']\n",
    "\n",
    "print(\"\\nVERDICT:\")\n",
    "print(f\"✓ Optimal retraining interval: {best_interval} days\")\n",
    "print(f\"  Mean Sharpe: {best_sharpe:.3f}\")\n",
    "print(f\"  Stability: {retrain_df.loc[best_interval, 'std_sharpe']:.3f} std\")\n",
    "print(f\"  Positive windows: {retrain_df.loc[best_interval, 'positive_pct']:.1f}%\")\n",
    "\n",
    "if best_interval == 30:\n",
    "    print(\"\\n  → Current strategy (30d) is optimal for adaptation\")\n",
    "elif best_interval == 60:\n",
    "    print(\"\\n  → RECOMMENDATION: Increase retraining interval to 60 days for better stability\")\n",
    "else:\n",
    "    print(\"\\n  → RECOMMENDATION: Increase retraining interval to 90 days (less overfitting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db961919",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 8: Hypothèse 4 - Comparaison au Buy-and-Hold\n",
    "# La stratégie ML bat-elle vraiment le HODL?\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HYPOTHESIS 4: ML vs Buy-and-Hold Benchmark\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Période de test: 2023 à aujourd'hui (out-of-sample)\n",
    "test_start = \"2023-01-01\"\n",
    "test_end = features_df.index[-1]\n",
    "\n",
    "# ML Strategy (train sur 2017-2022)\n",
    "ml_result = train_and_evaluate(\n",
    "    features_df,\n",
    "    train_start=\"2017-01-01\",\n",
    "    train_end=\"2022-12-31\",\n",
    "    test_start=test_start,\n",
    "    test_end=test_end,\n",
    "    label=\"ML Strategy (5Y training)\"\n",
    ")\n",
    "\n",
    "# Buy-and-Hold\n",
    "test_df = features_df.loc[test_start:test_end]\n",
    "bnh_returns = test_df['daily_return'].values\n",
    "bnh_cumulative = (1 + bnh_returns).cumprod()\n",
    "bnh_total_return = bnh_cumulative[-1] - 1\n",
    "bnh_sharpe = (bnh_returns.mean() * 365) / (bnh_returns.std() * np.sqrt(365))\n",
    "bnh_max_dd = (bnh_cumulative / np.maximum.accumulate(bnh_cumulative) - 1).min()\n",
    "\n",
    "# Comparaison\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': 'Buy-and-Hold BTC',\n",
    "        'Total Return (%)': bnh_total_return * 100,\n",
    "        'Sharpe Ratio': bnh_sharpe,\n",
    "        'Max Drawdown (%)': bnh_max_dd * 100,\n",
    "        'Annual Return (%)': (bnh_returns.mean() * 365) * 100,\n",
    "        'Volatility (%)': (bnh_returns.std() * np.sqrt(365)) * 100\n",
    "    },\n",
    "    {\n",
    "        'Strategy': 'ML Strategy',\n",
    "        'Total Return (%)': ml_result['total_return'] * 100,\n",
    "        'Sharpe Ratio': ml_result['sharpe'],\n",
    "        'Max Drawdown (%)': ml_result['max_drawdown'] * 100,\n",
    "        'Annual Return (%)': ml_result['annual_return'] * 100,\n",
    "        'Volatility (%)': ml_result['volatility'] * 100\n",
    "    }\n",
    "]).set_index('Strategy')\n",
    "\n",
    "print(comparison.round(2))\n",
    "\n",
    "# Visualisation comparative\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Total Return\n",
    "strategies = comparison.index.tolist()\n",
    "returns = comparison['Total Return (%)'].values\n",
    "axes[0, 0].bar(strategies, returns, color=['#3498db', '#2ecc71'])\n",
    "axes[0, 0].set_title('Total Return (%)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Return (%)')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Sharpe Ratio\n",
    "sharpes = comparison['Sharpe Ratio'].values\n",
    "axes[0, 1].bar(strategies, sharpes, color=['#3498db', '#2ecc71'])\n",
    "axes[0, 1].set_title('Sharpe Ratio (Risk-Adjusted)', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Sharpe')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Max Drawdown\n",
    "drawdowns = comparison['Max Drawdown (%)'].values\n",
    "axes[1, 0].bar(strategies, drawdowns, color=['#e74c3c', '#e67e22'])\n",
    "axes[1, 0].set_title('Max Drawdown (%) - Lower is Better', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Drawdown (%)')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Volatility\n",
    "vols = comparison['Volatility (%)'].values\n",
    "axes[1, 1].bar(strategies, vols, color=['#9b59b6', '#8e44ad'])\n",
    "axes[1, 1].set_title('Annualized Volatility (%)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Volatility (%)')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verdict\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERDICT - ML vs Buy-and-Hold\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "excess_return = ml_result['total_return'] - bnh_total_return\n",
    "sharpe_improvement = ml_result['sharpe'] - bnh_sharpe\n",
    "\n",
    "print(f\"\\nExcess Return: {excess_return * 100:+.2f}%\")\n",
    "print(f\"Sharpe Improvement: {sharpe_improvement:+.3f}\")\n",
    "\n",
    "if ml_result['sharpe'] > bnh_sharpe:\n",
    "    print(\"\\n✓ H4 CONFIRMED: ML strategy beats buy-and-hold on risk-adjusted basis\")\n",
    "    print(f\"  ML Sharpe: {ml_result['sharpe']:.3f} vs BnH Sharpe: {bnh_sharpe:.3f}\")\n",
    "else:\n",
    "    print(\"\\n✗ H4 REJECTED: Buy-and-hold has better risk-adjusted returns\")\n",
    "    print(f\"  BnH Sharpe: {bnh_sharpe:.3f} vs ML Sharpe: {ml_result['sharpe']:.3f}\")\n",
    "\n",
    "if ml_result['max_drawdown'] > bnh_max_dd:\n",
    "    print(f\"\\n⚠️ WARNING: ML has worse max drawdown ({ml_result['max_drawdown']*100:.1f}% vs {bnh_max_dd*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n✓ ML reduces max drawdown ({ml_result['max_drawdown']*100:.1f}% vs {bnh_max_dd*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d291ec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Findings - Synthèse des Résultats\n",
    "\n",
    "### Hypothèse 1: Période d'Entraînement Étendue\n",
    "\n",
    "**Résultat**: [À compléter après exécution]\n",
    "\n",
    "- **2021-2022 (2 ans)**: Sharpe = ?, Return = ?\n",
    "- **2017-2022 (5 ans)**: Sharpe = ?, Return = ?\n",
    "- **Conclusion**: L'entraînement sur 5 ans incluant plusieurs cycles bull/bear...\n",
    "\n",
    "### Hypothèse 2: Stabilité des Features par Régime\n",
    "\n",
    "**Résultat**: [À compléter après exécution]\n",
    "\n",
    "- **Features les plus stables**: [Top 3]\n",
    "- **Features changeantes**: [Lesquelles varient entre bull/bear]\n",
    "- **Conclusion**: Les features momentum (EMA) sont plus importantes en bull, RSI/ADX en bear...\n",
    "\n",
    "### Hypothèse 3: Fréquence Optimale de Retraining\n",
    "\n",
    "**Résultat**: [À compléter après exécution]\n",
    "\n",
    "| Interval | Mean Sharpe | Stability | Positive % |\n",
    "|----------|-------------|-----------|------------|\n",
    "| 30 jours | ? | ? | ? |\n",
    "| 60 jours | ? | ? | ? |\n",
    "| 90 jours | ? | ? | ? |\n",
    "\n",
    "- **Conclusion**: Un retraining tous les X jours offre le meilleur compromis...\n",
    "\n",
    "### Hypothèse 4: ML vs Buy-and-Hold\n",
    "\n",
    "**Résultat**: [À compléter après exécution]\n",
    "\n",
    "| Métrique | BnH | ML | Différence |\n",
    "|----------|-----|----|-----------|\n",
    "| Total Return | ? | ? | ? |\n",
    "| Sharpe | ? | ? | ? |\n",
    "| Max DD | ? | ? | ? |\n",
    "\n",
    "- **Conclusion**: Le ML [bat/ne bat pas] le buy-and-hold sur base ajustée au risque...\n",
    "\n",
    "### Hypothèse 5: Walk-Forward Consistency\n",
    "\n",
    "**Résultat**: [À compléter après exécution]\n",
    "\n",
    "- **Nombre de fenêtres**: ?\n",
    "- **Sharpe moyen**: ? ± ?\n",
    "- **Windows positifs**: ? / ?\n",
    "- **Conclusion**: La stratégie [montre/ne montre pas] une performance cohérente...\n",
    "\n",
    "## Insights Clés\n",
    "\n",
    "1. **Data Leakage**: [Observations sur la séparation train/test]\n",
    "2. **Regime Dependency**: [Comment la stratégie performe selon le régime]\n",
    "3. **Overfitting Risk**: [Signes d'overfitting détectés ou non]\n",
    "4. **Retraining Trade-off**: [Balance entre adaptation et stabilité]\n",
    "\n",
    "## Limites de l'Étude\n",
    "\n",
    "- **Coûts de transaction**: Non inclus (spread, slippage, fees)\n",
    "- **Exécution parfaite**: Assume fills instantanés au prix prédit\n",
    "- **Régime futur**: Les patterns passés peuvent ne pas se répéter\n",
    "- **Features statiques**: Pas d'adaptation dynamique des features\n",
    "\n",
    "## Recommandations pour la Stratégie Principale\n",
    "\n",
    "[À compléter après exécution avec les meilleurs paramètres identifiés]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead7172",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 10: Recommendations JSON - Pour implémentation dans main.py\n",
    "\n",
    "import json\n",
    "\n",
    "# Recommendations basées sur les résultats de recherche\n",
    "recommendations = {\n",
    "    \"research_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"project\": \"BTC-MachineLearning (21047688)\",\n",
    "    \"methodology\": \"Walk-forward validation + regime analysis\",\n",
    "    \n",
    "    \"findings\": {\n",
    "        \"h1_extended_training\": {\n",
    "            \"confirmed\": None,  # À remplir après exécution\n",
    "            \"optimal_training_period\": \"2017-2022 (5 years)\" if None else \"2021-2022 (2 years)\",\n",
    "            \"sharpe_improvement\": None\n",
    "        },\n",
    "        \"h2_feature_stability\": {\n",
    "            \"most_stable_features\": [],  # Top 3\n",
    "            \"regime_dependent_features\": [],  # Features changeantes\n",
    "            \"recommendation\": \"Consider regime-adaptive feature weighting\"\n",
    "        },\n",
    "        \"h3_retraining_frequency\": {\n",
    "            \"optimal_interval_days\": None,  # 30, 60, ou 90\n",
    "            \"current_interval\": 30,\n",
    "            \"change_recommended\": False\n",
    "        },\n",
    "        \"h4_ml_vs_bnh\": {\n",
    "            \"ml_beats_bnh\": None,\n",
    "            \"excess_return_pct\": None,\n",
    "            \"sharpe_improvement\": None\n",
    "        },\n",
    "        \"h5_walk_forward\": {\n",
    "            \"consistent_performance\": None,\n",
    "            \"mean_sharpe\": None,\n",
    "            \"positive_windows_pct\": None\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"implementation_recommendations\": [\n",
    "        {\n",
    "            \"parameter\": \"TRAIN_START\",\n",
    "            \"current_value\": \"2021-01-01\",\n",
    "            \"recommended_value\": None,  # Basé sur H1\n",
    "            \"reason\": \"Extended training period improves generalization\"\n",
    "        },\n",
    "        {\n",
    "            \"parameter\": \"RETRAIN_INTERVAL_DAYS\",\n",
    "            \"current_value\": 30,\n",
    "            \"recommended_value\": None,  # Basé sur H3\n",
    "            \"reason\": \"Optimal balance between adaptation and stability\"\n",
    "        },\n",
    "        {\n",
    "            \"parameter\": \"CONFIDENCE_LONG_THRESHOLD\",\n",
    "            \"current_value\": 0.6,\n",
    "            \"recommended_value\": 0.6,  # Peut ajuster selon walk-forward\n",
    "            \"reason\": \"Walk-forward validation suggests...\"\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"next_steps\": [\n",
    "        \"Update main.py with optimal training period\",\n",
    "        \"Adjust retraining interval if needed\",\n",
    "        \"Compile and backtest with new parameters\",\n",
    "        \"Monitor live performance vs walk-forward expectations\",\n",
    "        \"Consider adding transaction costs to backtest\"\n",
    "    ],\n",
    "    \n",
    "    \"risks\": [\n",
    "        \"Past performance does not guarantee future results\",\n",
    "        \"Crypto markets are highly volatile and non-stationary\",\n",
    "        \"Regime changes may invalidate historical patterns\",\n",
    "        \"Transaction costs not included in this research\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sauvegarde\n",
    "recommendations_json = json.dumps(recommendations, indent=2)\n",
    "print(recommendations_json)\n",
    "\n",
    "# Note: Remplir les valeurs None après exécution complète du notebook\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESEARCH COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Fill in the None values in recommendations based on cell outputs\")\n",
    "print(\"2. Update main.py with optimal parameters\")\n",
    "print(\"3. Compile via MCP QC\")\n",
    "print(\"4. Backtest via web UI (API requires paid account)\")\n",
    "print(\"5. Compare backtest results to walk-forward predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.321741,
   "end_time": "2026-02-17T09:32:50.464476",
   "environment_variables": {},
   "exception": true,
   "input_path": "c:\\dev\\CoursIA\\MyIA.AI.Notebooks\\QuantConnect\\ESGF-2026\\examples\\BTC-MachineLearning\\research_robustness.ipynb",
   "output_path": "c:\\dev\\CoursIA\\MyIA.AI.Notebooks\\QuantConnect\\ESGF-2026\\examples\\BTC-MachineLearning\\research_robustness_output.ipynb",
   "parameters": {},
   "start_time": "2026-02-17T09:32:47.142735",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}