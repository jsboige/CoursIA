{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research: Diagnostic et Correction de la Stratégie ETF Pairs Trading\n",
    "\n",
    "## Contexte\n",
    "\n",
    "La stratégie actuelle affiche un **Sharpe ratio de -0.759** sur la période 2020-2024, ce qui signifie qu'elle **perd de l'argent**. Cette recherche vise à identifier les causes profondes et proposer des corrections basées sur des données empiriques.\n",
    "\n",
    "## Problématique\n",
    "\n",
    "Le pairs trading repose sur l'hypothèse de cointégration : deux actifs évoluent ensemble à long terme, et leurs écarts temporaires (spreads) reviennent vers une moyenne. Pourquoi cette stratégie échoue-t-elle ?\n",
    "\n",
    "### Pièges courants du pairs trading\n",
    "\n",
    "1. **Divergence du spread** : Les paires perdent leur cointégration en cours de position\n",
    "2. **Sortie prématurée** : Exit à z-score = 0.5 au lieu de 0.0 (mean)\n",
    "3. **Instabilité du beta** : EWMA peut amplifier le bruit à court terme\n",
    "4. **Coûts de transaction** : Sur-trading avec des half-lives trop courts\n",
    "5. **Mauvaise gestion du risque** : Stop-loss par jambe au lieu de spread-level\n",
    "6. **Durée d'insight inadaptée** : 6h fixes au lieu de se baser sur le half-life\n",
    "\n",
    "## Hypothèses à tester\n",
    "\n",
    "1. **H1** : Corriger z-exit de 0.5 → 0.0 améliore le Sharpe de >0.3 points\n",
    "2. **H2** : Les paires restent cointégrées <50% du temps après 1 semaine\n",
    "3. **H3** : Half-life moyen > 10 jours (trop long pour du hourly trading)\n",
    "4. **H4** : Un filtre half-life < 30 jours élimine les paires instables\n",
    "5. **H5** : Walk-forward validation montre une dégradation >30% hors-sample\n",
    "\n",
    "## Méthodologie\n",
    "\n",
    "1. Charger les données ETF sectoriels (2015-2026) avec QuantBook\n",
    "2. Analyser la cointégration et stabilité temporelle des paires\n",
    "3. Calculer les half-lives de mean-reversion\n",
    "4. Backtester vectorisé avec différents paramètres (z-exit, stop-loss)\n",
    "5. Walk-forward validation (train 1 an, trade 3 mois)\n",
    "6. Synthétiser les corrections prioritaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup QuantBook et chargement des données\n",
    "from AlgorithmImports import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialiser QuantBook\n",
    "qb = QuantBook()\n",
    "\n",
    "# Secteur ETFs du S&P (SPDR)\n",
    "etf_tickers = [\"XLB\", \"XLE\", \"XLF\", \"XLI\", \"XLK\", \"XLP\", \"XLU\", \"XLV\", \"XLY\"]\n",
    "symbols = {}\n",
    "\n",
    "for ticker in etf_tickers:\n",
    "    symbols[ticker] = qb.AddEquity(ticker, Resolution.Daily).Symbol\n",
    "\n",
    "# Charger les données historiques (2015-01-01 → maintenant)\n",
    "start_date = datetime(2015, 1, 1)\n",
    "end_date = datetime.now()\n",
    "\n",
    "history = qb.History(list(symbols.values()), start_date, end_date, Resolution.Daily)\n",
    "\n",
    "# Extraire les prix de clôture\n",
    "if 'close' in history.columns:\n",
    "    prices = history['close'].unstack(level=0)\n",
    "    # Renommer les colonnes avec les tickers\n",
    "    prices.columns = [s.Value for s in prices.columns]\n",
    "    prices = prices[etf_tickers]  # Réordonner\n",
    "    prices = prices.dropna()\n",
    "    \n",
    "    print(f\"✓ Chargé {len(prices)} jours de données pour {len(prices.columns)} ETFs\")\n",
    "    print(f\"  Période: {prices.index[0].date()} → {prices.index[-1].date()}\")\n",
    "    print(f\"\\nAperçu:\")\n",
    "    print(prices.head())\n",
    "else:\n",
    "    print(\"⚠ Erreur: colonne 'close' non trouvée dans l'historique\")\n",
    "    prices = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Analyse de cointégration - test de toutes les paires\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import scipy.stats as stats\n",
    "\n",
    "def test_cointegration(p1, p2, pvalue_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Test de cointégration d'Engle-Granger.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'cointegrated': bool, 'pvalue': float, 'statistic': float}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score, pvalue, _ = coint(p1, p2)\n",
    "        return {\n",
    "            'cointegrated': pvalue < pvalue_threshold,\n",
    "            'pvalue': round(pvalue, 4),\n",
    "            'statistic': round(score, 4)\n",
    "        }\n",
    "    except:\n",
    "        return {'cointegrated': False, 'pvalue': 1.0, 'statistic': 0.0}\n",
    "\n",
    "# Générer toutes les paires possibles\n",
    "all_pairs = list(combinations(etf_tickers, 2))\n",
    "print(f\"Analyse de {len(all_pairs)} paires possibles\\n\")\n",
    "\n",
    "# Tester chaque paire sur la période complète\n",
    "coint_results = []\n",
    "\n",
    "for pair in all_pairs:\n",
    "    etf1, etf2 = pair\n",
    "    p1 = prices[etf1]\n",
    "    p2 = prices[etf2]\n",
    "    \n",
    "    result = test_cointegration(p1, p2)\n",
    "    coint_results.append({\n",
    "        'pair': f\"{etf1}-{etf2}\",\n",
    "        'etf1': etf1,\n",
    "        'etf2': etf2,\n",
    "        **result\n",
    "    })\n",
    "\n",
    "coint_df = pd.DataFrame(coint_results).sort_values('pvalue')\n",
    "\n",
    "# Statistiques globales\n",
    "n_cointegrated = coint_df['cointegrated'].sum()\n",
    "print(f\"Paires cointégrées (p < 0.05): {n_cointegrated}/{len(all_pairs)} ({100*n_cointegrated/len(all_pairs):.1f}%)\\n\")\n",
    "\n",
    "# Top 10 paires par p-value\n",
    "print(\"Top 10 paires les plus cointégrées:\")\n",
    "print(coint_df[['pair', 'pvalue', 'statistic']].head(10).to_string(index=False))\n",
    "\n",
    "# Bottom 5 (les moins cointégrées)\n",
    "print(\"\\nPires 5 paires (moins cointégrées):\")\n",
    "print(coint_df[['pair', 'pvalue', 'statistic']].tail(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Stabilité temporelle des paires - rolling cointegration test\n",
    "def rolling_cointegration(p1, p2, window_days=252, step_days=63):\n",
    "    \"\"\"\n",
    "    Test de cointégration glissant pour mesurer la stabilité temporelle.\n",
    "    \n",
    "    Args:\n",
    "        window_days: Fenêtre de test (252 jours = 1 an)\n",
    "        step_days: Pas de roulement (63 jours = ~3 mois)\n",
    "    \n",
    "    Returns:\n",
    "        list: Historique des p-values\n",
    "    \"\"\"\n",
    "    pvalues = []\n",
    "    dates = []\n",
    "    \n",
    "    for i in range(window_days, len(p1), step_days):\n",
    "        window_p1 = p1.iloc[i-window_days:i]\n",
    "        window_p2 = p2.iloc[i-window_days:i]\n",
    "        \n",
    "        result = test_cointegration(window_p1, window_p2)\n",
    "        pvalues.append(result['pvalue'])\n",
    "        dates.append(p1.index[i])\n",
    "    \n",
    "    return pd.Series(pvalues, index=dates)\n",
    "\n",
    "# Tester la stabilité des 5 meilleures paires\n",
    "print(\"Analyse de stabilité temporelle (rolling 1-year cointegration)\\n\")\n",
    "\n",
    "stability_results = {}\n",
    "\n",
    "for _, row in coint_df.head(5).iterrows():\n",
    "    pair_name = row['pair']\n",
    "    etf1, etf2 = row['etf1'], row['etf2']\n",
    "    \n",
    "    pvalue_history = rolling_cointegration(prices[etf1], prices[etf2])\n",
    "    stability_results[pair_name] = pvalue_history\n",
    "    \n",
    "    # Calculer le % de temps où la paire est cointégrée\n",
    "    stable_pct = (pvalue_history < 0.05).sum() / len(pvalue_history) * 100\n",
    "    \n",
    "    print(f\"{pair_name}:\")\n",
    "    print(f\"  P-value moyenne: {pvalue_history.mean():.4f}\")\n",
    "    print(f\"  Stable (p<0.05): {stable_pct:.1f}% du temps\")\n",
    "    print(f\"  Périodes testées: {len(pvalue_history)}\")\n",
    "    print()\n",
    "\n",
    "# Conclusion sur H2\n",
    "avg_stability = np.mean([((pv < 0.05).sum() / len(pv) * 100) for pv in stability_results.values()])\n",
    "print(f\"⚠ Stabilité moyenne des top 5 paires: {avg_stability:.1f}%\")\n",
    "print(f\"   {'✓ H2 CONFIRMÉE' if avg_stability < 50 else '✗ H2 INFIRMÉE'}: Les paires restent cointégrées {'<' if avg_stability < 50 else '>'}50% du temps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Calcul du half-life de mean-reversion\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def compute_spread_and_half_life(p1, p2):\n",
    "    \"\"\"\n",
    "    Calcule le spread cointégré et son half-life de mean-reversion.\n",
    "    \n",
    "    Méthode: OLS regression du spread_lag sur spread_diff\n",
    "    Half-life = -ln(2) / beta si beta < 0\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'spread': Series, 'beta': float, 'half_life_days': float}\n",
    "    \"\"\"\n",
    "    # 1. Estimer le beta de la paire (OLS)\n",
    "    X = sm.add_constant(p2)\n",
    "    model = sm.OLS(p1, X)\n",
    "    result = model.fit()\n",
    "    beta = result.params[1]\n",
    "    \n",
    "    # 2. Calculer le spread\n",
    "    spread = p1 - beta * p2\n",
    "    \n",
    "    # 3. Estimer le half-life (AR(1) mean-reversion speed)\n",
    "    spread_lag = spread.shift(1).dropna()\n",
    "    spread_diff = spread.diff().dropna()\n",
    "    \n",
    "    # Aligner les indices\n",
    "    common_idx = spread_lag.index.intersection(spread_diff.index)\n",
    "    X_ar = sm.add_constant(spread_lag.loc[common_idx])\n",
    "    y_ar = spread_diff.loc[common_idx]\n",
    "    \n",
    "    model_ar = sm.OLS(y_ar, X_ar)\n",
    "    result_ar = model_ar.fit()\n",
    "    \n",
    "    # Beta AR(1) (coefficient du spread_lag)\n",
    "    beta_ar = result_ar.params[1]\n",
    "    \n",
    "    if beta_ar < 0:\n",
    "        half_life = -np.log(2) / beta_ar\n",
    "    else:\n",
    "        # Pas de mean-reversion si beta_ar >= 0\n",
    "        half_life = float('inf')\n",
    "    \n",
    "    return {\n",
    "        'spread': spread,\n",
    "        'beta': round(beta, 4),\n",
    "        'beta_ar': round(beta_ar, 4),\n",
    "        'half_life_days': round(half_life, 2)\n",
    "    }\n",
    "\n",
    "# Calculer le half-life pour toutes les paires cointégrées\n",
    "print(\"Calcul des half-lives de mean-reversion\\n\")\n",
    "\n",
    "half_life_results = []\n",
    "\n",
    "for _, row in coint_df[coint_df['cointegrated']].iterrows():\n",
    "    pair_name = row['pair']\n",
    "    etf1, etf2 = row['etf1'], row['etf2']\n",
    "    \n",
    "    result = compute_spread_and_half_life(prices[etf1], prices[etf2])\n",
    "    \n",
    "    half_life_results.append({\n",
    "        'pair': pair_name,\n",
    "        'pvalue': row['pvalue'],\n",
    "        'beta': result['beta'],\n",
    "        'half_life_days': result['half_life_days']\n",
    "    })\n",
    "\n",
    "hl_df = pd.DataFrame(half_life_results).sort_values('half_life_days')\n",
    "\n",
    "# Statistiques globales\n",
    "valid_hl = hl_df[hl_df['half_life_days'] < 365]  # Exclure les inf\n",
    "print(f\"Paires avec half-life valide (<365j): {len(valid_hl)}/{len(hl_df)}\\n\")\n",
    "\n",
    "if len(valid_hl) > 0:\n",
    "    print(f\"Half-life moyen: {valid_hl['half_life_days'].mean():.1f} jours\")\n",
    "    print(f\"Half-life médian: {valid_hl['half_life_days'].median():.1f} jours\")\n",
    "    print(f\"Half-life min: {valid_hl['half_life_days'].min():.1f} jours\")\n",
    "    print(f\"Half-life max: {valid_hl['half_life_days'].max():.1f} jours\\n\")\n",
    "    \n",
    "    # Filtrage par half-life\n",
    "    fast_pairs = valid_hl[valid_hl['half_life_days'] < 30]\n",
    "    print(f\"Paires avec HL < 30 jours: {len(fast_pairs)} ({100*len(fast_pairs)/len(valid_hl):.1f}%)\")\n",
    "    \n",
    "    # Top 10 paires par half-life court\n",
    "    print(\"\\nTop 10 paires avec mean-reversion rapide:\")\n",
    "    print(hl_df[['pair', 'pvalue', 'half_life_days']].head(10).to_string(index=False))\n",
    "    \n",
    "    # Conclusion sur H3\n",
    "    median_hl = valid_hl['half_life_days'].median()\n",
    "    print(f\"\\n{'✓ H3 CONFIRMÉE' if median_hl > 10 else '✗ H3 INFIRMÉE'}: Half-life médian = {median_hl:.1f} jours {'>' if median_hl > 10 else '<='} 10 jours\")\n",
    "else:\n",
    "    print(\"⚠ Aucune paire avec half-life valide trouvée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Backtest vectorisé avec différents z-score exit\n",
    "def pairs_trading_backtest(p1, p2, z_entry=1.5, z_exit=0.0, lookback_ols=252, lookback_zscore=60):\n",
    "    \"\"\"\n",
    "    Backtest vectorisé d'une stratégie pairs trading.\n",
    "    \n",
    "    Args:\n",
    "        z_entry: Seuil d'entrée (ex: 1.5)\n",
    "        z_exit: Seuil de sortie (ex: 0.0 pour mean)\n",
    "        lookback_ols: Fenêtre de calcul du beta\n",
    "        lookback_zscore: Fenêtre de calcul du z-score\n",
    "    \n",
    "    Returns:\n",
    "        dict: Métriques de performance\n",
    "    \"\"\"\n",
    "    # Calculer le beta rolling (OLS)\n",
    "    betas = []\n",
    "    spreads = []\n",
    "    \n",
    "    for i in range(lookback_ols, len(p1)):\n",
    "        window_p1 = p1.iloc[i-lookback_ols:i]\n",
    "        window_p2 = p2.iloc[i-lookback_ols:i]\n",
    "        \n",
    "        # OLS beta\n",
    "        beta = np.cov(window_p1, window_p2)[0,1] / np.var(window_p2)\n",
    "        spread = p1.iloc[i] - beta * p2.iloc[i]\n",
    "        \n",
    "        betas.append(beta)\n",
    "        spreads.append(spread)\n",
    "    \n",
    "    spread_series = pd.Series(spreads, index=p1.index[lookback_ols:])\n",
    "    \n",
    "    # Calculer le z-score du spread\n",
    "    spread_mean = spread_series.rolling(lookback_zscore).mean()\n",
    "    spread_std = spread_series.rolling(lookback_zscore).std()\n",
    "    z_score = (spread_series - spread_mean) / spread_std\n",
    "    z_score = z_score.dropna()\n",
    "    \n",
    "    # Générer les signaux de trading\n",
    "    signal = pd.Series(0.0, index=z_score.index)\n",
    "    position = 0\n",
    "    entries = 0\n",
    "    exits = 0\n",
    "    \n",
    "    for i in range(1, len(z_score)):\n",
    "        z = z_score.iloc[i]\n",
    "        \n",
    "        if position == 0:\n",
    "            # Entrée long spread (long etf1, short etf2)\n",
    "            if z < -z_entry:\n",
    "                position = 1\n",
    "                entries += 1\n",
    "            # Entrée short spread (short etf1, long etf2)\n",
    "            elif z > z_entry:\n",
    "                position = -1\n",
    "                entries += 1\n",
    "        else:\n",
    "            # Sortie long spread\n",
    "            if position == 1 and z > z_exit:\n",
    "                position = 0\n",
    "                exits += 1\n",
    "            # Sortie short spread\n",
    "            elif position == -1 and z < -z_exit:\n",
    "                position = 0\n",
    "                exits += 1\n",
    "        \n",
    "        signal.iloc[i] = position\n",
    "    \n",
    "    # Calculer les returns du spread\n",
    "    spread_returns = spread_series.pct_change()\n",
    "    spread_returns = spread_returns.loc[signal.index]\n",
    "    \n",
    "    # Returns de la stratégie (signal décalé d'1 jour)\n",
    "    strategy_returns = spread_returns * signal.shift(1)\n",
    "    strategy_returns = strategy_returns.dropna()\n",
    "    \n",
    "    # Métriques\n",
    "    if len(strategy_returns) > 0 and strategy_returns.std() > 0:\n",
    "        sharpe = strategy_returns.mean() / strategy_returns.std() * np.sqrt(252)\n",
    "        cumulative_returns = (1 + strategy_returns).cumprod()\n",
    "        max_dd = (cumulative_returns / cumulative_returns.cummax() - 1).min()\n",
    "        win_rate = (strategy_returns > 0).sum() / len(strategy_returns) if len(strategy_returns) > 0 else 0\n",
    "    else:\n",
    "        sharpe = 0\n",
    "        max_dd = 0\n",
    "        win_rate = 0\n",
    "    \n",
    "    return {\n",
    "        'sharpe': round(sharpe, 3),\n",
    "        'max_dd': round(max_dd * 100, 2),\n",
    "        'n_trades': entries,\n",
    "        'win_rate': round(win_rate * 100, 1)\n",
    "    }\n",
    "\n",
    "# Tester différentes valeurs de z_exit sur les 3 meilleures paires\n",
    "print(\"Backtest avec différents seuils de z-score exit\\n\")\n",
    "\n",
    "z_exit_values = [0.5, 0.25, 0.0, -0.25]  # Négatif = attendre l'overshoot\n",
    "test_pairs = hl_df.head(3)\n",
    "\n",
    "results_by_zexit = []\n",
    "\n",
    "for z_exit in z_exit_values:\n",
    "    print(f\"\\n=== Z-exit = {z_exit} ===\")\n",
    "    \n",
    "    for _, row in test_pairs.iterrows():\n",
    "        pair = row['pair']\n",
    "        etf1, etf2 = pair.split('-')\n",
    "        \n",
    "        metrics = pairs_trading_backtest(\n",
    "            prices[etf1], prices[etf2],\n",
    "            z_entry=1.5,\n",
    "            z_exit=z_exit\n",
    "        )\n",
    "        \n",
    "        results_by_zexit.append({\n",
    "            'z_exit': z_exit,\n",
    "            'pair': pair,\n",
    "            **metrics\n",
    "        })\n",
    "        \n",
    "        print(f\"{pair}: Sharpe={metrics['sharpe']}, MaxDD={metrics['max_dd']}%, Trades={metrics['n_trades']}, WinRate={metrics['win_rate']}%\")\n",
    "\n",
    "# Analyser l'impact du z_exit\n",
    "results_df = pd.DataFrame(results_by_zexit)\n",
    "avg_by_zexit = results_df.groupby('z_exit')['sharpe'].mean()\n",
    "\n",
    "print(\"\\n=== Impact moyen du z-exit ===\")\n",
    "for z, sharpe in avg_by_zexit.items():\n",
    "    print(f\"Z-exit = {z:5.2f} → Sharpe moyen = {sharpe:6.3f}\")\n",
    "\n",
    "# Conclusion sur H1\n",
    "sharpe_baseline = avg_by_zexit.get(0.5, 0)\n",
    "sharpe_corrected = avg_by_zexit.get(0.0, 0)\n",
    "improvement = sharpe_corrected - sharpe_baseline\n",
    "\n",
    "print(f\"\\n{'✓ H1 CONFIRMÉE' if improvement > 0.3 else '✗ H1 INFIRMÉE'}: Amélioration Sharpe (0.5→0.0) = {improvement:+.3f} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Comparaison des approches de stop-loss\n",
    "def pairs_trading_with_stop(p1, p2, stop_type='spread_sigma', stop_threshold=2.5):\n",
    "    \"\"\"\n",
    "    Backtest avec différentes stratégies de stop-loss.\n",
    "    \n",
    "    Args:\n",
    "        stop_type: 'per_leg' (8% par jambe), 'spread_sigma' (2.5σ du spread), 'time_based'\n",
    "        stop_threshold: Valeur du seuil selon le type\n",
    "    \"\"\"\n",
    "    lookback_ols = 252\n",
    "    lookback_zscore = 60\n",
    "    z_entry = 1.5\n",
    "    z_exit = 0.0\n",
    "    \n",
    "    # Calculer spread et z-score (même logique que précédemment)\n",
    "    betas = []\n",
    "    spreads = []\n",
    "    \n",
    "    for i in range(lookback_ols, len(p1)):\n",
    "        window_p1 = p1.iloc[i-lookback_ols:i]\n",
    "        window_p2 = p2.iloc[i-lookback_ols:i]\n",
    "        beta = np.cov(window_p1, window_p2)[0,1] / np.var(window_p2)\n",
    "        spread = p1.iloc[i] - beta * p2.iloc[i]\n",
    "        spreads.append(spread)\n",
    "    \n",
    "    spread_series = pd.Series(spreads, index=p1.index[lookback_ols:])\n",
    "    spread_mean = spread_series.rolling(lookback_zscore).mean()\n",
    "    spread_std = spread_series.rolling(lookback_zscore).std()\n",
    "    z_score = (spread_series - spread_mean) / spread_std\n",
    "    z_score = z_score.dropna()\n",
    "    \n",
    "    # Générer les signaux avec stop-loss\n",
    "    signal = pd.Series(0.0, index=z_score.index)\n",
    "    position = 0\n",
    "    entry_spread = None\n",
    "    entry_date = None\n",
    "    stopped_out = 0\n",
    "    \n",
    "    for i in range(1, len(z_score)):\n",
    "        z = z_score.iloc[i]\n",
    "        current_spread = spread_series.loc[z_score.index[i]]\n",
    "        \n",
    "        if position == 0:\n",
    "            # Entrée\n",
    "            if z < -z_entry:\n",
    "                position = 1\n",
    "                entry_spread = current_spread\n",
    "                entry_date = z_score.index[i]\n",
    "            elif z > z_entry:\n",
    "                position = -1\n",
    "                entry_spread = current_spread\n",
    "                entry_date = z_score.index[i]\n",
    "        else:\n",
    "            # Vérifier stop-loss\n",
    "            stop_hit = False\n",
    "            \n",
    "            if stop_type == 'spread_sigma':\n",
    "                # Stop si spread dépasse threshold * sigma\n",
    "                current_sigma = spread_std.loc[z_score.index[i]]\n",
    "                if position == 1 and (current_spread - entry_spread) < -stop_threshold * current_sigma:\n",
    "                    stop_hit = True\n",
    "                elif position == -1 and (current_spread - entry_spread) > stop_threshold * current_sigma:\n",
    "                    stop_hit = True\n",
    "            \n",
    "            elif stop_type == 'time_based':\n",
    "                # Stop si position ouverte > threshold jours\n",
    "                days_open = (z_score.index[i] - entry_date).days\n",
    "                if days_open > stop_threshold:\n",
    "                    stop_hit = True\n",
    "            \n",
    "            if stop_hit:\n",
    "                position = 0\n",
    "                stopped_out += 1\n",
    "            # Sortie normale au z-exit\n",
    "            elif position == 1 and z > z_exit:\n",
    "                position = 0\n",
    "            elif position == -1 and z < -z_exit:\n",
    "                position = 0\n",
    "        \n",
    "        signal.iloc[i] = position\n",
    "    \n",
    "    # Calculer returns\n",
    "    spread_returns = spread_series.pct_change().loc[signal.index]\n",
    "    strategy_returns = spread_returns * signal.shift(1)\n",
    "    strategy_returns = strategy_returns.dropna()\n",
    "    \n",
    "    if len(strategy_returns) > 0 and strategy_returns.std() > 0:\n",
    "        sharpe = strategy_returns.mean() / strategy_returns.std() * np.sqrt(252)\n",
    "    else:\n",
    "        sharpe = 0\n",
    "    \n",
    "    return {\n",
    "        'sharpe': round(sharpe, 3),\n",
    "        'stopped_out': stopped_out\n",
    "    }\n",
    "\n",
    "# Comparer les approches de stop-loss\n",
    "print(\"Comparaison des stratégies de stop-loss\\n\")\n",
    "\n",
    "stop_configs = [\n",
    "    ('spread_sigma', 2.5, \"Spread-level: 2.5σ\"),\n",
    "    ('spread_sigma', 3.0, \"Spread-level: 3.0σ\"),\n",
    "    ('time_based', 60, \"Time-based: 60 jours\"),\n",
    "    ('time_based', 90, \"Time-based: 90 jours\")\n",
    "]\n",
    "\n",
    "best_pair = test_pairs.iloc[0]['pair']\n",
    "etf1, etf2 = best_pair.split('-')\n",
    "\n",
    "print(f\"Test sur la meilleure paire: {best_pair}\\n\")\n",
    "\n",
    "for stop_type, threshold, label in stop_configs:\n",
    "    metrics = pairs_trading_with_stop(\n",
    "        prices[etf1], prices[etf2],\n",
    "        stop_type=stop_type,\n",
    "        stop_threshold=threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"{label:30s} → Sharpe={metrics['sharpe']:6.3f}, Stopped={metrics['stopped_out']:3d}\")\n",
    "\n",
    "print(\"\\n⚠ Note: Le stop per-leg actuel (8%) ne peut pas être testé sans données tick\")\n",
    "print(\"   Recommandation: Utiliser spread-level stop pour préserver la neutralité\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Walk-forward validation\n",
    "def walk_forward_validation(p1, p2, train_days=252, test_days=63):\n",
    "    \"\"\"\n",
    "    Walk-forward validation: train sur N jours, trade sur M jours.\n",
    "    \n",
    "    Args:\n",
    "        train_days: Période d'entraînement (252 = 1 an)\n",
    "        test_days: Période de test (63 = 3 mois)\n",
    "    \n",
    "    Returns:\n",
    "        list: Résultats par période\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Itérer par périodes de test\n",
    "    for i in range(train_days, len(p1) - test_days, test_days):\n",
    "        # Fenêtre d'entraînement\n",
    "        train_p1 = p1.iloc[i-train_days:i]\n",
    "        train_p2 = p2.iloc[i-train_days:i]\n",
    "        \n",
    "        # Fenêtre de test\n",
    "        test_p1 = p1.iloc[i:i+test_days]\n",
    "        test_p2 = p2.iloc[i:i+test_days]\n",
    "        \n",
    "        # Vérifier la cointégration sur train\n",
    "        train_coint = test_cointegration(train_p1, train_p2)\n",
    "        \n",
    "        if not train_coint['cointegrated']:\n",
    "            # Skip cette période si pas cointégré\n",
    "            continue\n",
    "        \n",
    "        # Backtest sur la période de test\n",
    "        test_metrics = pairs_trading_backtest(test_p1, test_p2, z_exit=0.0)\n",
    "        \n",
    "        results.append({\n",
    "            'start_date': test_p1.index[0],\n",
    "            'end_date': test_p1.index[-1],\n",
    "            'train_pvalue': train_coint['pvalue'],\n",
    "            'test_sharpe': test_metrics['sharpe'],\n",
    "            'test_trades': test_metrics['n_trades']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Walk-forward sur les 3 meilleures paires\n",
    "print(\"Walk-forward validation (train 1 an, test 3 mois)\\n\")\n",
    "\n",
    "wf_results_all = []\n",
    "\n",
    "for _, row in test_pairs.iterrows():\n",
    "    pair = row['pair']\n",
    "    etf1, etf2 = pair.split('-')\n",
    "    \n",
    "    print(f\"\\n=== {pair} ===\")\n",
    "    \n",
    "    wf_results = walk_forward_validation(prices[etf1], prices[etf2])\n",
    "    \n",
    "    if len(wf_results) > 0:\n",
    "        wf_df = pd.DataFrame(wf_results)\n",
    "        \n",
    "        # Statistiques\n",
    "        mean_sharpe = wf_df['test_sharpe'].mean()\n",
    "        std_sharpe = wf_df['test_sharpe'].std()\n",
    "        positive_periods = (wf_df['test_sharpe'] > 0).sum()\n",
    "        \n",
    "        print(f\"Périodes testées: {len(wf_results)}\")\n",
    "        print(f\"Sharpe moyen: {mean_sharpe:.3f} ± {std_sharpe:.3f}\")\n",
    "        print(f\"Périodes profitables: {positive_periods}/{len(wf_results)} ({100*positive_periods/len(wf_results):.1f}%)\")\n",
    "        \n",
    "        # Ajouter le nom de paire\n",
    "        wf_df['pair'] = pair\n",
    "        wf_results_all.append(wf_df)\n",
    "    else:\n",
    "        print(\"⚠ Aucune période cointégrée trouvée\")\n",
    "\n",
    "# Conclusion sur H5\n",
    "if len(wf_results_all) > 0:\n",
    "    all_wf = pd.concat(wf_results_all, ignore_index=True)\n",
    "    overall_sharpe = all_wf['test_sharpe'].mean()\n",
    "    \n",
    "    print(f\"\\n=== Résultats globaux ===\")\n",
    "    print(f\"Sharpe moyen walk-forward: {overall_sharpe:.3f}\")\n",
    "    print(f\"Médian: {all_wf['test_sharpe'].median():.3f}\")\n",
    "    print(f\"Écart-type: {all_wf['test_sharpe'].std():.3f}\")\n",
    "    \n",
    "    # On considère que si sharpe WF < 0.3, il y a dégradation significative\n",
    "    print(f\"\\n{'✓ H5 CONFIRMÉE' if overall_sharpe < 0.3 else '✗ H5 INFIRMÉE'}: Sharpe WF = {overall_sharpe:.3f} {'<' if overall_sharpe < 0.3 else '>='} 0.3\")\n",
    "else:\n",
    "    print(\"\\n⚠ Aucun résultat walk-forward disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings - Synthèse des résultats\n",
    "\n",
    "### Résumé des hypothèses testées\n",
    "\n",
    "| Hypothèse | Statut | Métrique | Conclusion |\n",
    "|-----------|--------|----------|------------|\n",
    "| H1: z-exit 0.5→0.0 améliore Sharpe >0.3 | ... | Δ Sharpe = ... | ... |\n",
    "| H2: Paires stables <50% du temps | ... | Stabilité moyenne = ...% | ... |\n",
    "| H3: Half-life > 10 jours | ... | HL médian = ... jours | ... |\n",
    "| H4: Filtre HL < 30j élimine instables | ... | % paires HL<30j = ...% | ... |\n",
    "| H5: Dégradation WF >30% | ... | Sharpe WF = ... | ... |\n",
    "\n",
    "### Corrections prioritaires identifiées\n",
    "\n",
    "1. **Correction du z-exit** : Passer de 0.5 → 0.0 (attendre la mean-reversion complète)\n",
    "   - Impact estimé : +... points de Sharpe\n",
    "   - Justification pédagogique : Le z-score à 0 représente le retour à la moyenne du spread. Sortir à 0.5 revient à quitter la position **avant** que la convergence attendue ne se réalise.\n",
    "\n",
    "2. **Filtre de half-life** : Exclure les paires avec HL > 30 jours\n",
    "   - Raison : Half-life trop long → trop de risque de rupture de cointégration pendant la position\n",
    "   - Pairs restantes : .../... paires (suffisant pour diversification)\n",
    "\n",
    "3. **Stop-loss spread-level** : Remplacer stop per-leg 8% par spread-level 2.5σ\n",
    "   - Raison : Le stop per-leg **brise la neutralité** de la position (une jambe peut être stoppée indépendamment)\n",
    "   - Alternative : Time-based stop à 2× half-life moyen\n",
    "\n",
    "4. **Durée d'insight adaptative** : Baser sur le half-life au lieu de 6h fixes\n",
    "   - Formule proposée : `insight_duration = min(2 * half_life, 30 days)`\n",
    "   - Justification : Le half-life indique la vitesse naturelle de mean-reversion\n",
    "\n",
    "5. **Extension de période** : 2020-2024 → 2015-2026\n",
    "   - Objectif : Capturer différents régimes de marché (sideways 2015-2019, volatil 2020-2021)\n",
    "   - Pré-requis : Appliquer d'abord les corrections 1-4\n",
    "\n",
    "### Leçons pédagogiques sur le pairs trading\n",
    "\n",
    "1. **La cointégration n'est pas stationnaire** : Les paires qui sont cointégrées sur le passé peuvent perdre cette propriété.\n",
    "\n",
    "2. **Le half-life est un signal de robustesse** : Plus il est court, plus la mean-reversion est rapide et fiable.\n",
    "\n",
    "3. **La neutralité du spread est critique** : Un stop-loss par jambe transforme une position market-neutral en position directionnelle.\n",
    "\n",
    "4. **Walk-forward validation est essentielle** : Le pairs trading est particulièrement sujet à l'overfitting sur les paramètres de cointégration.\n",
    "\n",
    "5. **Les coûts de transaction sont cruciaux** : Hourly trading avec trop de rotations (HL < 5 jours) peut tuer la stratégie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Recommandations JSON pour implémentation\n",
    "import json\n",
    "\n",
    "# Calculer les métriques finales pour les recommandations\n",
    "# (Ces valeurs seront remplies après exécution du notebook)\n",
    "\n",
    "recommendations = {\n",
    "    \"strategy_name\": \"ETF-Pairs-Trading\",\n",
    "    \"current_sharpe\": -0.759,\n",
    "    \"target_sharpe\": 0.3,\n",
    "    \"corrections\": [\n",
    "        {\n",
    "            \"priority\": 1,\n",
    "            \"file\": \"alpha.py\",\n",
    "            \"line\": \"~130\",\n",
    "            \"change\": \"z_exit: 0.5 → 0.0\",\n",
    "            \"code_before\": \"if abs(z_score) < 0.5:\",\n",
    "            \"code_after\": \"if abs(z_score) < 0.0:\",\n",
    "            \"expected_impact\": \"+0.3 to +0.5 Sharpe points\"\n",
    "        },\n",
    "        {\n",
    "            \"priority\": 2,\n",
    "            \"file\": \"universe.py\",\n",
    "            \"line\": \"~85\",\n",
    "            \"change\": \"Add half-life filter in pair selection\",\n",
    "            \"code_after\": \"if half_life < 30:  # Filter pairs with HL < 30 days\",\n",
    "            \"expected_impact\": \"+0.1 to +0.2 Sharpe points (stability)\"\n",
    "        },\n",
    "        {\n",
    "            \"priority\": 3,\n",
    "            \"file\": \"risk.py\",\n",
    "            \"line\": \"~40\",\n",
    "            \"change\": \"Replace per-leg stop with spread-level stop\",\n",
    "            \"code_before\": \"TrailingStopRiskManagementModel(0.08)\",\n",
    "            \"code_after\": \"# Custom spread-level stop at 2.5 sigma\",\n",
    "            \"expected_impact\": \"Preserve market neutrality\"\n",
    "        },\n",
    "        {\n",
    "            \"priority\": 4,\n",
    "            \"file\": \"alpha.py\",\n",
    "            \"line\": \"~110\",\n",
    "            \"change\": \"Adaptive insight duration based on half-life\",\n",
    "            \"code_before\": \"timedelta(hours=6)\",\n",
    "            \"code_after\": \"timedelta(days=min(2 * half_life, 30))\",\n",
    "            \"expected_impact\": \"Better position timing\"\n",
    "        },\n",
    "        {\n",
    "            \"priority\": 5,\n",
    "            \"file\": \"main.py\",\n",
    "            \"line\": \"~20\",\n",
    "            \"change\": \"Extend backtest period\",\n",
    "            \"code_before\": \"self.SetStartDate(2020, 1, 1)\",\n",
    "            \"code_after\": \"self.SetStartDate(2015, 1, 1)\",\n",
    "            \"expected_impact\": \"More robust regime testing\"\n",
    "        }\n",
    "    ],\n",
    "    \"next_steps\": [\n",
    "        \"Apply corrections 1-4 to the strategy code\",\n",
    "        \"Compile the project on QuantConnect cloud\",\n",
    "        \"Run backtest via web UI (2015-2026)\",\n",
    "        \"Verify Sharpe > 0.3 before considering live deployment\",\n",
    "        \"If still negative, consider daily resolution instead of hourly\"\n",
    "    ],\n",
    "    \"hypothesis_results\": {\n",
    "        \"H1_zexit_improvement\": \"To be filled after execution\",\n",
    "        \"H2_stability_pct\": \"To be filled after execution\",\n",
    "        \"H3_median_halflife_days\": \"To be filled after execution\",\n",
    "        \"H4_fast_pairs_pct\": \"To be filled after execution\",\n",
    "        \"H5_wf_sharpe\": \"To be filled after execution\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder en JSON\n",
    "output_path = \"research_recommendations.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(recommendations, f, indent=2)\n",
    "\n",
    "print(\"✓ Recommandations sauvegardées dans:\", output_path)\n",
    "print(\"\\n\" + json.dumps(recommendations, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
