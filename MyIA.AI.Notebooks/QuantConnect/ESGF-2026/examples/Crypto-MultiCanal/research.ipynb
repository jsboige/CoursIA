{"cells":[{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["# Analyse de Canaux Multi-Échelles avec ZigZag et Lignes de Régression Pondérées\n","\n","Ce notebook explore une approche d'analyse technique multi-échelles pour l'actif BTCUSDT (sur Binance), basée sur l'identification de pivots via l'indicateur ZigZag et la construction de canaux de tendance hiérarchiques (Macro, Méso, Micro).\n","\n","**Objectifs :**\n","\n","1.  **Extraire les points pivots significatifs** du prix horaire à l'aide d'une implémentation personnalisée de l'indicateur ZigZag.\n","2.  **Développer un algorithme** pour tracer des lignes de support et de résistance formant des canaux, en privilégiant les lignes qui \"contiennent\" strictement les pivots récents et en minimisant une erreur quadratique pondérée par le temps (`WSSE`).\n","3.  **Construire une hiérarchie de canaux** (Macro, Méso, Micro) où chaque niveau utilise les pivots définissant le niveau supérieur comme point de départ temporel.\n","4.  **Visualiser** les pivots, les canaux et leur évolution temporelle.\n","5.  **Explorer la sensibilité** de l'algorithme aux paramètres de pondération (`weight_power`, `wp`) et de fraction de pivots récents (`recent_pivot_fraction`, `rpf`).\n","6.  **(Optionnel / Futur)** Définir et backtester des stratégies de trading basées sur les interactions du prix avec ces canaux multi-échelles.\n","\n","**Étapes Initiales :**\n","\n","* Mise en place de l'environnement QuantConnect et téléchargement des données horaires BTCUSDT.\n","* Calcul d'une stratégie HODL simple comme référence de performance.\n","* Implémentation et visualisation de l'indicateur ZigZag."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Initialisation : Environnement et Données\n","\n","Mise en place de l'environnement QuantConnect, définition de l'actif (BTCUSDT), de la période d'analyse et téléchargement des données horaires nécessaires."]},{"cell_type":"code","execution_count":32,"metadata":{"dotnet_interactive":{"language":"csharp"},"polyglot_notebook":{"kernelName":"csharp"}},"outputs":[],"source":["# Import necessary libraries from the QC framework\n","from AlgorithmImports import *\n","from datetime import datetime\n","import pandas as pd\n","\n","# Initialize QuantBook\n","qb = QuantBook()\n","qb.SetBrokerageModel(BrokerageName.Binance, AccountType.Cash)\n","# For this example, we use BTCUSDT and set it as our benchmark.\n","btc_symbol = qb.AddCrypto('BTCUSDT', Resolution.Hour, Market.Binance).Symbol\n","qb.SetBenchmark(btc_symbol)\n","\n","# Define the backtesting period\n","start_date = datetime(2022, 1, 1)\n","end_date = datetime(2025, 4, 21)\n","\n","# Download hourly history for BTCUSDT\n","bars_df = qb.History(btc_symbol, start_date, end_date, Resolution.Hour)\n","bars_df = bars_df.reset_index()\n","\n","print('History loaded: ')\n","print(bars_df.head())\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Stratégie de Référence : HODL\n","\n","Calcul et visualisation de la performance d'une stratégie simple \"Buy and Hold\" (HODL) sur la période d'analyse. Ceci sert de benchmark pour évaluer toute stratégie de trading ultérieure."]},{"cell_type":"code","execution_count":33,"metadata":{"dotnet_interactive":{"language":"csharp"},"polyglot_notebook":{"kernelName":"csharp"}},"outputs":[],"source":["# Assume we use the close prices from the hourly DataFrame as the HODL basis\n","initial_investment = 10000  # in USDT\n","first_price = bars_df['close'].iloc[0]\n","bars_df['HODL_Value'] = initial_investment * (bars_df['close'] / first_price)\n","\n","# Print final portfolio value\n","final_value = bars_df['HODL_Value'].iloc[-1]\n","print(f\"Final HODL portfolio value: {final_value:.2f} USDT\")\n","\n","# Plot HODL performance (optional: using matplotlib)\n","import matplotlib.pyplot as plt\n","\n","# Disabling visualization temporarily for easier json copy\n","\n","# plt.figure(figsize=(10, 5))\n","# plt.plot(bars_df['time'], bars_df['HODL_Value'], label='HODL Portfolio Value ')\n","# plt.xlabel('Time')\n","# plt.ylabel('Portfolio Value (USDT)')\n","# plt.title('HODL Strategy Performance')\n","# plt.legend()\n","# plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Détection des Pivots : Indicateur ZigZag\n","\n","Cette section se concentre sur l'identification des points de retournement significatifs du marché à l'aide de l'indicateur ZigZag.\n","\n","### 3.1. Fonction `classic_chart_zigzag`\n","\n","Pour identifier les retournements de tendance significatifs, nous utilisons l'indicateur ZigZag. L'indicateur natif de QuantConnect pouvant présenter des comportements inattendus dans certains contextes de recherche, nous employons ici une fonction personnalisée `classic_chart_zigzag`.\n","\n","**Fonctionnement :**\n","\n","*   Elle identifie les points hauts (pivots High, type -1) et bas (pivots Low, type +1) sur les prix de clôture (`close`).\n","*   Un pivot est confirmé uniquement si le prix retrace d'un certain pourcentage (`thresholdPercent`) depuis le dernier extrême enregistré.\n","*   Dans cet exemple, `thresholdPercent=0.05` signifie qu'une baisse de 5% depuis le dernier plus haut est nécessaire pour confirmer ce plus haut comme un pivot High, et une hausse de 5% depuis le dernier plus bas est requise pour confirmer ce plus bas comme un pivot Low.\n","*   La fonction retourne une liste de pivots `(timestamp, price, type)`, garantissant une alternance stricte entre High et Low.\n","\n","Cette liste de pivots (`pivotList`) servira de base à la construction de nos canaux."]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def classic_chart_zigzag(df, thresholdPercent=0.05):\n","    \"\"\"\n","    Manually computes ZigZag pivots in the style of typical charting platforms.\n","    df must have columns 'time' and 'close'.\n","    thresholdPercent=0.05 => 5% retracement needed to lock in a pivot.\n","    Returns a list of pivots: (time, price, +1 or -1)\n","    +1 => Low pivot\n","    -1 => High pivot\n","    \"\"\"\n","    if len(df) < 2:\n","        return []\n","    # We'll store (time, price, sign)\n","    pivots = []\n","\n","    # 1) Start with the first bar as pivot\n","    firstBar = df.iloc[0]\n","    lastPivotPrice = firstBar['close']\n","    lastPivotTime  = firstBar['time']\n","\n","    # 2) Determine initial direction (up or down) by comparing the second bar\n","    secondBar = df.iloc[1]\n","    directionUp = secondBar['close'] > lastPivotPrice  # True => up, False => down\n","\n","    # If directionUp => we want to find new 'high extremes'\n","    # If directionUp=False => we want to find new 'low extremes'\n","\n","    # Track the extreme in the current direction\n","    currentExtremePrice = lastPivotPrice\n","    currentExtremeTime  = lastPivotTime\n","\n","    # We record the sign for the first pivot:\n","    #   if directionUp => first pivot is a Low (+1)\n","    #   else => first pivot is a High (-1)\n","    lastSign = +1 if directionUp else -1\n","\n","    # We'll add that initial pivot to the list\n","    pivots.append((lastPivotTime, lastPivotPrice, lastSign))\n","\n","    for i in range(1, len(df)):\n","        row = df.iloc[i]\n","        price = row['close']\n","        time  = row['time']\n","\n","        # Update extremes in the current direction\n","        if directionUp:\n","            # If price is higher than currentExtreme => update extreme\n","            if price > currentExtremePrice:\n","                currentExtremePrice = price\n","                currentExtremeTime  = time\n","            else:\n","                # Check if we retraced enough to confirm a new pivot\n","                retrace = 1.0 - (price / currentExtremePrice)\n","                if retrace >= thresholdPercent:\n","                    # That means we lock in the old extreme as a High pivot\n","                    # Switch direction to down\n","                    pivots.append((currentExtremeTime, currentExtremePrice, -1))\n","\n","                    # Now the new pivot is the current bar as 'low start'\n","                    directionUp = False\n","                    currentExtremePrice = price\n","                    currentExtremeTime  = time\n","        else:\n","            # directionDown => track new low extremes\n","            if price < currentExtremePrice:\n","                currentExtremePrice = price\n","                currentExtremeTime  = time\n","            else:\n","                # Check if we rallied enough to confirm a new pivot\n","                rally = (price / currentExtremePrice) - 1.0\n","                if rally >= thresholdPercent:\n","                    # Lock in the old extreme as a Low pivot\n","                    pivots.append((currentExtremeTime, currentExtremePrice, +1))\n","\n","                    directionUp = True\n","                    currentExtremePrice = price\n","                    currentExtremeTime  = time\n","\n","    # The last extreme can be appended as the final pivot if you want\n","    # e.g., treat the last bar as a pivot if you prefer\n","    finalPrice = currentExtremePrice\n","    finalTime  = currentExtremeTime\n","    finalSign  = +1 if not directionUp else -1  # Because if we ended going up, the last pivot is High\n","    pivots.append((finalTime, finalPrice, finalSign))\n","\n","    # Now we have a strictly alternating pivot list\n","    return pivots\n","\n","pivotList = classic_chart_zigzag(bars_df[['time','close']], thresholdPercent=0.05)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2. Visualisation du ZigZag\n","\n","Nous affichons ici les prix de clôture horaires superposés avec l'indicateur ZigZag (lignes de connexion et marqueurs de pivots High/Low) pour vérifier visuellement la pertinence des points détectés."]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["def plot_manual_zigzag(bars_df, pivotList):\n","    import plotly.graph_objects as go\n","\n","    # A) lines for raw price\n","    fig = go.Figure()\n","    fig.add_trace(go.Scatter(\n","        x=bars_df['time'],\n","        y=bars_df['close'],\n","        mode='lines',\n","        name='Close Price',\n","        line=dict(color='blue')\n","    ))\n","\n","    # B) build x and y arrays to connect pivot → pivot\n","    xline = []\n","    yline = []\n","    for (dt, price, _) in pivotList:\n","        xline.append(dt)\n","        yline.append(price)\n","\n","    fig.add_trace(go.Scatter(\n","        x=xline, \n","        y=yline,\n","        mode='lines',\n","        name='Classic ZigZag',\n","        line=dict(color='orange')\n","    ))\n","\n","    # C) separate pivot sign for red or green markers\n","    high_x, high_y = [], []\n","    low_x,  low_y  = [], []\n","\n","    for (dt, price, sign) in pivotList:\n","        if sign < 0:\n","            high_x.append(dt)\n","            high_y.append(price)\n","        else:\n","            low_x.append(dt)\n","            low_y.append(price)\n","\n","    fig.add_trace(go.Scatter(\n","        x=high_x, \n","        y=high_y,\n","        mode='markers',\n","        marker=dict(color='green', size=7),\n","        name='High Pivot'\n","    ))\n","    fig.add_trace(go.Scatter(\n","        x=low_x, \n","        y=low_y,\n","        mode='markers',\n","        marker=dict(color='red', size=7),\n","        name='Low Pivot'\n","    ))\n","\n","    fig.update_layout(\n","        title='BTCUSDT Hourly - Classic ZigZag (from scratch)',\n","        xaxis_title='Time',\n","        yaxis_title='Price (USDT)'\n","    )\n","    fig.show()\n","\n","# THEN:\n","\n","# plot_manual_zigzag(bars_df, pivotList)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3. Préparation des Données Pivots pour les Canaux\n","\n","Avant de construire les canaux, une étape cruciale est de préparer les données des pivots extraits par le ZigZag :\n","\n","1.  **Conversion en DataFrame :** Pour faciliter les manipulations.\n","2.  **Ajout de `time_numeric` :** Conversion du timestamp en secondes depuis l'epoch Unix. **Essentiel** pour les calculs géométriques (pente, ordonnée à l'origine) qui nécessitent une coordonnée 'x' numérique et linéaire.\n","3.  **Séparation High/Low :** Création de DataFrames distincts (`high_pivots`, `low_pivots`).\n","4.  **Stockage pour Vérification :** Création de copies NumPy (`all_high_pivots_for_check`, `all_low_pivots_for_check`) pour des vérifications rapides de contention lors de la recherche des meilleures lignes."]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["# Convert pivot list to DataFrame\n","pivots_df = pd.DataFrame(pivotList, columns=['time', 'price', 'type']) # type: +1 Low, -1 High\n","\n","# Convert time to numerical representation (seconds since epoch) for accurate line calculations\n","# Note: Using time.timestamp() is generally good. Ensure your pandas version handles it correctly.\n","# Alternatively, convert to numpy datetime64 and then to integer/float.\n","pivots_df['time_numeric'] = pivots_df['time'].apply(lambda x: x.timestamp())\n","\n","# Keep the original 'index' from the dataframe reset_index if needed for other purposes,\n","# but DO NOT use it for slope/channel calculations.\n","# pivots_df = pivots_df.reset_index() # Keep DataFrame index if needed\n","\n","# Separate High and Low pivots\n","high_pivots = pivots_df[pivots_df['type'] == -1].copy()\n","low_pivots = pivots_df[pivots_df['type'] == +1].copy()\n","\n","print(\"High Pivots with time_numeric:\")\n","print(high_pivots.head())\n","print(\"\\nLow Pivots with time_numeric:\")\n","print(low_pivots.head())\n","\n","# Store all pivots for global checks later\n","all_high_pivots_for_check = high_pivots[['time_numeric', 'price']].values\n","all_low_pivots_for_check = low_pivots[['time_numeric', 'price']].values"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Construction des Canaux : Algorithme et Paramètres\n","\n","Cette section détaille l'algorithme utilisé pour tracer les lignes de support et de résistance formant les canaux, ainsi que la configuration des paramètres qui influencent ce processus.\n","\n","\n","### 4.1. Algorithme Principal : `find_best_channel_line_strict_weighted`\n","\n","La fonction `find_best_channel_line_strict_weighted` est au cœur de notre construction de canaux. Elle vise à identifier la \"meilleure\" ligne de support ou de résistance possible, définie par une paire de pivots (`p1`, `p2`), en se basant sur des critères stricts et une optimisation pondérée.\n","\n","**Principes Clés :**\n","\n","1.  **Contention Stricte (Zéro Violation) :** Une ligne candidate (support ou résistance) définie par `p1` et `p2` n'est considérée comme **valide** que si **tous** les autres pivots pertinents (tous les Lows pour un support, tous les Highs pour une résistance) se trouvent du \"bon\" côté de la ligne sur toute la période considérée. Si aucune ligne ne satisfait ce critère, la fonction retourne `None`.\n","2.  **Minimisation de l'Erreur Pondérée (WSSE) sur Fraction Récente :** Parmi toutes les lignes _strictement valides_, l'algorithme sélectionne celle qui minimise la **Somme Pondérée des Erreurs Quadratiques (Weighted Sum of Squared Errors - WSSE)**.\n","    *   **Pondération Temporelle (`weight_power`, `wp`) :** L'erreur de chaque pivot est pondérée par son ancienneté relative (`wp > 0` donne plus d'importance aux pivots récents).\n","    *   **Fraction Récente (`recent_pivot_fraction`, `rpf`) :** La WSSE n'est calculée que sur une fraction (`rpf`) des pivots les plus récents, focalisant l'optimisation sur la pertinence récente.\n","3.  **Paramétrisation :** `wp` et `rpf` sont ajustables indépendamment pour chaque type de ligne et échelle.\n","\n","Cette approche garantit le respect de la structure globale des pivots tout en privilégiant la dynamique récente."]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["# CELLULE 13 : Définitions des Fonctions \n","\n","import numpy as np\n","import pandas as pd\n","import math \n","\n","# --- Helpers: get_line_params_time, check_point_position, calculate_weighted_sse ---\n","# (Copier ici les définitions complètes de ces 3 fonctions depuis la réponse précédente)\n","def get_line_params_time(p1_time_num, p1_price, p2_time_num, p2_price):\n","    time_diff = p2_time_num - p1_time_num\n","    if abs(time_diff) < 1e-9: return np.inf, p1_time_num\n","    m = (p2_price - p1_price) / time_diff\n","    c = p1_price - m * p1_time_num\n","    return m, c\n","\n","def check_point_position(point_time_num, point_price, m, c, check_above, epsilon=1e-9):\n","    if m == np.inf: return False \n","    line_y_at_point_x = m * point_time_num + c\n","    if check_above: return point_price >= line_y_at_point_x - epsilon\n","    else: return point_price <= line_y_at_point_x + epsilon\n","\n","def calculate_weighted_sse(p1, p2, pivots_for_sse_np, time_min_wsse, time_max_wsse, weight_power=1.0):\n","    p1_time_num, p1_price = p1['time_numeric'], p1['price']\n","    p2_time_num, p2_price = p2['time_numeric'], p2['price']\n","    m, c = get_line_params_time(p1_time_num, p1_price, p2_time_num, p2_price)\n","    if m == np.inf: return np.inf\n","    total_wsse = 0.0; total_weight = 0.0; time_range = time_max_wsse - time_min_wsse\n","    if time_range < 1e-9: time_range = 1.0\n","    if len(pivots_for_sse_np) == 0: return 0.0\n","    for k in range(len(pivots_for_sse_np)):\n","        pk_time_num, pk_price = pivots_for_sse_np[k, 0], pivots_for_sse_np[k, 1]\n","        if abs(pk_time_num - p1_time_num) < 1e-9 or abs(pk_time_num - p2_time_num) < 1e-9: continue\n","        normalized_time = (pk_time_num - time_min_wsse) / time_range if time_range > 1e-9 else 0.5 \n","        weight = max(0, normalized_time) ** weight_power + 1e-9\n","        line_y_at_pk = m * pk_time_num + c\n","        error = pk_price - line_y_at_pk\n","        total_wsse += weight * (error**2)\n","        total_weight += weight\n","    return total_wsse / total_weight if total_weight > 1e-9 else 0.0\n","\n","# --- Fonction Principale ---\n","def find_best_channel_line_strict_weighted(pivots_df, all_pivots_for_check_np, is_resistance,\n","                                           weight_power=1.0, recent_pivot_fraction=1.0):\n","    strictly_valid_lines_info = []\n","    n_pivots = len(pivots_df)\n","    if n_pivots < 2 or len(all_pivots_for_check_np) < 1: return None, None\n","\n","    check_pivots_df = pd.DataFrame(all_pivots_for_check_np, columns=['time_numeric', 'price'])\n","    check_pivots_df = check_pivots_df.sort_values('time_numeric', ascending=True)\n","    n_total_check = len(check_pivots_df)\n","    safe_rpf = max(0.0, min(1.0, recent_pivot_fraction))\n","    n_keep_for_wsse = max(1, math.ceil(n_total_check * safe_rpf))\n","    recent_pivots_for_wsse_df = check_pivots_df.tail(n_keep_for_wsse).copy()\n","    \n","    if recent_pivots_for_wsse_df.empty or len(recent_pivots_for_wsse_df) < 1: return None, None \n","    time_min_wsse = recent_pivots_for_wsse_df['time_numeric'].min()\n","    time_max_wsse = recent_pivots_for_wsse_df['time_numeric'].max()\n","\n","    for i in range(n_pivots):\n","        p1 = pivots_df.iloc[i]\n","        for j in range(i + 1, n_pivots):\n","            p2 = pivots_df.iloc[j]\n","            if p1['time_numeric'] >= p2['time_numeric']: continue\n","            p1_time_num, p1_price = p1['time_numeric'], p1['price']\n","            p2_time_num, p2_price = p2['time_numeric'], p2['price']\n","            m, c = get_line_params_time(p1_time_num, p1_price, p2_time_num, p2_price)\n","            if m == np.inf: continue\n","            line_is_strictly_valid = True\n","            for k in range(n_total_check): \n","                pk_time_num, pk_price = all_pivots_for_check_np[k, 0], all_pivots_for_check_np[k, 1]\n","                if abs(pk_time_num - p1_time_num) < 1e-9 or abs(pk_time_num - p2_time_num) < 1e-9: continue\n","                if not check_point_position(pk_time_num, pk_price, m, c, check_above=(not is_resistance)):\n","                    line_is_strictly_valid = False; break\n","            if line_is_strictly_valid:\n","                pivots_to_calc_wsse_on_df = recent_pivots_for_wsse_df[\n","                    (np.abs(recent_pivots_for_wsse_df['time_numeric'] - p1_time_num) > 1e-9) &\n","                    (np.abs(recent_pivots_for_wsse_df['time_numeric'] - p2_time_num) > 1e-9)\n","                ]\n","                if not pivots_to_calc_wsse_on_df.empty:\n","                    time_min_wsse_actual = pivots_to_calc_wsse_on_df['time_numeric'].min()\n","                    time_max_wsse_actual = pivots_to_calc_wsse_on_df['time_numeric'].max()\n","                    current_wsse = calculate_weighted_sse(p1, p2, pivots_to_calc_wsse_on_df[['time_numeric', 'price']].values,\n","                                                          time_min_wsse_actual, time_max_wsse_actual, weight_power)\n","                else: current_wsse = 0.0\n","                if current_wsse != np.inf:\n","                     strictly_valid_lines_info.append({ \"p1\": p1, \"p2\": p2, \"wsse_recent\": current_wsse })\n","\n","    if not strictly_valid_lines_info: return None, None\n","    strictly_valid_lines_info.sort(key=lambda x: x['wsse_recent'])\n","    best_line_info = strictly_valid_lines_info[0]\n","    return best_line_info['p1'], best_line_info['p2']"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2. Configuration des Paramètres de Canal\n","\n","Ici, nous définissons les jeux de paramètres (`wp` et `rpf`) qui seront utilisés pour construire les canaux à différentes échelles.\n","\n","1.  **`final_config` :** Configuration unique sélectionnée (potentiellement après optimisation) pour le tracé final, les snapshots et le backtesting éventuel. Contient `wp`/`rpf` pour Support/Résistance des niveaux Macro, Méso, Micro.\n","2.  **`configurations_to_explore` :** Liste de configurations générées pour tester la sensibilité de l'algorithme (ex: variation de `wp_micro_sup` et `rpf_micro_sup`). Les résultats alimenteront une visualisation comparative."]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["# CELLULE 14 : Définition des Configurations\n","\n","import itertools\n","\n","\n","# --- Configurations à Explorer pour la Visualisation ---\n","print(\"\\nGénération des configurations pour l'exploration...\")\n","wp_ref = 2.0\n","rpf_ref = 1.0\n","# rpf_micro_sup_options = [1, 0.8, 0.66, 0.5, 0.3] \n","rpf_micro_sup_options = [0.66, 0.5, 0.3] \n","wp_micro_sup_options = [0.3, 0.5, 1.0, 2.0, 4.0, 8.0] \n","\n","configurations_to_explore = []\n","config_id_counter = 0\n","for rpf_micsup in rpf_micro_sup_options:\n","    for wp_micsup in wp_micro_sup_options:\n","        config_id_counter += 1\n","        config = {\n","            \"label\": f\"Cfg{config_id_counter}_wpM={wp_micsup:.1f}_rpfM={rpf_micsup:.2f}\",\n","            \"wp_macro_res\": wp_ref, \"rpf_macro_res\": rpf_ref,\n","            \"wp_macro_sup\": wp_ref, \"rpf_macro_sup\": rpf_ref,\n","            \"wp_meso_res\" : wp_ref, \"rpf_meso_res\" : rpf_ref,\n","            \"wp_meso_sup\" : wp_ref, \"rpf_meso_sup\" : rpf_ref,\n","            \"wp_micro_res\": wp_ref, \"rpf_micro_res\": rpf_ref,\n","            \"wp_micro_sup\": wp_micsup, \"rpf_micro_sup\": rpf_micsup,\n","        }\n","        configurations_to_explore.append(config)\n","\n","print(f\"Nombre de configurations à explorer: {len(configurations_to_explore)}\")\n","results_list_explore = [] # Liste séparée pour les résultats de l'exploration"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Calcul et Analyse des Canaux\n","\n","Exécution des calculs pour déterminer les lignes de canal selon les configurations définies, suivie de l'analyse et de la visualisation des résultats.\n","\n","### 5.1. Exploration des Paramètres (Calcul)\n","\n","Cette cellule exécute le calcul des canaux pour **toutes** les configurations définies dans `configurations_to_explore`. Les pivots résultants pour chaque ligne de chaque configuration sont stockés dans `results_df_explore`. Ceci permet une analyse comparative ultérieure."]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# CELLULE 15 : Calcul en Boucle pour l'Exploration (CORRIGÉE - Ajout time_numeric dans get_pivot_info)\n","\n","import pandas as pd\n","import numpy as np\n","import math\n","from tqdm.notebook import tqdm # S'assurer que tqdm est importé\n","\n","print(\"Starting channel calculations for parameter exploration...\")\n","if 'high_pivots' not in locals() or 'low_pivots' not in locals(): raise NameError(\"Pivots non définis.\")\n","if 'find_best_channel_line_strict_weighted' not in locals(): raise NameError(\"Fonction de calcul non définie.\")\n","if 'get_line_params_time' not in locals(): # S'assurer que la dépendance est là\n","    def get_line_params_time(p1_time_num, p1_price, p2_time_num, p2_price):\n","        time_diff = p2_time_num - p1_time_num\n","        if abs(time_diff) < 1e-9: return np.inf, p1_time_num\n","        m = (p2_price - p1_price) / time_diff\n","        c = p1_price - m * p1_time_num\n","        return m, c\n","if 'np' not in locals(): import numpy as np # Assurer import numpy\n","if 'math' not in locals(): import math # Assurer import math\n","\n","\n","results_list_explore = [] # S'assurer qu'elle est vide avant de commencer\n","\n","# --- Définition de get_pivot_info DANS la cellule ---\n","# Correction: Ajouter time_numeric\n","def get_pivot_info(pivot):\n","    \"\"\"Convertit une Series pivot en dictionnaire pour stockage, incluant time_numeric.\"\"\"\n","    if pivot is None or not isinstance(pivot, pd.Series):\n","        return {'time': pd.NaT, 'price': np.nan, 'idx': -1, 'time_numeric': np.nan} # Valeurs par défaut\n","    # Récupérer 'idx' depuis le nom de la Series si disponible\n","    idx_val = pivot.name if hasattr(pivot, 'name') else -1\n","    # Assurer que time_numeric existe dans la Series source, le recalculer si besoin\n","    time_numeric_val = pivot.get('time_numeric', np.nan)\n","    if pd.isna(time_numeric_val) and not pd.isna(pivot.get('time')):\n","         try:\n","            # Convertir le timestamp pandas en objet datetime natif si nécessaire\n","            dt_obj = pivot['time']\n","            if isinstance(dt_obj, pd.Timestamp):\n","                 dt_obj = dt_obj.to_pydatetime()\n","            time_numeric_val = dt_obj.timestamp()\n","         except Exception as e:\n","             #print(f\"WARN: Could not get timestamp for {pivot.get('time')}: {e}\")\n","             time_numeric_val = np.nan # Fallback\n","\n","    return {\n","        'time': pivot.get('time', pd.NaT),\n","        'price': pivot.get('price', np.nan),\n","        'idx': idx_val,\n","        'time_numeric': time_numeric_val # Utiliser la valeur récupérée/calculée\n","    }\n","# --- Fin définition get_pivot_info ---\n","\n","\n","for idx, config in enumerate(tqdm(configurations_to_explore, desc=\"Exploring Configs\")): # Ajout tqdm ici\n","    # Stockage temporaire des résultats (tuples de Series ou (None, None))\n","    temp_channel_definitions = { \"macro\": {}, \"meso\": {}, \"micro\": {} }\n","\n","    # --- Extraire params ---\n","    wp_macro_res=config['wp_macro_res']; rpf_macro_res=config['rpf_macro_res']\n","    wp_macro_sup=config['wp_macro_sup']; rpf_macro_sup=config['rpf_macro_sup']\n","    wp_meso_res =config['wp_meso_res'];  rpf_meso_res =config['rpf_meso_res']\n","    wp_meso_sup =config['wp_meso_sup'];  rpf_meso_sup =config['rpf_meso_sup']\n","    wp_micro_res=config['wp_micro_res']; rpf_micro_res=config['rpf_micro_res']\n","    wp_micro_sup=config['wp_micro_sup']; rpf_micro_sup=config['rpf_micro_sup']\n","\n","    # --- Calcul Macro ---\n","    macro_success = False\n","    try:\n","        macro_res_series = find_best_channel_line_strict_weighted(high_pivots, all_high_pivots_for_check, True, wp_macro_res, rpf_macro_res)\n","        macro_sup_series = find_best_channel_line_strict_weighted(low_pivots, all_low_pivots_for_check, False, wp_macro_sup, rpf_macro_sup)\n","        temp_channel_definitions[\"macro\"][\"resistance\"] = macro_res_series if macro_res_series else (None, None)\n","        temp_channel_definitions[\"macro\"][\"support\"] = macro_sup_series if macro_sup_series else (None, None)\n","        if temp_channel_definitions[\"macro\"][\"resistance\"][0] is not None and temp_channel_definitions[\"macro\"][\"support\"][0] is not None:\n","            macro_success = True\n","    except Exception as e: print(f\"ERROR Macro in {config['label']}: {e}\")\n","\n","    # --- Calcul Meso ---\n","    meso_success = False\n","    meso_start_time = None\n","    temp_channel_definitions[\"meso\"][\"resistance\"] = (None, None)\n","    temp_channel_definitions[\"meso\"][\"support\"] = (None, None)\n","    if macro_success:\n","        macro_pivots_list = [p for pair in temp_channel_definitions[\"macro\"].values() for p in pair if p is not None]\n","        if len(macro_pivots_list) >= 2:\n","            try:\n","                meso_start_time = sorted([p['time'] for p in macro_pivots_list], reverse=True)[1]\n","                meso_high_f=high_pivots[high_pivots['time'] >= meso_start_time].copy(); meso_low_f=low_pivots[low_pivots['time'] >= meso_start_time].copy()\n","                if len(meso_high_f) >= 2 and len(meso_low_f) >= 2:\n","                    meso_high_np_check=meso_high_f[['time_numeric', 'price']].values; meso_low_np_check = meso_low_f[['time_numeric', 'price']].values\n","                    meso_res_series = find_best_channel_line_strict_weighted(meso_high_f, meso_high_np_check, True, wp_meso_res, rpf_meso_res)\n","                    meso_sup_series = find_best_channel_line_strict_weighted(meso_low_f, meso_low_np_check, False, wp_meso_sup, rpf_meso_sup)\n","                    temp_channel_definitions[\"meso\"][\"resistance\"] = meso_res_series if meso_res_series else (None, None)\n","                    temp_channel_definitions[\"meso\"][\"support\"] = meso_sup_series if meso_sup_series else (None, None)\n","                    if temp_channel_definitions[\"meso\"][\"resistance\"][0] is not None and temp_channel_definitions[\"meso\"][\"support\"][0] is not None:\n","                        meso_success = True\n","            except IndexError: pass\n","            except Exception as e: print(f\"ERROR Meso in {config['label']}: {e}\")\n","\n","    # --- Calcul Micro ---\n","    micro_start_time = None\n","    temp_channel_definitions[\"micro\"][\"resistance\"] = (None, None)\n","    temp_channel_definitions[\"micro\"][\"support\"] = (None, None)\n","    if meso_success:\n","        meso_pivots_list = [p for pair in temp_channel_definitions[\"meso\"].values() for p in pair if p is not None]\n","        if len(meso_pivots_list) >= 2:\n","            try:\n","                micro_start_time = sorted([p['time'] for p in meso_pivots_list], reverse=True)[1]\n","                micro_high_f=high_pivots[high_pivots['time'] >= micro_start_time].copy(); micro_low_f=low_pivots[low_pivots['time'] >= micro_start_time].copy()\n","                if len(micro_high_f) >= 2 and len(micro_low_f) >= 2:\n","                    micro_high_np_check=micro_high_f[['time_numeric', 'price']].values; micro_low_np_check=micro_low_f[['time_numeric', 'price']].values\n","                    micro_res_series = find_best_channel_line_strict_weighted(micro_high_f, micro_high_np_check, True, wp_micro_res, rpf_micro_res)\n","                    micro_sup_series = find_best_channel_line_strict_weighted(micro_low_f, micro_low_np_check, False, wp_micro_sup, rpf_micro_sup)\n","                    temp_channel_definitions[\"micro\"][\"resistance\"] = micro_res_series if micro_res_series else (None, None)\n","                    temp_channel_definitions[\"micro\"][\"support\"] = micro_sup_series if micro_sup_series else (None, None)\n","            except IndexError: pass\n","            except Exception as e: print(f\"ERROR Micro in {config['label']}: {e}\")\n","\n","    # --- Stocker les résultats (UTILISER get_pivot_info ICI) ---\n","    result_data = {\n","        \"config_index\": idx, \"config_label\": config['label'], \"params\": config,\n","        \"macro_res_p1\": get_pivot_info(temp_channel_definitions['macro'].get('resistance', (None, None))[0]),\n","        \"macro_res_p2\": get_pivot_info(temp_channel_definitions['macro'].get('resistance', (None, None))[1]),\n","        \"macro_sup_p1\": get_pivot_info(temp_channel_definitions['macro'].get('support', (None, None))[0]),\n","        \"macro_sup_p2\": get_pivot_info(temp_channel_definitions['macro'].get('support', (None, None))[1]),\n","        \"meso_res_p1\": get_pivot_info(temp_channel_definitions['meso'].get('resistance', (None, None))[0]),\n","        \"meso_res_p2\": get_pivot_info(temp_channel_definitions['meso'].get('resistance', (None, None))[1]),\n","        \"meso_sup_p1\": get_pivot_info(temp_channel_definitions['meso'].get('support', (None, None))[0]),\n","        \"meso_sup_p2\": get_pivot_info(temp_channel_definitions['meso'].get('support', (None, None))[1]),\n","        \"micro_res_p1\": get_pivot_info(temp_channel_definitions['micro'].get('resistance', (None, None))[0]),\n","        \"micro_res_p2\": get_pivot_info(temp_channel_definitions['micro'].get('resistance', (None, None))[1]),\n","        \"micro_sup_p1\": get_pivot_info(temp_channel_definitions['micro'].get('support', (None, None))[0]),\n","        \"micro_sup_p2\": get_pivot_info(temp_channel_definitions['micro'].get('support', (None, None))[1]),\n","        \"meso_start_t\": meso_start_time if meso_start_time is not None else pd.NaT,\n","        \"micro_start_t\": micro_start_time if micro_start_time is not None else pd.NaT,\n","    }\n","    results_list_explore.append(result_data)\n","    # print(f\"--> Result Micro Support: p1={result_data['micro_sup_p1'].get('time')} - p2={result_data['micro_sup_p2'].get('time')}\") # Moins verbeux\n","\n","print(\"\\n--- Channel exploration calculations finished ---\")\n","results_df_explore = pd.DataFrame(results_list_explore)\n","print(\"\\n--- Exploration Results Summary (Micro Support Pivots) ---\")\n","if not results_df_explore.empty:\n","    if 'micro_sup_p1' in results_df_explore.columns and 'micro_sup_p2' in results_df_explore.columns:\n","        pd.set_option('display.max_rows', None); pd.set_option('display.max_columns', None); pd.set_option('display.width', 2000)\n","        results_df_explore['mic_s1_t'] = results_df_explore['micro_sup_p1'].apply(lambda x: x.get('time') if isinstance(x, dict) else pd.NaT)\n","        results_df_explore['mic_s2_t'] = results_df_explore['micro_sup_p2'].apply(lambda x: x.get('time') if isinstance(x, dict) else pd.NaT)\n","        print(results_df_explore[['config_index', 'config_label', 'mic_s1_t', 'mic_s2_t']].to_string())\n","        pd.reset_option('display.max_rows'); pd.reset_option('display.max_columns'); pd.reset_option('display.width')\n","    else:\n","        print(\"WARN: Colonnes 'micro_sup_p1' ou 'micro_sup_p2' manquantes dans results_df_explore.\")\n","        print(results_df_explore.head())\n","else: print(\"results_df_explore is empty.\")"]},{"cell_type":"markdown","metadata":{},"source":["### 5.2. Identification de la Configuration Cible (`final_config`)\n","\n","Examen des résultats de l'exploration (`results_df_explore`), potentiellement basé sur des critères visuels ou quantitatifs (non implémentés ici), pour sélectionner la configuration (`final_config`) qui semble la plus pertinente pour une analyse plus approfondie (visualisation finale, snapshots, backtesting)."]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["# CELLULE 16 : Identification Configuration Cible\n","\n","target_p1_date = pd.Timestamp(\"2025-04-07 07:00:00\")\n","target_p2_date = pd.Timestamp(\"2025-04-09 04:00:00\")\n","tolerance = pd.Timedelta(hours=6) \n","best_config_index = -1 # Default\n","\n","if 'results_df_explore' in locals() and not results_df_explore.empty:\n","    # Convertir les colonnes de temps si elles ne sont pas déjà datetime (elles devraient l'être)\n","    results_df_explore['mic_s1_t_dt'] = pd.to_datetime(results_df_explore['micro_sup_p1'].apply(lambda x: x['time']))\n","    results_df_explore['mic_s2_t_dt'] = pd.to_datetime(results_df_explore['micro_sup_p2'].apply(lambda x: x['time']))\n","    \n","    target_configs_df = results_df_explore[\n","        (results_df_explore['mic_s1_t_dt'] >= target_p1_date - tolerance) & (results_df_explore['mic_s1_t_dt'] <= target_p1_date + tolerance) &\n","        (results_df_explore['mic_s2_t_dt'] >= target_p2_date - tolerance) & (results_df_explore['mic_s2_t_dt'] <= target_p2_date + tolerance)\n","    ]\n","    \n","    if not target_configs_df.empty:\n","        print(f\"\\n--- Configurations Cible Trouvées ---\")\n","        print(target_configs_df[['config_index', 'config_label', 'mic_s1_t', 'mic_s2_t']].to_string())\n","        best_config_index = target_configs_df.iloc[0]['config_index']\n","        print(f\"\\n==> Sélection auto de la 1ère config cible: Index {best_config_index} ({target_configs_df.iloc[0]['config_label']})\\n\")\n","        # Stocker la meilleure configuration trouvée\n","        final_config = configurations_to_explore[best_config_index]\n","    else:\n","        print(f\"\\n--- ATTENTION : Aucune config trouvée pour Support Micro cible ({target_p1_date} - {target_p2_date}) ---\")\n","        # Fallback: Utiliser la config par défaut ou la dernière ? Utilisons la config par défaut définie plus haut.\n","        best_config_index = -99 # Marqueur spécial\n","        final_config = { \"label\": \"Default_Config\", # Fallback\n","            \"wp_macro_res\": 2.0, \"rpf_macro_res\": 1.0, \"wp_macro_sup\": 2.0, \"rpf_macro_sup\": 1.0,\n","            \"wp_meso_res\" : 2.0, \"rpf_meso_res\" : 1.0, \"wp_meso_sup\" : 2.0, \"rpf_meso_sup\" : 1.0,\n","            \"wp_micro_res\": 2.0, \"rpf_micro_res\": 1.0, \"wp_micro_sup\": 2.0, \"rpf_micro_sup\": 1.0, }\n","        print(f\"WARN: Utilisation de la configuration par défaut: {final_config['label']}\")\n","        \n","else:\n","    print(\"results_df_explore non trouvé ou vide. Utilisation d'une config par défaut.\")\n","    best_config_index = -99\n","    final_config = { \"label\": \"Default_Config\", # Fallback\n","            \"wp_macro_res\": 2.0, \"rpf_macro_res\": 1.0, \"wp_macro_sup\": 2.0, \"rpf_macro_sup\": 1.0,\n","            # ... (autres params par défaut) ...\n","             }\n","\n","# --- Calcul Final avec la Meilleure Configuration ---\n","# (Optionnel ici, on peut le faire juste avant le plot 2D final si on veut séparer)\n","print(\"Recalculating channels with the selected final configuration...\")\n","final_channel_definitions = {\"macro\": {}, \"meso\": {}, \"micro\": {}}\n","# ... (Copier/Coller la logique de calcul complète pour UNE config, utilisant final_config) ...\n","# (Identique à la structure interne de la boucle dans la Cellule 15, mais sans boucle)\n","# --- Macro ---\n","try:\n","    macro_res = find_best_channel_line_strict_weighted(high_pivots, all_high_pivots_for_check, True, final_config['wp_macro_res'], final_config['rpf_macro_res'])\n","    macro_sup = find_best_channel_line_strict_weighted(low_pivots, all_low_pivots_for_check, False, final_config['wp_macro_sup'], final_config['rpf_macro_sup'])\n","    final_channel_definitions[\"macro\"][\"resistance\"] = macro_res if macro_res else (None, None)\n","    final_channel_definitions[\"macro\"][\"support\"] = macro_sup if macro_sup else (None, None)\n","except Exception as e: print(f\"ERROR Final Macro: {e}\")\n","# --- Meso ---\n","meso_start_time = None; macro_pivots_list = [p for pair in final_channel_definitions[\"macro\"].values() if pair and pair[0] is not None and pair[1] is not None for p in pair]\n","if len(macro_pivots_list) >= 2:\n","    try:\n","        meso_start_time = sorted([p['time'] for p in macro_pivots_list], reverse=True)[1]\n","        meso_high_f=high_pivots[high_pivots['time'] >= meso_start_time].copy(); meso_low_f=low_pivots[low_pivots['time'] >= meso_start_time].copy()\n","        meso_high_np=meso_high_f[['time_numeric', 'price']].values; meso_low_np = meso_low_f[['time_numeric', 'price']].values\n","        if len(meso_high_f) >= 2 and len(meso_low_f) >= 2:\n","             meso_res = find_best_channel_line_strict_weighted(meso_high_f, meso_high_np, True, final_config['wp_meso_res'], final_config['rpf_meso_res'])\n","             meso_sup = find_best_channel_line_strict_weighted(meso_low_f, meso_low_np, False, final_config['wp_meso_sup'], final_config['rpf_meso_sup'])\n","             final_channel_definitions[\"meso\"][\"resistance\"] = meso_res if meso_res else (None, None)\n","             final_channel_definitions[\"meso\"][\"support\"] = meso_sup if meso_sup else (None, None)\n","    except Exception as e: print(f\"ERROR Final Meso: {e}\")\n","# --- Micro ---\n","micro_start_time = None; meso_pivots_list = [p for pair in final_channel_definitions[\"meso\"].values() if pair and pair[0] is not None and pair[1] is not None for p in pair]\n","if len(meso_pivots_list) >= 2:\n","    try:\n","        micro_start_time = sorted([p['time'] for p in meso_pivots_list], reverse=True)[1]\n","        micro_high_f=high_pivots[high_pivots['time'] >= micro_start_time].copy(); micro_low_f=low_pivots[low_pivots['time'] >= micro_start_time].copy()\n","        micro_high_np=micro_high_f[['time_numeric', 'price']].values; micro_low_np=micro_low_f[['time_numeric', 'price']].values\n","        if len(micro_high_f) >= 2 and len(micro_low_f) >= 2:\n","             micro_res = find_best_channel_line_strict_weighted(micro_high_f, micro_high_np, True, final_config['wp_micro_res'], final_config['rpf_micro_res'])\n","             micro_sup = find_best_channel_line_strict_weighted(micro_low_f, micro_low_np, False, final_config['wp_micro_sup'], final_config['rpf_micro_sup'])\n","             final_channel_definitions[\"micro\"][\"resistance\"] = micro_res if micro_res else (None, None)\n","             final_channel_definitions[\"micro\"][\"support\"] = micro_sup if micro_sup else (None, None)\n","    except Exception as e: print(f\"ERROR Final Micro: {e}\")\n","\n","print(\"\\nFinal channels calculated using selected configuration.\")"]},{"cell_type":"markdown","metadata":{},"source":["### 5.3. Visualisation Finale (Configuration Cible)\n","\n","Cette section utilise Plotly pour générer une visualisation complète intégrant :\n","\n","*   Le **prix** de clôture horaire.\n","*   Les **pivots** ZigZag (High/Low).\n","*   Les **lignes de connexion** du ZigZag.\n","*   Les **canaux Macro, Méso et Micro** (support et résistance) calculés en utilisant la configuration sélectionnée (`final_config`).\n","\n","**Caractéristiques du Tracé :**\n","\n","*   Niveaux de canal distincts par couleur/style.\n","*   Lignes étendues jusqu'à la fin des données.\n","*   Pivots définissant chaque ligne mis en évidence (marqueurs étoiles)."]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["import plotly.graph_objects as go\n","\n","def plot_channels_and_zigzag(bars_df, pivotList, channel_definitions):\n","    \n","    fig = go.Figure()\n","\n","    # A) Plot raw price\n","    fig.add_trace(go.Scatter(\n","        x=bars_df['time'],\n","        y=bars_df['close'],\n","        mode='lines',\n","        name='Close Price',\n","        line=dict(color='rgba(100, 100, 200, 0.8)') # Light blue\n","    ))\n","\n","    # B) Plot ZigZag lines connecting pivots\n","    xline = [p[0] for p in pivotList]\n","    yline = [p[1] for p in pivotList]\n","    fig.add_trace(go.Scatter(\n","        x=xline, \n","        y=yline,\n","        mode='lines',\n","        name='ZigZag Line',\n","        line=dict(color='orange', dash='dot')\n","    ))\n","\n","    # C) Plot High/Low pivot markers\n","    high_x = [p[0] for p in pivotList if p[2] < 0]\n","    high_y = [p[1] for p in pivotList if p[2] < 0]\n","    low_x  = [p[0] for p in pivotList if p[2] > 0]\n","    low_y  = [p[1] for p in pivotList if p[2] > 0]\n","\n","    fig.add_trace(go.Scatter(\n","        x=high_x, y=high_y, mode='markers',\n","        marker=dict(color='red', size=7, symbol='diamond'), name='High Pivot'\n","    ))\n","    fig.add_trace(go.Scatter(\n","        x=low_x, y=low_y, mode='markers',\n","        marker=dict(color='green', size=7, symbol='circle'), name='Low Pivot'\n","    ))\n","\n","    # D) Plot Channel Lines (Revised for Extension)\n","    channel_colors = {\n","        \"macro\": \"rgba(60, 60, 60, 0.8)\", # Dark Grey\n","        \"meso\": \"rgba(0, 0, 255, 0.6)\",   # Blue\n","        \"micro\": \"rgba(255, 0, 255, 0.7)\" # Magenta\n","    }\n","    channel_styles = {\n","        \"macro\": \"solid\",\n","        \"meso\": \"dash\",\n","        \"micro\": \"dot\"\n","    }\n","    # Get the overall time range for extending lines\n","    plot_start_time = bars_df['time'].iloc[0]\n","    plot_end_time = bars_df['time'].iloc[-1]\n","    plot_start_time_num = plot_start_time.timestamp()\n","    plot_end_time_num = plot_end_time.timestamp()\n","    for name, definition in channel_definitions.items():\n","        color = channel_colors[name]\n","        style = channel_styles[name]\n","        for line_type in [\"resistance\", \"support\"]:\n","            p1, p2 = definition[line_type]\n","            if p1 is not None and p2 is not None:\n","                # Calculate line parameters using time_numeric\n","                m, c = get_line_params_time(p1['time_numeric'], p1['price'], p2['time_numeric'], p2['price'])\n","                # Calculate line extension points for plotting\n","                # Start from the time of the first defining pivot\n","                line_plot_start_time = p1['time']\n","                line_plot_start_time_num = p1['time_numeric']\n","                line_plot_start_y = p1['price'] # More accurately: m * line_plot_start_time_num + c\n","                # End at the time of the last data point in the plot\n","                line_plot_end_time = plot_end_time\n","                line_plot_end_time_num = plot_end_time_num\n","                line_plot_end_y = m * line_plot_end_time_num + c\n","                # Add the extended line trace\n","                fig.add_trace(go.Scatter(\n","                    x=[line_plot_start_time, line_plot_end_time],\n","                    y=[line_plot_start_y, line_plot_end_y],\n","                    mode='lines',\n","                    name=f'{name.capitalize()} {line_type.capitalize()}',\n","                    line=dict(color=color, width=2, dash=style)\n","                ))\n","                # Highlight the defining pivots\n","                fig.add_trace(go.Scatter(\n","                    x=[p1['time'], p2['time']],\n","                    y=[p1['price'], p2['price']],\n","                    mode='markers',\n","                    marker=dict(color=color, size=9,\n","                                symbol='star' if line_type == 'resistance' else 'star-open'),\n","                    showlegend=False\n","                ))\n","    fig.update_layout(\n","        title=\"BTCUSDT Hourly - ZigZag Pivots with Macro/Meso/Micro Channels (Hierarchical Envelope)\",\n","        xaxis_title=\"Time\",\n","        yaxis_title=\"Price (USDT)\",\n","        xaxis_rangeslider_visible=False\n","    )\n","    fig.show()\n","\n","\n","# --- Exécuter le Plotting avec la configuration FINALE ---\n","\n","# S'assurer que final_channel_definitions existe et contient les bons canaux\n","if 'final_channel_definitions' in locals():\n","    plot_channels_and_zigzag(bars_df, pivotList, final_channel_definitions)\n","    print(\"Plotting avec la configuration finale terminée.\")\n","else:\n","    print(\"ERREUR: `final_channel_definitions` non trouvées. Exécuter la cellule 16 pour recalculer les canaux finaux.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### 5.4. Visualisation Comparative (Exploration Paramètres)\n","\n","Ce graphique utilise une grille de sous-graphiques (subplots) pour visualiser comment les canaux **Macro, Méso et Micro** varient en fonction des **différentes configurations testées** dans `configurations_to_explore`.\n","\n","**Organisation de la Grille :**\n","\n","*   Chaque sous-graphique correspond à une configuration testée.\n","*   Le titre indique les paramètres variables (ex: `wp_micro_sup`, `rpf_micro_sup`).\n","*   Chaque sous-graphique **zoome automatiquement** sur une fenêtre temporelle (X) et une plage de prix (Y) déterminées par les **pivots définissant le canal Méso** de cette configuration spécifique, pour mieux voir l'impact local des paramètres.\n","*   Tous les canaux (Macro, Méso, Micro) et pivots ZigZag pertinents sont affichés dans chaque sous-graphique zoomé.\n","\n","Aide à comprendre la **stabilité** et la **sensibilité** de l'algorithme aux variations des paramètres `wp` et `rpf`."]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["# CELLULE 28 : Visualisation 2D (Grille 4x4) Exploration - Tous Canaux, Zoom X/Y sur Pivots Meso par Config\n","\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import pandas as pd\n","import numpy as np\n","import math\n","\n","# --- Pré-requis ---\n","if 'results_df_explore' not in locals() or results_df_explore.empty:\n","    print(\"ERREUR: results_df_explore non disponible pour la visualisation.\")\n","elif 'get_line_params_time' not in locals():\n","    print(\"ERREUR: La fonction get_line_params_time n'est pas définie.\")\n","elif 'bars_df' not in locals():\n","     print(\"ERREUR: `bars_df` non disponible pour tracer le prix.\")\n","elif 'pivotList' not in locals():\n","     print(\"ERREUR: `pivotList` global non disponible pour tracer les pivots.\")\n","else:\n","    # --- MODIFICATION : Sélectionner N premières configurations (16 pour une grille 4x4) ---\n","    n_configs_to_plot = 16 # <--- Changé de 9 à 16\n","    configs_to_plot = results_df_explore.head(n_configs_to_plot).copy()\n","    if len(configs_to_plot) < 1: # Vérifier s'il y a au moins une config\n","         print(\"ERREUR: Aucune configuration à plotter.\")\n","         configs_to_plot = None # Marqueur pour ne pas continuer\n","    elif len(configs_to_plot) < n_configs_to_plot:\n","        print(f\"WARN: Moins de {n_configs_to_plot} configurations disponibles ({len(configs_to_plot)}). Affichage d'une grille partielle.\")\n","        n_configs_to_plot = len(configs_to_plot) # Ajuster le nombre réel\n","\n","if 'configs_to_plot' in locals() and configs_to_plot is not None:\n","    cols = 4 # <--- Changé de 3 à 4\n","    rows = math.ceil(n_configs_to_plot / cols)\n","    subplot_titles = [row['config_label'] for index, row in configs_to_plot.iterrows()]\n","\n","    # S'assurer qu'il y a assez de titres pour la grille (remplir si besoin)\n","    subplot_titles.extend([''] * (rows * cols - len(subplot_titles)))\n","\n","    fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles,\n","                        shared_xaxes=False, # Axe X indépendant\n","                        shared_yaxes=False, # Axe Y indépendant\n","                        vertical_spacing=0.06, horizontal_spacing=0.04) # Espacement ajusté\n","\n","    channel_colors = {\"macro\": \"rgba(60, 60, 60, 0.8)\", \"meso\": \"rgba(0, 0, 255, 0.6)\", \"micro\": \"rgba(255, 0, 255, 0.7)\"}\n","    channel_styles = {\"macro\": \"solid\", \"meso\": \"dash\", \"micro\": \"dot\"}\n","\n","    # Pivots globaux (ceux calculés sur tout l'historique initialement)\n","    high_x_all = [p[0] for p in pivotList if p[2] < 0]\n","    high_y_all = [p[1] for p in pivotList if p[2] < 0]\n","    low_x_all  = [p[0] for p in pivotList if p[2] > 0]\n","    low_y_all  = [p[1] for p in pivotList if p[2] > 0]\n","\n","    # --- Itérer sur les configurations SÉLECTIONNÉES ---\n","    for plot_index, (index, row_data) in enumerate(configs_to_plot.iterrows()):\n","        row_idx = (plot_index // cols) + 1\n","        col_idx = (plot_index % cols) + 1\n","\n","        config_label = row_data['config_label']\n","\n","        # 1. Récupérer les canaux pour CETTE configuration\n","        channels_for_this_config = {\n","            \"macro\": {\"resistance\": (row_data[\"macro_res_p1\"], row_data[\"macro_res_p2\"]), \"support\": (row_data[\"macro_sup_p1\"], row_data[\"macro_sup_p2\"])},\n","            \"meso\": {\"resistance\": (row_data[\"meso_res_p1\"], row_data[\"meso_res_p2\"]), \"support\": (row_data[\"meso_sup_p1\"], row_data[\"meso_sup_p2\"])},\n","            \"micro\": {\"resistance\": (row_data[\"micro_res_p1\"], row_data[\"micro_res_p2\"]), \"support\": (row_data[\"micro_sup_p1\"], row_data[\"micro_sup_p2\"])}\n","        }\n","\n","        # 2. Calculer la plage X et Y basée sur les pivots Meso de CETTE config\n","        meso_pivots_data = []\n","        yaxis_range_subplot = None\n","        xaxis_range_subplot = None\n","\n","        meso_def = channels_for_this_config.get(\"meso\", {})\n","        meso_res_p1_info, meso_res_p2_info = meso_def.get(\"resistance\", ({},{}))\n","        meso_sup_p1_info, meso_sup_p2_info = meso_def.get(\"support\", ({},{}))\n","\n","        for p_info in [meso_res_p1_info, meso_res_p2_info, meso_sup_p1_info, meso_sup_p2_info]:\n","            if isinstance(p_info, dict) and not pd.isna(p_info.get('time')) and not pd.isna(p_info.get('price')):\n","                meso_pivots_data.append({'time': pd.to_datetime(p_info['time']), 'price': p_info['price']})\n","\n","        if len(meso_pivots_data) >= 2:\n","            meso_pivots_df = pd.DataFrame(meso_pivots_data)\n","            min_p_meso = meso_pivots_df['price'].min()\n","            max_p_meso = meso_pivots_df['price'].max()\n","            min_t_meso = meso_pivots_df['time'].min()\n","            max_t_meso = meso_pivots_df['time'].max()\n","\n","            # Calcul Plage Y\n","            height_meso = max_p_meso - min_p_meso\n","            margin_y = max(height_meso * 0.30, (min_p_meso + max_p_meso)/2 * 0.05)\n","            yaxis_range_subplot = [min_p_meso - margin_y, max_p_meso + margin_y]\n","\n","            # Calcul Plage X\n","            duration_meso = max_t_meso - min_t_meso if max_t_meso > min_t_meso else pd.Timedelta(days=1)\n","            margin_t_before = max(duration_meso * 0.5, pd.Timedelta(days=60))\n","            margin_t_after = max(duration_meso * 0.2, pd.Timedelta(days=30))\n","            xaxis_range_subplot = [min_t_meso - margin_t_before, max_t_meso + margin_t_after]\n","        else:\n","            # Fallback si pas de pivots meso trouvés pour cette config\n","            if not bars_df.empty:\n","                fallback_end_time = bars_df['time'].iloc[-1]\n","                fallback_start_time = fallback_end_time - pd.Timedelta(days=180) # Lookback par défaut\n","                xaxis_range_subplot = [fallback_start_time, fallback_end_time]\n","\n","\n","        # 3. Filtrer les données à afficher dans cette plage temporelle\n","        if xaxis_range_subplot:\n","            bars_subplot = bars_df[(bars_df['time'] >= xaxis_range_subplot[0]) & (bars_df['time'] <= xaxis_range_subplot[1])].copy()\n","            high_x_plot = [t for t in high_x_all if t >= xaxis_range_subplot[0] and t <= xaxis_range_subplot[1]]\n","            high_y_plot = [high_y_all[i] for i, t in enumerate(high_x_all) if t >= xaxis_range_subplot[0] and t <= xaxis_range_subplot[1]]\n","            low_x_plot = [t for t in low_x_all if t >= xaxis_range_subplot[0] and t <= xaxis_range_subplot[1]]\n","            low_y_plot = [low_y_all[i] for i, t in enumerate(low_x_all) if t >= xaxis_range_subplot[0] and t <= xaxis_range_subplot[1]]\n","        else: # Si pas de plage X définie (ne devrait pas arriver avec le fallback)\n","            bars_subplot = bars_df.copy()\n","            high_x_plot, high_y_plot = high_x_all, high_y_all\n","            low_x_plot, low_y_plot = low_x_all, low_y_all\n","            if not bars_subplot.empty:\n","                 xaxis_range_subplot = [bars_subplot['time'].min(), bars_subplot['time'].max()] # Fallback range X\n","\n","        # Fallback pour Y si toujours None\n","        if yaxis_range_subplot is None and not bars_subplot.empty:\n","            min_p = bars_subplot['low'].min()\n","            max_p = bars_subplot['high'].max()\n","            margin_y = (max_p - min_p) * 0.1\n","            yaxis_range_subplot = [min_p - margin_y, max_p + margin_y]\n","\n","        # 4. Tracer le prix (section filtrée)\n","        if not bars_subplot.empty:\n","            fig.add_trace(go.Scatter(x=bars_subplot['time'], y=bars_subplot['close'], mode='lines',\n","                                     line=dict(color='rgba(150, 150, 150, 0.5)'), name='Close',\n","                                     showlegend=False),\n","                          row=row_idx, col=col_idx)\n","\n","        # 5. Tracer les pivots globaux (section filtrée)\n","        fig.add_trace(go.Scatter(x=high_x_plot, y=high_y_plot, mode='markers', name='High Pivot',\n","                                 marker=dict(color='red', size=4, symbol='diamond-open'), showlegend=(plot_index==0)), # Légende 1 fois\n","                      row=row_idx, col=col_idx)\n","        fig.add_trace(go.Scatter(x=low_x_plot, y=low_y_plot, mode='markers', name='Low Pivot',\n","                                 marker=dict(color='green', size=4, symbol='circle-open'), showlegend=(plot_index==0)), # Légende 1 fois\n","                      row=row_idx, col=col_idx)\n","\n","\n","        # 6. Tracer les 3 canaux pour CETTE configuration\n","        if not bars_subplot.empty:\n","            plot_start_time_dt_eff = bars_subplot['time'].min()\n","            plot_end_time_dt_eff = bars_subplot['time'].max()\n","            plot_start_time_num_eff = plot_start_time_dt_eff.timestamp()\n","            plot_end_time_num_eff = plot_end_time_dt_eff.timestamp()\n","\n","            for channel_name, definition in channels_for_this_config.items():\n","                for line_type in [\"resistance\", \"support\"]:\n","                    p1_info, p2_info = definition.get(line_type, ({}, {}))\n","\n","                    if (isinstance(p1_info, dict) and isinstance(p2_info, dict) and\n","                        not pd.isna(p1_info.get('time')) and not pd.isna(p2_info.get('time')) and\n","                        not pd.isna(p1_info.get('time_numeric')) and not pd.isna(p2_info.get('time_numeric')) and\n","                        not pd.isna(p1_info.get('price')) and not pd.isna(p2_info.get('price'))):\n","\n","                        p1_time_num = p1_info['time_numeric']\n","                        p2_time_num = p2_info['time_numeric']\n","                        p1_price = p1_info['price']\n","                        p2_price = p2_info['price']\n","                        p1_time = pd.to_datetime(p1_info['time'])\n","                        p2_time = pd.to_datetime(p2_info['time'])\n","\n","                        try:\n","                            m, c = get_line_params_time(p1_time_num, p1_price, p2_time_num, p2_price)\n","                            if m != np.inf:\n","                                # Calculer les points Y aux extrémités de la FENETRE VISIBLE\n","                                line_start_y_plot = m * plot_start_time_num_eff + c\n","                                line_end_y_plot = m * plot_end_time_num_eff + c\n","\n","                                fig.add_trace(go.Scatter(\n","                                    x=[plot_start_time_dt_eff, plot_end_time_dt_eff], y=[line_start_y_plot, line_end_y_plot],\n","                                    mode='lines', name=f\"{channel_name.capitalize()} {line_type.capitalize()}\",\n","                                    line=dict(color=channel_colors[channel_name], width=1.5, dash=channel_styles[channel_name]),\n","                                    legendgroup=f\"{channel_name}_{line_type}\",\n","                                    showlegend=(plot_index==0) # Légende juste pour le premier subplot\n","                                ), row=row_idx, col=col_idx)\n","\n","                                # Marquer les pivots utilisés (plus gros) s'ils sont visibles\n","                                pivots_x_to_mark = []\n","                                pivots_y_to_mark = []\n","                                if xaxis_range_subplot and p1_time >= xaxis_range_subplot[0] and p1_time <= xaxis_range_subplot[1]:\n","                                    pivots_x_to_mark.append(p1_time)\n","                                    pivots_y_to_mark.append(p1_price)\n","                                if xaxis_range_subplot and p2_time >= xaxis_range_subplot[0] and p2_time <= xaxis_range_subplot[1]:\n","                                    pivots_x_to_mark.append(p2_time)\n","                                    pivots_y_to_mark.append(p2_price)\n","\n","                                if pivots_x_to_mark:\n","                                    fig.add_trace(go.Scatter(\n","                                        x=pivots_x_to_mark, y=pivots_y_to_mark,\n","                                        mode='markers', marker=dict(color=channel_colors[channel_name], size=7,\n","                                                                      symbol='star' if line_type == 'resistance' else 'star-open'),\n","                                        showlegend=False, hoverinfo='skip'\n","                                    ), row=row_idx, col=col_idx)\n","                        except Exception as e: print(f\"Error plotting line {channel_name} {line_type} for {config_label}: {e}\")\n","\n","        # Appliquer les zooms X et Y calculés pour ce subplot\n","        if yaxis_range_subplot: fig.update_yaxes(range=yaxis_range_subplot, row=row_idx, col=col_idx)\n","        if xaxis_range_subplot: fig.update_xaxes(range=xaxis_range_subplot, row=row_idx, col=col_idx)\n","\n","        # Masquer les ticks X/Y inutiles\n","        if row_idx < rows: fig.update_xaxes(showticklabels=False, row=row_idx, col=col_idx)\n","        if col_idx > 1: fig.update_yaxes(showticklabels=False, row=row_idx, col=col_idx)\n","\n","\n","    # --- Configurer Layout Final ---\n","    fig.update_layout(\n","        title=f'Exploration Paramètres ({n_configs_to_plot} Configs): Canaux & Zoom X/Y sur Pivots Meso',\n","        height=250 * rows + 80, # Ajuster hauteur pour 4 lignes\n","        width=1200, # Un peu plus large pour 4 colonnes\n","        hovermode='x unified',\n","        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.05, xanchor=\"center\", x=0.5) # Légende un peu plus bas\n","    )\n","    # Mettre à jour la taille des titres des subplots\n","    for i, annot in enumerate(fig.layout.annotations):\n","        if i < n_configs_to_plot: # Seulement pour les subplots utilisés\n","            annot.update(font=dict(size=9))\n","\n","    fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Analyse Temporelle : Snapshots Historiques\n","\n","Pour évaluer la robustesse et l'évolution des canaux dans le temps, nous recalculons l'ensemble de la structure de canaux (Macro, Méso, Micro) tels qu'ils auraient été tracés à différentes **dates passées (\"snapshots\")**, en utilisant la **configuration fixe** `final_config`.\n","\n","### 6.1. Calcul des Snapshots\n","\n","**Processus :**\n","\n","1.  **Dates de Snapshot :** Une liste de dates historiques est définie (ex: fin de mois).\n","2.  **Configuration Fixe :** `final_config` est utilisée pour tous les calculs.\n","3.  **Calcul Itératif :** Pour chaque date de snapshot :\n","    *   L'historique de prix *jusqu'à cette date* est utilisé.\n","    *   Les pivots ZigZag sont recalculés sur cet historique tronqué.\n","    *   Les canaux Macro, Méso et Micro sont entièrement recalculés (avec la logique de **fallback** si nécessaire pour Meso/Micro) en utilisant les paramètres de `final_config`.\n","4.  **Stockage :** Les pivots et définitions de canaux résultants (`snapshot_results`) sont stockés pour chaque date.\n","\n","Permet de vérifier si les canaux récents étaient déjà présents ou similaires dans le passé."]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["# CELLULE HELPER (MODIFIÉE) : Fonction pour Calculer Tous les Canaux à une Date Donnée AVEC FALLBACK\n","\n","import pandas as pd\n","import numpy as np\n","import math\n","import traceback # Pour le débogage si besoin\n","\n","# Assurer que les dépendances sont chargées (elles devraient l'être par les cellules précédentes)\n","if 'classic_chart_zigzag' not in locals(): raise NameError(\"classic_chart_zigzag non définie\")\n","if 'find_best_channel_line_strict_weighted' not in locals(): raise NameError(\"find_best_channel_line_strict_weighted non définie\")\n","if 'get_pivot_info_snap' not in locals(): # S'assurer que la version snapshot est définie\n","    def get_pivot_info_snap(pivot_series):\n","         if pivot_series is None or not isinstance(pivot_series, pd.Series): return {'time': pd.NaT, 'price': np.nan, 'time_numeric': np.nan, 'original_index': -1}\n","         time_numeric_val = pivot_series.get('time_numeric', np.nan)\n","         if pd.isna(time_numeric_val) and not pd.isna(pivot_series.get('time')):\n","             try:\n","                 dt_obj = pivot_series['time']\n","                 if isinstance(dt_obj, pd.Timestamp): dt_obj = dt_obj.to_pydatetime()\n","                 time_numeric_val = dt_obj.timestamp()\n","             except Exception as e: time_numeric_val = np.nan\n","         return {'time': pivot_series.get('time', pd.NaT), 'price': pivot_series.get('price', np.nan), 'time_numeric': time_numeric_val, 'original_index': pivot_series.name if hasattr(pivot_series, 'name') else -1}\n","\n","def calculate_all_channels_at_date(end_date_dt, full_history_df, config, zigzag_threshold=0.05):\n","    \"\"\"\n","    Calcule les pivots ZigZag et les canaux Macro, Meso, Micro avec logique de fallback\n","    pour les dates de début de Meso et Micro.\n","    \"\"\"\n","    try:\n","        # 1. Filtrer l'historique\n","        hist_for_calc = full_history_df[full_history_df['time'] <= end_date_dt].copy()\n","        if len(hist_for_calc) < 5: return None, None\n","\n","        # 2. Calculer les pivots ZigZag\n","        snap_pivots_list = classic_chart_zigzag(hist_for_calc[['time','close']], thresholdPercent=zigzag_threshold)\n","        if not snap_pivots_list or len(snap_pivots_list) < 2: return snap_pivots_list, None\n","\n","        snap_pivots_df = pd.DataFrame(snap_pivots_list, columns=['time', 'price', 'type'])\n","        snap_pivots_df['time_numeric'] = snap_pivots_df['time'].apply(lambda x: x.timestamp())\n","        snap_high_pivots = snap_pivots_df[snap_pivots_df['type'] == -1].copy()\n","        snap_low_pivots = snap_pivots_df[snap_pivots_df['type'] == +1].copy()\n","        snap_all_high_np = snap_high_pivots[['time_numeric', 'price']].values\n","        snap_all_low_np = snap_low_pivots[['time_numeric', 'price']].values\n","\n","        if len(snap_high_pivots) < 2 or len(snap_low_pivots) < 2: return snap_pivots_list, None\n","\n","        # 3. Calculer les canaux hiérarchiques\n","        channels = {\"macro\": {}, \"meso\": {}, \"micro\": {}}\n","        # Initialiser avec None pour faciliter la détection d'échec\n","        for scale in channels:\n","            channels[scale][\"resistance\"] = (None, None)\n","            channels[scale][\"support\"] = (None, None)\n","\n","        # --- Macro ---\n","        macro_success = False\n","        try:\n","            res1_m, res2_m = find_best_channel_line_strict_weighted(snap_high_pivots, snap_all_high_np, True, config['wp_macro_res'], config['rpf_macro_res'])\n","            sup1_m, sup2_m = find_best_channel_line_strict_weighted(snap_low_pivots, snap_all_low_np, False, config['wp_macro_sup'], config['rpf_macro_sup'])\n","            # Stocker les infos seulement si les deux pivots sont trouvés\n","            if res1_m is not None and res2_m is not None:\n","                 channels[\"macro\"][\"resistance\"] = (get_pivot_info_snap(res1_m), get_pivot_info_snap(res2_m))\n","            if sup1_m is not None and sup2_m is not None:\n","                 channels[\"macro\"][\"support\"] = (get_pivot_info_snap(sup1_m), get_pivot_info_snap(sup2_m))\n","            # Macro est un succès si AU MOINS un support ET une résistance sont définis\n","            if channels[\"macro\"][\"resistance\"][0] is not None and channels[\"macro\"][\"support\"][0] is not None:\n","                macro_success = True\n","        except Exception as e_macro: pass\n","\n","        # --- Meso (avec Fallback) ---\n","        meso_success = False\n","        if macro_success:\n","            macro_pivots_info = [\n","                channels[\"macro\"][\"resistance\"][0], channels[\"macro\"][\"resistance\"][1],\n","                channels[\"macro\"][\"support\"][0], channels[\"macro\"][\"support\"][1]\n","            ]\n","            # Filtrer les pivots valides (non None et avec une date)\n","            valid_macro_pivots = [p for p in macro_pivots_info if p and not pd.isna(p.get('time'))]\n","            valid_macro_pivots.sort(key=lambda p: pd.to_datetime(p['time']), reverse=True) # Trier par date décroissante\n","\n","            meso_start_time = None\n","            # Essai 1: 2ème pivot le plus récent\n","            if len(valid_macro_pivots) >= 2:\n","                meso_start_time = pd.to_datetime(valid_macro_pivots[1]['time']) # 2ème plus récent\n","\n","            # Essai 2 (Fallback): 3ème pivot le plus récent (si essai 1 échoue et si assez de pivots)\n","            attempted_fallback_meso = False\n","            if meso_start_time is not None:\n","                meso_high_f = snap_high_pivots[snap_high_pivots['time'] >= meso_start_time].copy()\n","                meso_low_f = snap_low_pivots[snap_low_pivots['time'] >= meso_start_time].copy()\n","                # Si pas assez de pivots après le 2ème, et si on a au moins 4 pivots macro pour tenter le 3ème\n","                if (len(meso_high_f) < 2 or len(meso_low_f) < 2) and len(valid_macro_pivots) >= 4:\n","                    meso_start_time = pd.to_datetime(valid_macro_pivots[2]['time']) # 3ème plus récent\n","                    attempted_fallback_meso = True\n","            elif len(valid_macro_pivots) >= 3: # Si l'essai 1 n'a même pas pu avoir lieu (moins de 2 pivots) mais qu'on en a 3 ou 4\n","                 meso_start_time = pd.to_datetime(valid_macro_pivots[2]['time']) # Essayer directement le 3ème\n","                 attempted_fallback_meso = True\n","\n","\n","            # Calcul Meso si une date de début valide a été trouvée\n","            if meso_start_time is not None:\n","                 try:\n","                    meso_high_f = snap_high_pivots[snap_high_pivots['time'] >= meso_start_time].copy()\n","                    meso_low_f = snap_low_pivots[snap_low_pivots['time'] >= meso_start_time].copy()\n","                    if len(meso_high_f) >= 2 and len(meso_low_f) >= 2:\n","                        meso_high_np = meso_high_f[['time_numeric', 'price']].values\n","                        meso_low_np = meso_low_f[['time_numeric', 'price']].values\n","                        res1_me, res2_me = find_best_channel_line_strict_weighted(meso_high_f, meso_high_np, True, config['wp_meso_res'], config['rpf_meso_res'])\n","                        sup1_me, sup2_me = find_best_channel_line_strict_weighted(meso_low_f, meso_low_np, False, config['wp_meso_sup'], config['rpf_meso_sup'])\n","                        if res1_me is not None and res2_me is not None:\n","                            channels[\"meso\"][\"resistance\"] = (get_pivot_info_snap(res1_me), get_pivot_info_snap(res2_me))\n","                        if sup1_me is not None and sup2_me is not None:\n","                            channels[\"meso\"][\"support\"] = (get_pivot_info_snap(sup1_me), get_pivot_info_snap(sup2_me))\n","                        # Meso succès si résistance ET support trouvés\n","                        if channels[\"meso\"][\"resistance\"][0] is not None and channels[\"meso\"][\"support\"][0] is not None:\n","                             meso_success = True\n","                             # print(f\"DEBUG [{end_date_dt.date()}] Meso OK {'(Fallback)' if attempted_fallback_meso else ''}\")\n","                    # else:\n","                         # print(f\"DEBUG [{end_date_dt.date()}] Meso FAIL: Not enough pivots after start {'(Fallback)' if attempted_fallback_meso else ''}\")\n","\n","                 except Exception as e_meso: pass # print(f\"DEBUG [{end_date_dt.date()}] Meso Calc Error: {e_meso}\")\n","            # else:\n","                 # print(f\"DEBUG [{end_date_dt.date()}] Meso FAIL: No valid start time found.\")\n","\n","\n","        # --- Micro (avec Fallback) ---\n","        # Note: On ne définit pas micro_success, on calcule juste si possible\n","        if meso_success: # Micro ne peut exister que si Meso a réussi\n","            meso_pivots_info = [\n","                channels[\"meso\"][\"resistance\"][0], channels[\"meso\"][\"resistance\"][1],\n","                channels[\"meso\"][\"support\"][0], channels[\"meso\"][\"support\"][1]\n","            ]\n","            valid_meso_pivots = [p for p in meso_pivots_info if p and not pd.isna(p.get('time'))]\n","            valid_meso_pivots.sort(key=lambda p: pd.to_datetime(p['time']), reverse=True)\n","\n","            micro_start_time = None\n","            # Essai 1: 2ème pivot meso le plus récent\n","            if len(valid_meso_pivots) >= 2:\n","                micro_start_time = pd.to_datetime(valid_meso_pivots[1]['time'])\n","\n","            # Essai 2 (Fallback): 3ème pivot meso le plus récent\n","            attempted_fallback_micro = False\n","            if micro_start_time is not None:\n","                 micro_high_f_test = snap_high_pivots[snap_high_pivots['time'] >= micro_start_time].copy()\n","                 micro_low_f_test = snap_low_pivots[snap_low_pivots['time'] >= micro_start_time].copy()\n","                 if (len(micro_high_f_test) < 2 or len(micro_low_f_test) < 2) and len(valid_meso_pivots) >= 4:\n","                      micro_start_time = pd.to_datetime(valid_meso_pivots[2]['time'])\n","                      attempted_fallback_micro = True\n","            elif len(valid_meso_pivots) >= 3:\n","                 micro_start_time = pd.to_datetime(valid_meso_pivots[2]['time'])\n","                 attempted_fallback_micro = True\n","\n","\n","            # Calcul Micro si une date de début valide a été trouvée\n","            if micro_start_time is not None:\n","                try:\n","                    micro_high_f = snap_high_pivots[snap_high_pivots['time'] >= micro_start_time].copy()\n","                    micro_low_f = snap_low_pivots[snap_low_pivots['time'] >= micro_start_time].copy()\n","                    if len(micro_high_f) >= 2 and len(micro_low_f) >= 2:\n","                        micro_high_np = micro_high_f[['time_numeric', 'price']].values\n","                        micro_low_np = micro_low_f[['time_numeric', 'price']].values\n","                        res1_mi, res2_mi = find_best_channel_line_strict_weighted(micro_high_f, micro_high_np, True, config['wp_micro_res'], config['rpf_micro_res'])\n","                        sup1_mi, sup2_mi = find_best_channel_line_strict_weighted(micro_low_f, micro_low_np, False, config['wp_micro_sup'], config['rpf_micro_sup'])\n","                        if res1_mi is not None and res2_mi is not None:\n","                             channels[\"micro\"][\"resistance\"] = (get_pivot_info_snap(res1_mi), get_pivot_info_snap(res2_mi))\n","                        if sup1_mi is not None and sup2_mi is not None:\n","                            channels[\"micro\"][\"support\"] = (get_pivot_info_snap(sup1_mi), get_pivot_info_snap(sup2_mi))\n","                        # if channels[\"micro\"][\"resistance\"][0] or channels[\"micro\"][\"support\"][0]:\n","                        #     print(f\"DEBUG [{end_date_dt.date()}] Micro OK {'(Fallback)' if attempted_fallback_micro else ''}\")\n","                    # else:\n","                        # print(f\"DEBUG [{end_date_dt.date()}] Micro FAIL: Not enough pivots after start {'(Fallback)' if attempted_fallback_micro else ''}\")\n","\n","                except Exception as e_micro: pass # print(f\"DEBUG [{end_date_dt.date()}] Micro Calc Error: {e_micro}\")\n","            # else:\n","                # print(f\"DEBUG [{end_date_dt.date()}] Micro FAIL: No valid start time found.\")\n","\n","        return snap_pivots_list, channels\n","\n","    except Exception as e:\n","        print(f\"ERREUR Majeure dans calculate_all_channels_at_date pour {end_date_dt}: {e}\")\n","        # traceback.print_exc()\n","        return None, None\n"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["# CELLULE 29 : Calcul des Snapshots Historiques (Révisée)\n","\n","import pandas as pd\n","import numpy as np\n","import math\n","from datetime import datetime, timedelta\n","from tqdm.notebook import tqdm\n","import traceback # Importé pour le debug au besoin\n","\n","# --- Vérifier les dépendances clés ---\n","if 'bars_df' not in locals(): raise NameError(\"bars_df non défini\")\n","if 'final_config' not in locals(): raise NameError(\"final_config non définie\")\n","if 'calculate_all_channels_at_date' not in locals() or not callable(calculate_all_channels_at_date):\n","    raise NameError(\"Fonction 'calculate_all_channels_at_date' non définie ou non appelable.\")\n","\n","# --- Dates de Snapshot (Logique de génération inchangée) ---\n","data_tz = bars_df['time'].dt.tz # Récupérer le timezone des données sources\n","\n","# Utiliser une date de début pour les snapshots (ex: 1 an avant la fin des données ou date spécifique)\n","snapshot_start_calc_date = bars_df['time'].iloc[0] # Ou une date spécifique: pd.Timestamp(\"2024-01-01\", tz=data_tz)\n","if snapshot_start_calc_date > bars_df['time'].iloc[-1] - pd.Timedelta(days=60): # S'assurer qu'on a au moins 2 mois de snapshots\n","     snapshot_start_calc_date = bars_df['time'].iloc[-1] - pd.Timedelta(days=60)\n","\n","last_hist_date = bars_df['time'].iloc[-1]\n","\n","# Générer les dates de fin de mois (ou autre fréquence)\n","freq_snapshots = 'M' # 'M'=Fin de Mois, 'W'=Fin de Semaine, 'D'=Jour etc.\n","print(f\"Génération des dates de snapshots (freq='{freq_snapshots}') entre {snapshot_start_calc_date.date()} et {last_hist_date.date()}\")\n","try:\n","    # Générer les dates de fin de période\n","    snapshot_dates_gen = pd.date_range(\n","        start=snapshot_start_calc_date,\n","        end=last_hist_date,\n","        freq=freq_snapshots,\n","        tz=data_tz # Appliquer le timezone des données sources\n","    )\n","    # S'assurer que les dates générées ne dépassent pas la dernière date historique\n","    snapshot_dates_gen = snapshot_dates_gen[snapshot_dates_gen <= last_hist_date]\n","\n","except Exception as e:\n","    print(f\"ERREUR lors de la génération de date_range: {e}\")\n","    snapshot_dates = pd.DatetimeIndex([]) # Créer un index vide pour éviter erreur aval\n","    # raise # Optionnel: arrêter l'exécution ici\n","\n","if not snapshot_dates_gen.empty:\n","     # Ajouter la toute dernière date de l'historique si elle est significativement après la dernière date générée\n","    if last_hist_date > snapshot_dates_gen[-1] + pd.Timedelta(hours=1):\n","         snapshot_dates_list = snapshot_dates_gen.to_list()\n","         snapshot_dates_list.append(last_hist_date)\n","         snapshot_dates = pd.DatetimeIndex(snapshot_dates_list).sort_values()\n","    else:\n","         snapshot_dates = snapshot_dates_gen\n","    # S'assurer qu'il n'y a pas de doublons (si last_hist_date tombe sur une fin de période)\n","    snapshot_dates = snapshot_dates.unique()\n","else: # Si date_range est vide ou période très courte\n","     snapshot_dates = pd.DatetimeIndex([last_hist_date], tz=data_tz) # Garder le timezone\n","\n","\n","print(f\"Calcul pour {len(snapshot_dates)} dates de snapshots : {snapshot_dates.strftime('%Y-%m-%d').tolist()}\")\n","\n","# --- Calcul Effectif des Snapshots ---\n","snapshot_results = {}\n","config_for_snapshots = final_config # Utiliser la configuration finale sélectionnée\n","\n","print(f\"\\nUtilisation de final_config '{config_for_snapshots.get('label', 'N/A')}' pour les calculs de snapshots.\")\n","\n","for snap_date in tqdm(snapshot_dates, desc=\"Calculating Snapshots\"):\n","    # La fonction `calculate_all_channels_at_date` gère le filtrage de l'historique\n","    # S'assurer que la date passée a le bon TZ ou est naive si les données sont naives\n","    snap_date_to_pass = snap_date\n","    if data_tz is None and getattr(snap_date, 'tz', None) is not None:\n","        snap_date_to_pass = snap_date.tz_convert(None)\n","    elif data_tz is not None and getattr(snap_date, 'tz', None) is None:\n","         try:\n","             snap_date_to_pass = snap_date.tz_localize(data_tz)\n","         except Exception as e:\n","              print(f\"Warn: Could not localize snap_date {snap_date} to {data_tz}. Skipping snapshot.\")\n","              snapshot_results[snap_date] = {\"pivots\": None, \"channels\": None}\n","              continue # Passer au snapshot suivant\n","\n","    # Appeler la fonction de calcul dédiée\n","    pivots, channels = calculate_all_channels_at_date(snap_date_to_pass, bars_df, config_for_snapshots)\n","\n","    # Stocker les résultats (même si channels est None, on garde les pivots s'ils existent)\n","    snapshot_results[snap_date] = {\"pivots\": pivots, \"channels\": channels}\n","\n","print(\"\\nCalcul des snapshots terminé.\")\n","\n","# Optionnel: Vérifier combien de snapshots ont réussi à calculer des canaux\n","successful_channel_calcs = sum(1 for r in snapshot_results.values() if r and r.get('channels') is not None)\n","print(f\"{successful_channel_calcs} / {len(snapshot_dates)} snapshots ont pu calculer au moins une partie des canaux.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### 6.2. Analyse de Stabilité des Pivots (Optionnel)\n","\n","Cette cellule analyse les résultats des snapshots (`snapshot_results`) pour quantifier la fréquence à laquelle les pivots définissant chaque ligne de canal (Macro, Méso, Micro - Support & Résistance) changent d'un snapshot à l'autre. Une faible fréquence de changement suggère une plus grande robustesse des canaux identifiés."]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["# CELLULE 29b (Nouvelle) : Analyse de la Stabilité des Pivots Définissant les Canaux\n","\n","import pandas as pd\n","\n","if 'snapshot_results' not in locals() or not snapshot_results:\n","    print(\"ERREUR: Pas de résultats de snapshots à analyser.\")\n","else:\n","    stability_data = []\n","    previous_pivots = {} # Stocker les pivots du snapshot précédent\n","\n","    for snap_date in sorted(snapshot_results.keys()):\n","        result = snapshot_results[snap_date]\n","        if not result or not result.get('channels'):\n","            continue\n","\n","        channels = result['channels']\n","        current_pivots = {}\n","        row_data = {'snapshot_date': snap_date}\n","\n","        for scale in ['macro', 'meso', 'micro']:\n","            for line_type in ['support', 'resistance']:\n","                key = f\"{scale}_{line_type}\"\n","                p1_info, p2_info = channels.get(scale, {}).get(line_type, ({}, {}))\n","\n","                # Utiliser le timestamp comme identifiant unique (plus robuste que l'index)\n","                p1_id = pd.to_datetime(p1_info.get('time')).isoformat() if isinstance(p1_info, dict) and not pd.isna(p1_info.get('time')) else None\n","                p2_id = pd.to_datetime(p2_info.get('time')).isoformat() if isinstance(p2_info, dict) and not pd.isna(p2_info.get('time')) else None\n","                current_pivots[key] = (p1_id, p2_id)\n","\n","                row_data[f\"{key}_p1_time\"] = p1_id\n","                row_data[f\"{key}_p2_time\"] = p2_id\n","\n","                # Comparer avec le snapshot précédent\n","                changed = False\n","                if key in previous_pivots:\n","                    if previous_pivots[key] != current_pivots[key]:\n","                        changed = True\n","                row_data[f\"{key}_changed\"] = changed\n","\n","        stability_data.append(row_data)\n","        previous_pivots = current_pivots # Mettre à jour pour la prochaine itération\n","\n","    if stability_data:\n","        stability_df = pd.DataFrame(stability_data)\n","        stability_df = stability_df.set_index('snapshot_date')\n","\n","        print(\"\\n--- Analyse de Stabilité des Pivots Définissant les Canaux ---\")\n","        # Afficher les colonnes indiquant les changements\n","        change_cols = [col for col in stability_df.columns if 'changed' in col]\n","        if change_cols:\n","             print(stability_df[change_cols].to_string())\n","             # Compter le nombre total de changements pour chaque canal\n","             print(\"\\nNombre total de changements de pivots par canal:\")\n","             print(stability_df[change_cols].sum())\n","        else:\n","             print(\"Aucune colonne de changement trouvée.\")\n","\n","    else:\n","        print(\"Impossible de générer les données de stabilité.\")"]},{"cell_type":"markdown","metadata":{},"source":["### 6.3. Visualisation des Snapshots (Configuration Cible)\n","\n","Ce graphique présente les résultats de l'analyse temporelle (basée sur `final_config`) sous forme de grille de sous-graphiques (subplots).\n","\n","**Organisation de la Grille :**\n","\n","*   Chaque sous-graphique correspond à une **date de snapshot**.\n","*   Le titre indique la date de fin de l'historique utilisé.\n","*   Chaque sous-graphique affiche :\n","    *   Prix de clôture jusqu'au snapshot.\n","    *   Pivots ZigZag du snapshot.\n","    *   Canaux Macro, Méso et Micro déterminés à cette date.\n","*   Chaque sous-graphique **zoome automatiquement** sur une fenêtre temporelle et une plage de prix définies par les **pivots des canaux Meso et Micro** (si disponibles, sinon Macro) calculés pour ce snapshot spécifique, pour mieux visualiser la structure récente.\n","\n","Permet d'observer comment la structure hiérarchique (calculée avec `final_config`) a **évolué au fil du temps**."]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["# CELLULE 30 (MODIFIÉE) : Visualisation 2D Snapshots - Zoom Meso/Micro\n","\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import pandas as pd\n","import numpy as np\n","import math\n","\n","# --- Pré-requis ---\n","if 'snapshot_results' not in locals() or not snapshot_results: print(\"ERREUR: Pas de résultats de snapshots ('snapshot_results') à visualiser.\")\n","elif 'final_config' not in locals(): print(\"ERREUR: `final_config` non définie.\")\n","elif 'get_line_params_time' not in locals(): print(\"ERREUR: `get_line_params_time` non définie.\")\n","elif 'bars_df' not in locals(): print(\"ERREUR: `bars_df` non disponible.\")\n","elif not any(res is not None and res.get('channels') is not None for date, res in snapshot_results.items() if res is not None):\n","    print(\"WARN: Aucun snapshot n'a pu calculer de canaux valides. Impossible de générer le graphique.\")\n","else:\n","    valid_snapshots = {date: res for date, res in snapshot_results.items() if res and res.get('channels') is not None}\n","    if not valid_snapshots: print(\"ERREUR: Aucun snapshot avec des canaux valides trouvé pour le plot.\")\n","    else:\n","        sorted_snap_dates = sorted(valid_snapshots.keys())\n","        n_snapshots_to_plot = len(sorted_snap_dates)\n","        print(f\"Nombre de snapshots valides à afficher: {n_snapshots_to_plot}\")\n","\n","        if n_snapshots_to_plot > 0:\n","            # --- Configuration Grille ---\n","            cols = 4; rows = math.ceil(n_snapshots_to_plot / cols)\n","            print(f\"Affichage de {n_snapshots_to_plot} snapshots sur une grille de {rows}x{cols}.\")\n","            subplot_titles=[d.strftime('%Y-%m-%d') for d in sorted_snap_dates]\n","            subplot_titles.extend([''] * (rows * cols - len(subplot_titles)))\n","            fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles,\n","                                shared_xaxes=False, shared_yaxes=False,\n","                                vertical_spacing=0.08, horizontal_spacing=0.05)\n","            channel_colors = {\"macro\": \"rgba(60, 60, 60, 0.8)\", \"meso\": \"rgba(0, 0, 255, 0.6)\", \"micro\": \"rgba(255, 0, 255, 0.7)\"}\n","            channel_styles = {\"macro\": \"solid\", \"meso\": \"dash\", \"micro\": \"dot\"}\n","            data_tz = bars_df['time'].dt.tz\n","\n","            # --- Boucle de Plotting ---\n","            for i, snap_date in enumerate(sorted_snap_dates):\n","                row_idx = (i // cols) + 1; col_idx = (i % cols) + 1\n","                result = valid_snapshots[snap_date]\n","                snap_pivots_list = result.get(\"pivots\")\n","                snap_channels = result.get(\"channels\")\n","\n","                # --- Calcul Zoom X/Y (NOUVELLE LOGIQUE: Priorité Meso/Micro) ---\n","                zoom_pivots_data = []\n","                yaxis_range_subplot = None\n","                xaxis_range_subplot = None\n","                zoom_level_used = \"Fallback\" # Pour info\n","\n","                if snap_channels:\n","                    # 1. Essayer de collecter les pivots Meso ET Micro\n","                    for scale in [\"meso\", \"micro\"]:\n","                        scale_def = snap_channels.get(scale, {})\n","                        res_p12 = scale_def.get(\"resistance\", ({},{}))\n","                        sup_p12 = scale_def.get(\"support\", ({},{}))\n","                        for p_info in [res_p12[0], res_p12[1], sup_p12[0], sup_p12[1]]:\n","                            if isinstance(p_info, dict) and not pd.isna(p_info.get('time')) and not pd.isna(p_info.get('price')):\n","                                 time_val = pd.to_datetime(p_info['time']).to_pydatetime()\n","                                 if getattr(time_val, 'tzinfo', None) is not None: time_val = time_val.replace(tzinfo=None)\n","                                 zoom_pivots_data.append({'time': time_val, 'price': p_info['price']})\n","\n","                    if len(zoom_pivots_data) >= 2:\n","                         zoom_level_used = \"Meso/Micro\"\n","                    else:\n","                         # 2. Si échec, essayer avec Macro seulement\n","                         zoom_pivots_data = [] # Reset\n","                         scale_def = snap_channels.get(\"macro\", {})\n","                         res_p12 = scale_def.get(\"resistance\", ({},{}))\n","                         sup_p12 = scale_def.get(\"support\", ({},{}))\n","                         for p_info in [res_p12[0], res_p12[1], sup_p12[0], sup_p12[1]]:\n","                              if isinstance(p_info, dict) and not pd.isna(p_info.get('time')) and not pd.isna(p_info.get('price')):\n","                                   time_val = pd.to_datetime(p_info['time']).to_pydatetime()\n","                                   if getattr(time_val, 'tzinfo', None) is not None: time_val = time_val.replace(tzinfo=None)\n","                                   zoom_pivots_data.append({'time': time_val, 'price': p_info['price']})\n","                         if len(zoom_pivots_data) >= 2:\n","                              zoom_level_used = \"Macro\"\n","\n","                # 3. Calculer les ranges à partir des pivots collectés (Meso/Micro ou Macro)\n","                if len(zoom_pivots_data) >= 2:\n","                    zoom_pivots_df = pd.DataFrame(zoom_pivots_data)\n","                    min_p_zoom = zoom_pivots_df['price'].min(); max_p_zoom = zoom_pivots_df['price'].max()\n","                    min_t_zoom = zoom_pivots_df['time'].min(); max_t_zoom = zoom_pivots_df['time'].max()\n","\n","                    height_zoom = max_p_zoom - min_p_zoom\n","                    margin_y = max(height_zoom * 0.30, (min_p_zoom + max_p_zoom)/2 * 0.05) # Marge Y 30%\n","                    yaxis_range_subplot = [min_p_zoom - margin_y, max_p_zoom + margin_y]\n","\n","                    min_t_zoom_ts = pd.Timestamp(min_t_zoom, tz=data_tz); max_t_zoom_ts = pd.Timestamp(max_t_zoom, tz=data_tz)\n","                    duration_zoom = max_t_zoom_ts - min_t_zoom_ts if max_t_zoom_ts > min_t_zoom_ts else pd.Timedelta(days=1)\n","                    # Ajuster marges temporelles pour bien voir la structure ciblée\n","                    margin_t_before = max(duration_zoom * 0.5, pd.Timedelta(days=45)) # Marge avant 50%\n","                    margin_t_after = max(duration_zoom * 0.2, pd.Timedelta(days=20))  # Marge après 20%\n","                    xaxis_range_subplot = [min_t_zoom_ts - margin_t_before, max_t_zoom_ts + margin_t_after]\n","                    xaxis_range_subplot[1] = min(xaxis_range_subplot[1], pd.Timestamp(snap_date + pd.Timedelta(days=5), tz=data_tz))\n","                    # print(f\"DEBUG Zoom {snap_date.date()}: Used {zoom_level_used} pivots.\") # Décommenter pour vérifier\n","\n","                # 4. Si AUCUN pivot trouvé pour zoomer (Meso/Micro ET Macro échouent) -> Fallback temporel\n","                if xaxis_range_subplot is None:\n","                    zoom_level_used = \"Fallback Time\"\n","                    lookback_plot_days = 90\n","                    xaxis_range_subplot = [pd.Timestamp(snap_date - pd.Timedelta(days=lookback_plot_days), tz=data_tz), pd.Timestamp(snap_date + pd.Timedelta(days=5), tz=data_tz)]\n","                    bars_subplot_fallback = bars_df[(bars_df['time'] >= xaxis_range_subplot[0]) & (bars_df['time'] <= snap_date)].copy()\n","                    if not bars_subplot_fallback.empty:\n","                         min_p = bars_subplot_fallback['low'].min(); max_p = bars_subplot_fallback['high'].max(); margin_y = (max_p - min_p) * 0.1 if (max_p - min_p) > 1e-6 else max_p * 0.05\n","                         yaxis_range_subplot = [min_p - margin_y, max_p + margin_y]\n","                    # print(f\"DEBUG Zoom {snap_date.date()}: Used Fallback Time.\") # Décommenter pour vérifier\n","                # --- Fin Calcul Zoom X/Y ---\n","\n","                # --- Filtrage Données et Tracé (reste identique à la version précédente de Cell 30) ---\n","                # ... (coller ici la partie filtrage et tracé de la cellule 30 précédente) ...\n","                # ... (Assurez-vous d'utiliser xaxis_range_subplot et yaxis_range_subplot calculés ci-dessus) ...\n","\n","                # 1. Filtrage\n","                bars_subplot = bars_df[(bars_df['time'] >= xaxis_range_subplot[0]) & (bars_df['time'] <= snap_date)].copy()\n","                pivots_in_view_df = pd.DataFrame()\n","                if snap_pivots_list:\n","                    pivots_plot_df = pd.DataFrame(snap_pivots_list, columns=['time','price','type']); pivots_plot_df['time'] = pd.to_datetime(pivots_plot_df['time'])\n","                    pivot_tz = pivots_plot_df['time'].dt.tz\n","                    if data_tz is not None and pivot_tz is None:\n","                         try: pivots_plot_df['time'] = pivots_plot_df['time'].dt.tz_localize(data_tz, ambiguous='infer', nonexistent='shift_forward')\n","                         except: pass\n","                    elif data_tz is None and pivot_tz is not None: pivots_plot_df['time'] = pivots_plot_df['time'].dt.tz_convert(None)\n","                    elif data_tz is not None and pivot_tz is not None and hasattr(data_tz, 'zone') and hasattr(pivot_tz, 'zone') and data_tz.zone != pivot_tz.zone: pivots_plot_df['time'] = pivots_plot_df['time'].dt.tz_convert(data_tz)\n","                    pivots_in_view_df = pivots_plot_df[(pivots_plot_df['time'] >= xaxis_range_subplot[0]) & (pivots_plot_df['time'] <= snap_date)]\n","                if yaxis_range_subplot is None: # Fallback Y si besoin\n","                     all_prices_in_view = pd.concat([bars_subplot['low'], bars_subplot['high'], pivots_in_view_df['price']]).dropna()\n","                     if not all_prices_in_view.empty:\n","                         min_p = all_prices_in_view.min(); max_p = all_prices_in_view.max(); margin_y = (max_p - min_p) * 0.1 if (max_p - min_p) > 1e-6 else max_p * 0.05\n","                         yaxis_range_subplot = [min_p - margin_y, max_p + margin_y]\n","\n","                # 2. Tracé Prix\n","                if not bars_subplot.empty: fig.add_trace(go.Scatter(x=bars_subplot['time'], y=bars_subplot['close'], mode='lines', name='Close', line=dict(color='rgba(150, 150, 150, 0.5)'), showlegend=(i==0)), row=row_idx, col=col_idx)\n","\n","                # 3. Tracé Pivots\n","                if not pivots_in_view_df.empty:\n","                    high_x = pivots_in_view_df[pivots_in_view_df['type'] < 0]['time']; high_y = pivots_in_view_df[pivots_in_view_df['type'] < 0]['price']\n","                    low_x  = pivots_in_view_df[pivots_in_view_df['type'] > 0]['time']; low_y  = pivots_in_view_df[pivots_in_view_df['type'] > 0]['price']\n","                    fig.add_trace(go.Scatter(x=high_x, y=high_y, mode='markers', name='High Pivot', marker=dict(color='red', size=5, symbol='diamond-open'), showlegend=(i==0)), row=row_idx, col=col_idx)\n","                    fig.add_trace(go.Scatter(x=low_x, y=low_y, mode='markers', name='Low Pivot', marker=dict(color='green', size=5, symbol='circle-open'), showlegend=(i==0)), row=row_idx, col=col_idx)\n","\n","                # 4. Tracé Canaux\n","                if snap_channels and not bars_subplot.empty:\n","                    plot_start_time_eff = bars_subplot['time'].min(); plot_end_time_eff = snap_date\n","                    plot_start_time_num_eff = plot_start_time_eff.timestamp(); plot_end_time_num_eff = plot_end_time_eff.timestamp()\n","                    for channel_name in [\"macro\", \"meso\", \"micro\"]:\n","                         definition = snap_channels.get(channel_name, {})\n","                         for line_type in [\"resistance\", \"support\"]:\n","                             p1_info, p2_info = definition.get(line_type, ({}, {}))\n","                             if (isinstance(p1_info, dict) and isinstance(p2_info, dict) and # Check validité\n","                                 not pd.isna(p1_info.get('time')) and not pd.isna(p2_info.get('time')) and\n","                                 not pd.isna(p1_info.get('time_numeric')) and not pd.isna(p2_info.get('time_numeric')) and\n","                                 not pd.isna(p1_info.get('price')) and not pd.isna(p2_info.get('price'))):\n","                                 p1_time_num = p1_info['time_numeric']; p2_time_num = p2_info['time_numeric']\n","                                 p1_price = p1_info['price']; p2_price = p2_info['price']\n","                                 p1_time = pd.to_datetime(p1_info['time']); p2_time = pd.to_datetime(p2_info['time'])\n","                                 try:\n","                                     m, c = get_line_params_time(p1_time_num, p1_price, p2_time_num, p2_price)\n","                                     if m != np.inf:\n","                                         line_plot_start_time_dt = max(p1_time, plot_start_time_eff); line_plot_start_time_num = line_plot_start_time_dt.timestamp()\n","                                         line_plot_end_time_dt = plot_end_time_eff\n","                                         line_start_y_plot = m * line_plot_start_time_num + c; line_end_y_plot = m * plot_end_time_num_eff + c\n","                                         fig.add_trace(go.Scatter(x=[line_plot_start_time_dt, line_plot_end_time_dt], y=[line_start_y_plot, line_end_y_plot], mode='lines', name=f\"{channel_name.capitalize()} {line_type.capitalize()}\", line=dict(color=channel_colors[channel_name], width=1.5, dash=channel_styles[channel_name]), legendgroup=f\"{channel_name}_{line_type}\", showlegend=(i==0)), row=row_idx, col=col_idx)\n","                                         pivots_x_to_mark = []; pivots_y_to_mark = []\n","                                         if xaxis_range_subplot and p1_time >= xaxis_range_subplot[0] and p1_time <= xaxis_range_subplot[1]: pivots_x_to_mark.append(p1_time); pivots_y_to_mark.append(p1_price)\n","                                         if xaxis_range_subplot and p2_time >= xaxis_range_subplot[0] and p2_time <= xaxis_range_subplot[1]: pivots_x_to_mark.append(p2_time); pivots_y_to_mark.append(p2_price)\n","                                         if pivots_x_to_mark: fig.add_trace(go.Scatter(x=pivots_x_to_mark, y=pivots_y_to_mark, mode='markers', marker=dict(color=channel_colors[channel_name], size=7, symbol='star' if line_type == 'resistance' else 'star-open'), showlegend=False, hoverinfo='skip'), row=row_idx, col=col_idx)\n","                                 except Exception as e_plotline: pass # print(f\"WARN: Plot line {snap_date.date()}: {e_plotline}\")\n","\n","\n","                # --- Application Zoom et Mise en Forme Axes ---\n","                if yaxis_range_subplot: fig.update_yaxes(range=yaxis_range_subplot, row=row_idx, col=col_idx)\n","                if xaxis_range_subplot: fig.update_xaxes(range=xaxis_range_subplot, row=row_idx, col=col_idx)\n","                if row_idx < rows: fig.update_xaxes(showticklabels=False, row=row_idx, col=col_idx)\n","                if col_idx > 1: fig.update_yaxes(showticklabels=False, row=row_idx, col=col_idx)\n","\n","            # --- Layout Final ---\n","            fig.update_layout(\n","                title=f\"Évolution Temporelle des Canaux ({n_snapshots_to_plot} Snapshots - Config: {final_config.get('label','N/A')}) - Zoom Meso/Micro\",\n","                height=250 * rows + 80, width=1200, hovermode='x unified',\n","                legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.08, xanchor=\"center\", x=0.5)\n","            )\n","            for i_annot, annot in enumerate(fig.layout.annotations):\n","                 if i_annot < n_snapshots_to_plot: annot.update(font=dict(size=9))\n","            fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 6.4. Visualisation des Snapshots (Configurations Multiples)\n","\n","Similaire à la visualisation précédente, mais cette cellule génère une grille de snapshots **pour plusieurs configurations de canaux sélectionnées** (définies par `indices_configs_snapshots`).\n","\n","Cela permet de comparer directement comment différentes configurations de `wp`/`rpf` auraient influencé l'évolution historique perçue des canaux. Chaque configuration génère son propre graphique de snapshots complet. Le zoom est également basé sur les pivots Meso/Micro (ou Macro en fallback) pour chaque snapshot de chaque configuration."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CELLULE 30b (MODIFIÉE) : Affichage Snapshots pour Plusieurs Configs - Zoom Meso/Micro\n","\n","import pandas as pd\n","import numpy as np\n","import math\n","from tqdm.notebook import tqdm\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import traceback\n","\n","# --- Indices des Configurations à Visualiser ---\n","indices_configs_snapshots = [3, 7, 11] # <--- !!! UTILISATEUR: MODIFIER CETTE LISTE D'INDICES !!!\n","\n","# --- Vérifications ---\n","if 'configurations_to_explore' not in locals(): raise NameError(\"configurations_to_explore non définie\")\n","if 'bars_df' not in locals(): raise NameError(\"bars_df non défini\")\n","if 'calculate_all_channels_at_date' not in locals() or not callable(calculate_all_channels_at_date): raise NameError(\"calculate_all_channels_at_date non définie\")\n","if 'get_line_params_time' not in locals(): raise NameError(\"get_line_params_time non définie\")\n","\n","# --- Boucle sur les Configurations ---\n","for config_idx in indices_configs_snapshots:\n","    if not (0 <= config_idx < len(configurations_to_explore)):\n","        print(f\"WARN: Index {config_idx} invalide. Skip.\"); continue\n","\n","    config_to_snapshot = configurations_to_explore[config_idx]\n","    config_label = config_to_snapshot.get('label', f'Config {config_idx}')\n","    print(f\"\\n{'='*10} Génération Snapshots pour Config: {config_label} (Index {config_idx}) {'='*10}\")\n","\n","    # --- 1. RECALCUL SNAPSHOTS pour CETTE config ---\n","    current_config_snapshot_results = {}\n","    data_tz = bars_df['time'].dt.tz\n","    snapshot_start_calc_date_cfg = bars_df['time'].iloc[0]\n","    if snapshot_start_calc_date_cfg > bars_df['time'].iloc[-1] - pd.Timedelta(days=60): snapshot_start_calc_date_cfg = bars_df['time'].iloc[-1] - pd.Timedelta(days=60)\n","    last_hist_date_cfg = bars_df['time'].iloc[-1]\n","    freq_snapshots_cfg = 'M'\n","    try:\n","        snapshot_dates_gen_cfg = pd.date_range(start=snapshot_start_calc_date_cfg, end=last_hist_date_cfg, freq=freq_snapshots_cfg, tz=data_tz)\n","        snapshot_dates_gen_cfg = snapshot_dates_gen_cfg[snapshot_dates_gen_cfg <= last_hist_date_cfg]\n","    except Exception as e_dr: print(f\"ERREUR date_range {config_label}: {e_dr}\"); continue\n","    if not snapshot_dates_gen_cfg.empty:\n","         if last_hist_date_cfg > snapshot_dates_gen_cfg[-1] + pd.Timedelta(hours=1):\n","             snapshot_dates_list_cfg = snapshot_dates_gen_cfg.to_list(); snapshot_dates_list_cfg.append(last_hist_date_cfg)\n","             snapshot_dates_cfg = pd.DatetimeIndex(snapshot_dates_list_cfg).sort_values().unique()\n","         else: snapshot_dates_cfg = snapshot_dates_gen_cfg.unique()\n","    else: snapshot_dates_cfg = pd.DatetimeIndex([last_hist_date_cfg], tz=data_tz)\n","    print(f\"Calcul pour {len(snapshot_dates_cfg)} snapshots pour {config_label}...\")\n","    for snap_date_cfg in tqdm(snapshot_dates_cfg, desc=f\"Calc Snapshots {config_label[:15]}...\", leave=False):\n","         snap_date_to_pass_cfg = snap_date_cfg\n","         if data_tz is None and getattr(snap_date_cfg, 'tz', None) is not None: snap_date_to_pass_cfg = snap_date_cfg.tz_convert(None)\n","         elif data_tz is not None and getattr(snap_date_cfg, 'tz', None) is None:\n","              try: snap_date_to_pass_cfg = snap_date_cfg.tz_localize(data_tz)\n","              except: print(f\"Warn TZ loc snap {config_label}\"); continue\n","         pivots, channels = calculate_all_channels_at_date(snap_date_to_pass_cfg, bars_df, config_to_snapshot) # Utilise config_to_snapshot\n","         current_config_snapshot_results[snap_date_cfg] = {\"pivots\": pivots, \"channels\": channels}\n","    print(f\"Calcul snapshots pour {config_label} terminé.\")\n","\n","    # --- 2. AFFICHAGE GRAPHIQUE pour CETTE config ---\n","    valid_snapshots_cfg = {date: res for date, res in current_config_snapshot_results.items() if res and res.get('channels') is not None}\n","    if not valid_snapshots_cfg:\n","        print(f\"Aucun snapshot valide pour {config_label}. Skip plot.\"); continue\n","\n","    sorted_snap_dates_cfg = sorted(valid_snapshots_cfg.keys())\n","    n_snapshots_to_plot_cfg = len(sorted_snap_dates_cfg)\n","    cols_cfg = 4; rows_cfg = math.ceil(n_snapshots_to_plot_cfg / cols_cfg)\n","    subplot_titles_cfg=[d.strftime('%Y-%m-%d') for d in sorted_snap_dates_cfg]\n","    subplot_titles_cfg.extend([''] * (rows_cfg * cols_cfg - len(subplot_titles_cfg)))\n","    fig_cfg = make_subplots(rows=rows_cfg, cols=cols_cfg, subplot_titles=subplot_titles_cfg, shared_xaxes=False, shared_yaxes=False, vertical_spacing=0.08, horizontal_spacing=0.05)\n","    channel_colors_cfg = {\"macro\": \"rgba(60, 60, 60, 0.8)\", \"meso\": \"rgba(0, 0, 255, 0.6)\", \"micro\": \"rgba(255, 0, 255, 0.7)\"}\n","    channel_styles_cfg = {\"macro\": \"solid\", \"meso\": \"dash\", \"micro\": \"dot\"}\n","\n","    # Boucle de plotting interne (utilise la logique de zoom de la Cell 30 modifiée)\n","    for i_cfg, snap_date_cfg_plot in enumerate(sorted_snap_dates_cfg):\n","         row_idx_cfg = (i_cfg // cols_cfg) + 1; col_idx_cfg = (i_cfg % cols_cfg) + 1\n","         result_cfg = valid_snapshots_cfg[snap_date_cfg_plot]\n","         snap_pivots_list_cfg = result_cfg.get(\"pivots\"); snap_channels_cfg = result_cfg.get(\"channels\")\n","\n","         # Calcul zoom X/Y (Priorité Meso/Micro - identique à Cell 30 modifiée)\n","         zoom_pivots_data_cfg = []; yaxis_range_subplot_cfg = None; xaxis_range_subplot_cfg = None; zoom_level_used_cfg = \"Fallback\"\n","         if snap_channels_cfg:\n","             for scale in [\"meso\", \"micro\"]: # Essai Meso/Micro\n","                 scale_def = snap_channels_cfg.get(scale, {}); res_p12 = scale_def.get(\"resistance\", ({},{})); sup_p12 = scale_def.get(\"support\", ({},{}))\n","                 for p_info in [res_p12[0], res_p12[1], sup_p12[0], sup_p12[1]]:\n","                      if isinstance(p_info, dict) and not pd.isna(p_info.get('time')) and not pd.isna(p_info.get('price')):\n","                           time_val = pd.to_datetime(p_info['time']).to_pydatetime();\n","                           if getattr(time_val, 'tzinfo', None) is not None: time_val = time_val.replace(tzinfo=None)\n","                           zoom_pivots_data_cfg.append({'time': time_val, 'price': p_info['price']})\n","             if len(zoom_pivots_data_cfg) >= 2: zoom_level_used_cfg = \"Meso/Micro\"\n","             else: # Essai Macro\n","                  zoom_pivots_data_cfg = []\n","                  scale_def = snap_channels_cfg.get(\"macro\", {}); res_p12 = scale_def.get(\"resistance\", ({},{})); sup_p12 = scale_def.get(\"support\", ({},{}))\n","                  for p_info in [res_p12[0], res_p12[1], sup_p12[0], sup_p12[1]]:\n","                      if isinstance(p_info, dict) and not pd.isna(p_info.get('time')) and not pd.isna(p_info.get('price')):\n","                           time_val = pd.to_datetime(p_info['time']).to_pydatetime();\n","                           if getattr(time_val, 'tzinfo', None) is not None: time_val = time_val.replace(tzinfo=None)\n","                           zoom_pivots_data_cfg.append({'time': time_val, 'price': p_info['price']})\n","                  if len(zoom_pivots_data_cfg) >= 2: zoom_level_used_cfg = \"Macro\"\n","         # Calcul ranges si pivots trouvés\n","         if len(zoom_pivots_data_cfg) >= 2:\n","              zoom_pivots_df_cfg = pd.DataFrame(zoom_pivots_data_cfg)\n","              min_p_zoom_cfg = zoom_pivots_df_cfg['price'].min(); max_p_zoom_cfg = zoom_pivots_df_cfg['price'].max()\n","              min_t_zoom_cfg = zoom_pivots_df_cfg['time'].min(); max_t_zoom_cfg = zoom_pivots_df_cfg['time'].max()\n","              height_zoom_cfg = max_p_zoom_cfg - min_p_zoom_cfg; margin_y_cfg = max(height_zoom_cfg * 0.30, (min_p_zoom_cfg + max_p_zoom_cfg)/2 * 0.05)\n","              yaxis_range_subplot_cfg = [min_p_zoom_cfg - margin_y_cfg, max_p_zoom_cfg + margin_y_cfg]\n","              min_t_zoom_ts_cfg = pd.Timestamp(min_t_zoom_cfg, tz=data_tz); max_t_zoom_ts_cfg = pd.Timestamp(max_t_zoom_cfg, tz=data_tz)\n","              duration_zoom_cfg = max_t_zoom_ts_cfg - min_t_zoom_ts_cfg if max_t_zoom_ts_cfg > min_t_zoom_ts_cfg else pd.Timedelta(days=1)\n","              margin_t_before_cfg = max(duration_zoom_cfg * 0.5, pd.Timedelta(days=45)); margin_t_after_cfg = max(duration_zoom_cfg * 0.2, pd.Timedelta(days=20))\n","              xaxis_range_subplot_cfg = [min_t_zoom_ts_cfg - margin_t_before_cfg, max_t_zoom_ts_cfg + margin_t_after_cfg]\n","              xaxis_range_subplot_cfg[1] = min(xaxis_range_subplot_cfg[1], pd.Timestamp(snap_date_cfg_plot + pd.Timedelta(days=5), tz=data_tz))\n","         # Fallback temporel si aucun pivot\n","         if xaxis_range_subplot_cfg is None:\n","              zoom_level_used_cfg = \"Fallback Time\"; lookback_plot_days_cfg = 90\n","              xaxis_range_subplot_cfg = [pd.Timestamp(snap_date_cfg_plot - pd.Timedelta(days=lookback_plot_days_cfg), tz=data_tz), pd.Timestamp(snap_date_cfg_plot + pd.Timedelta(days=5), tz=data_tz)]\n","              bars_subplot_fallback_cfg = bars_df[(bars_df['time'] >= xaxis_range_subplot_cfg[0]) & (bars_df['time'] <= snap_date_cfg_plot)].copy()\n","              if not bars_subplot_fallback_cfg.empty:\n","                   min_p_cfg = bars_subplot_fallback_cfg['low'].min(); max_p_cfg = bars_subplot_fallback_cfg['high'].max(); margin_y_cfg = (max_p_cfg - min_p_cfg) * 0.1 if (max_p_cfg - min_p_cfg) > 1e-6 else max_p_cfg * 0.05\n","                   yaxis_range_subplot_cfg = [min_p_cfg - margin_y_cfg, max_p_cfg + margin_y_cfg]\n","\n","         # Filtrage données & pivots (identique à Cell 30 modifiée)\n","         # ... (coller ici la partie filtrage de la cellule 30 modifiée, avec suffixe _cfg) ...\n","         bars_subplot_cfg = bars_df[(bars_df['time'] >= xaxis_range_subplot_cfg[0]) & (bars_df['time'] <= snap_date_cfg_plot)].copy()\n","         pivots_in_view_df_cfg = pd.DataFrame()\n","         if snap_pivots_list_cfg:\n","             pivots_plot_df_cfg = pd.DataFrame(snap_pivots_list_cfg, columns=['time','price','type']); pivots_plot_df_cfg['time'] = pd.to_datetime(pivots_plot_df_cfg['time'])\n","             pivot_tz_cfg = pivots_plot_df_cfg['time'].dt.tz # Gérer TZ...\n","             if data_tz is not None and pivot_tz_cfg is None:\n","                  try: pivots_plot_df_cfg['time'] = pivots_plot_df_cfg['time'].dt.tz_localize(data_tz, ambiguous='infer', nonexistent='shift_forward')\n","                  except: pass\n","             elif data_tz is None and pivot_tz_cfg is not None: pivots_plot_df_cfg['time'] = pivots_plot_df_cfg['time'].dt.tz_convert(None)\n","             elif data_tz is not None and pivot_tz_cfg is not None and hasattr(data_tz, 'zone') and hasattr(pivot_tz_cfg, 'zone') and data_tz.zone != pivot_tz_cfg.zone: pivots_plot_df_cfg['time'] = pivots_plot_df_cfg['time'].dt.tz_convert(data_tz)\n","             pivots_in_view_df_cfg = pivots_plot_df_cfg[(pivots_plot_df_cfg['time'] >= xaxis_range_subplot_cfg[0]) & (pivots_plot_df_cfg['time'] <= snap_date_cfg_plot)]\n","         if yaxis_range_subplot_cfg is None: # Fallback Y\n","              all_prices_in_view_cfg = pd.concat([bars_subplot_cfg['low'], bars_subplot_cfg['high'], pivots_in_view_df_cfg['price']]).dropna()\n","              if not all_prices_in_view_cfg.empty:\n","                   min_p_cfg = all_prices_in_view_cfg.min(); max_p_cfg = all_prices_in_view_cfg.max(); margin_y_cfg = (max_p_cfg - min_p_cfg) * 0.1 if (max_p_cfg - min_p_cfg) > 1e-6 else max_p_cfg * 0.05\n","                   yaxis_range_subplot_cfg = [min_p_cfg - margin_y_cfg, max_p_cfg + margin_y_cfg]\n","\n","         # Tracer Prix, Pivots, Canaux (identique à Cell 30 modifiée)\n","         # ... (coller ici la partie tracé de la cellule 30 modifiée, avec suffixe _cfg) ...\n","         # Plot Prix\n","         if not bars_subplot_cfg.empty: fig_cfg.add_trace(go.Scatter(x=bars_subplot_cfg['time'], y=bars_subplot_cfg['close'], mode='lines', name='Close', line=dict(color='rgba(150, 150, 150, 0.5)'), showlegend=(i_cfg==0)), row=row_idx_cfg, col=col_idx_cfg)\n","         # Plot Pivots\n","         if not pivots_in_view_df_cfg.empty:\n","              high_x_cfg = pivots_in_view_df_cfg[pivots_in_view_df_cfg['type'] < 0]['time']; high_y_cfg = pivots_in_view_df_cfg[pivots_in_view_df_cfg['type'] < 0]['price']\n","              low_x_cfg  = pivots_in_view_df_cfg[pivots_in_view_df_cfg['type'] > 0]['time']; low_y_cfg  = pivots_in_view_df_cfg[pivots_in_view_df_cfg['type'] > 0]['price']\n","              fig_cfg.add_trace(go.Scatter(x=high_x_cfg, y=high_y_cfg, mode='markers', name='High Pivot', marker=dict(color='red', size=5, symbol='diamond-open'), showlegend=(i_cfg==0)), row=row_idx_cfg, col=col_idx_cfg)\n","              fig_cfg.add_trace(go.Scatter(x=low_x_cfg, y=low_y_cfg, mode='markers', name='Low Pivot', marker=dict(color='green', size=5, symbol='circle-open'), showlegend=(i_cfg==0)), row=row_idx_cfg, col=col_idx_cfg)\n","         # Plot Canaux\n","         if snap_channels_cfg and not bars_subplot_cfg.empty:\n","             plot_start_time_eff_cfg = bars_subplot_cfg['time'].min(); plot_end_time_eff_cfg = snap_date_cfg_plot\n","             plot_start_time_num_eff_cfg = plot_start_time_eff_cfg.timestamp(); plot_end_time_num_eff_cfg = plot_end_time_eff_cfg.timestamp()\n","             for channel_name_cfg in [\"macro\", \"meso\", \"micro\"]: # ... (reste identique) ...\n","                 definition_cfg = snap_channels_cfg.get(channel_name_cfg, {})\n","                 for line_type_cfg in [\"resistance\", \"support\"]:\n","                     p1_info_cfg, p2_info_cfg = definition_cfg.get(line_type_cfg, ({}, {}))\n","                     if (isinstance(p1_info_cfg, dict) and isinstance(p2_info_cfg, dict) and # Check validité\n","                         not pd.isna(p1_info_cfg.get('time')) and not pd.isna(p2_info_cfg.get('time')) and\n","                         not pd.isna(p1_info_cfg.get('time_numeric')) and not pd.isna(p2_info_cfg.get('time_numeric')) and\n","                         not pd.isna(p1_info_cfg.get('price')) and not pd.isna(p2_info_cfg.get('price'))):\n","                         p1_time_num_cfg = p1_info_cfg['time_numeric']; p2_time_num_cfg = p2_info_cfg['time_numeric']; p1_price_cfg = p1_info_cfg['price']; p2_price_cfg = p2_info_cfg['price']; p1_time_cfg = pd.to_datetime(p1_info_cfg['time']); p2_time_cfg = pd.to_datetime(p2_info_cfg['time'])\n","                         try: # ... (m, c, lignes, markers identiques) ...\n","                             m_cfg, c_cfg = get_line_params_time(p1_time_num_cfg, p1_price_cfg, p2_time_num_cfg, p2_price_cfg)\n","                             if m_cfg != np.inf:\n","                                 line_plot_start_time_dt_cfg = max(p1_time_cfg, plot_start_time_eff_cfg); line_plot_start_time_num_cfg = line_plot_start_time_dt_cfg.timestamp(); line_plot_end_time_dt_cfg = plot_end_time_eff_cfg\n","                                 line_start_y_plot_cfg = m_cfg * line_plot_start_time_num_cfg + c_cfg; line_end_y_plot_cfg = m_cfg * plot_end_time_num_eff_cfg + c_cfg\n","                                 fig_cfg.add_trace(go.Scatter(x=[line_plot_start_time_dt_cfg, line_plot_end_time_dt_cfg], y=[line_start_y_plot_cfg, line_end_y_plot_cfg], mode='lines', name=f\"{channel_name_cfg.capitalize()} {line_type_cfg.capitalize()}\", line=dict(color=channel_colors_cfg[channel_name_cfg], width=1.5, dash=channel_styles_cfg[channel_name_cfg]), legendgroup=f\"{channel_name_cfg}_{line_type_cfg}\", showlegend=(i_cfg==0)), row=row_idx_cfg, col=col_idx_cfg)\n","                                 pivots_x_to_mark_cfg = []; pivots_y_to_mark_cfg = []\n","                                 if xaxis_range_subplot_cfg and p1_time_cfg >= xaxis_range_subplot_cfg[0] and p1_time_cfg <= xaxis_range_subplot_cfg[1]: pivots_x_to_mark_cfg.append(p1_time_cfg); pivots_y_to_mark_cfg.append(p1_price_cfg)\n","                                 if xaxis_range_subplot_cfg and p2_time_cfg >= xaxis_range_subplot_cfg[0] and p2_time_cfg <= xaxis_range_subplot_cfg[1]: pivots_x_to_mark_cfg.append(p2_time_cfg); pivots_y_to_mark_cfg.append(p2_price_cfg)\n","                                 if pivots_x_to_mark_cfg: fig_cfg.add_trace(go.Scatter(x=pivots_x_to_mark_cfg, y=pivots_y_to_mark_cfg, mode='markers', marker=dict(color=channel_colors_cfg[channel_name_cfg], size=7, symbol='star' if line_type_cfg == 'resistance' else 'star-open'), showlegend=False, hoverinfo='skip'), row=row_idx_cfg, col=col_idx_cfg)\n","                         except Exception as e_plotline_cfg: pass # print(f\"WARN: Plot line {config_label} {snap_date_cfg_plot.date()}: {e_plotline_cfg}\")\n","\n","\n","         # Appliquer zooms et masquer ticks (identique à Cell 30 modifiée)\n","         if yaxis_range_subplot_cfg: fig_cfg.update_yaxes(range=yaxis_range_subplot_cfg, row=row_idx_cfg, col=col_idx_cfg)\n","         if xaxis_range_subplot_cfg: fig_cfg.update_xaxes(range=xaxis_range_subplot_cfg, row=row_idx_cfg, col=col_idx_cfg)\n","         if row_idx_cfg < rows_cfg: fig_cfg.update_xaxes(showticklabels=False, row=row_idx_cfg, col=col_idx_cfg)\n","         if col_idx_cfg > 1: fig_cfg.update_yaxes(showticklabels=False, row=row_idx_cfg, col=col_idx_cfg)\n","\n","    # Layout Final pour ce graphique\n","    fig_cfg.update_layout(title=f\"Snapshots pour {config_label} (Index {config_idx}) - Zoom Meso/Micro\",\n","                          height=250 * rows_cfg + 80, width=1200, hovermode='x unified',\n","                          legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.08, xanchor=\"center\", x=0.5))\n","    for i_annot_cfg, annot_cfg in enumerate(fig_cfg.layout.annotations):\n","         if i_annot_cfg < n_snapshots_to_plot_cfg: annot_cfg.update(font=dict(size=9))\n","    fig_cfg.show()\n","    # --------------------------------------------------------\n","\n","print(\"\\n--- Visualisation des snapshots pour les configurations sélectionnées terminée ---\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 8. Optimisation par Algorithme Génétique (GA)\n","\n","Cette section utilise un algorithme génétique (implémenté avec la bibliothèque DEAP) pour explorer l'espace des paramètres de stratégies de trading basées sur les canaux multi-échelles pré-calculés. L'objectif est d'identifier des combinaisons de règles et de paramètres qui maximisent une métrique de performance (ici, l'équité finale) sur la période de simulation.\n","\n","**Étapes Clés :**\n","\n","1.  **Définition de l'Espace Paramétrique :** Spécifier les différentes options pour chaque paramètre de la stratégie (niveau de canal, type de signal, filtre de tendance, gestion du risque, SL/TP, etc.).\n","2.  **Création d'Individus (Chromosomes) :** Définir une fonction qui transforme une combinaison de paramètres (un individu) en une structure de stratégie complète (règles, actions).\n","3.  **Fonction de Fitness :** Implémenter une fonction (`evaluate_strategy`) qui simule une stratégie donnée sur l'historique des prix et des canaux (calculé avec `final_config`) et retourne une valeur de fitness (équité finale).\n","4.  **Configuration DEAP :** Mettre en place les opérateurs génétiques (sélection, croisement, mutation) et les outils DEAP pour gérer la population et l'évolution.\n","5.  **Exécution de l'AG :** Lancer le processus d'optimisation sur plusieurs générations.\n","6.  **Analyse des Résultats :** Visualiser la convergence de la fitness, identifier la meilleure stratégie trouvée et re-simuler sa performance en détail.\n","7.  **(Optionnel) Re-simulation Manuelle :** Permettre de tester manuellement une configuration spécifique découverte lors d'une exécution précédente ou définie par l'utilisateur.\n","\n","**Important :** L'historique des canaux utilisé pour l'évaluation de la fitness dans le GA est basé sur la `final_config` sélectionnée dans la section précédente. La performance du GA dépend donc directement de la pertinence de cette configuration de canaux fixe."]},{"cell_type":"markdown","metadata":{},"source":["### 8.1. Préparation de l'Environnement GA\n","\n","Cette sous-section configure les éléments nécessaires au fonctionnement de l'algorithme génétique.\n","\n","*   **Espace Paramétrique (`param_space`) :** Définit les choix possibles pour chaque hyperparamètre de la stratégie de trading (ex: quel canal utiliser, type de signal, paramètres de SL/TP, etc.).\n","*   **Création de Stratégie (`create_strategy_from_params`) :** Une fonction qui prend une combinaison de paramètres (un \"individu\" ou \"chromosome\" pour le GA) et la traduit en un ensemble de règles de trading structurées.\n","*   **Fonctions Helpers (`get_channel_value_at_time`, `check_conditions`, `calculate_position_size`, `apply_fees`) :** Fonctions utilitaires pour la simulation (calcul de valeur de canal, vérification des conditions de trade, calcul de taille de position, application des frais). La fonction `apply_fees` tente d'utiliser le modèle de frais de QuantConnect ou un fallback simple.\n","*   **Calcul de l'Historique des Canaux Fixes (`channel_history_df`) :** **Étape cruciale**. Recalcule l'état des canaux Macro/Meso/Micro (en utilisant la `final_config` sélectionnée précédemment) à intervalles réguliers sur toute la période de simulation. Ce DataFrame historique sera utilisé par la fonction de fitness pour déterminer l'état des canaux à chaque pas de temps sans avoir à les recalculer pour chaque individu du GA, optimisant ainsi considérablement le processus.\n","*   **Configuration DEAP :** Initialise les outils de la bibliothèque DEAP, définissant comment les individus sont créés, évalués (via `evaluate_strategy`), croisés (`mate`), mutés (`mutate`), et sélectionnés (`select`) pour les générations suivantes."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CELLULE 7.1.1 : Espace Paramétrique pour le GA\n","\n","import itertools\n","import random # Import random ici si pas déjà fait globalement\n","\n","print(\"Définition de l'espace paramétrique des stratégies pour le GA...\")\n","\n","# --- Options pour chaque paramètre de la stratégie ---\n","# (Assurez-vous que ces options correspondent à ce que vous voulez optimiser)\n","param_space = {\n","    'trade_level': ['micro', 'meso'],\n","    'signal_type': ['bounce', 'breakout', 'both'],\n","    'trend_filter_level': ['none', 'meso', 'macro'],\n","    'risk_per_trade_pct': [0.0075, 0.01, 0.015, 0.02],\n","    'min_channel_width_pct': [0.004, 0.008, 0.012, 0.016],\n","    'bounce_sl_type': ['pct_entry', 'pct_level'],\n","    'bounce_sl_value': [0.005, 0.01, 0.015, 0.02],\n","    'bounce_tp_type': ['rr_ratio'], # Type de TP fixe pour l'instant\n","    'bounce_tp_value': [1.5, 2.0, 2.5, 3.0],\n","    'bounce_entry_offset': [0.001, 0.002, 0.003],\n","    'breakout_sl_type': ['pct_level'], # Type de SL fixe pour breakout\n","    'breakout_sl_value': [0.005, 0.01, 0.015, 0.02],\n","    'breakout_tp_type': ['rr_ratio'], # Type de TP fixe\n","    'breakout_tp_value': [1.5, 2.0, 2.5, 3.0],\n","}\n","\n","# Obtenir la liste ordonnée des clés (important pour DEAP)\n","param_keys = list(param_space.keys())\n","\n","print(f\"Espace paramétrique défini avec {len(param_keys)} paramètres.\")\n","\n","# Calculer le nombre total de combinaisons (pour info, peut être très grand)\n","# total_combinations_possible = 1\n","# for key in param_keys:\n","#     total_combinations_possible *= len(param_space[key])\n","# print(f\"Nombre total de combinaisons possibles dans cet espace : {total_combinations_possible:,}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CELLULE NOUVELLE : Fonction pour Créer une Stratégie (Chromosome) depuis les Paramètres\n","\n","print(\"Définition de la fonction de création de stratégie...\")\n","\n","def create_strategy_from_params(param_combination, param_keys):\n","    \"\"\"\n","    Crée un dictionnaire de stratégie (chromosome) à partir d'une combinaison de paramètres.\n","    \"\"\"\n","    params = dict(zip(param_keys, param_combination))\n","    strategy_name = f\"L{params['trade_level']}_S{params['signal_type']}_TF{params['trend_filter_level']}\" \\\n","                    f\"_R{params['risk_per_trade_pct']*100:.0f}_SLB{params['bounce_sl_value']*1000:.0f}\" \\\n","                    f\"_SLK{params['breakout_sl_value']*1000:.0f}_TP{params['bounce_tp_value']}\"\n","\n","    rules = []\n","\n","    # --- Actions Modèles (basées sur les params actuels) ---\n","    current_bounce_long = {\n","        \"action_type\": \"bounce_long\", \"order_type\": \"market\",\n","        f\"stop_loss_{params['bounce_sl_type'].replace('_', '_pct_')}\" : params['bounce_sl_value'],\n","        \"target_rr_ratio\": params['bounce_tp_value']\n","    }\n","    current_bounce_short = {\n","        \"action_type\": \"bounce_short\", \"order_type\": \"market\",\n","         f\"stop_loss_{params['bounce_sl_type'].replace('pct_entry', 'pct_above_entry').replace('pct_level', 'pct_above_level')}\" : params['bounce_sl_value'],\n","        \"target_rr_ratio\": params['bounce_tp_value']\n","    }\n","    current_breakout_long = {\n","        \"action_type\": \"breakout_long\", \"order_type\": \"market\",\n","        f\"stop_loss_{params['breakout_sl_type'].replace('_', '_pct_')}\" : params['breakout_sl_value'],\n","        \"target_rr_ratio\": params['breakout_tp_value']\n","    }\n","    current_breakout_short = {\n","        \"action_type\": \"breakout_short\", \"order_type\": \"market\",\n","        f\"stop_loss_{params['breakout_sl_type'].replace('pct_level', 'pct_above_level')}\" : params['breakout_sl_value'],\n","        \"target_rr_ratio\": params['breakout_tp_value']\n","    }\n","\n","    # --- Génération des Règles ---\n","    trade_level = params['trade_level']\n","    min_width = params['min_channel_width_pct']\n","    bounce_offset = params['bounce_entry_offset']\n","    trend_filter = params['trend_filter_level']\n","\n","    # Conditions communes pour le filtre de tendance\n","    trend_cond_long = {}\n","    trend_cond_short = {}\n","    if trend_filter != 'none':\n","        trend_cond_long = {\"channel_trend\": {\"level\": trend_filter, \"direction\": \"up\"}}\n","        trend_cond_short = {\"channel_trend\": {\"level\": trend_filter, \"direction\": \"down\"}}\n","\n","    # Ajouter règles BOUNCE si signal_type est 'bounce' or 'both'\n","    if params['signal_type'] in ['bounce', 'both']:\n","        # Bounce Long Rule\n","        conditions_bl = {\n","            \"in_position\": \"none\",\n","            \"price_near_level\": {\"level\": f\"{trade_level}_support\", \"threshold_pct\": bounce_offset},\n","            \"channel_width_ok\": {\"level\": trade_level, \"min_pct\": min_width},\n","            **trend_cond_long # Ajoute le filtre de tendance s'il est défini\n","        }\n","        rules.append({\"conditions\": conditions_bl, \"action\": current_bounce_long})\n","\n","        # Bounce Short Rule\n","        conditions_bs = {\n","            \"in_position\": \"none\",\n","            \"price_near_level\": {\"level\": f\"{trade_level}_resistance\", \"threshold_pct\": bounce_offset},\n","            \"channel_width_ok\": {\"level\": trade_level, \"min_pct\": min_width},\n","             **trend_cond_short\n","        }\n","        rules.append({\"conditions\": conditions_bs, \"action\": current_bounce_short})\n","\n","    # Ajouter règles BREAKOUT si signal_type est 'breakout' or 'both'\n","    if params['signal_type'] in ['breakout', 'both']:\n","         # Breakout Long Rule\n","        conditions_kl = {\n","            \"in_position\": \"none\",\n","            \"price_closes_above\": {\"level\": f\"{trade_level}_resistance\"},\n","             # \"channel_width_ok\": {\"level\": trade_level, \"min_pct\": min_width}, # Largeur peut être moins pertinente pour breakout pur\n","             **trend_cond_long # Filtre optionnel\n","        }\n","        rules.append({\"conditions\": conditions_kl, \"action\": current_breakout_long})\n","\n","         # Breakout Short Rule\n","        conditions_ks = {\n","            \"in_position\": \"none\",\n","            \"price_closes_below\": {\"level\": f\"{trade_level}_support\"},\n","             # \"channel_width_ok\": {\"level\": trade_level, \"min_pct\": min_width},\n","             **trend_cond_short # Filtre optionnel\n","        }\n","        rules.append({\"conditions\": conditions_ks, \"action\": current_breakout_short})\n","\n","    # Règle par défaut\n","    rules.append({ \"conditions\": {}, \"action\": \"do_nothing\" })\n","\n","    # Assembler le chromosome final\n","    strategy_chromosome = {\n","        \"name\": strategy_name,\n","        \"parameters\": params, # Garder une trace des paramètres utilisés\n","        \"general_params\": {\n","            \"max_concurrent_trades\": 1,\n","            \"risk_per_trade_pct_equity\": params['risk_per_trade_pct'],\n","            \"min_channel_width_pct_for_bounce\": min_width if params['signal_type'] in ['bounce', 'both'] else 999, # Inactif si pas de bounce\n","        },\n","        \"rules\": rules\n","    }\n","    return strategy_chromosome\n","\n","print(\"Fonction de création de stratégie définie.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CELLULE 7.1.3 : Définition des Fonctions Helper pour Simulation GA (Syntaxe Finale Corrigée v9)\n","\n","import pandas as pd\n","import numpy as np\n","from datetime import timezone, datetime # Assurer imports\n","import traceback # Pour le debug si besoin\n","\n","# --- Imports QC ou Placeholders ---\n","try:\n","    from AlgorithmImports import OrderFeeParameters, OrderType, Security, Symbol, ConstantFeeModel, MarketOrder\n","    print(\"Imports QC OK pour helpers GA.\")\n","    qcimports_ok_helper = True\n","except ImportError:\n","    print(\"WARN: QC Imports manquants pour helpers GA. Définition de placeholders.\")\n","    qcimports_ok_helper = False\n","    # --- Placeholders Globaux (Syntaxe Finale Corrigée) ---\n","    if 'MarketOrder' not in locals():\n","        class MarketOrder:                              # Correct: Classe sur sa ligne\n","            def __init__(self, symbol, quantity, time): # Correct: Init indenté\n","                self.Symbol = symbol\n","                self.Quantity = quantity\n","                self.Time = time\n","                self.Price = 1 # Dummy\n","                self.Status = 0 # Dummy\n","\n","    if 'OrderFeeValue' not in locals():             # Check pour la classe externe\n","        class OrderFeeValue: Amount = 0.0           # Si elle manque, la définir\n","\n","    if 'OrderFeeParameters' not in locals():\n","        class OrderFeeParameters:                       # Correct: Classe sur sa ligne\n","            def __init__(self, security, order):      # Correct: Init indenté\n","                pass\n","            class OrderFeeValueInternal_7_1_3: Amount = 0.0 # Nom unique\n","            Value = OrderFeeValueInternal_7_1_3()         # Correct: Instance comme attribut de classe\n","\n","    if 'OrderType' not in locals():\n","        class OrderType:                                # Correct: Classe sur sa ligne\n","            Market = 1                                  # Correct: Attribut indenté\n","\n","    if 'Security' not in locals():\n","        class Security:                                 # Correct: Classe sur sa ligne\n","            pass                                        # Correct: pass indenté\n","\n","    if 'Symbol' not in locals():\n","        class Symbol:                                   # Correct: Classe sur sa ligne\n","            pass                                        # Correct: pass indenté\n","\n","    if 'ConstantFeeModel' not in locals():\n","        class ConstantFeeModel:                         # Correct: Classe sur sa ligne\n","            def __init__(self, fee):                  # Correct: Init indenté\n","                self.fee = fee\n","            def GetOrderFee(self, parameters):        # Correct: Méthode indentée\n","                # Suppose que OrderFeeParameters (global/placeholder) existe déjà\n","                if 'OrderFeeParameters' not in globals():\n","                    raise NameError(\"Placeholder global OrderFeeParameters non défini\")\n","                # Doit retourner un objet avec .Value.Amount ou un numérique\n","                return OrderFeeParameters.Value\n","    # --- Fin Placeholders ---\n","\n","# --- Définition symbol_security (tentative - Syntaxe Corrigée et Lisibilité Améliorée) ---\n","print(\"Définition de l'objet Security pour le symbole...\")\n","if 'symbol_security' not in locals():\n","    if 'qb' in locals() and 'btc_symbol' in locals() and qcimports_ok_helper:\n","        try:\n","            symbol_security = qb.Securities[btc_symbol]\n","            print(f\"Objet Security pour {btc_symbol} récupéré.\")\n","        except Exception as e:\n","            print(f\"WARN: Récupération Security échouée: {e}.\")\n","            qcimports_ok_helper = False # Marquer comme échoué si la récupération rate\n","\n","    # Vérifier si on a besoin du placeholder (si l'import ou la récupération a échoué, ou si non défini)\n","    if not qcimports_ok_helper or 'symbol_security' not in locals():\n","        print(\"Création placeholder MinimalSecurity...\")\n","        # *** SYNTAXE CORRIGÉE ICI ***\n","        class MinimalSecurity:                      # Correct: Classe sur sa ligne\n","            Symbol = 'BTCUSDT'                      # Correct: Attribut indenté\n","            class QuoteCurrencyClass:               # Correct: Classe imbriquée indentée\n","                Symbol = 'USDT'\n","            QuoteCurrency = QuoteCurrencyClass()    # Correct: Instance comme attribut indenté\n","        symbol_security = MinimalSecurity()         # Correct: Instanciation après définition\n","else:\n","    print(\"Objet 'symbol_security' déjà défini.\")\n","\n","# --- Définition fee_model (tentative - Syntaxe Corrigée et Lisibilité Améliorée) ---\n","print(\"Définition du modèle de frais...\")\n","if 'fee_model' not in locals():\n","    # Tenter de récupérer via qb seulement si les imports sont OK ET symbol_security a QuoteCurrency\n","    if 'qb' in locals() and qcimports_ok_helper and hasattr(symbol_security, 'QuoteCurrency'):\n","        try:\n","            fee_model = qb.BrokerageModel.GetFeeModel(symbol_security)\n","            print(f\"fee_model OK (qb): {type(fee_model)}\")\n","        except Exception as e:\n","            print(f\"WARN: Récupération FeeModel échouée: {e}.\")\n","            qcimports_ok_helper = False # Marquer comme échoué ici aussi pourrait être pertinent\n","\n","    # Créer placeholder si nécessaire (imports KO, récupération KO, ou fee_model non défini)\n","    if not qcimports_ok_helper or 'fee_model' not in locals():\n","        print(\"Création placeholder ConstantFeeModel(0)...\")\n","        if 'ConstantFeeModel' not in globals(): # Vérifier si la classe placeholder existe\n","             raise NameError(\"Placeholder ConstantFeeModel n'a pas été défini globalement\")\n","        fee_model = ConstantFeeModel(0)\n","else:\n","    print(f\"Modèle de frais 'fee_model' déjà défini: {type(fee_model)}\")\n","\n","\n","print(\"Définition/Mise à jour des fonctions helper pour la simulation GA...\")\n","\n","# --- Fonctions get_line_params_time, get_channel_value_at_time ---\n","# (Supposées correctes, mais ajout d'un placeholder pour get_line_params_time si manquant)\n","if 'get_line_params_time' not in locals():\n","    print(\"WARN: Fonction get_line_params_time non trouvée, redéfinition locale.\")\n","    def get_line_params_time(p1tn, p1p, p2tn, p2p):\n","        diff = p2tn - p1tn\n","        if abs(diff) < 1e-9:\n","            return (np.inf, p1tn) # Retourner infini pour pente, et une coordonnée comme 'intercept'\n","        elif abs(p2p - p1p) < 1e-9:\n","             return (0.0, p1p) # Pente 0, intercept = prix constant\n","        else:\n","            slope = (p2p - p1p) / diff\n","            intercept = p1p - slope * p1tn\n","            return (slope, intercept)\n","\n","def get_channel_value_at_time(channel_info, time_numeric):\n","    # Vérification initiale plus robuste\n","    if not isinstance(channel_info, (list, tuple)) or len(channel_info) != 2:\n","        return np.nan\n","    if not all(isinstance(p, dict) for p in channel_info):\n","        return np.nan\n","\n","    # Utiliser .get avec default pour éviter les erreurs si les clés n'existent pas\n","    p1 = channel_info[0]\n","    p2 = channel_info[1]\n","    p1_time = p1.get('time_numeric', np.nan)\n","    p1_price = p1.get('price', np.nan)\n","    p2_time = p2.get('time_numeric', np.nan)\n","    p2_price = p2.get('price', np.nan)\n","\n","    # Vérifier si toutes les valeurs nécessaires sont valides\n","    if any(pd.isna(x) for x in [p1_time, p1_price, p2_time, p2_price]):\n","        return np.nan\n","\n","    try:\n","        slope, intercept = get_line_params_time(p1_time, p1_price, p2_time, p2_price)\n","        # Si la pente est infinie (ligne verticale), on ne peut pas calculer de valeur à un temps donné\n","        if slope == np.inf:\n","            return np.nan\n","        # Si la pente est nulle, la valeur est constante (l'intercept)\n","        elif slope == 0.0:\n","            return intercept\n","        # Calcul standard\n","        else:\n","            return slope * time_numeric + intercept\n","    except Exception as e:\n","        # print(f\"Erreur dans get_channel_value_at_time: {e}\") # Pour debug\n","        return np.nan\n","\n","def check_conditions(conditions_dict, current_state):\n","    if not conditions_dict: return True\n","\n","    price = current_state.get('price')\n","    time_numeric = current_state.get('time_numeric')\n","    position_status_numeric = current_state.get('position_status_numeric') # Changed key\n","    active_channels = current_state.get('active_channels')\n","\n","    if any(x is None for x in [price, time_numeric, position_status_numeric, active_channels]):\n","        return False\n","\n","    for condition_key, condition_value in conditions_dict.items():\n","        if condition_key == \"in_position\":\n","            is_in_position = (position_status_numeric != 0)\n","            if is_in_position != condition_value: return False\n","        elif condition_key == \"price_near_level\":\n","            level_id = condition_value.get(\"level\")\n","            threshold_pct = condition_value.get(\"threshold_pct\", 0.001)\n","            if not level_id or '_' not in level_id: return False\n","            slope_name, line_type = level_id.split('_', 1)\n","            channel_info = active_channels.get(slope_name, {}).get(line_type)\n","            if not channel_info: return False\n","            level_value = get_channel_value_at_time(channel_info, time_numeric)\n","            if pd.isna(level_value): return False\n","            if not (level_value * (1 - threshold_pct) <= price <= level_value * (1 + threshold_pct)): return False\n","        elif condition_key == \"price_closes_above\":\n","            level_id = condition_value.get(\"level\")\n","            if not level_id or \"_\" not in level_id: return False\n","            slope_name, line_type = level_id.split(\"_\", 1)\n","            channel_info = active_channels.get(slope_name, {}).get(line_type)\n","            if not channel_info: return False\n","            level_value = get_channel_value_at_time(channel_info, time_numeric)\n","            if pd.isna(level_value) or not (price > level_value): return False\n","        elif condition_key == \"price_closes_below\":\n","            level_id = condition_value.get(\"level\")\n","            if not level_id or \"_\" not in level_id: return False\n","            slope_name, line_type = level_id.split(\"_\", 1)\n","            channel_info = active_channels.get(slope_name, {}).get(line_type)\n","            if not channel_info: return False\n","            level_value = get_channel_value_at_time(channel_info, time_numeric)\n","            if pd.isna(level_value) or not (price < level_value): return False\n","        elif condition_key == \"channel_width_ok\":\n","            slope_name = condition_value.get(\"level\")\n","            min_width_pct = condition_value.get(\"min_pct\", 0.01)\n","            if not slope_name: return False\n","            resistance_info = active_channels.get(slope_name, {}).get(\"resistance\")\n","            support_info = active_channels.get(slope_name, {}).get(\"support\")\n","            if not resistance_info or not support_info: return False\n","            resistance_value = get_channel_value_at_time(resistance_info, time_numeric)\n","            support_value = get_channel_value_at_time(support_info, time_numeric)\n","            if pd.isna(resistance_value) or pd.isna(support_value) or resistance_value <= support_value: return False\n","            if (resistance_value - support_value) < (price * min_width_pct): return False\n","        elif condition_key == \"channel_trend\":\n","            slope_name = condition_value.get(\"level\")\n","            required_direction = condition_value.get(\"direction\")\n","            if not slope_name or not required_direction: return False\n","            resistance_info = active_channels.get(slope_name, {}).get(\"resistance\")\n","            support_info = active_channels.get(slope_name, {}).get(\"support\")\n","            def is_valid_pivot_pair(p_pair):\n","                 return (isinstance(p_pair, (list, tuple)) and len(p_pair) == 2 and\n","                         all(isinstance(p, dict) and 'time_numeric' in p and 'price' in p and\n","                             not pd.isna(p['time_numeric']) and not pd.isna(p['price']) for p in p_pair))\n","            if not is_valid_pivot_pair(resistance_info) or not is_valid_pivot_pair(support_info): return False\n","            try:\n","                mr, _ = get_line_params_time(resistance_info[0]['time_numeric'], resistance_info[0]['price'], resistance_info[1]['time_numeric'], resistance_info[1]['price'])\n","                ms, _ = get_line_params_time(support_info[0]['time_numeric'], support_info[0]['price'], support_info[1]['time_numeric'], support_info[1]['price'])\n","                if mr == np.inf or ms == np.inf: return False\n","                tolerance = 1e-9\n","                is_trending_up = (mr >= -tolerance and ms >= -tolerance) and not (abs(mr) < tolerance and abs(ms) < tolerance)\n","                is_trending_down = (mr <= tolerance and ms <= tolerance) and not (abs(mr) < tolerance and abs(ms) < tolerance)\n","                if required_direction == \"up\" and not is_trending_up: return False\n","                if required_direction == \"down\" and not is_trending_down: return False\n","            except Exception as e: return False\n","    return True # All conditions passed\n","\n","def calculate_position_size(equity, risk_per_trade_pct, entry_price, stop_loss_price):\n","    if entry_price <= 0 or equity <= 0: return 0.0\n","    if stop_loss_price is None or pd.isna(stop_loss_price): return 0.0\n","    cash_at_risk = equity * risk_per_trade_pct\n","    risk_per_unit = abs(entry_price - stop_loss_price)\n","    if risk_per_unit < 1e-9: return 0.0\n","    quantity = cash_at_risk / risk_per_unit\n","    min_quantity = 1e-5 # Example\n","    if quantity < min_quantity: return 0.0\n","    else: return quantity\n","\n","# --- Fonction apply_fees (Amélioration Lisibilité) ---\n","def apply_fees(order, symbol_security_obj, fee_model_obj):\n","    \"\"\" Simule l'application des frais en utilisant les objets passés. \"\"\"\n","    try:\n","        # Vérifier si les objets nécessaires et leurs méthodes/attributs existent\n","        can_use_model = (hasattr(fee_model_obj, 'GetOrderFee') and\n","                         hasattr(symbol_security_obj, 'Symbol') and\n","                         hasattr(order, 'Quantity') and\n","                         hasattr(order, 'Symbol'))\n","\n","        if can_use_model:\n","            # Vérifier la présence de la classe nécessaire (importée ou placeholder)\n","            if 'OrderFeeParameters' not in globals():\n","                raise NameError(\"Classe OrderFeeParameters non trouvée globalement.\")\n","\n","            fee_params = OrderFeeParameters(symbol_security_obj, order)\n","            fee_result = fee_model_obj.GetOrderFee(fee_params)\n","\n","            if hasattr(fee_result, 'Value') and hasattr(fee_result.Value, 'Amount'):\n","                fee = fee_result.Value.Amount\n","            elif isinstance(fee_result, (int, float)):\n","                fee = fee_result\n","            else:\n","                # print(f\"WARN: Structure retour GetOrderFee inconnue ({type(fee_result)}). Frais=0.\")\n","                fee = 0.0\n","            return abs(fee)\n","        else:\n","            # Fallback si le modèle ne peut pas être utilisé\n","            # print(\"WARN: Utilisation calcul frais fallback (0.1%) dans apply_fees.\")\n","            order_price = getattr(order, 'Price', 1)\n","            order_price = order_price if order_price is not None and order_price > 0 else 1\n","            order_quantity = getattr(order, 'Quantity', 0)\n","            order_value = abs(order_quantity * order_price)\n","            return order_value * 0.001 # Frais de 0.1%\n","\n","    except Exception as e:\n","        print(f\"ERREUR dans apply_fees: {e}\")\n","        # traceback.print_exc() # Décommenter pour la trace complète\n","        return 0.0\n","\n","print(\"Fonctions helper pour la simulation GA définies/mises à jour (Syntaxe v9 Finale, Lisibilité Améliorée).\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CELLULE 7.1.4 : Calcul de l'Historique des Canaux (basé sur final_config) (DÉBUT MODIFIÉ)\n","\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import traceback\n","import numpy as np # Assurer import\n","import math # Assurer import\n","\n","print(\"Calcul de l'historique des canaux basé sur la configuration fixe...\")\n","\n","# --- Vérifications des dépendances ---\n","if 'bars_df' not in locals(): raise NameError(\"bars_df non défini.\")\n","\n","# *** AJOUT : Assurer que final_config existe et l'assigner ***\n","if 'final_config' not in locals():\n","    raise NameError(\"La variable 'final_config' (contenant les paramètres de canaux choisis) n'a pas été définie. Exécutez la cellule 'Identification Configuration Cible' (anciennement 16/27) avant celle-ci.\")\n","fixed_channel_config = final_config\n","# *** FIN AJOUT ***\n","\n","# Vérifier la fonction de calcul\n","if 'calculate_all_channels_at_date' not in locals() or not callable(calculate_all_channels_at_date):\n","    raise NameError(\"Fonction 'calculate_all_channels_at_date' non définie. Assurez-vous que la cellule précédente (Helper) a été exécutée.\")\n","\n","# --- Paramètres pour ce calcul (Reste identique) ---\n","lookback_days_hist = 180\n","zigzag_thresh_hist = 0.05 # Utiliser le seuil standardisé\n","recalc_frequency_hist = pd.Timedelta(days=1)\n","hist_start_date = bars_df['time'].min()\n","hist_end_date = bars_df['time'].max()\n","data_tz = bars_df['time'].dt.tz\n","\n","channel_history = {}\n","last_calculated_channels_hist = None\n","\n","# --- Génération Dates Recalcul (Reste identique) ---\n","recalc_dates_hist = pd.date_range(start=hist_start_date + pd.Timedelta(days=lookback_days_hist),\n","                                end=hist_end_date, freq=recalc_frequency_hist, tz=data_tz)\n","if recalc_dates_hist.empty:\n","    raise ValueError(\"Pas de dates de recalcul générées.\")\n","\n","print(f\"Utilisation de la configuration de canaux fixe : '{fixed_channel_config.get('label', 'N/A')}' pour le calcul de l'historique...\")\n","\n","# --- Boucle de Calcul (Reste identique) ---\n","for recalc_date in tqdm(recalc_dates_hist, desc=\"Calcul Historique Canaux Fixes\"):\n","    actual_recalc_time = bars_df[bars_df['time'] <= recalc_date]['time'].max()\n","    if pd.isna(actual_recalc_time) or actual_recalc_time in channel_history: continue\n","\n","    # Appel à la fonction de calcul (utilise maintenant fixed_channel_config qui est défini)\n","    pivots, channels = calculate_all_channels_at_date(actual_recalc_time, bars_df, fixed_channel_config, zigzag_threshold=zigzag_thresh_hist)\n","\n","    if channels is not None:\n","        channel_history[actual_recalc_time] = channels\n","        last_calculated_channels_hist = channels\n","    elif last_calculated_channels_hist is not None:\n","        channel_history[actual_recalc_time] = last_calculated_channels_hist\n","\n","if not channel_history:\n","    raise ValueError(\"Échec complet du calcul de l'historique des canaux.\")\n","\n","# --- Création DataFrame (Reste identique) ---\n","channel_history_df = pd.DataFrame.from_dict(channel_history, orient='index')\n","channel_history_df.index = pd.to_datetime(channel_history_df.index)\n","channel_history_df = channel_history_df.sort_index()\n","print(f\"Historique des canaux (channel_history_df) créé avec {len(channel_history_df)} entrées.\")\n","# print(channel_history_df.head()) # Décommenter pour vérifier\n","\n","# --- Préparation loop_bars_all (Reste identique) ---\n","sim_start_date_ga = pd.Timestamp(\"2024-01-01\", tz=data_tz)\n","loop_bars_all = bars_df[bars_df['time'] >= sim_start_date_ga].copy()\n","if loop_bars_all.empty:\n","    raise ValueError(f\"Aucune donnée de barre disponible après {sim_start_date_ga} pour la simulation GA.\")\n","print(f\"Période de simulation pour le GA: {loop_bars_all['time'].min()} à {loop_bars_all['time'].max()}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CELLULE GA-1 : Setup de l'Algorithme Génétique avec DEAP (CORRIGÉE v2 + LOGGING)\n","\n","import random\n","import numpy as np\n","import pandas as pd\n","from deap import base, creator, tools, algorithms\n","from tqdm.notebook import tqdm\n","import traceback\n","import logging\n","\n","# S'assurer que les imports spécifiques QC sont présents si nécessaire hors de l'env QC\n","try:\n","    from AlgorithmImports import MarketOrder, OrderFeeParameters, ConstantFeeModel\n","except ImportError:\n","    print(\"WARN: QuantConnect imports non trouvés. Simulation avec ConstantFeeModel(0).\")\n","    class MarketOrder:\n","        def __init__(self, symbol, quantity, time): pass\n","    class OrderFeeParameters:\n","         def __init__(self, security, order): pass\n","         class OrderFeeValue:\n","             Amount = 0.0\n","         Value = OrderFeeValue()\n","    class ConstantFeeModel:\n","        def __init__(self, fee): self.fee = fee\n","        def GetOrderFee(self, parameters): return OrderFeeParameters.Value\n","\n","\n","# --- Configuration du Logging ---\n","log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n","# Mettre logging.DEBUG pour voir l'évaluation de chaque individu (très verbeux)\n","logging.basicConfig(level=logging.INFO, format=log_format)\n","logger = logging.getLogger(\"GA_Optimizer\")\n","# ------------------------------\n","\n","logger.info(\"Configuration de l'algorithme génétique (DEAP)...\")\n","\n","# --- 1. Vérification des Dépendances ---\n","if 'strategy_simulation_results' not in locals():\n","    logger.warning(\"'strategy_simulation_results' non trouvé.\")\n","if 'calculate_position_size' not in locals(): raise NameError(\"Fonction 'calculate_position_size' non définie.\")\n","if 'apply_fees' not in locals(): raise NameError(\"Fonction 'apply_fees' non définie.\")\n","if 'check_conditions' not in locals(): raise NameError(\"Fonction 'check_conditions' non définie.\")\n","if 'get_channel_value_at_time' not in locals(): raise NameError(\"Fonction 'get_channel_value_at_time' non définie.\")\n","if 'channel_history_df' not in locals() or channel_history_df.empty:\n","     raise NameError(\"L'historique des canaux ('channel_history_df') doit être calculé avant d'exécuter le GA.\")\n","if 'fixed_channel_config' not in locals():\n","     raise NameError(\"La configuration fixe des canaux ('fixed_channel_config') doit être définie.\")\n","if 'loop_bars_all' not in locals() or loop_bars_all.empty:\n","     raise NameError(\"Le DataFrame 'loop_bars_all' contenant les barres de simulation n'est pas défini ou est vide.\")\n","if 'symbol_security' not in locals(): raise NameError(\"'symbol_security' non défini.\")\n","if 'fee_model' not in locals():\n","    logger.warning(\"Modèle de frais non défini, simulation sans frais.\")\n","    fee_model = ConstantFeeModel(0)\n","if 'initial_cash' not in locals(): initial_cash = 10000 # Définir une valeur par défaut\n","if 'btc_symbol' not in locals(): btc_symbol = None # Définir placeholder\n","\n","\n","# --- 2. Espace Paramétrique (Chromosome) ---\n","if 'param_space' not in locals(): raise NameError(\"L'espace paramétrique 'param_space' n'est pas défini.\")\n","if 'param_keys' not in locals(): raise NameError(\"'param_keys' n'est pas défini.\")\n","\n","# --- 3. Fonction Fitness (Évaluation d'un individu) ---\n","memoization_cache = {}\n","\n","def evaluate_strategy(individual):\n","    \"\"\"\n","    Fonction de fitness pour l'algorithme génétique.\n","    Évalue une stratégie (représentée par 'individual') en utilisant l'historique\n","    de canaux pré-calculé ('channel_history_df').\n","    Retourne une tuple (fitness_value,) - DEAP requiert un tuple.\n","    \"\"\"\n","    global initial_cash, loop_bars_all, channel_history_df, btc_symbol, symbol_security, fee_model # Accéder aux variables globales nécessaires\n","\n","    individual_tuple = tuple(individual)\n","    # logger.debug(f\"Début évaluation individu: {individual_tuple}\")\n","    if individual_tuple in memoization_cache:\n","        return memoization_cache[individual_tuple]\n","\n","    try:\n","        if 'create_strategy_from_params' not in globals(): raise NameError(\"create_strategy_from_params non définie\")\n","        strategy_config = create_strategy_from_params(individual, param_keys)\n","        strat_gen_params = strategy_config.get('general_params', {})\n","        strat_rules = strategy_config.get('rules', [])\n","        risk_per_trade_pct = strat_gen_params.get('risk_per_trade_pct_equity', 0.01)\n","\n","        sim_cash = initial_cash; sim_position_qty = 0.0; sim_entry_price = 0.0\n","        sim_equity = initial_cash; active_stop_loss = None; active_take_profit = None\n","        last_known_channels_sim_ga = None\n","\n","        for index, row in loop_bars_all.iterrows():\n","            current_time = row['time']; current_price = row['close']\n","            current_high = row['high']; current_low = row['low']\n","            current_time_num = current_time.timestamp()\n","\n","            valid_channel_time_index = channel_history_df.index[channel_history_df.index <= current_time]\n","            current_active_channels = None\n","            if not valid_channel_time_index.empty:\n","                valid_channel_time = valid_channel_time_index.max()\n","                try:\n","                    current_active_channels = channel_history_df.loc[valid_channel_time].to_dict()\n","                    last_known_channels_sim_ga = current_active_channels\n","                except Exception as e: current_active_channels = last_known_channels_sim_ga\n","            else: current_active_channels = last_known_channels_sim_ga\n","            if current_active_channels is None: continue\n","\n","            exit_executed = False\n","            if sim_position_qty != 0:\n","                 pnl = 0.0; exit_reason = \"\"; close_position_now = False; exit_price = current_price\n","                 if active_stop_loss is not None:\n","                     if sim_position_qty > 0 and current_low <= active_stop_loss: close_position_now=True; exit_reason=\"Stop Loss\"; exit_price=active_stop_loss\n","                     elif sim_position_qty < 0 and current_high >= active_stop_loss: close_position_now=True; exit_reason=\"Stop Loss\"; exit_price=active_stop_loss\n","                 if not close_position_now and active_take_profit is not None:\n","                      if sim_position_qty > 0 and current_high >= active_take_profit: close_position_now=True; exit_reason=\"Take Profit\"; exit_price=active_take_profit\n","                      elif sim_position_qty < 0 and current_low <= active_take_profit: close_position_now=True; exit_reason=\"Take Profit\"; exit_price=active_take_profit\n","                 if close_position_now:\n","                     exit_order = MarketOrder(btc_symbol, -sim_position_qty, current_time)\n","                     exit_fee = apply_fees(exit_order, symbol_security, fee_model)\n","                     pnl = sim_position_qty * (exit_price - sim_entry_price)\n","                     sim_cash += sim_position_qty * exit_price; sim_cash -= exit_fee\n","                     sim_position_qty = 0.0; sim_entry_price = 0.0; active_stop_loss = None; active_take_profit = None; exit_executed = True\n","\n","            if sim_position_qty == 0 and not exit_executed:\n","                current_state = { 'price': current_price, 'time_numeric': current_time_num, 'position_status': 'none', 'active_channels': current_active_channels }\n","                for rule in strat_rules:\n","                    conditions = rule.get('conditions', {}); action_template = rule.get('action')\n","\n","                    # <-- Logique corrigée pour action_details -->\n","                    if isinstance(action_template, dict):\n","                        action_details = action_template\n","                    elif action_template == \"do_nothing\":\n","                        action_details = {\"action_type\": \"do_nothing\"}\n","                    else:\n","                        action_details = {} # Cas par défaut ou erreur\n","\n","                    if check_conditions(conditions, current_state):\n","                        action_type = action_details.get(\"action_type\")\n","                        if action_type in [\"bounce_long\", \"breakout_long\", \"bounce_short\", \"breakout_short\"]:\n","                            entry_price = current_price; signal = 1 if \"long\" in action_type else -1\n","                            sl_price = None; tp_price = None; risk_amount_per_unit = 0\n","\n","                            # --- Calcul SL/TP ---\n","                            is_bounce_action = 'bounce' in action_type\n","                            # !! Utilise strategy_config['parameters'] pour les params de l'individu actuel !!\n","                            strat_origin_params = strategy_config.get('parameters', {})\n","                            original_sl_type = strat_origin_params.get('bounce_sl_type' if is_bounce_action else 'breakout_sl_type')\n","                            original_sl_value_key = 'bounce_sl_value' if is_bounce_action else 'breakout_sl_value'\n","                            sl_val = strat_origin_params.get(original_sl_value_key)\n","\n","                            if original_sl_type is None or sl_val is None: # Vérifier si les clés existent\n","                                logger.warning(f\"Paramètres SL manquants pour {action_type} dans {strategy_config.get('name')}\")\n","                                continue # Ne pas prendre le trade si SL non défini\n","\n","                            sl_pct_entry = sl_val if original_sl_type == 'pct_entry' else None\n","                            sl_pct_level = sl_val if original_sl_type == 'pct_level' else None\n","\n","                            if signal == 1: # Long SL\n","                                if sl_pct_entry: sl_price = entry_price * (1 - sl_pct_entry)\n","                                elif sl_pct_level:\n","                                    level_name = conditions.get(\"price_near_level\",{}).get(\"level\") or conditions.get(\"price_closes_above\",{}).get(\"level\") or f\"{strat_origin_params.get('trade_level','micro')}_resistance\" # Ajout fallback niveau\n","                                    scale, line_type = level_name.split('_')\n","                                    level_val = get_channel_value_at_time(current_active_channels.get(scale, {}).get(line_type), current_time_num)\n","                                    sl_price = level_val * (1 - sl_pct_level) if not pd.isna(level_val) else entry_price * (1 - sl_val)\n","                                else: sl_price = entry_price * (1 - sl_val)\n","                            else: # Short SL\n","                                if sl_pct_entry: sl_price = entry_price * (1 + sl_pct_entry)\n","                                elif sl_pct_level:\n","                                    level_name = conditions.get(\"price_near_level\",{}).get(\"level\") or conditions.get(\"price_closes_below\",{}).get(\"level\") or f\"{strat_origin_params.get('trade_level','micro')}_support\" # Ajout fallback niveau\n","                                    scale, line_type = level_name.split('_')\n","                                    level_val = get_channel_value_at_time(current_active_channels.get(scale, {}).get(line_type), current_time_num)\n","                                    sl_price = level_val * (1 + sl_pct_level) if not pd.isna(level_val) else entry_price * (1 + sl_val)\n","                                else: sl_price = entry_price * (1 + sl_val)\n","\n","                            target_rr_key = 'bounce_tp_value' if is_bounce_action else 'breakout_tp_value'\n","                            target_rr = strat_origin_params.get(target_rr_key)\n","\n","                            if sl_price is not None and target_rr is not None:\n","                                risk_amount_per_unit = abs(entry_price - sl_price)\n","                                if risk_amount_per_unit > 1e-9:\n","                                    tp_price = entry_price + signal * risk_amount_per_unit * target_rr\n","                            # --- Fin Calcul SL/TP ---\n","\n","                            if sl_price is not None and risk_amount_per_unit > 1e-9:\n","                                 sim_equity_before_trade = sim_cash\n","                                 qty_to_trade = calculate_position_size(sim_equity_before_trade, risk_per_trade_pct, entry_price, sl_price)\n","                                 if qty_to_trade > 0:\n","                                     entry_order = MarketOrder(btc_symbol, signal * qty_to_trade, current_time)\n","                                     entry_fee = apply_fees(entry_order, symbol_security, fee_model)\n","                                     sim_position_qty = signal * qty_to_trade; sim_entry_price = entry_price\n","                                     sim_cash -= sim_position_qty * sim_entry_price; sim_cash -= entry_fee\n","                                     active_stop_loss = sl_price; active_take_profit = tp_price\n","                                     # Pas de log de trade ici pour la performance\n","                                     break # Sortir boucle règles\n","                        elif action_type == \"do_nothing\":\n","                            break # Sortir boucle règles\n","\n","        # --- Fin Logique Trading ---\n","        sim_equity = sim_cash + sim_position_qty * current_price\n","\n","        final_equity = sim_cash + sim_position_qty * current_price\n","        fitness = final_equity if final_equity > 1 else 1.0\n","        result_tuple = (fitness,)\n","        memoization_cache[individual_tuple] = result_tuple\n","        # logger.debug(f\"Fin évaluation individu -> Fitness: {fitness:.2f}\")\n","        return result_tuple\n","\n","    except Exception as e:\n","        logger.exception(f\"Erreur pendant l'évaluation de l'individu {individual_tuple}:\")\n","        memoization_cache[individual_tuple] = (0.0,)\n","        return (0.0,)\n","\n","\n","# --- 4. Configuration DEAP (Identique) ---\n","# (Assurez-vous que cette partie est bien présente et correcte dans votre cellule)\n","# Si FitnessMax existe déjà, la ligne creator.create peut lever une erreur,\n","# ce qui est normal si vous exécutez la cellule plusieurs fois.\n","try:\n","    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n","    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n","except Exception:\n","    logger.info(\"DEAP creators 'FitnessMax' et/ou 'Individual' existent déjà.\")\n","    pass # Continuer si elles existent déjà\n","\n","toolbox = base.Toolbox()\n","gene_generators = []\n","for key in param_keys:\n","    options = param_space[key]\n","    if isinstance(options[0], str): gene_generators.append(lambda opts=options: random.choice(opts))\n","    elif isinstance(options[0], (int, float)):\n","         if isinstance(options[0], float) or len(options) == 2:\n","             min_val, max_val = min(options), max(options)\n","             if isinstance(min_val, float) or isinstance(max_val, float): gene_generators.append(lambda mn=min_val, mx=max_val: random.uniform(mn, mx))\n","             else: gene_generators.append(lambda opts=options: random.choice(opts))\n","         else: gene_generators.append(lambda opts=options: random.choice(opts))\n","    else: raise TypeError(f\"Type de paramètre non supporté pour {key}: {type(options[0])}\")\n","\n","toolbox.register(\"individual\", tools.initCycle, creator.Individual, gene_generators, n=1)\n","toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n","toolbox.register(\"evaluate\", evaluate_strategy)\n","toolbox.register(\"mate\", tools.cxTwoPoint)\n","def mutate_individual(individual, indpb=0.1): # Fonction mutate identique\n","    for i in range(len(individual)):\n","        if random.random() < indpb:\n","            key = param_keys[i]; options = param_space[key]\n","            if isinstance(options[0], str): individual[i] = random.choice(options)\n","            elif isinstance(options[0], (int, float)):\n","                 if isinstance(options[0], float) or len(options) == 2:\n","                     min_val, max_val = min(options), max(options)\n","                     if isinstance(min_val, float) or isinstance(max_val, float): individual[i] = random.uniform(min_val, max_val)\n","                     else: individual[i] = random.choice(options)\n","                 else: individual[i] = random.choice(options)\n","    return individual,\n","toolbox.register(\"mutate\", mutate_individual, indpb=0.1)\n","toolbox.register(\"select\", tools.selTournament, tournsize=3)\n","\n","logger.info(\"Configuration DEAP (corrigée v2 avec logging) terminée.\")"]},{"cell_type":"markdown","metadata":{},"source":["### 8.2. Exécution de l'Algorithme Génétique\n","\n","Cette cellule lance l'algorithme génétique DEAP configuré précédemment.\n","\n","*   **Paramètres GA :** `POPULATION_SIZE`, `GENERATIONS`, `CXPB` (probabilité de croisement), `MUTPB` (probabilité de mutation) contrôlent le processus d'évolution.\n","*   **Initialisation :** Une population initiale d'individus (stratégies aléatoires) est créée.\n","*   **Évolution (`algorithms.eaSimple`) :** La boucle principale du GA est exécutée. À chaque génération :\n","    *   La fitness de chaque individu est évaluée (simulation de la stratégie).\n","    *   Les meilleurs individus sont sélectionnés.\n","    *   Des opérateurs de croisement et de mutation sont appliqués pour créer la nouvelle génération.\n","*   **Suivi :** Des statistiques (min, max, avg fitness) sont collectées à chaque génération. Le meilleur individu trouvé (`hof`) est conservé.\n","*   **Résultat :** Affiche la meilleure fitness (équité finale) atteinte et les paramètres de la stratégie correspondante."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CELLULE GA-2 : Exécution de l'Algorithme Génétique (Avec Logging)\n","\n","logger.info(\"Lancement de l'algorithme génétique...\")\n","memoization_cache.clear() # Vider le cache avant une nouvelle exécution\n","\n","# Paramètres du GA\n","POPULATION_SIZE = 50\n","GENERATIONS = 20\n","CXPB = 0.7\n","MUTPB = 0.2\n","\n","# Initialiser la population\n","logger.info(f\"Initialisation de la population (taille={POPULATION_SIZE})...\")\n","pop = toolbox.population(n=POPULATION_SIZE)\n","logger.info(\"Population initialisée.\")\n","\n","# Garder une trace du meilleur individu\n","hof = tools.HallOfFame(1)\n","\n","# Statistiques pour suivre l'évolution\n","stats = tools.Statistics(lambda ind: ind.fitness.values)\n","stats.register(\"avg\", np.mean)\n","stats.register(\"std\", np.std)\n","stats.register(\"min\", np.min)\n","stats.register(\"max\", np.max)\n","\n","# Lancer l'algorithme\n","# Note : eaSimple loggue déjà des infos si verbose=True. On ajoute des logs avant/après.\n","logger.info(f\"Début de l'évolution pour {GENERATIONS} générations...\")\n","pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=CXPB, mutpb=MUTPB, ngen=GENERATIONS,\n","                                   stats=stats, halloffame=hof, verbose=True) # verbose=True donne les stats DEAP par génération\n","\n","logger.info(\"Algorithme génétique terminé.\")\n","\n","if hof: # Vérifier si le HallOfFame n'est pas vide\n","    best_individual = hof[0]\n","    best_fitness = best_individual.fitness.values[0]\n","    logger.info(f\"Meilleure Fitness (Equity Finale) trouvée : {best_fitness:,.2f}\")\n","    logger.info(\"Paramètres du meilleur individu :\")\n","    best_params_dict = dict(zip(param_keys, best_individual))\n","    for key, value in best_params_dict.items():\n","        if isinstance(value, float):\n","            logger.info(f\"  - {key}: {value:.4f}\")\n","        else:\n","            logger.info(f\"  - {key}: {value}\")\n","else:\n","    logger.warning(\"Aucun individu dans le HallOfFame après l'exécution du GA.\")\n","    best_individual = None # Pour éviter des erreurs plus loin\n","\n","# Vider le cache après l'exécution\n","memoization_cache.clear()\n","logger.debug(\"Cache de fitness vidé.\")"]},{"cell_type":"markdown","metadata":{},"source":["### 8.3. Analyse des Résultats du GA\n","\n","Après l'exécution de l'algorithme génétique, cette cellule analyse les résultats :\n","\n","1.  **Visualisation de la Convergence :** Un graphique montre l'évolution de la fitness maximale et moyenne au fil des générations. Cela permet de vérifier si l'algorithme a convergé vers une solution stable.\n","2.  **Re-simulation de la Meilleure Stratégie :** La stratégie correspondant au meilleur individu trouvé par le GA (stocké dans `hof[0]`) est simulée à nouveau, cette fois en enregistrant plus de détails (courbe d'équité, liste des trades).\n","3.  **Calcul des Métriques de Performance :** Pour la meilleure stratégie, des métriques clés sont calculées et affichées (Équité Finale, PnL Total, Win Rate, Max Drawdown, Nombre de Trades, Frais Totaux).\n","4.  **Affichage de la Courbe d'Équité :** La courbe de performance de la meilleure stratégie est tracée.\n","5.  **Rappel des Paramètres :** Les paramètres exacts de la meilleure stratégie trouvée sont affichés."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CELLULE GA-3 : Analyse des Résultats de l'AG (CORRIGÉE - Erreur action_details)\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as mtick\n","import heapq\n","import logging # Assurer l'import si ce n'est pas fait plus haut\n","import pandas as pd # Assurer l'import\n","import numpy as np # Assurer l'import\n","\n","# Récupérer le logger défini précédemment\n","logger = logging.getLogger(\"GA_Optimizer\")\n","\n","logger.info(\"--- Analyse des Résultats de l'Algorithme Génétique ---\")\n","\n","# --- 1. Visualisation de la Convergence (Identique) ---\n","if 'logbook' in locals() and logbook: # S'assurer que logbook existe\n","    gen = logbook.select(\"gen\")\n","    fit_max = logbook.select(\"max\")\n","    fit_avg = logbook.select(\"avg\")\n","\n","    fig, ax1 = plt.subplots(figsize=(12, 6))\n","    color = 'tab:red'; ax1.set_xlabel('Génération'); ax1.set_ylabel('Max Fitness (Equity)', color=color)\n","    ax1.plot(gen, fit_max, color=color, label=\"Max Fitness\"); ax1.tick_params(axis='y', labelcolor=color)\n","    ax1.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.0f'))\n","    ax2 = ax1.twinx(); color = 'tab:blue'; ax2.set_ylabel('Avg Fitness (Equity)', color=color)\n","    ax2.plot(gen, fit_avg, color=color, linestyle='--', label=\"Avg Fitness\"); ax2.tick_params(axis='y', labelcolor=color)\n","    ax2.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.0f'))\n","    fig.tight_layout(); plt.title('Convergence de la Fitness (Equity) au fil des Générations')\n","    lines, labels = ax1.get_legend_handles_labels(); lines2, labels2 = ax2.get_legend_handles_labels()\n","    ax2.legend(lines + lines2, labels + labels2, loc='center right'); plt.grid(True); plt.show()\n","else:\n","    logger.warning(\"Logbook non disponible, impossible de tracer la convergence.\")\n","\n","# --- 2. Re-simulation de la Meilleure Stratégie Trouvée (Avec Logging) ---\n","# Assurer que 'best_individual' existe et a été assigné dans la cellule précédente\n","if 'best_individual' in locals() and best_individual is not None:\n","    logger.info(\"Re-simulation de la meilleure stratégie trouvée pour analyse détaillée...\")\n","    # S'assurer que les fonctions nécessaires sont disponibles\n","    if 'create_strategy_from_params' not in globals(): raise NameError(\"create_strategy_from_params non définie\")\n","    if 'param_keys' not in globals(): raise NameError(\"param_keys non définis\")\n","\n","    best_strategy_config = create_strategy_from_params(best_individual, param_keys)\n","    best_strategy_name = best_strategy_config['name']\n","    logger.info(f\"Stratégie à re-simuler: {best_strategy_name}\")\n","\n","    # (Logique de simulation - identique à evaluate_strategy mais stocke trades/equity)\n","    sim_cash = initial_cash; sim_position_qty = 0.0; sim_entry_price = 0.0\n","    sim_equity = initial_cash; active_stop_loss = None; active_take_profit = None\n","    equity_curve_list_best = [{'time': sim_start_date - pd.Timedelta(hours=1), 'equity': initial_cash}]\n","    sim_trades_best = []\n","    strat_rules_best = best_strategy_config.get('rules', [])\n","    strat_gen_params_best = best_strategy_config.get('general_params', {})\n","    risk_per_trade_pct_best = strat_gen_params_best.get('risk_per_trade_pct_equity', 0.01)\n","    last_known_channels_sim_best = None\n","\n","    # Utiliser loop_bars_all qui doit être défini (contient les barres pour la simulation)\n","    if 'loop_bars_all' not in locals() or loop_bars_all.empty:\n","         raise NameError(\"Le DataFrame 'loop_bars_all' pour la simulation n'est pas défini ou est vide.\")\n","\n","    for index, row in loop_bars_all.iterrows(): # Pas de tqdm ici pour éviter surcharge logs\n","        current_time = row['time']; current_price = row['close']\n","        current_high = row['high']; current_low = row['low']\n","        current_time_num = current_time.timestamp()\n","\n","        # Trouver canaux actifs\n","        valid_channel_time_index = channel_history_df.index[channel_history_df.index <= current_time]\n","        current_active_channels = None\n","        if not valid_channel_time_index.empty:\n","            valid_channel_time = valid_channel_time_index.max()\n","            try:\n","                current_active_channels = channel_history_df.loc[valid_channel_time].to_dict()\n","                last_known_channels_sim_best = current_active_channels\n","            except Exception as e: current_active_channels = last_known_channels_sim_best\n","        else: current_active_channels = last_known_channels_sim_best\n","        if current_active_channels is None:\n","            sim_equity = sim_cash + sim_position_qty * current_price\n","            equity_curve_list_best.append({'time': current_time, 'equity': sim_equity})\n","            continue\n","\n","        # --- Logique de Trading (Sorties puis Entrées) ---\n","        exit_executed = False\n","        # 1. Gestion Sorties (SL/TP)\n","        if sim_position_qty != 0:\n","            pnl = 0.0; exit_reason = \"\"; close_position_now = False; exit_price = current_price\n","            if active_stop_loss is not None:\n","                if sim_position_qty > 0 and current_low <= active_stop_loss: close_position_now=True; exit_reason=\"Stop Loss\"; exit_price=active_stop_loss\n","                elif sim_position_qty < 0 and current_high >= active_stop_loss: close_position_now=True; exit_reason=\"Stop Loss\"; exit_price=active_stop_loss\n","            if not close_position_now and active_take_profit is not None:\n","                 if sim_position_qty > 0 and current_high >= active_take_profit: close_position_now=True; exit_reason=\"Take Profit\"; exit_price=active_take_profit\n","                 elif sim_position_qty < 0 and current_low <= active_take_profit: close_position_now=True; exit_reason=\"Take Profit\"; exit_price=active_take_profit\n","            if close_position_now:\n","                exit_order = MarketOrder(btc_symbol, -sim_position_qty, current_time)\n","                exit_fee = apply_fees(exit_order, symbol_security, fee_model)\n","                pnl = sim_position_qty * (exit_price - sim_entry_price)\n","                sim_cash += sim_position_qty * exit_price; sim_cash -= exit_fee\n","                sim_trades_best.append({'time': current_time, 'type': f'exit_{\"long\" if sim_position_qty > 0 else \"short\"}', 'price': exit_price, 'size': -sim_position_qty, 'pnl': pnl - exit_fee, 'fee': exit_fee, 'reason': exit_reason})\n","                sim_position_qty = 0.0; sim_entry_price = 0.0; active_stop_loss = None; active_take_profit = None; exit_executed = True\n","\n","        # 2. Gestion Entrées\n","        if sim_position_qty == 0 and not exit_executed:\n","            current_state = { 'price': current_price, 'time_numeric': current_time_num, 'position_status': 'none', 'active_channels': current_active_channels }\n","            for rule in strat_rules_best:\n","                conditions = rule.get('conditions', {}); action_template = rule.get('action')\n","\n","                # <-- CORRECTION ICI (appliquée dans la boucle de re-simulation)\n","                if isinstance(action_template, dict):\n","                    action_details = action_template\n","                elif action_template == \"do_nothing\":\n","                    action_details = {\"action_type\": \"do_nothing\"}\n","                else:\n","                    action_details = {}\n","\n","                if check_conditions(conditions, current_state):\n","                    action_type = action_details.get(\"action_type\")\n","                    if action_type in [\"bounce_long\", \"breakout_long\", \"bounce_short\", \"breakout_short\"]:\n","                        entry_price = current_price; signal = 1 if \"long\" in action_type else -1\n","                        sl_price = None; tp_price = None; risk_amount_per_unit = 0\n","\n","                        # --- Recalcul SL/TP (Identique) ---\n","                        is_bounce_action = 'bounce' in action_type\n","                        best_strat_origin_params = best_strategy_config.get('parameters', {})\n","                        original_sl_type = best_strat_origin_params['bounce_sl_type'] if is_bounce_action else best_strat_origin_params['breakout_sl_type']\n","                        original_sl_value_key_suffix = 'bounce_sl_value' if is_bounce_action else 'breakout_sl_value'\n","                        sl_val = best_strat_origin_params[original_sl_value_key_suffix]\n","                        sl_pct_entry = sl_val if original_sl_type == 'pct_entry' else None\n","                        sl_pct_level = sl_val if original_sl_type == 'pct_level' else None\n","                        if signal == 1: # Long SL\n","                            if sl_pct_entry: sl_price = entry_price * (1 - sl_pct_entry)\n","                            elif sl_pct_level:\n","                                level_name = conditions.get(\"price_near_level\",{}).get(\"level\") or conditions.get(\"price_closes_above\",{}).get(\"level\") or f\"{best_strat_origin_params['trade_level']}_resistance\"\n","                                scale, line_type = level_name.split('_')\n","                                level_val = get_channel_value_at_time(current_active_channels.get(scale, {}).get(line_type), current_time_num)\n","                                sl_price = level_val * (1 - sl_pct_level) if not pd.isna(level_val) else entry_price * (1 - sl_val)\n","                            else: sl_price = entry_price * (1 - sl_val)\n","                        else: # Short SL\n","                            if sl_pct_entry: sl_price = entry_price * (1 + sl_pct_entry)\n","                            elif sl_pct_level:\n","                                level_name = conditions.get(\"price_near_level\",{}).get(\"level\") or conditions.get(\"price_closes_below\",{}).get(\"level\") or f\"{best_strat_origin_params['trade_level']}_support\"\n","                                scale, line_type = level_name.split('_')\n","                                level_val = get_channel_value_at_time(current_active_channels.get(scale, {}).get(line_type), current_time_num)\n","                                sl_price = level_val * (1 + sl_pct_level) if not pd.isna(level_val) else entry_price * (1 + sl_val)\n","                            else: sl_price = entry_price * (1 + sl_val)\n","                        target_rr_key_suffix = 'bounce_tp_value' if is_bounce_action else 'breakout_tp_value'\n","                        target_rr = best_strat_origin_params[target_rr_key_suffix]\n","                        if sl_price is not None and target_rr is not None:\n","                            risk_amount_per_unit = abs(entry_price - sl_price)\n","                            if risk_amount_per_unit > 1e-9:\n","                                tp_price = entry_price + signal * risk_amount_per_unit * target_rr\n","                        # --- Fin Recalcul SL/TP ---\n","\n","                        if sl_price is not None and risk_amount_per_unit > 1e-9:\n","                             sim_equity_before_trade = sim_cash\n","                             qty_to_trade = calculate_position_size(sim_equity_before_trade, risk_per_trade_pct_best, entry_price, sl_price)\n","                             if qty_to_trade > 0:\n","                                 entry_order = MarketOrder(btc_symbol, signal * qty_to_trade, current_time)\n","                                 entry_fee = apply_fees(entry_order, symbol_security, fee_model)\n","                                 sim_position_qty = signal * qty_to_trade; sim_entry_price = entry_price\n","                                 sim_cash -= sim_position_qty * sim_entry_price; sim_cash -= entry_fee\n","                                 active_stop_loss = sl_price; active_take_profit = tp_price\n","                                 # Enregistrer le trade simulé pour analyse\n","                                 sim_trades_best.append({'time': current_time, 'type': 'buy' if signal > 0 else 'sell', 'price': entry_price, 'size': sim_position_qty, 'fee': entry_fee, 'sl': active_stop_loss, 'tp': active_take_profit, 'reason': action_type})\n","                                 break # Sortir boucle règles\n","                    elif action_type == \"do_nothing\":\n","                        break # Sortir boucle règles\n","\n","        # 3. Mise à jour Equity finale\n","        sim_equity = sim_cash + sim_position_qty * current_price\n","        equity_curve_list_best.append({'time': current_time, 'equity': sim_equity})\n","\n","    # --- Fin Boucle Barre par Barre (Re-simulation) ---\n","\n","    # --- 3. Affichage des Résultats de la Meilleure Stratégie (Identique) ---\n","    best_equity_curve = pd.DataFrame(equity_curve_list_best).set_index('time')['equity']\n","    best_trades_df = pd.DataFrame(sim_trades_best)\n","    best_final_equity = best_equity_curve.iloc[-1] if not best_equity_curve.empty else initial_cash\n","\n","    logger.info(f\"--- Performance Détaillée de la Meilleure Stratégie Trouvée ({best_strategy_name}) ---\")\n","    logger.info(f\"Équité Finale: {best_final_equity:,.2f} USDT\")\n","    # (Calcul des métriques et affichage des logs identiques à la version précédente)\n","    nb_trades_best = len(best_trades_df[best_trades_df['type'].isin(['buy', 'sell'])])\n","    total_pnl_best = best_trades_df['pnl'].sum() if 'pnl' in best_trades_df.columns else 0.0\n","    total_fees_best = best_trades_df['fee'].sum() if 'fee' in best_trades_df.columns else 0.0\n","    winning_trades_best = best_trades_df[best_trades_df['pnl'] > 0]['pnl'].count() if 'pnl' in best_trades_df.columns else 0\n","    losing_trades_best = best_trades_df[best_trades_df['pnl'] <= 0]['pnl'].count() if 'pnl' in best_trades_df.columns else 0\n","    total_closed_trades_best = winning_trades_best + losing_trades_best\n","    win_rate_best = (winning_trades_best / total_closed_trades_best * 100) if total_closed_trades_best > 0 else 0.0\n","    max_dd_best = 0.0; peak_best = -np.inf\n","    if not best_equity_curve.empty:\n","        for equity_value in best_equity_curve:\n","            if equity_value > peak_best: peak_best = equity_value\n","            drawdown = (peak_best - equity_value) / peak_best if peak_best > 0 else 0\n","            if drawdown > max_dd_best: max_dd_best = drawdown\n","\n","    logger.info(f\"Nombre de Trades (Entrées): {nb_trades_best}\")\n","    logger.info(f\"Total PnL Net: {total_pnl_best:,.2f} USDT\")\n","    logger.info(f\"Total Fees: {total_fees_best:,.2f} USDT\")\n","    logger.info(f\"Win Rate: {win_rate_best:.1f}%\")\n","    logger.info(f\"Max Drawdown: {max_dd_best*100:.1f}%\")\n","\n","    plt.figure(figsize=(14, 7))\n","    best_equity_curve.plot(title=f'Courbe d\\'Équité - Meilleure Stratégie GA: {best_strategy_name[:60]}...', grid=True)\n","    plt.ylabel(\"Valeur Portefeuille (USDT)\")\n","    plt.xlabel(\"Date\")\n","    plt.show()\n","\n","    logger.info(\"Paramètres de la meilleure stratégie trouvée par le GA:\")\n","    best_params_final = best_strategy_config.get('parameters', {})\n","    for key, value in best_params_final.items():\n","         if isinstance(value, float): logger.info(f\"  - {key}: {value:.4f}\")\n","         else: logger.info(f\"  - {key}: {value}\")\n","else:\n","    logger.error(\"Aucun meilleur individu trouvé par le GA. Impossible d'analyser les résultats.\")"]},{"cell_type":"markdown","metadata":{},"source":["### 8.4. Re-simulation Manuelle d'une Configuration Spécifique\n","\n","Cette cellule permet de tester ou de vérifier la performance d'une configuration de stratégie spécifique en dehors du processus d'optimisation du GA.\n","\n","*   **Définition Manuelle des Paramètres :** L'utilisateur définit explicitement un dictionnaire (`params_run1` dans l'exemple) contenant tous les paramètres de la stratégie à simuler.\n","*   **Création de la Stratégie :** La fonction `create_strategy_from_params` est utilisée pour construire la structure de la stratégie à partir des paramètres manuels.\n","*   **Simulation :** La même logique de simulation que celle utilisée dans la fonction de fitness du GA est exécutée pour cette stratégie spécifique, en enregistrant la courbe d'équité et les trades.\n","*   **Analyse et Affichage :** Les métriques de performance et la courbe d'équité pour cette simulation manuelle sont calculées et affichées, permettant une comparaison avec les résultats du GA ou d'autres benchmarks."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CELLULE 7.4 : Re-simulation d'une Configuration Spécifique (Syntaxe Placeholder MinimalSecurity CORRIGÉE v8)\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as mtick\n","import heapq\n","import logging\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime, timedelta, timezone\n","import traceback\n","\n","# --- Assurer imports QC ou placeholders (Syntaxe Corrigée pour cette cellule) ---\n","try:\n","    from AlgorithmImports import MarketOrder, OrderFeeParameters, ConstantFeeModel, OrderType, Security, Symbol\n","    print(\"Imports QuantConnect OK pour re-sim spécifique (v8).\")\n","    _qc_imports_ok_resim = True\n","except ImportError:\n","    print(\"WARN: QC Imports manquants pour re-sim spécifique (v8). Définition de placeholders.\")\n","    _qc_imports_ok_resim = False\n","    # --- Placeholders (Syntaxe Corrigée avec Indentation) ---\n","    if 'MarketOrder' not in locals():\n","        class MarketOrder:\n","            def __init__(self, symbol, quantity, time):\n","                self.Symbol = symbol; self.Quantity = quantity; self.Time = time; self.Price = 1; self.Status = 0\n","\n","    if 'OrderFeeValue' not in locals():\n","         class OrderFeeValue:\n","              Amount = 0.0\n","\n","    if 'OrderFeeParameters' not in locals():\n","        class OrderFeeParameters:\n","            def __init__(self, security, order): pass\n","            class OrderFeeValueInternal: Amount = 0.0 # Nom différent pour éviter conflit\n","            Value = OrderFeeValueInternal()\n","\n","    if 'ConstantFeeModel' not in locals():\n","        class ConstantFeeModel:\n","            def __init__(self, fee): self.fee = fee\n","            def GetOrderFee(self, parameters):\n","                if 'OrderFeeParameters' not in globals(): raise NameError(\"Placeholder OrderFeeParameters non défini\")\n","                return OrderFeeParameters.Value\n","\n","    if 'OrderType' not in locals():\n","        class OrderType:\n","            Market = 1\n","\n","    if 'Security' not in locals():\n","        class Security:\n","            pass\n","\n","    if 'Symbol' not in locals():\n","        class Symbol:\n","            pass\n","    # --- Fin Placeholders ImportError ---\n","\n","# --- Assurer définition des variables globales ---\n","if 'btc_symbol' not in locals(): btc_symbol = 'BTCUSDT'\n","\n","# --- Définition/Vérification symbol_security (Syntaxe MinimalSecurity CORRIGÉE) ---\n","if 'symbol_security' not in locals():\n","    print(\"Vérification/Création 'symbol_security' pour re-sim...\")\n","    if 'qb' in locals() and _qc_imports_ok_resim:\n","        try:\n","            symbol_security = qb.Securities[btc_symbol]\n","            print(f\"Objet Security pour {btc_symbol} récupéré via qb.\")\n","        except Exception as e_sec:\n","            print(f\"WARN: Impossible de récupérer Security via qb: {e_sec}. Création placeholder.\")\n","            _qc_imports_ok_resim = False # Marquer comme échoué\n","\n","    # Créer placeholder si import/récupération échoue OU si déjà marqué comme échoué\n","    if not _qc_imports_ok_resim or 'symbol_security' not in locals():\n","        print(\"Création placeholder MinimalSecurity pour re-sim...\")\n","        # *** SYNTAXE CORRIGÉE ICI ***\n","        class MinimalSecurity:\n","            Symbol = btc_symbol         # Attribut de classe\n","            class QCC:                  # Classe interne indentée\n","                Symbol = 'USDT'         # Attribut de classe interne indenté\n","            QuoteCurrency = QCC()       # Instanciation indentée\n","        symbol_security = MinimalSecurity()\n","        # *** FIN CORRECTION SYNTAXE ***\n","else:\n","    print(\"'symbol_security' déjà défini pour re-sim.\")\n","\n","# --- Définition/Vérification fee_model (Bloc vérifié) ---\n","if 'fee_model' not in locals():\n","    print(\"Vérification/Création 'fee_model' pour re-sim...\")\n","    if 'qb' in locals() and _qc_imports_ok_resim and hasattr(symbol_security, 'QuoteCurrency') and not isinstance(symbol_security, MinimalSecurity):\n","        try:\n","            fee_model = qb.BrokerageModel.GetFeeModel(symbol_security)\n","            print(f\"Modèle de frais récupéré via qb: {fee_model.__class__.__name__}\")\n","        except Exception as e_fee:\n","            print(f\"WARN: Impossible de récupérer FeeModel via qb: {e_fee}. Utilisation ConstantFeeModel(0).\")\n","            _qc_imports_ok_resim = False # Marquer comme échoué\n","\n","    if not _qc_imports_ok_resim or 'fee_model' not in locals():\n","        print(\"Création placeholder ConstantFeeModel(0) pour re-sim...\")\n","        if 'ConstantFeeModel' not in locals(): # S'assurer que la classe placeholder est définie\n","             class ConstantFeeModel:\n","                 def __init__(self, fee): self.fee = fee\n","                 def GetOrderFee(self, parameters):\n","                     if 'OrderFeeParameters' not in globals(): raise NameError(\"Placeholder OrderFeeParameters non trouvé\")\n","                     return OrderFeeParameters.Value\n","        fee_model = ConstantFeeModel(0)\n","else:\n","     print(\"'fee_model' déjà défini pour re-sim.\")\n","\n","if 'initial_cash' not in locals(): initial_cash = 10000\n","# --- Fin Vérifications Globales ---\n","\n","# Récupérer logger\n","logger = logging.getLogger(\"GA_Optimizer_Resim\")\n","if not logger.hasHandlers(): # Configurer si pas déjà fait\n","    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n","    logging.basicConfig(level=logging.INFO, format=log_format, force=True)\n","\n","logger.info(\"--- Re-simulation Manuelle d'une Stratégie Spécifique (v8) ---\")\n","\n","# --- Définir les Paramètres de la Stratégie à Re-tester ---\n","params_run1 = { # (Paramètres Run1 ~19.4k - Vérifiez si ce sont les bons)\n","    'trade_level': 'meso', 'signal_type': 'breakout', 'trend_filter_level': 'none',\n","    'risk_per_trade_pct': 0.0199, 'min_channel_width_pct': 0.0062,\n","    'bounce_sl_type': 'pct_entry', 'bounce_sl_value': 0.0105, 'bounce_tp_type': 'rr_ratio',\n","    'bounce_tp_value': 2.1194, 'bounce_entry_offset': 0.0015,\n","    'breakout_sl_type': 'pct_level', 'breakout_sl_value': 0.0120,\n","    'breakout_tp_type': 'rr_ratio', 'breakout_tp_value': 2.9670\n","}\n","if 'param_keys' not in locals(): raise NameError(\"Variable 'param_keys' non définie.\")\n","# S'assurer que param_keys correspond bien aux clés de params_run1\n","missing_keys_in_run1 = [k for k in param_keys if k not in params_run1]\n","if missing_keys_in_run1: raise ValueError(f\"Clés manquantes: {missing_keys_in_run1}\")\n","param_combination_run1 = [params_run1.get(key) for key in param_keys]\n","\n","# --- Vérifier les fonctions nécessaires ---\n","required_funcs = ['create_strategy_from_params', 'check_conditions', 'get_channel_value_at_time', 'calculate_position_size', 'apply_fees', 'get_line_params_time']\n","for func_name in required_funcs:\n","    if func_name not in globals() and func_name not in locals(): raise NameError(f\"Fonction '{func_name}' non définie.\")\n","if 'loop_bars_all' not in locals() or loop_bars_all.empty: raise NameError(\"'loop_bars_all' requis.\")\n","if 'channel_history_df' not in locals() or channel_history_df.empty: raise NameError(\"'channel_history_df' requis.\")\n","\n","# --- Créer et Simuler la Stratégie ---\n","# (Utilise la version de create_strategy_from_params qui prend param_keys globalement)\n","if 'create_strategy_from_params' not in globals(): raise NameError(\"create_strategy_from_params non définie\")\n","strategy_config_run1 = create_strategy_from_params(param_combination_run1)\n","strategy_name_run1 = strategy_config_run1['name']\n","logger.info(f\"Re-simulation de la configuration spécifique : {strategy_name_run1}\")\n","\n","# --- Initialisation et Boucle de Simulation (Identique aux cellules précédentes) ---\n","sim_cash_run1 = initial_cash; sim_position_qty_run1 = 0.0; sim_entry_price_run1 = 0.0\n","sim_equity_run1 = initial_cash; active_stop_loss_run1 = None; active_take_profit_run1 = None\n","sim_start_time_run1 = loop_bars_all['time'].iloc[0] if not loop_bars_all.empty else pd.Timestamp.now(tz='UTC')\n","equity_curve_list_run1 = [{'time': sim_start_time_run1 - pd.Timedelta(hours=1), 'equity': initial_cash}]\n","sim_trades_run1 = []\n","strat_rules_run1 = strategy_config_run1.get('rules', [])\n","risk_per_trade_pct_run1 = strategy_config_run1['parameters']['risk_per_trade_pct']\n","origin_params_run1 = strategy_config_run1.get('parameters', {})\n","last_known_channels_sim_run1 = None\n","position_status_numeric_run1 = 0 # Ajouter statut numérique\n","\n","for index, row in tqdm(loop_bars_all.iterrows(), total=len(loop_bars_all), desc=f\"Simulating {strategy_name_run1[:20]}...\", leave=False):\n","    # --- Début Boucle Simulation (Copier/Coller depuis Analyse GA) ---\n","    current_time=row['time']; current_price=row['close']; current_high=row['high']; current_low=row['low']; current_time_num=current_time.timestamp()\n","    current_active_channels = None;\n","    valid_channel_time_index = channel_history_df.index[channel_history_df.index <= current_time]\n","    if not valid_channel_time_index.empty:\n","        valid_channel_time = valid_channel_time_index.max();\n","        try: current_active_channels = channel_history_df.loc[valid_channel_time].to_dict(); last_known_channels_sim_run1=current_active_channels;\n","        except: current_active_channels=last_known_channels_sim_run1;\n","    else: current_active_channels=last_known_channels_sim_run1;\n","    if current_active_channels is None:\n","         sim_equity_run1 = sim_cash_run1 + sim_position_qty_run1 * current_price; equity_curve_list_run1.append({'time': current_time, 'equity': sim_equity_run1}); continue;\n","\n","    exit_executed=False;\n","    if position_status_numeric_run1 != 0: # Sorties\n","        pnl=0.; exit_reason=''; close=False; exit_p=current_price; sl=active_stop_loss_run1; tp=active_take_profit_run1; qty=sim_position_qty_run1; ep=sim_entry_price_run1;\n","        if sl is not None:\n","            if qty>0 and current_low<=sl: close=True; exit_reason=\"SL\"; exit_p=sl;\n","            elif qty<0 and current_high>=sl: close=True; exit_reason=\"SL\"; exit_p=sl;\n","        if not close and tp is not None:\n","            if qty>0 and current_high>=tp: close=True; exit_reason=\"TP\"; exit_p=tp;\n","            elif qty<0 and current_low<=tp: close=True; exit_reason=\"TP\"; exit_p=tp;\n","        if close:\n","            # S'assurer que MarketOrder est défini (placeholder ou réel)\n","            if 'MarketOrder' not in globals(): raise NameError(\"MarketOrder non défini\")\n","            order=MarketOrder(btc_symbol,-qty,current_time); fee=apply_fees(order,symbol_security,fee_model); pnl=qty*(exit_p-ep); sim_cash_run1+=qty*exit_p-fee;\n","            sim_trades_run1.append({'time':current_time,'type':f'exit_{\"long\" if qty>0 else \"short\"}','price':exit_p,'size':-qty,'pnl':pnl-fee,'fee':fee,'reason':exit_reason,'equity':sim_cash_run1});\n","            sim_position_qty_run1=0.; sim_entry_price_run1=0.; active_stop_loss_run1=None; active_take_profit_run1=None; exit_executed=True; position_status_numeric_run1=0;\n","\n","    if position_status_numeric_run1 == 0 and not exit_executed: # Entrées\n","        cs={'price':current_price,'time_numeric':current_time_num,'position_status_numeric':position_status_numeric_run1,'active_channels':current_active_channels};\n","        for rule in strat_rules_run1:\n","            cond=rule.get('conditions',{}); act_tmpl=rule.get('action');\n","            if isinstance(act_tmpl,dict): ad=act_tmpl;\n","            elif act_tmpl==\"do_nothing\": ad={\"action_type\":\"do_nothing\"};\n","            else: ad={};\n","            if check_conditions(cond, cs):\n","                aty=ad.get(\"action_type\");\n","                if aty in [\"bounce_long\",\"breakout_long\",\"bounce_short\",\"breakout_short\"]:\n","                    entry_p=current_price; sig=1 if \"long\" in aty else -1; slp=None; tpp=None; rpu=0;\n","                    is_b='bounce' in aty;\n","                    sltk='bounce_sl_type' if is_b else 'breakout_sl_type'; slvk='bounce_sl_value' if is_b else 'breakout_sl_value'; tpvk='bounce_tp_value' if is_b else 'breakout_tp_value';\n","                    oslt=origin_params_run1.get(sltk); slv=origin_params_run1.get(slvk); trr=origin_params_run1.get(tpvk);\n","                    if oslt is None or slv is None or trr is None: continue;\n","                    # Logique SL/TP\n","                    if sig==1:\n","                        if oslt=='pct_entry': slp=entry_p*(1-slv);\n","                        elif oslt=='pct_level': ln_cond=cond.get(\"price_near_level\",{}).get(\"level\") or cond.get(\"price_closes_above\",{}).get(\"level\"); ln=ln_cond or f\"{origin_params_run1['trade_level']}_resistance\"; s,lt=ln.split('_'); lv=get_channel_value_at_time(current_active_channels.get(s,{}).get(lt),current_time_num); slp=lv*(1-slv) if not pd.isna(lv) else entry_p*(1-slv);\n","                        else: slp=entry_p*(1-slv);\n","                    else:\n","                        if oslt=='pct_entry': slp=entry_p*(1+slv);\n","                        elif oslt=='pct_level': ln_cond=cond.get(\"price_near_level\",{}).get(\"level\") or cond.get(\"price_closes_below\",{}).get(\"level\"); ln=ln_cond or f\"{origin_params_run1['trade_level']}_support\"; s,lt=ln.split('_'); lv=get_channel_value_at_time(current_active_channels.get(s,{}).get(lt),current_time_num); slp=lv*(1+slv) if not pd.isna(lv) else entry_p*(1+slv);\n","                        else: slp=entry_p*(1+slv);\n","                    if slp is not None: rpu=abs(entry_p-slp);\n","                    if rpu>1e-9: tpp=entry_p+sig*rpu*trr;\n","                    if slp is not None and rpu>1e-9:\n","                        eq_b4=sim_cash_run1; qty=calculate_position_size(eq_b4,risk_per_trade_pct_run1,entry_p,slp);\n","                        if qty>0:\n","                             # S'assurer que MarketOrder est défini\n","                             if 'MarketOrder' not in globals(): raise NameError(\"MarketOrder non défini\")\n","                             order=MarketOrder(btc_symbol,sig*qty,current_time); fee=apply_fees(order,symbol_security,fee_model);\n","                             sim_position_qty_run1=sig*qty; sim_entry_price_run1=entry_p; sim_cash_run1-=sim_position_qty_run1*sim_entry_price_run1+fee;\n","                             active_stop_loss_run1=slp; active_take_profit_run1=tpp; position_status_numeric_run1=sig;\n","                             sim_trades_run1.append({'time':current_time,'type':'buy' if sig>0 else 'sell','price':entry_p,'size':sim_position_qty_run1,'fee':fee,'sl':slp,'tp':tpp,'reason':aty,'equity':sim_cash_run1});\n","                             break;\n","                elif aty==\"do_nothing\": break;\n","    # --- Fin Boucle Simulation ---\n","\n","    sim_equity_run1 = sim_cash_run1 + sim_position_qty_run1 * current_price\n","    equity_curve_list_run1.append({'time': current_time, 'equity': sim_equity_run1})\n","# --- Fin Boucle Barre par Barre ---\n","\n","\n","# --- Analyse et Affichage Résultats pour Run1 ---\n","equity_curve_run1 = pd.DataFrame(equity_curve_list_run1).set_index('time')['equity']\n","trades_run1_df = pd.DataFrame(sim_trades_run1)\n","final_equity_run1 = equity_curve_run1.iloc[-1] if not equity_curve_run1.empty else initial_cash\n","logger.info(f\"--- Performance Détaillée Stratégie Spécifique ({strategy_name_run1}) ---\")\n","logger.info(f\"Équité Finale: {final_equity_run1:,.2f} USDT\")\n","if not trades_run1_df.empty:\n","    nb_trades_run1 = len(trades_run1_df[trades_run1_df['type'].isin(['buy', 'sell'])])\n","    exit_trades_run1 = trades_run1_df[trades_run1_df['type'].str.startswith('exit')] # Correct: Utiliser les exits pour PnL\n","    total_pnl_run1 = exit_trades_run1['pnl'].sum() if 'pnl' in exit_trades_run1.columns and not exit_trades_run1.empty else 0.0\n","    total_fees_run1 = trades_run1_df['fee'].sum() if 'fee' in trades_run1_df.columns else 0.0\n","    winning_trades_run1 = exit_trades_run1[exit_trades_run1['pnl'] > 0]['pnl'].count() if 'pnl' in exit_trades_run1.columns else 0\n","    losing_trades_run1 = exit_trades_run1[exit_trades_run1['pnl'] <= 0]['pnl'].count() if 'pnl' in exit_trades_run1.columns else 0\n","    total_closed_trades_run1 = len(exit_trades_run1) # Utiliser le nombre de sorties\n","    win_rate_run1 = (winning_trades_run1 / total_closed_trades_run1 * 100) if total_closed_trades_run1 > 0 else 0.0\n","else: nb_trades_run1=0; total_pnl_run1=0.0; total_fees_run1=0.0; win_rate_run1=0.0; logger.info(\"Aucun trade exécuté.\")\n","max_dd_run1 = 0.0;\n","if not equity_curve_run1.empty:\n","    rolling_max_run1 = equity_curve_run1.cummax(); daily_drawdown_run1 = equity_curve_run1 / rolling_max_run1 - 1.0\n","    max_dd_run1 = abs(daily_drawdown_run1.min()) if not daily_drawdown_run1.empty else 0.0\n","logger.info(f\"Nb Trades (Entrées): {nb_trades_run1}\")\n","logger.info(f\"Total PnL Net (Sorties): {total_pnl_run1:,.2f} USDT\")\n","logger.info(f\"Total Fees: {total_fees_run1:,.2f} USDT\")\n","logger.info(f\"Win Rate (Sorties): {win_rate_run1:.1f}%\")\n","logger.info(f\"Max Drawdown: {max_dd_run1*100:.1f}%\")\n","\n","# --- Plotting ---\n","plt.figure(figsize=(14, 7))\n","plot_title_run1 = f\"Courbe d'Équité - Re-Sim Spécifique: {strategy_name_run1[:60]}...\"\n","if not equity_curve_run1.empty:\n","    equity_curve_run1.plot(title=plot_title_run1, grid=True)\n","    # Ajouter les marqueurs de trades\n","    if not trades_run1_df.empty:\n","        entries = trades_run1_df[trades_run1_df['type'].isin(['buy','sell'])]\n","        exits = trades_run1_df[trades_run1_df['type'].str.startswith('exit')]\n","        # S'assurer que les index de temps existent dans la courbe d'équité avant de plotter\n","        valid_entry_times = entries.index.intersection(equity_curve_run1.index)\n","        valid_exit_times = exits.index.intersection(equity_curve_run1.index)\n","        if not entries.loc[valid_entry_times].empty:\n","            plt.scatter(entries.loc[valid_entry_times].index, equity_curve_run1.loc[valid_entry_times], marker='^', color='lime', s=60, label='Entrée', zorder=5)\n","        if not exits.loc[valid_exit_times].empty:\n","             plt.scatter(exits.loc[valid_exit_times].index, equity_curve_run1.loc[valid_exit_times], marker='v', color='red', s=60, label='Sortie', zorder=5)\n","        if not entries.empty or not exits.empty:\n","             plt.legend()\n","else:\n","    plt.title(f\"{plot_title_run1} (Pas de données d'équité)\")\n","\n","plt.ylabel(\"Valeur Portefeuille (USDT)\"); plt.xlabel(\"Date\"); plt.show()\n","\n","logger.info(\"Paramètres de cette stratégie spécifique:\")\n","for key, value in origin_params_run1.items():\n","     if isinstance(value, float): logger.info(f\"  - {key}: {value:.4f}\")\n","     else: logger.info(f\"  - {key}: {value}\")"]}],"metadata":{"kernelspec":{"display_name":"Foundation-Py-Default","language":"python","name":"python3"},"polyglot_notebook":{"kernelInfo":{"defaultKernelName":"csharp","items":[{"aliases":[],"name":"csharp"}]}}},"nbformat":4,"nbformat_minor":2}