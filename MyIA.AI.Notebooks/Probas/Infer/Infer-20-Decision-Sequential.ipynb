{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Infer-20-Decision-Sequential : MDPs, Bandits et POMDPs\n",
        "\n",
        "**Serie** : Programmation Probabiliste avec Infer.NET (20/20)  \n",
        "**Duree estimee** : 60 minutes  \n",
        "**Prerequis** : Notebooks 14-19 (Decision Theory)\n",
        "\n",
        "---\n",
        "\n",
        "## Objectifs\n",
        "\n",
        "- Comprendre les **Processus de Decision Markoviens** (MDPs)\n",
        "- Maitriser l'**iteration de valeur** et l'**iteration de politique**\n",
        "- Decouvrir les alternatives : **LP, Expectimax, RTDP**\n",
        "- Appliquer le **reward shaping** avec le theoreme de preservation de politique\n",
        "- Introduire les **bandits multi-bras** et l'**indice de Gittins**\n",
        "- Presenter les **POMDPs** et les belief states\n",
        "\n",
        "---\n",
        "\n",
        "## Navigation\n",
        "\n",
        "| Precedent | Suivant |\n",
        "|-----------|--------|\n",
        "| [Infer-19-Decision-Expert-Systems](Infer-19-Decision-Expert-Systems.ipynb) | Serie RL (`MyIA.AI.Notebooks/RL/`) |\n",
        "\n",
        "---\n",
        "\n",
        "**Note** : Ce notebook est une introduction conceptuelle aux decisions sequentielles. Les implementations avancees avec Deep RL sont dans la serie `RL/`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Decisions Sequentielles vs One-Shot\n",
        "\n",
        "### Difference fondamentale\n",
        "\n",
        "| Type | Caracteristique | Exemple |\n",
        "|------|-----------------|--------|\n",
        "| **One-shot** | Une seule decision | Accepter une offre d'emploi |\n",
        "| **Sequentielle** | Serie de decisions interdependantes | Jouer aux echecs |\n",
        "\n",
        "### Horizon\n",
        "\n",
        "- **Fini** : Nombre fixe d'etapes (T etapes)\n",
        "- **Infini** : Le processus continue indefiniment\n",
        "\n",
        "### Facteur d'actualisation γ (gamma)\n",
        "\n",
        "Pour les horizons infinis, on utilise un facteur γ ∈ [0,1) pour :\n",
        "\n",
        "- Garantir la convergence des sommes infinies\n",
        "- Modeliser la preference pour les recompenses immediates\n",
        "- γ = 0.99 : vision long terme\n",
        "- γ = 0.5 : vision court terme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "csharp"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.ML.Probabilistic, 0.4.2504.701</span></li><li><span>Microsoft.ML.Probabilistic.Compiler, 0.4.2504.701</span></li></ul></div></div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Infer.NET charge !\n"
          ]
        }
      ],
      "source": [
        "// Installation Infer.NET\n",
        "#r \"nuget: Microsoft.ML.Probabilistic\"\n",
        "#r \"nuget: Microsoft.ML.Probabilistic.Compiler\"\n",
        "\n",
        "using Microsoft.ML.Probabilistic;\n",
        "using Microsoft.ML.Probabilistic.Distributions;\n",
        "using Microsoft.ML.Probabilistic.Models;\n",
        "using Microsoft.ML.Probabilistic.Algorithms;\n",
        "\n",
        "Console.WriteLine(\"Infer.NET charge !\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Processus de Decision Markoviens (MDP)\n",
        "\n",
        "### Definition formelle\n",
        "\n",
        "Un MDP est un tuple (S, A, P, R, γ) :\n",
        "\n",
        "| Element | Notation | Description |\n",
        "|---------|----------|-------------|\n",
        "| **Etats** | S | Ensemble des etats possibles |\n",
        "| **Actions** | A | Ensemble des actions |\n",
        "| **Transitions** | P(s'\\|s,a) | Probabilite de transition |\n",
        "| **Recompenses** | R(s,a) ou R(s,a,s') | Recompense immediate |\n",
        "| **Discount** | γ | Facteur d'actualisation |\n",
        "\n",
        "### Politique et Fonction de Valeur\n",
        "\n",
        "- **Politique** π : S → A (ou distribution sur A)\n",
        "- **Fonction de valeur** V^π(s) : Utilite esperee depuis s en suivant π\n",
        "- **Fonction action-valeur** Q^π(s, a) : Utilite esperee en faisant a puis suivant π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "csharp"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MDP Grille 4x3 cree :\n",
            "  But (+1) en (3,2)\n",
            "  Piege (-1) en (3,1)\n",
            "  Mur en (1,1)\n",
            "  Gamma = 0,9\n"
          ]
        }
      ],
      "source": [
        "// Definition d'un MDP simple : Navigation dans une grille\n",
        "\n",
        "public class GridMDP\n",
        "{\n",
        "    public int Width { get; }\n",
        "    public int Height { get; }\n",
        "    public double Gamma { get; }\n",
        "    public Dictionary<(int, int), double> Rewards { get; }\n",
        "    public HashSet<(int, int)> TerminalStates { get; }\n",
        "    public HashSet<(int, int)> Walls { get; }\n",
        "    \n",
        "    public string[] Actions { get; } = { \"N\", \"S\", \"E\", \"W\" };\n",
        "    \n",
        "    // Directions\n",
        "    private Dictionary<string, (int dx, int dy)> _directions = new()\n",
        "    {\n",
        "        { \"N\", (0, 1) }, { \"S\", (0, -1) }, { \"E\", (1, 0) }, { \"W\", (-1, 0) }\n",
        "    };\n",
        "    \n",
        "    public GridMDP(int width, int height, double gamma = 0.9)\n",
        "    {\n",
        "        Width = width;\n",
        "        Height = height;\n",
        "        Gamma = gamma;\n",
        "        Rewards = new Dictionary<(int, int), double>();\n",
        "        TerminalStates = new HashSet<(int, int)>();\n",
        "        Walls = new HashSet<(int, int)>();\n",
        "    }\n",
        "    \n",
        "    public List<(int, int)> GetStates()\n",
        "    {\n",
        "        var states = new List<(int, int)>();\n",
        "        for (int x = 0; x < Width; x++)\n",
        "            for (int y = 0; y < Height; y++)\n",
        "                if (!Walls.Contains((x, y)))\n",
        "                    states.Add((x, y));\n",
        "        return states;\n",
        "    }\n",
        "    \n",
        "    public double GetReward((int, int) state) => \n",
        "        Rewards.TryGetValue(state, out var r) ? r : -0.04; // Cout de mouvement par defaut\n",
        "    \n",
        "    public List<((int, int) nextState, double prob)> GetTransitions((int, int) state, string action)\n",
        "    {\n",
        "        if (TerminalStates.Contains(state))\n",
        "            return new List<((int, int), double)> { (state, 1.0) };\n",
        "        \n",
        "        var transitions = new List<((int, int), double)>();\n",
        "        var (dx, dy) = _directions[action];\n",
        "        \n",
        "        // 80% de chance d'aller dans la direction voulue\n",
        "        transitions.Add((NextState(state, action), 0.8));\n",
        "        \n",
        "        // 10% de chance d'aller perpendiculairement (droite ou gauche)\n",
        "        var perpActions = action == \"N\" || action == \"S\" \n",
        "            ? new[] { \"E\", \"W\" } \n",
        "            : new[] { \"N\", \"S\" };\n",
        "        \n",
        "        foreach (var perpAction in perpActions)\n",
        "            transitions.Add((NextState(state, perpAction), 0.1));\n",
        "        \n",
        "        return transitions;\n",
        "    }\n",
        "    \n",
        "    private (int, int) NextState((int, int) state, string action)\n",
        "    {\n",
        "        var (x, y) = state;\n",
        "        var (dx, dy) = _directions[action];\n",
        "        int nx = x + dx, ny = y + dy;\n",
        "        \n",
        "        // Collision avec mur ou bord = rester sur place\n",
        "        if (nx < 0 || nx >= Width || ny < 0 || ny >= Height || Walls.Contains((nx, ny)))\n",
        "            return state;\n",
        "        return (nx, ny);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Creer le MDP classique de Russell & Norvig\n",
        "var mdp = new GridMDP(4, 3, gamma: 0.9);\n",
        "mdp.Rewards[(3, 2)] = 1.0;   // But positif\n",
        "mdp.Rewards[(3, 1)] = -1.0;  // Piege\n",
        "mdp.TerminalStates.Add((3, 2));\n",
        "mdp.TerminalStates.Add((3, 1));\n",
        "mdp.Walls.Add((1, 1));       // Mur\n",
        "\n",
        "Console.WriteLine(\"MDP Grille 4x3 cree :\");\n",
        "Console.WriteLine(\"  But (+1) en (3,2)\");\n",
        "Console.WriteLine(\"  Piege (-1) en (3,1)\");\n",
        "Console.WriteLine(\"  Mur en (1,1)\");\n",
        "Console.WriteLine($\"  Gamma = {mdp.Gamma}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Equation de Bellman\n",
        "\n",
        "### Equation de Bellman pour V*\n",
        "\n",
        "La fonction de valeur optimale satisfait :\n",
        "\n",
        "$$V^*(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right]$$\n",
        "\n",
        "### Equation de Bellman pour Q*\n",
        "\n",
        "$$Q^*(s, a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q^*(s', a')$$\n",
        "\n",
        "### Politique optimale\n",
        "\n",
        "$$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Iteration de Valeur\n",
        "\n",
        "### Algorithme\n",
        "\n",
        "1. Initialiser V(s) = 0 pour tout s\n",
        "2. Repeter jusqu'a convergence :\n",
        "   - Pour chaque etat s :\n",
        "     - V(s) ← max_a [R(s,a) + γ Σ_s' P(s'|s,a) V(s')]\n",
        "3. Extraire la politique : π(s) = argmax_a Q(s,a)\n",
        "\n",
        "### Complexite\n",
        "\n",
        "- O(|S|² |A|) par iteration\n",
        "- Converge en O(log(1/ε) / (1-γ)) iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "csharp"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Convergence apres 14 iterations (delta = 6,01E-004)\n",
            "\n",
            "Fonction de Valeur V* :\n",
            "  0,509   0,650   0,795   1,000 \n",
            "  0,398   ####    0,486  -1,000 \n",
            "  0,296   0,254   0,345   0,130 \n",
            "\n",
            "Politique optimale π* :\n",
            " →  →  →  ● \n",
            " ↑  #  ↑  ● \n",
            " ↑  →  ↑  ← \n"
          ]
        }
      ],
      "source": [
        "// Iteration de Valeur\n",
        "\n",
        "public class ValueIteration\n",
        "{\n",
        "    public static (Dictionary<(int,int), double> V, Dictionary<(int,int), string> policy) \n",
        "        Solve(GridMDP mdp, double epsilon = 0.001, int maxIter = 100)\n",
        "    {\n",
        "        var states = mdp.GetStates();\n",
        "        var V = states.ToDictionary(s => s, s => 0.0);\n",
        "        \n",
        "        for (int iter = 0; iter < maxIter; iter++)\n",
        "        {\n",
        "            double delta = 0;\n",
        "            var newV = new Dictionary<(int, int), double>(V);\n",
        "            \n",
        "            foreach (var s in states)\n",
        "            {\n",
        "                if (mdp.TerminalStates.Contains(s))\n",
        "                {\n",
        "                    newV[s] = mdp.GetReward(s);\n",
        "                    continue;\n",
        "                }\n",
        "                \n",
        "                double maxQ = double.NegativeInfinity;\n",
        "                foreach (var a in mdp.Actions)\n",
        "                {\n",
        "                    double q = mdp.GetReward(s);\n",
        "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
        "                        q += mdp.Gamma * prob * V[nextState];\n",
        "                    maxQ = Math.Max(maxQ, q);\n",
        "                }\n",
        "                \n",
        "                newV[s] = maxQ;\n",
        "                delta = Math.Max(delta, Math.Abs(V[s] - newV[s]));\n",
        "            }\n",
        "            \n",
        "            V = newV;\n",
        "            \n",
        "            if (delta < epsilon)\n",
        "            {\n",
        "                Console.WriteLine($\"Convergence apres {iter + 1} iterations (delta = {delta:E2})\");\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        // Extraire la politique\n",
        "        var policy = new Dictionary<(int, int), string>();\n",
        "        foreach (var s in states)\n",
        "        {\n",
        "            if (mdp.TerminalStates.Contains(s))\n",
        "            {\n",
        "                policy[s] = \"T\"; // Terminal\n",
        "                continue;\n",
        "            }\n",
        "            \n",
        "            string bestAction = null;\n",
        "            double maxQ = double.NegativeInfinity;\n",
        "            \n",
        "            foreach (var a in mdp.Actions)\n",
        "            {\n",
        "                double q = mdp.GetReward(s);\n",
        "                foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
        "                    q += mdp.Gamma * prob * V[nextState];\n",
        "                \n",
        "                if (q > maxQ)\n",
        "                {\n",
        "                    maxQ = q;\n",
        "                    bestAction = a;\n",
        "                }\n",
        "            }\n",
        "            policy[s] = bestAction;\n",
        "        }\n",
        "        \n",
        "        return (V, policy);\n",
        "    }\n",
        "}\n",
        "\n",
        "var (V, policy) = ValueIteration.Solve(mdp);\n",
        "\n",
        "// Afficher la grille\n",
        "Console.WriteLine(\"\\nFonction de Valeur V* :\");\n",
        "for (int y = mdp.Height - 1; y >= 0; y--)\n",
        "{\n",
        "    for (int x = 0; x < mdp.Width; x++)\n",
        "    {\n",
        "        if (mdp.Walls.Contains((x, y)))\n",
        "            Console.Write(\"  ####  \");\n",
        "        else\n",
        "            Console.Write($\" {V[(x, y)],6:F3} \");\n",
        "    }\n",
        "    Console.WriteLine();\n",
        "}\n",
        "\n",
        "Console.WriteLine(\"\\nPolitique optimale π* :\");\n",
        "var arrows = new Dictionary<string, string> { {\"N\",\"↑\"}, {\"S\",\"↓\"}, {\"E\",\"→\"}, {\"W\",\"←\"}, {\"T\",\"●\"} };\n",
        "for (int y = mdp.Height - 1; y >= 0; y--)\n",
        "{\n",
        "    for (int x = 0; x < mdp.Width; x++)\n",
        "    {\n",
        "        if (mdp.Walls.Contains((x, y)))\n",
        "            Console.Write(\" # \");\n",
        "        else\n",
        "            Console.Write($\" {arrows[policy[(x, y)]]} \");\n",
        "    }\n",
        "    Console.WriteLine();\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Iteration de Politique\n",
        "\n",
        "### Algorithme\n",
        "\n",
        "1. Initialiser π arbitrairement\n",
        "2. Repeter jusqu'a stabilite :\n",
        "   - **Evaluation** : Calculer V^π (resoudre systeme lineaire)\n",
        "   - **Amelioration** : π(s) ← argmax_a Q^π(s,a)\n",
        "\n",
        "### Avantages vs Iteration de Valeur\n",
        "\n",
        "| Critere | Value Iteration | Policy Iteration |\n",
        "|---------|-----------------|------------------|\n",
        "| Iterations | Beaucoup | Peu (souvent < 10) |\n",
        "| Cout/iteration | O(\\|S\\|²\\|A\\|) | O(\\|S\\|³) pour evaluation |\n",
        "| Convergence | Asymptotique | Exacte en fini |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "csharp"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Iteration de Politique ===\n",
            "\n",
            "Iteration 1 : stable = False\n",
            "Iteration 2 : stable = False\n",
            "Iteration 3 : stable = True\n",
            "\n",
            "Comparaison avec Value Iteration :\n",
            "  Politiques identiques !\n"
          ]
        }
      ],
      "source": [
        "// Iteration de Politique\n",
        "\n",
        "public class PolicyIteration\n",
        "{\n",
        "    public static (Dictionary<(int,int), double> V, Dictionary<(int,int), string> policy) \n",
        "        Solve(GridMDP mdp, int maxIter = 20)\n",
        "    {\n",
        "        var states = mdp.GetStates();\n",
        "        \n",
        "        // Initialiser politique aleatoire\n",
        "        var policy = states.ToDictionary(s => s, s => \n",
        "            mdp.TerminalStates.Contains(s) ? \"T\" : \"N\");\n",
        "        \n",
        "        var V = states.ToDictionary(s => s, s => 0.0);\n",
        "        \n",
        "        for (int iter = 0; iter < maxIter; iter++)\n",
        "        {\n",
        "            // 1. Evaluation de politique (iterative simplifiee)\n",
        "            for (int evalIter = 0; evalIter < 50; evalIter++)\n",
        "            {\n",
        "                var newV = new Dictionary<(int, int), double>(V);\n",
        "                foreach (var s in states)\n",
        "                {\n",
        "                    if (mdp.TerminalStates.Contains(s))\n",
        "                    {\n",
        "                        newV[s] = mdp.GetReward(s);\n",
        "                        continue;\n",
        "                    }\n",
        "                    \n",
        "                    string a = policy[s];\n",
        "                    double v = mdp.GetReward(s);\n",
        "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
        "                        v += mdp.Gamma * prob * V[nextState];\n",
        "                    newV[s] = v;\n",
        "                }\n",
        "                V = newV;\n",
        "            }\n",
        "            \n",
        "            // 2. Amelioration de politique\n",
        "            bool stable = true;\n",
        "            foreach (var s in states)\n",
        "            {\n",
        "                if (mdp.TerminalStates.Contains(s)) continue;\n",
        "                \n",
        "                string oldAction = policy[s];\n",
        "                string bestAction = null;\n",
        "                double maxQ = double.NegativeInfinity;\n",
        "                \n",
        "                foreach (var a in mdp.Actions)\n",
        "                {\n",
        "                    double q = mdp.GetReward(s);\n",
        "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
        "                        q += mdp.Gamma * prob * V[nextState];\n",
        "                    \n",
        "                    if (q > maxQ)\n",
        "                    {\n",
        "                        maxQ = q;\n",
        "                        bestAction = a;\n",
        "                    }\n",
        "                }\n",
        "                \n",
        "                policy[s] = bestAction;\n",
        "                if (bestAction != oldAction) stable = false;\n",
        "            }\n",
        "            \n",
        "            Console.WriteLine($\"Iteration {iter + 1} : stable = {stable}\");\n",
        "            if (stable) break;\n",
        "        }\n",
        "        \n",
        "        return (V, policy);\n",
        "    }\n",
        "}\n",
        "\n",
        "Console.WriteLine(\"=== Iteration de Politique ===\\n\");\n",
        "var (V_pi, policy_pi) = PolicyIteration.Solve(mdp);\n",
        "\n",
        "// Verifier que les resultats sont identiques\n",
        "Console.WriteLine(\"\\nComparaison avec Value Iteration :\");\n",
        "bool same = true;\n",
        "foreach (var s in mdp.GetStates())\n",
        "{\n",
        "    if (policy[s] != policy_pi[s])\n",
        "    {\n",
        "        same = false;\n",
        "        Console.WriteLine($\"  Difference en {s}: VI={policy[s]}, PI={policy_pi[s]}\");\n",
        "    }\n",
        "}\n",
        "if (same) Console.WriteLine(\"  Politiques identiques !\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Alternatives : LP, Expectimax, RTDP\n",
        "\n",
        "### Programmation Lineaire (LP)\n",
        "\n",
        "Le MDP peut etre formule comme un programme lineaire :\n",
        "\n",
        "- **Variables** : V(s) pour chaque etat\n",
        "- **Objectif** : min Σ_s V(s)\n",
        "- **Contraintes** : V(s) ≥ R(s,a) + γ Σ_s' P(s'|s,a) V(s') pour tout a\n",
        "\n",
        "### Expectimax\n",
        "\n",
        "Pour les MDPs a horizon fini, on peut utiliser un arbre de recherche :\n",
        "\n",
        "```\n",
        "       (s0)\n",
        "      / | \\\n",
        "   a1  a2  a3     <- Noeuds max (choix agent)\n",
        "   / \\ \n",
        " s1  s2           <- Noeuds chance (transition)\n",
        "```\n",
        "\n",
        "### RTDP (Real-Time Dynamic Programming)\n",
        "\n",
        "- Mise a jour le long de trajectoires simulees\n",
        "- Focus sur les etats atteignables\n",
        "- Algorithme **anytime** : ameliore avec plus de temps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "csharp"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== RTDP (100 trials depuis (0,0)) ===\n",
            "\n",
            "Comparaison V_RTDP vs V_VI :\n",
            "  Etat   |  V_RTDP  |   V_VI   |   Diff\n",
            "---------|----------|----------|--------\n",
            " (0,0)   |   0,1666 |   0,2960 | 0,1294\n",
            " (0,2)   |  -0,1123 |   0,5094 | 0,6217\n",
            " (2,2)   |   0,7954 |   0,7954 | 0,0000\n",
            " (3,2)   |   1,0000 |   1,0000 | 0,0000\n"
          ]
        }
      ],
      "source": [
        "// RTDP simplifie\n",
        "\n",
        "public class RTDP\n",
        "{\n",
        "    public static Dictionary<(int,int), double> Solve(\n",
        "        GridMDP mdp, (int, int) startState, int nTrials = 100)\n",
        "    {\n",
        "        var states = mdp.GetStates();\n",
        "        var V = states.ToDictionary(s => s, s => 0.0);\n",
        "        var rng = new Random(42);\n",
        "        \n",
        "        for (int trial = 0; trial < nTrials; trial++)\n",
        "        {\n",
        "            var s = startState;\n",
        "            int steps = 0;\n",
        "            \n",
        "            while (!mdp.TerminalStates.Contains(s) && steps < 100)\n",
        "            {\n",
        "                // Mise a jour de Bellman\n",
        "                double maxQ = double.NegativeInfinity;\n",
        "                string bestAction = null;\n",
        "                \n",
        "                foreach (var a in mdp.Actions)\n",
        "                {\n",
        "                    double q = mdp.GetReward(s);\n",
        "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
        "                        q += mdp.Gamma * prob * V[nextState];\n",
        "                    \n",
        "                    if (q > maxQ)\n",
        "                    {\n",
        "                        maxQ = q;\n",
        "                        bestAction = a;\n",
        "                    }\n",
        "                }\n",
        "                \n",
        "                V[s] = maxQ;\n",
        "                \n",
        "                // Simuler transition\n",
        "                var transitions = mdp.GetTransitions(s, bestAction);\n",
        "                double r = rng.NextDouble();\n",
        "                double cumProb = 0;\n",
        "                foreach (var (nextState, prob) in transitions)\n",
        "                {\n",
        "                    cumProb += prob;\n",
        "                    if (r <= cumProb)\n",
        "                    {\n",
        "                        s = nextState;\n",
        "                        break;\n",
        "                    }\n",
        "                }\n",
        "                \n",
        "                steps++;\n",
        "            }\n",
        "            \n",
        "            // Update terminal state\n",
        "            if (mdp.TerminalStates.Contains(s))\n",
        "                V[s] = mdp.GetReward(s);\n",
        "        }\n",
        "        \n",
        "        return V;\n",
        "    }\n",
        "}\n",
        "\n",
        "Console.WriteLine(\"=== RTDP (100 trials depuis (0,0)) ===\\n\");\n",
        "var V_rtdp = RTDP.Solve(mdp, (0, 0), nTrials: 100);\n",
        "\n",
        "// Comparer avec Value Iteration\n",
        "Console.WriteLine(\"Comparaison V_RTDP vs V_VI :\");\n",
        "Console.WriteLine(\"  Etat   |  V_RTDP  |   V_VI   |   Diff\");\n",
        "Console.WriteLine(\"---------|----------|----------|--------\");\n",
        "foreach (var s in new[] { (0,0), (0,2), (2,2), (3,2) })\n",
        "{\n",
        "    double diff = Math.Abs(V_rtdp[s] - V[s]);\n",
        "    Console.WriteLine($\" ({s.Item1},{s.Item2})   | {V_rtdp[s],8:F4} | {V[s],8:F4} | {diff,6:F4}\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Reward Shaping\n",
        "\n",
        "### Le probleme des recompenses sparses\n",
        "\n",
        "Quand les recompenses sont rares (ex: +1 seulement au but), l'apprentissage est tres lent.\n",
        "\n",
        "### Solution : Ajouter une recompense de faconnage\n",
        "\n",
        "$$R'(s, a, s') = R(s, a, s') + F(s, a, s')$$\n",
        "\n",
        "### Theoreme de preservation de politique (Ng et al., 1999)\n",
        "\n",
        "> Si F a la forme d'une **fonction de potentiel** :\n",
        "> \n",
        "> $$F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)$$\n",
        "> \n",
        "> Alors **la politique optimale est preservee** !\n",
        "\n",
        "### Intuition\n",
        "\n",
        "- Φ(s) represente \"a quel point s est proche du but\"\n",
        "- F recompense les transitions vers des etats meilleurs\n",
        "- La forme specifique garantit que les raccourcis ne sont pas crees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "csharp"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Reward Shaping avec Fonction de Potentiel ===\n",
            "\n",
            "But en (3, 2)\n",
            "Phi(s) = -distance(s, but)\n",
            "\n",
            "Fonction de potentiel Phi(s) :\n",
            "  -3,00   -2,00   -1,00   -0,00 \n",
            "  -3,16   ####    -1,41   -1,00 \n",
            "  -3,61   -2,83   -2,24   -2,00 \n",
            "\n",
            "Exemples de shaping reward F(s, a, s') = gamma*Phi(s') - Phi(s) :\n",
            "  F((0,0) -> (1,0)) = 1,060 (vers le but)\n",
            "  F((1,0) -> (0,0)) = -0,417 (loin du but)\n",
            "  F((2,2) -> (3,2)) = 1,000 (atteindre le but)\n",
            "\n",
            "=> Le shaping recompense les mouvements vers le but,\n",
            "   mais le theoreme garantit que la politique optimale est preservee.\n"
          ]
        }
      ],
      "source": [
        "// Demonstration du Reward Shaping\n",
        "\n",
        "public class ShapedGridMDP : GridMDP\n",
        "{\n",
        "    private Func<(int, int), double> _potential;\n",
        "    private double _gamma;\n",
        "    \n",
        "    public ShapedGridMDP(int width, int height, double gamma, Func<(int, int), double> potential) \n",
        "        : base(width, height, gamma)\n",
        "    {\n",
        "        _potential = potential;\n",
        "        _gamma = gamma;\n",
        "    }\n",
        "    \n",
        "    public double GetShapedReward((int, int) s, (int, int) sPrime)\n",
        "    {\n",
        "        double R = GetReward(s);\n",
        "        double F = _gamma * _potential(sPrime) - _potential(s);\n",
        "        return R + F;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Fonction de potentiel : distance negative au but\n",
        "(int, int) goal = (3, 2);\n",
        "Func<(int, int), double> Phi = s => -Math.Sqrt(Math.Pow(s.Item1 - goal.Item1, 2) + Math.Pow(s.Item2 - goal.Item2, 2));\n",
        "\n",
        "Console.WriteLine(\"=== Reward Shaping avec Fonction de Potentiel ===\\n\");\n",
        "Console.WriteLine($\"But en {goal}\");\n",
        "Console.WriteLine(\"Phi(s) = -distance(s, but)\\n\");\n",
        "\n",
        "Console.WriteLine(\"Fonction de potentiel Phi(s) :\");\n",
        "for (int y = 2; y >= 0; y--)\n",
        "{\n",
        "    for (int x = 0; x < 4; x++)\n",
        "    {\n",
        "        if (mdp.Walls.Contains((x, y)))\n",
        "            Console.Write(\"  ####  \");\n",
        "        else\n",
        "            Console.Write($\" {Phi((x, y)),6:F2} \");\n",
        "    }\n",
        "    Console.WriteLine();\n",
        "}\n",
        "\n",
        "Console.WriteLine(\"\\nExemples de shaping reward F(s, a, s') = gamma*Phi(s') - Phi(s) :\");\n",
        "Console.WriteLine($\"  F((0,0) -> (1,0)) = {0.9 * Phi((1, 0)) - Phi((0, 0)):F3} (vers le but)\");\n",
        "Console.WriteLine($\"  F((1,0) -> (0,0)) = {0.9 * Phi((0, 0)) - Phi((1, 0)):F3} (loin du but)\");\n",
        "Console.WriteLine($\"  F((2,2) -> (3,2)) = {0.9 * Phi((3, 2)) - Phi((2, 2)):F3} (atteindre le but)\");\n",
        "\n",
        "Console.WriteLine();\n",
        "Console.WriteLine(\"=> Le shaping recompense les mouvements vers le but,\");\n",
        "Console.WriteLine(\"   mais le theoreme garantit que la politique optimale est preservee.\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Bandits Multi-Bras\n",
        "\n",
        "### Le probleme\n",
        "\n",
        "Vous avez K machines a sous (\"bras\"). Chaque bras i donne une recompense selon une distribution inconnue.\n",
        "\n",
        "**Dilemme exploration/exploitation** :\n",
        "- Explorer : essayer de nouveaux bras pour estimer leurs distributions\n",
        "- Exploiter : jouer le meilleur bras connu\n",
        "\n",
        "### Approches classiques\n",
        "\n",
        "| Methode | Description |\n",
        "|---------|-------------|\n",
        "| ε-greedy | Exploiter avec proba 1-ε, explorer avec ε |\n",
        "| UCB | Upper Confidence Bound : optimisme face a l'incertitude |\n",
        "| Thompson | Echantillonner selon la posterior des recompenses |\n",
        "| **Gittins** | Solution optimale pour bandits avec discount |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "csharp"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bandit avec 4 bras, moyennes vraies inconnues\n",
            "Meilleure moyenne : 0,7\n",
            "\n",
            "ε-greedy (ε=0,1):\n",
            "  Recompense totale : 684,6\n",
            "  Regret cumule : 15,4\n",
            "  Tirages par bras : 28, 69, 882, 21\n",
            "\n",
            "UCB1:\n",
            "  Recompense totale : 616,1\n",
            "  Regret cumule : 83,9\n",
            "  Tirages par bras : 54, 166, 719, 61\n",
            "\n"
          ]
        }
      ],
      "source": [
        "// Bandit multi-bras avec differentes strategies\n",
        "\n",
        "public class MultiArmedBandit\n",
        "{\n",
        "    private double[] _trueMeans;\n",
        "    private Random _rng;\n",
        "    \n",
        "    public int K { get; }\n",
        "    \n",
        "    public MultiArmedBandit(double[] means, int seed = 42)\n",
        "    {\n",
        "        _trueMeans = means;\n",
        "        K = means.Length;\n",
        "        _rng = new Random(seed);\n",
        "    }\n",
        "    \n",
        "    public double Pull(int arm)\n",
        "    {\n",
        "        // Recompense gaussienne autour de la moyenne vraie\n",
        "        double u1 = _rng.NextDouble();\n",
        "        double u2 = _rng.NextDouble();\n",
        "        double z = Math.Sqrt(-2 * Math.Log(u1)) * Math.Cos(2 * Math.PI * u2);\n",
        "        return _trueMeans[arm] + 0.5 * z;\n",
        "    }\n",
        "    \n",
        "    public double OptimalMean => _trueMeans.Max();\n",
        "}\n",
        "\n",
        "// Strategies\n",
        "public interface IBanditStrategy\n",
        "{\n",
        "    int SelectArm(int[] counts, double[] sumRewards);\n",
        "    string Name { get; }\n",
        "}\n",
        "\n",
        "public class EpsilonGreedy : IBanditStrategy\n",
        "{\n",
        "    private double _epsilon;\n",
        "    private Random _rng = new Random();\n",
        "    public string Name => $\"ε-greedy (ε={_epsilon})\";\n",
        "    \n",
        "    public EpsilonGreedy(double epsilon) { _epsilon = epsilon; }\n",
        "    \n",
        "    public int SelectArm(int[] counts, double[] sumRewards)\n",
        "    {\n",
        "        if (_rng.NextDouble() < _epsilon)\n",
        "            return _rng.Next(counts.Length);\n",
        "        \n",
        "        int best = 0;\n",
        "        double bestMean = double.NegativeInfinity;\n",
        "        for (int i = 0; i < counts.Length; i++)\n",
        "        {\n",
        "            double mean = counts[i] > 0 ? sumRewards[i] / counts[i] : 0;\n",
        "            if (mean > bestMean) { bestMean = mean; best = i; }\n",
        "        }\n",
        "        return best;\n",
        "    }\n",
        "}\n",
        "\n",
        "public class UCB : IBanditStrategy\n",
        "{\n",
        "    public string Name => \"UCB1\";\n",
        "    \n",
        "    public int SelectArm(int[] counts, double[] sumRewards)\n",
        "    {\n",
        "        int totalPulls = counts.Sum();\n",
        "        if (totalPulls < counts.Length)\n",
        "            return totalPulls; // Essayer chaque bras une fois\n",
        "        \n",
        "        int best = 0;\n",
        "        double bestUCB = double.NegativeInfinity;\n",
        "        for (int i = 0; i < counts.Length; i++)\n",
        "        {\n",
        "            double mean = sumRewards[i] / counts[i];\n",
        "            double bonus = Math.Sqrt(2 * Math.Log(totalPulls) / counts[i]);\n",
        "            double ucb = mean + bonus;\n",
        "            if (ucb > bestUCB) { bestUCB = ucb; best = i; }\n",
        "        }\n",
        "        return best;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Simulation\n",
        "var bandit = new MultiArmedBandit(new[] { 0.3, 0.5, 0.7, 0.4 });\n",
        "var strategies = new IBanditStrategy[] { new EpsilonGreedy(0.1), new UCB() };\n",
        "\n",
        "Console.WriteLine($\"Bandit avec {bandit.K} bras, moyennes vraies inconnues\");\n",
        "Console.WriteLine($\"Meilleure moyenne : {bandit.OptimalMean}\\n\");\n",
        "\n",
        "int T = 1000;\n",
        "\n",
        "foreach (var strategy in strategies)\n",
        "{\n",
        "    var counts = new int[bandit.K];\n",
        "    var sumRewards = new double[bandit.K];\n",
        "    double totalReward = 0;\n",
        "    double totalRegret = 0;\n",
        "    \n",
        "    for (int t = 0; t < T; t++)\n",
        "    {\n",
        "        int arm = strategy.SelectArm(counts, sumRewards);\n",
        "        double reward = bandit.Pull(arm);\n",
        "        \n",
        "        counts[arm]++;\n",
        "        sumRewards[arm] += reward;\n",
        "        totalReward += reward;\n",
        "        totalRegret += bandit.OptimalMean - reward;\n",
        "    }\n",
        "    \n",
        "    Console.WriteLine($\"{strategy.Name}:\");\n",
        "    Console.WriteLine($\"  Recompense totale : {totalReward:F1}\");\n",
        "    Console.WriteLine($\"  Regret cumule : {totalRegret:F1}\");\n",
        "    Console.WriteLine($\"  Tirages par bras : {string.Join(\", \", counts)}\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Indice de Gittins\n",
        "\n",
        "### Theoreme de Gittins (1979)\n",
        "\n",
        "> Pour un bandit multi-bras avec facteur de discount γ, la strategie optimale est de **toujours jouer le bras avec l'indice de Gittins le plus eleve**.\n",
        "\n",
        "### Definition intuitive\n",
        "\n",
        "L'indice de Gittins d'un bras represente :\n",
        "\n",
        "> \"Le prix equivalent certain\" de ce bras si on pouvait l'abandonner a tout moment.\n",
        "\n",
        "### Calcul\n",
        "\n",
        "L'indice est la solution d'un probleme d'arret optimal :\n",
        "\n",
        "$$G(s) = \\sup_{\\tau \\geq 1} \\frac{E\\left[\\sum_{t=0}^{\\tau-1} \\gamma^t R_t | s_0 = s\\right]}{E\\left[\\sum_{t=0}^{\\tau-1} \\gamma^t | s_0 = s\\right]}$$\n",
        "\n",
        "### Importance\n",
        "\n",
        "- Reduit un probleme multi-bras complexe a des problemes independants\n",
        "- Justifie theoriquement l'optimisme face a l'incertitude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. POMDPs : MDPs Partiellement Observables\n",
        "\n",
        "### Extension du MDP\n",
        "\n",
        "Dans un POMDP, l'agent **ne connait pas l'etat exact**. Il recoit des **observations** bruitees.\n",
        "\n",
        "### Definition formelle\n",
        "\n",
        "Un POMDP est un tuple (S, A, P, R, O, Ω, γ) :\n",
        "\n",
        "| Element | Description |\n",
        "|---------|-------------|\n",
        "| S, A, P, R, γ | Comme MDP |\n",
        "| **O** | Ensemble des observations |\n",
        "| **Ω** | P(o\\|s, a) : modele de capteur |\n",
        "\n",
        "### Belief State\n",
        "\n",
        "L'agent maintient une **distribution de croyance** b(s) sur les etats :\n",
        "\n",
        "$$b'(s') = \\eta \\cdot \\Omega(o|s', a) \\sum_s P(s'|s, a) b(s)$$\n",
        "\n",
        "### Plans conditionnels\n",
        "\n",
        "La politique d'un POMDP est un **arbre de decision** :\n",
        "\n",
        "```\n",
        "        a1\n",
        "       /  \\\n",
        "     o1    o2\n",
        "     |      |\n",
        "    a2     a3\n",
        "   / \\    / \\\n",
        "  o1 o2  o1 o2\n",
        "  ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "dotnet_interactive": {
          "language": "csharp"
        },
        "vscode": {
          "languageId": "csharp"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== POMDP : Probleme du Tigre ===\n",
            "\n",
            "Deux portes : gauche (G) et droite (D)\n",
            "Un tigre est cache derriere l'une des portes.\n",
            "Actions : Ouvrir G, Ouvrir D, Ecouter\n",
            "\n",
            "Belief initial : P(tigre gauche) = 50 %\n",
            "\n",
            "Utilites esperees :\n",
            "  E[U(ouvrir gauche) | b=50 %] = -45,0\n",
            "  E[U(ouvrir droite) | b=50 %] = -45,0\n",
            "  Ecouter : cout immediat = -1, mais reduit l'incertitude\n",
            "\n",
            "Simulation : 3 ecoutes donnent 'bruit gauche'\n",
            "\n",
            "Apres observation 1 : P(tigre gauche) = 85,0 %\n",
            "Apres observation 2 : P(tigre gauche) = 97,0 %\n",
            "Apres observation 3 : P(tigre gauche) = 99,5 %\n",
            "\n",
            "E[U(ouvrir gauche)] = -99,4\n",
            "E[U(ouvrir droite)] = 9,4\n",
            "\n",
            "=> Decision : OUVRIR DROITE (tigre probablement a gauche)\n"
          ]
        }
      ],
      "source": [
        "// POMDP simple : Tigre derriere une porte\n",
        "\n",
        "Console.WriteLine(\"=== POMDP : Probleme du Tigre ===\\n\");\n",
        "Console.WriteLine(\"Deux portes : gauche (G) et droite (D)\");\n",
        "Console.WriteLine(\"Un tigre est cache derriere l'une des portes.\");\n",
        "Console.WriteLine(\"Actions : Ouvrir G, Ouvrir D, Ecouter\\n\");\n",
        "\n",
        "// Etats : tigre gauche (TG), tigre droite (TD)\n",
        "// Actions : ouvrir_gauche, ouvrir_droite, ecouter\n",
        "// Observations (si ecouter) : bruit_gauche, bruit_droite\n",
        "\n",
        "double pCorrectHearing = 0.85; // P(bruit_gauche | tigre_gauche)\n",
        "double rewardTresor = 10;\n",
        "double rewardTigre = -100;\n",
        "double costEcoute = -1;\n",
        "\n",
        "// Belief state : b = P(tigre_gauche)\n",
        "double b = 0.5; // Prior uniforme\n",
        "\n",
        "Console.WriteLine($\"Belief initial : P(tigre gauche) = {b:P0}\\n\");\n",
        "\n",
        "// Fonction de valeur pour chaque action\n",
        "double EU_ouvrirGauche(double belief) => \n",
        "    belief * rewardTigre + (1 - belief) * rewardTresor;\n",
        "\n",
        "double EU_ouvrirDroite(double belief) => \n",
        "    belief * rewardTresor + (1 - belief) * rewardTigre;\n",
        "\n",
        "// Pour ecouter, on doit calculer la valeur esperee apres observation\n",
        "// (simplifie ici)\n",
        "\n",
        "Console.WriteLine(\"Utilites esperees :\");\n",
        "Console.WriteLine($\"  E[U(ouvrir gauche) | b={b:P0}] = {EU_ouvrirGauche(b):F1}\");\n",
        "Console.WriteLine($\"  E[U(ouvrir droite) | b={b:P0}] = {EU_ouvrirDroite(b):F1}\");\n",
        "Console.WriteLine($\"  Ecouter : cout immediat = {costEcoute}, mais reduit l'incertitude\\n\");\n",
        "\n",
        "// Simuler une sequence d'observations\n",
        "Console.WriteLine(\"Simulation : 3 ecoutes donnent 'bruit gauche'\\n\");\n",
        "\n",
        "for (int i = 0; i < 3; i++)\n",
        "{\n",
        "    // Mise a jour bayesienne apres observation \"bruit gauche\"\n",
        "    // P(TG|bruit_g) = P(bruit_g|TG) * P(TG) / P(bruit_g)\n",
        "    double pBruitGauche = b * pCorrectHearing + (1 - b) * (1 - pCorrectHearing);\n",
        "    b = (pCorrectHearing * b) / pBruitGauche;\n",
        "    \n",
        "    Console.WriteLine($\"Apres observation {i+1} : P(tigre gauche) = {b:P1}\");\n",
        "}\n",
        "\n",
        "Console.WriteLine();\n",
        "Console.WriteLine($\"E[U(ouvrir gauche)] = {EU_ouvrirGauche(b):F1}\");\n",
        "Console.WriteLine($\"E[U(ouvrir droite)] = {EU_ouvrirDroite(b):F1}\");\n",
        "Console.WriteLine();\n",
        "Console.WriteLine($\"=> Decision : OUVRIR DROITE (tigre probablement a gauche)\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Lien avec la Serie RL\n",
        "\n",
        "### Ce notebook : Concepts fondamentaux\n",
        "\n",
        "- MDPs et equations de Bellman\n",
        "- Methodes tabulaires (Value Iteration, Policy Iteration)\n",
        "- Concepts theoriques (Gittins, POMDPs)\n",
        "\n",
        "### Serie RL (`MyIA.AI.Notebooks/RL/`) : Implementations avancees\n",
        "\n",
        "| Notebook | Contenu |\n",
        "|----------|--------|\n",
        "| RL-1-Introduction | Q-Learning tabulaire |\n",
        "| RL-2-DeepRL | DQN avec reseaux de neurones |\n",
        "| RL-3-PolicyGradient | REINFORCE, Actor-Critic |\n",
        "| RL-4-AdvancedMethods | PPO, A2C, SAC |\n",
        "| RL-5-Applications | Gym, Stable-Baselines3 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Resume\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **MDP** | (S, A, P, R, γ) - cadre formel pour decisions sequentielles |\n",
        "| **Bellman** | V*(s) = max_a [R + γ Σ P V*] |\n",
        "| **Value Iteration** | Mise a jour iterative de V jusqu'a convergence |\n",
        "| **Policy Iteration** | Evaluation + Amelioration alternees |\n",
        "| **Reward Shaping** | F = γΦ(s') - Φ(s) preserve la politique optimale |\n",
        "| **Bandits** | Exploration vs Exploitation |\n",
        "| **Gittins** | Indice optimal pour bandits avec discount |\n",
        "| **POMDP** | MDP avec observations bruitees, belief states |\n",
        "\n",
        "---\n",
        "\n",
        "## Pour aller plus loin\n",
        "\n",
        "| Si vous voulez... | Consultez... |\n",
        "|-------------------|-------------|\n",
        "| Deep Reinforcement Learning | Serie `MyIA.AI.Notebooks/RL/` |\n",
        "| Implementations PyTorch/TF | Stable-Baselines3 |\n",
        "| Theorie avancee | Sutton & Barto \"Reinforcement Learning\" |\n",
        "\n",
        "---\n",
        "\n",
        "## Fin de la Serie Decision Theory\n",
        "\n",
        "Felicitations ! Vous avez termine les 7 notebooks sur la Decision Theory.\n",
        "\n",
        "### Recapitulatif de la serie 14-20\n",
        "\n",
        "| # | Titre | Concepts cles |\n",
        "|---|-------|---------------|\n",
        "| 14 | Utility Foundations | Axiomes VNM, loteries, agent rationnel |\n",
        "| 15 | Utility Money | CARA/CRRA, aversion au risque, dominance |\n",
        "| 16 | Multi-Attribute | MAUT, independance, SMART |\n",
        "| 17 | Decision Networks | Influence diagrams, arcs informationnels |\n",
        "| 18 | Value of Information | EVPI, EVSI, droits de forage |\n",
        "| 19 | Expert Systems | Minimax, regret, robustesse |\n",
        "| 20 | Sequential | MDPs, bandits, POMDPs |\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "- Bellman (1957) : Dynamic Programming\n",
        "- Gittins (1979) : Bandit Processes and Dynamic Allocation Indices\n",
        "- Ng, Harada, Russell (1999) : Policy Invariance Under Reward Transformations\n",
        "- Kaelbling, Littman, Cassandra (1998) : Planning and Acting in Partially Observable Stochastic Domains\n",
        "- Sutton & Barto (2018) : Reinforcement Learning: An Introduction"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".NET (C#)",
      "language": "C#",
      "name": ".net-csharp"
    },
    "language_info": {
      "file_extension": ".cs",
      "mimetype": "text/x-csharp",
      "name": "C#",
      "pygments_lexer": "csharp",
      "version": "13.0"
    },
    "polyglot_notebook": {
      "kernelInfo": {
        "defaultKernelName": "csharp",
        "items": [
          {
            "aliases": [],
            "languageName": "csharp",
            "name": "csharp"
          }
        ]
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
