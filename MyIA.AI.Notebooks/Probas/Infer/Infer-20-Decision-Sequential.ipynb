{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer-20-Decision-Sequential : MDPs, Bandits et POMDPs\n",
    "\n",
    "**Serie** : Programmation Probabiliste avec Infer.NET (20/20)  \n",
    "**Duree estimee** : 60 minutes  \n",
    "**Prerequis** : Notebooks 14-19 (Decision Theory)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Comprendre les **Processus de Decision Markoviens** (MDPs)\n",
    "- Maitriser l'**iteration de valeur** et l'**iteration de politique**\n",
    "- Decouvrir les alternatives : **LP, Expectimax, RTDP**\n",
    "- Appliquer le **reward shaping** avec le theoreme de preservation de politique\n",
    "- Introduire les **bandits multi-bras** et l'**indice de Gittins**\n",
    "- Presenter les **POMDPs** et les belief states\n",
    "\n",
    "---\n",
    "\n",
    "## Navigation\n",
    "\n",
    "| Precedent | Suivant |\n",
    "|-----------|--------|\n",
    "| [Infer-19-Decision-Expert-Systems](Infer-19-Decision-Expert-Systems.ipynb) | Serie RL (`MyIA.AI.Notebooks/RL/`) |\n",
    "\n",
    "---\n",
    "\n",
    "**Note** : Ce notebook est une introduction conceptuelle aux decisions sequentielles. Les implementations avancees avec Deep RL sont dans la serie `RL/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decisions Sequentielles vs One-Shot\n",
    "\n",
    "### Difference fondamentale\n",
    "\n",
    "| Type | Caracteristique | Exemple |\n",
    "|------|-----------------|--------|\n",
    "| **One-shot** | Une seule decision | Accepter une offre d'emploi |\n",
    "| **Sequentielle** | Serie de decisions interdependantes | Jouer aux echecs |\n",
    "\n",
    "### Horizon\n",
    "\n",
    "- **Fini** : Nombre fixe d'etapes (T etapes)\n",
    "- **Infini** : Le processus continue indefiniment\n",
    "\n",
    "### Facteur d'actualisation γ (gamma)\n",
    "\n",
    "Pour les horizons infinis, on utilise un facteur γ ∈ [0,1) pour :\n",
    "\n",
    "- Garantir la convergence des sommes infinies\n",
    "- Modeliser la preference pour les recompenses immediates\n",
    "- γ = 0.99 : vision long terme\n",
    "- γ = 0.5 : vision court terme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.ML.Probabilistic, 0.4.2504.701</span></li><li><span>Microsoft.ML.Probabilistic.Compiler, 0.4.2504.701</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer.NET charge !\n"
     ]
    }
   ],
   "source": [
    "// Installation Infer.NET\n",
    "#r \"nuget: Microsoft.ML.Probabilistic\"\n",
    "#r \"nuget: Microsoft.ML.Probabilistic.Compiler\"\n",
    "\n",
    "using Microsoft.ML.Probabilistic;\n",
    "using Microsoft.ML.Probabilistic.Distributions;\n",
    "using Microsoft.ML.Probabilistic.Models;\n",
    "using Microsoft.ML.Probabilistic.Algorithms;\n",
    "\n",
    "Console.WriteLine(\"Infer.NET charge !\");"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Environnement pret\n\nL'infrastructure Infer.NET est chargee. Dans ce notebook, nous utiliserons principalement Infer.NET pour la section sur les **belief states** dans les POMDPs, ou l'inference bayesienne automatique simplifie considerablement les calculs.\n\nCommencons par definir un MDP simple : une grille de navigation avec recompenses et obstacles.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processus de Decision Markoviens (MDP)\n",
    "\n",
    "### Definition formelle\n",
    "\n",
    "Un MDP est un tuple (S, A, P, R, γ) :\n",
    "\n",
    "| Element | Notation | Description |\n",
    "|---------|----------|-------------|\n",
    "| **Etats** | S | Ensemble des etats possibles |\n",
    "| **Actions** | A | Ensemble des actions |\n",
    "| **Transitions** | P(s'\\|s,a) | Probabilite de transition |\n",
    "| **Recompenses** | R(s,a) ou R(s,a,s') | Recompense immediate |\n",
    "| **Discount** | γ | Facteur d'actualisation |\n",
    "\n",
    "### Politique et Fonction de Valeur\n",
    "\n",
    "- **Politique** π : S → A (ou distribution sur A)\n",
    "- **Fonction de valeur** V^π(s) : Utilite esperee depuis s en suivant π\n",
    "- **Fonction action-valeur** Q^π(s, a) : Utilite esperee en faisant a puis suivant π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Grille 4x3 cree :\n",
      "  But (+1) en (3,2)\n",
      "  Piege (-1) en (3,1)\n",
      "  Mur en (1,1)\n",
      "  Gamma = 0,9\n"
     ]
    }
   ],
   "source": [
    "// Definition d'un MDP simple : Navigation dans une grille\n",
    "\n",
    "public class GridMDP\n",
    "{\n",
    "    public int Width { get; }\n",
    "    public int Height { get; }\n",
    "    public double Gamma { get; }\n",
    "    public Dictionary<(int, int), double> Rewards { get; }\n",
    "    public HashSet<(int, int)> TerminalStates { get; }\n",
    "    public HashSet<(int, int)> Walls { get; }\n",
    "    \n",
    "    public string[] Actions { get; } = { \"N\", \"S\", \"E\", \"W\" };\n",
    "    \n",
    "    // Directions\n",
    "    private Dictionary<string, (int dx, int dy)> _directions = new()\n",
    "    {\n",
    "        { \"N\", (0, 1) }, { \"S\", (0, -1) }, { \"E\", (1, 0) }, { \"W\", (-1, 0) }\n",
    "    };\n",
    "    \n",
    "    public GridMDP(int width, int height, double gamma = 0.9)\n",
    "    {\n",
    "        Width = width;\n",
    "        Height = height;\n",
    "        Gamma = gamma;\n",
    "        Rewards = new Dictionary<(int, int), double>();\n",
    "        TerminalStates = new HashSet<(int, int)>();\n",
    "        Walls = new HashSet<(int, int)>();\n",
    "    }\n",
    "    \n",
    "    public List<(int, int)> GetStates()\n",
    "    {\n",
    "        var states = new List<(int, int)>();\n",
    "        for (int x = 0; x < Width; x++)\n",
    "            for (int y = 0; y < Height; y++)\n",
    "                if (!Walls.Contains((x, y)))\n",
    "                    states.Add((x, y));\n",
    "        return states;\n",
    "    }\n",
    "    \n",
    "    public double GetReward((int, int) state) => \n",
    "        Rewards.TryGetValue(state, out var r) ? r : -0.04; // Cout de mouvement par defaut\n",
    "    \n",
    "    public List<((int, int) nextState, double prob)> GetTransitions((int, int) state, string action)\n",
    "    {\n",
    "        if (TerminalStates.Contains(state))\n",
    "            return new List<((int, int), double)> { (state, 1.0) };\n",
    "        \n",
    "        var transitions = new List<((int, int), double)>();\n",
    "        var (dx, dy) = _directions[action];\n",
    "        \n",
    "        // 80% de chance d'aller dans la direction voulue\n",
    "        transitions.Add((NextState(state, action), 0.8));\n",
    "        \n",
    "        // 10% de chance d'aller perpendiculairement (droite ou gauche)\n",
    "        var perpActions = action == \"N\" || action == \"S\" \n",
    "            ? new[] { \"E\", \"W\" } \n",
    "            : new[] { \"N\", \"S\" };\n",
    "        \n",
    "        foreach (var perpAction in perpActions)\n",
    "            transitions.Add((NextState(state, perpAction), 0.1));\n",
    "        \n",
    "        return transitions;\n",
    "    }\n",
    "    \n",
    "    private (int, int) NextState((int, int) state, string action)\n",
    "    {\n",
    "        var (x, y) = state;\n",
    "        var (dx, dy) = _directions[action];\n",
    "        int nx = x + dx, ny = y + dy;\n",
    "        \n",
    "        // Collision avec mur ou bord = rester sur place\n",
    "        if (nx < 0 || nx >= Width || ny < 0 || ny >= Height || Walls.Contains((nx, ny)))\n",
    "            return state;\n",
    "        return (nx, ny);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Creer le MDP classique de Russell & Norvig\n",
    "var mdp = new GridMDP(4, 3, gamma: 0.9);\n",
    "mdp.Rewards[(3, 2)] = 1.0;   // But positif\n",
    "mdp.Rewards[(3, 1)] = -1.0;  // Piege\n",
    "mdp.TerminalStates.Add((3, 2));\n",
    "mdp.TerminalStates.Add((3, 1));\n",
    "mdp.Walls.Add((1, 1));       // Mur\n",
    "\n",
    "Console.WriteLine(\"MDP Grille 4x3 cree :\");\n",
    "Console.WriteLine(\"  But (+1) en (3,2)\");\n",
    "Console.WriteLine(\"  Piege (-1) en (3,1)\");\n",
    "Console.WriteLine(\"  Mur en (1,1)\");\n",
    "Console.WriteLine($\"  Gamma = {mdp.Gamma}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Equation de Bellman\n",
    "\n",
    "### Intuition\n",
    "\n",
    "L'equation de Bellman exprime un principe fondamental de **consistance temporelle** :\n",
    "\n",
    "> La valeur d'un etat = recompense immediate + valeur actualisee des etats futurs\n",
    "\n",
    "C'est une relation **recursive** : la valeur de chaque etat depend de la valeur des etats atteignables. Cette structure permet de resoudre le probleme \"a l'envers\" (backward induction) ou iterativement.\n",
    "\n",
    "### Equation de Bellman pour V*\n",
    "\n",
    "La fonction de valeur optimale satisfait :\n",
    "\n",
    "$$V^*(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right]$$\n",
    "\n",
    "**Interpretation terme par terme** :\n",
    "- $\\max_a$ : On choisit la meilleure action\n",
    "- $R(s,a)$ : Recompense immediate\n",
    "- $\\gamma$ : Facteur d'actualisation (valeur du futur)\n",
    "- $\\sum_{s'} P(s'|s,a) V^*(s')$ : Esperance de la valeur future\n",
    "\n",
    "### Equation de Bellman pour Q*\n",
    "\n",
    "$$Q^*(s, a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q^*(s', a')$$\n",
    "\n",
    "La fonction Q donne la valeur de **faire a puis agir optimalement**.\n",
    "\n",
    "### Politique optimale\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$$\n",
    "\n",
    "La politique optimale est **deterministe** : dans chaque etat, une action domine.\n",
    "\n",
    "### Pourquoi Bellman est-il si important ?\n",
    "\n",
    "L'equation de Bellman est la **cle de voute** de la programmation dynamique et du reinforcement learning car :\n",
    "\n",
    "1. Elle decompose un probleme global en sous-problemes locaux\n",
    "2. Elle fournit une condition d'optimalite verifiable\n",
    "3. Elle inspire les algorithmes (VI, PI, Q-learning, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Iteration de Valeur\n",
    "\n",
    "### Algorithme\n",
    "\n",
    "1. Initialiser V(s) = 0 pour tout s\n",
    "2. Repeter jusqu'a convergence :\n",
    "   - Pour chaque etat s :\n",
    "     - V(s) ← max_a [R(s,a) + γ Σ_s' P(s'|s,a) V(s')]\n",
    "3. Extraire la politique : π(s) = argmax_a Q(s,a)\n",
    "\n",
    "### Complexite\n",
    "\n",
    "- O(|S|² |A|) par iteration\n",
    "- Converge en O(log(1/ε) / (1-γ)) iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence apres 14 iterations (delta = 6,01E-004)\n",
      "\n",
      "Fonction de Valeur V* :\n",
      "  0,509   0,650   0,795   1,000 \n",
      "  0,398   ####    0,486  -1,000 \n",
      "  0,296   0,254   0,345   0,130 \n",
      "\n",
      "Politique optimale π* :\n",
      " →  →  →  ● \n",
      " ↑  #  ↑  ● \n",
      " ↑  →  ↑  ← \n"
     ]
    }
   ],
   "source": [
    "// Iteration de Valeur\n",
    "\n",
    "public class ValueIteration\n",
    "{\n",
    "    public static (Dictionary<(int,int), double> V, Dictionary<(int,int), string> policy) \n",
    "        Solve(GridMDP mdp, double epsilon = 0.001, int maxIter = 100)\n",
    "    {\n",
    "        var states = mdp.GetStates();\n",
    "        var V = states.ToDictionary(s => s, s => 0.0);\n",
    "        \n",
    "        for (int iter = 0; iter < maxIter; iter++)\n",
    "        {\n",
    "            double delta = 0;\n",
    "            var newV = new Dictionary<(int, int), double>(V);\n",
    "            \n",
    "            foreach (var s in states)\n",
    "            {\n",
    "                if (mdp.TerminalStates.Contains(s))\n",
    "                {\n",
    "                    newV[s] = mdp.GetReward(s);\n",
    "                    continue;\n",
    "                }\n",
    "                \n",
    "                double maxQ = double.NegativeInfinity;\n",
    "                foreach (var a in mdp.Actions)\n",
    "                {\n",
    "                    double q = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        q += mdp.Gamma * prob * V[nextState];\n",
    "                    maxQ = Math.Max(maxQ, q);\n",
    "                }\n",
    "                \n",
    "                newV[s] = maxQ;\n",
    "                delta = Math.Max(delta, Math.Abs(V[s] - newV[s]));\n",
    "            }\n",
    "            \n",
    "            V = newV;\n",
    "            \n",
    "            if (delta < epsilon)\n",
    "            {\n",
    "                Console.WriteLine($\"Convergence apres {iter + 1} iterations (delta = {delta:E2})\");\n",
    "                break;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Extraire la politique\n",
    "        var policy = new Dictionary<(int, int), string>();\n",
    "        foreach (var s in states)\n",
    "        {\n",
    "            if (mdp.TerminalStates.Contains(s))\n",
    "            {\n",
    "                policy[s] = \"T\"; // Terminal\n",
    "                continue;\n",
    "            }\n",
    "            \n",
    "            string bestAction = null;\n",
    "            double maxQ = double.NegativeInfinity;\n",
    "            \n",
    "            foreach (var a in mdp.Actions)\n",
    "            {\n",
    "                double q = mdp.GetReward(s);\n",
    "                foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                    q += mdp.Gamma * prob * V[nextState];\n",
    "                \n",
    "                if (q > maxQ)\n",
    "                {\n",
    "                    maxQ = q;\n",
    "                    bestAction = a;\n",
    "                }\n",
    "            }\n",
    "            policy[s] = bestAction;\n",
    "        }\n",
    "        \n",
    "        return (V, policy);\n",
    "    }\n",
    "}\n",
    "\n",
    "var (V, policy) = ValueIteration.Solve(mdp);\n",
    "\n",
    "// Afficher la grille\n",
    "Console.WriteLine(\"\\nFonction de Valeur V* :\");\n",
    "for (int y = mdp.Height - 1; y >= 0; y--)\n",
    "{\n",
    "    for (int x = 0; x < mdp.Width; x++)\n",
    "    {\n",
    "        if (mdp.Walls.Contains((x, y)))\n",
    "            Console.Write(\"  ####  \");\n",
    "        else\n",
    "            Console.Write($\" {V[(x, y)],6:F3} \");\n",
    "    }\n",
    "    Console.WriteLine();\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"\\nPolitique optimale π* :\");\n",
    "var arrows = new Dictionary<string, string> { {\"N\",\"↑\"}, {\"S\",\"↓\"}, {\"E\",\"→\"}, {\"W\",\"←\"}, {\"T\",\"●\"} };\n",
    "for (int y = mdp.Height - 1; y >= 0; y--)\n",
    "{\n",
    "    for (int x = 0; x < mdp.Width; x++)\n",
    "    {\n",
    "        if (mdp.Walls.Contains((x, y)))\n",
    "            Console.Write(\" # \");\n",
    "        else\n",
    "            Console.Write($\" {arrows[policy[(x, y)]]} \");\n",
    "    }\n",
    "    Console.WriteLine();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation de l'Iteration de Valeur\n",
    "\n",
    "**Fonction de valeur V* :**\n",
    "- Les valeurs decroissent en s'eloignant du but (+1 en (3,2))\n",
    "- La case (3,1) vaut -1 car c'est un piege (terminal negatif)\n",
    "- Les valeurs reflètent la **distance actualisee au but** avec risque de deviation\n",
    "\n",
    "**Politique optimale π* :**\n",
    "- Toutes les fleches pointent vers le but (3,2) en evitant le piege (3,1)\n",
    "- La case (3,0) pointe vers la **gauche** (←) pour eviter de tomber dans le piege !\n",
    "- Cette politique non-intuitive illustre la puissance de l'optimisation\n",
    "\n",
    "**Pourquoi 14 iterations ?**\n",
    "- Le facteur γ = 0.9 propage les valeurs lentement\n",
    "- Les etats eloignes du but ont besoin de plusieurs iterations pour \"sentir\" la recompense\n",
    "- Critere d'arret : delta < 0.001 (precision suffisante)\n",
    "\n",
    "**Verification pratique :**\n",
    "- V(0,0) ≈ 0.30 : depuis (0,0), on peut atteindre le but avec un profit actualise de ~0.30\n",
    "- V(2,2) ≈ 0.80 : juste a cote du but, on y arrive presque certainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Iteration de Politique\n",
    "\n",
    "### Algorithme\n",
    "\n",
    "1. Initialiser π arbitrairement\n",
    "2. Repeter jusqu'a stabilite :\n",
    "   - **Evaluation** : Calculer V^π (resoudre systeme lineaire)\n",
    "   - **Amelioration** : π(s) ← argmax_a Q^π(s,a)\n",
    "\n",
    "### Compromis Value Iteration vs Policy Iteration\n",
    "\n",
    "| Critere | Value Iteration | Policy Iteration |\n",
    "|---------|-----------------|------------------|\n",
    "| **Iterations** | Beaucoup (log(1/ε)/(1-γ)) | Peu (souvent 5-10) |\n",
    "| **Cout/iteration** | O(\\|S\\|² \\|A\\|) leger | O(\\|S\\|³) evaluation exacte |\n",
    "| **Convergence** | Asymptotique (ε-optimal) | Exacte en nombre fini |\n",
    "| **Memoire** | V(s) seulement | V(s) + π(s) |\n",
    "\n",
    "### Quand utiliser chaque methode ?\n",
    "\n",
    "**Preferer Value Iteration si** :\n",
    "- Grand espace d'etats (\\|S\\| > 10,000)\n",
    "- Precision approximative suffisante\n",
    "- γ proche de 1 (convergence lente de PI)\n",
    "- Implementation simple souhaitee\n",
    "\n",
    "**Preferer Policy Iteration si** :\n",
    "- Petit/moyen espace d'etats\n",
    "- Solution exacte requise\n",
    "- Politique stable rapidement\n",
    "- Evaluation peut etre acceleree (ex: methodes matricielles)\n",
    "\n",
    "### Variantes hybrides\n",
    "\n",
    "- **Modified Policy Iteration** : Evaluation partielle (k iterations de Bellman au lieu de resolution exacte)\n",
    "- **Asynchronous VI** : Mise a jour d'un sous-ensemble d'etats a chaque iteration\n",
    "- **Prioritized Sweeping** : Mettre a jour les etats les plus \"surprenants\" en premier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Iteration de Politique ===\n",
      "\n",
      "Iteration 1 : stable = False\n",
      "Iteration 2 : stable = False\n",
      "Iteration 3 : stable = True\n",
      "\n",
      "Comparaison avec Value Iteration :\n",
      "  Politiques identiques !\n"
     ]
    }
   ],
   "source": [
    "// Iteration de Politique\n",
    "\n",
    "public class PolicyIteration\n",
    "{\n",
    "    public static (Dictionary<(int,int), double> V, Dictionary<(int,int), string> policy) \n",
    "        Solve(GridMDP mdp, int maxIter = 20)\n",
    "    {\n",
    "        var states = mdp.GetStates();\n",
    "        \n",
    "        // Initialiser politique aleatoire\n",
    "        var policy = states.ToDictionary(s => s, s => \n",
    "            mdp.TerminalStates.Contains(s) ? \"T\" : \"N\");\n",
    "        \n",
    "        var V = states.ToDictionary(s => s, s => 0.0);\n",
    "        \n",
    "        for (int iter = 0; iter < maxIter; iter++)\n",
    "        {\n",
    "            // 1. Evaluation de politique (iterative simplifiee)\n",
    "            for (int evalIter = 0; evalIter < 50; evalIter++)\n",
    "            {\n",
    "                var newV = new Dictionary<(int, int), double>(V);\n",
    "                foreach (var s in states)\n",
    "                {\n",
    "                    if (mdp.TerminalStates.Contains(s))\n",
    "                    {\n",
    "                        newV[s] = mdp.GetReward(s);\n",
    "                        continue;\n",
    "                    }\n",
    "                    \n",
    "                    string a = policy[s];\n",
    "                    double v = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        v += mdp.Gamma * prob * V[nextState];\n",
    "                    newV[s] = v;\n",
    "                }\n",
    "                V = newV;\n",
    "            }\n",
    "            \n",
    "            // 2. Amelioration de politique\n",
    "            bool stable = true;\n",
    "            foreach (var s in states)\n",
    "            {\n",
    "                if (mdp.TerminalStates.Contains(s)) continue;\n",
    "                \n",
    "                string oldAction = policy[s];\n",
    "                string bestAction = null;\n",
    "                double maxQ = double.NegativeInfinity;\n",
    "                \n",
    "                foreach (var a in mdp.Actions)\n",
    "                {\n",
    "                    double q = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        q += mdp.Gamma * prob * V[nextState];\n",
    "                    \n",
    "                    if (q > maxQ)\n",
    "                    {\n",
    "                        maxQ = q;\n",
    "                        bestAction = a;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                policy[s] = bestAction;\n",
    "                if (bestAction != oldAction) stable = false;\n",
    "            }\n",
    "            \n",
    "            Console.WriteLine($\"Iteration {iter + 1} : stable = {stable}\");\n",
    "            if (stable) break;\n",
    "        }\n",
    "        \n",
    "        return (V, policy);\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"=== Iteration de Politique ===\\n\");\n",
    "var (V_pi, policy_pi) = PolicyIteration.Solve(mdp);\n",
    "\n",
    "// Verifier que les resultats sont identiques\n",
    "Console.WriteLine(\"\\nComparaison avec Value Iteration :\");\n",
    "bool same = true;\n",
    "foreach (var s in mdp.GetStates())\n",
    "{\n",
    "    if (policy[s] != policy_pi[s])\n",
    "    {\n",
    "        same = false;\n",
    "        Console.WriteLine($\"  Difference en {s}: VI={policy[s]}, PI={policy_pi[s]}\");\n",
    "    }\n",
    "}\n",
    "if (same) Console.WriteLine(\"  Politiques identiques !\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation de l'Iteration de Politique\n",
    "\n",
    "**Convergence rapide :**\n",
    "- Seulement **3 iterations** contre 14 pour Value Iteration !\n",
    "- Chaque iteration est plus couteuse (evaluation complete), mais beaucoup moins d'iterations\n",
    "\n",
    "**Pourquoi moins d'iterations ?**\n",
    "- Policy Iteration fait des **sauts** dans l'espace des politiques\n",
    "- Value Iteration fait des **petits pas** dans l'espace des valeurs\n",
    "- La politique peut etre optimale bien avant que les valeurs convergent exactement\n",
    "\n",
    "**Verification de coherence :**\n",
    "- Les deux methodes donnent la **meme politique optimale**\n",
    "- C'est rassurant : le probleme a une solution unique\n",
    "\n",
    "**Quand utiliser chaque methode ?**\n",
    "\n",
    "| Situation | Methode recommandee |\n",
    "|-----------|---------------------|\n",
    "| Grand espace d'etats | Value Iteration (moins de memoire) |\n",
    "| Besoin de precision exacte | Policy Iteration |\n",
    "| Implementation simple | Value Iteration |\n",
    "| γ tres proche de 1 | Policy Iteration (VI converge lentement) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Alternatives : LP, Expectimax, RTDP\n",
    "\n",
    "### Programmation Lineaire (LP)\n",
    "\n",
    "Le MDP peut etre formule comme un programme lineaire :\n",
    "\n",
    "- **Variables** : V(s) pour chaque etat\n",
    "- **Objectif** : min Σ_s V(s)\n",
    "- **Contraintes** : V(s) ≥ R(s,a) + γ Σ_s' P(s'|s,a) V(s') pour tout a\n",
    "\n",
    "### Expectimax\n",
    "\n",
    "Pour les MDPs a horizon fini, on peut utiliser un arbre de recherche :\n",
    "\n",
    "```\n",
    "       (s0)\n",
    "      / | \\\n",
    "   a1  a2  a3     <- Noeuds max (choix agent)\n",
    "   / \\ \n",
    " s1  s2           <- Noeuds chance (transition)\n",
    "```\n",
    "\n",
    "### RTDP (Real-Time Dynamic Programming)\n",
    "\n",
    "- Mise a jour le long de trajectoires simulees\n",
    "- Focus sur les etats atteignables\n",
    "- Algorithme **anytime** : ameliore avec plus de temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RTDP (100 trials depuis (0,0)) ===\n",
      "\n",
      "Comparaison V_RTDP vs V_VI :\n",
      "  Etat   |  V_RTDP  |   V_VI   |   Diff\n",
      "---------|----------|----------|--------\n",
      " (0,0)   |   0,1666 |   0,2960 | 0,1294\n",
      " (0,2)   |  -0,1123 |   0,5094 | 0,6217\n",
      " (2,2)   |   0,7954 |   0,7954 | 0,0000\n",
      " (3,2)   |   1,0000 |   1,0000 | 0,0000\n"
     ]
    }
   ],
   "source": [
    "// RTDP simplifie\n",
    "\n",
    "public class RTDP\n",
    "{\n",
    "    public static Dictionary<(int,int), double> Solve(\n",
    "        GridMDP mdp, (int, int) startState, int nTrials = 100)\n",
    "    {\n",
    "        var states = mdp.GetStates();\n",
    "        var V = states.ToDictionary(s => s, s => 0.0);\n",
    "        var rng = new Random(42);\n",
    "        \n",
    "        for (int trial = 0; trial < nTrials; trial++)\n",
    "        {\n",
    "            var s = startState;\n",
    "            int steps = 0;\n",
    "            \n",
    "            while (!mdp.TerminalStates.Contains(s) && steps < 100)\n",
    "            {\n",
    "                // Mise a jour de Bellman\n",
    "                double maxQ = double.NegativeInfinity;\n",
    "                string bestAction = null;\n",
    "                \n",
    "                foreach (var a in mdp.Actions)\n",
    "                {\n",
    "                    double q = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        q += mdp.Gamma * prob * V[nextState];\n",
    "                    \n",
    "                    if (q > maxQ)\n",
    "                    {\n",
    "                        maxQ = q;\n",
    "                        bestAction = a;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                V[s] = maxQ;\n",
    "                \n",
    "                // Simuler transition\n",
    "                var transitions = mdp.GetTransitions(s, bestAction);\n",
    "                double r = rng.NextDouble();\n",
    "                double cumProb = 0;\n",
    "                foreach (var (nextState, prob) in transitions)\n",
    "                {\n",
    "                    cumProb += prob;\n",
    "                    if (r <= cumProb)\n",
    "                    {\n",
    "                        s = nextState;\n",
    "                        break;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                steps++;\n",
    "            }\n",
    "            \n",
    "            // Update terminal state\n",
    "            if (mdp.TerminalStates.Contains(s))\n",
    "                V[s] = mdp.GetReward(s);\n",
    "        }\n",
    "        \n",
    "        return V;\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"=== RTDP (100 trials depuis (0,0)) ===\\n\");\n",
    "var V_rtdp = RTDP.Solve(mdp, (0, 0), nTrials: 100);\n",
    "\n",
    "// Comparer avec Value Iteration\n",
    "Console.WriteLine(\"Comparaison V_RTDP vs V_VI :\");\n",
    "Console.WriteLine(\"  Etat   |  V_RTDP  |   V_VI   |   Diff\");\n",
    "Console.WriteLine(\"---------|----------|----------|--------\");\n",
    "foreach (var s in new[] { (0,0), (0,2), (2,2), (3,2) })\n",
    "{\n",
    "    double diff = Math.Abs(V_rtdp[s] - V[s]);\n",
    "    Console.WriteLine($\" ({s.Item1},{s.Item2})   | {V_rtdp[s],8:F4} | {V[s],8:F4} | {diff,6:F4}\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Interpretation de RTDP (Real-Time Dynamic Programming)\n\n**Comparaison RTDP vs Value Iteration :**\n\n| Etat | V_RTDP | V_VI | Ecart | Commentaire |\n|------|--------|------|-------|-------------|\n| (0,0) | 0.17 | 0.30 | 0.13 | Sous-estime (visite moderee) |\n| (0,2) | -0.11 | 0.51 | 0.62 | Non visite depuis (0,0) |\n| (2,2) | 0.80 | 0.80 | 0.00 | Exact (sur le chemin optimal) |\n| (3,2) | 1.00 | 1.00 | 0.00 | Terminal (toujours exact) |\n\n**Pourquoi ces ecarts ?**\n\nRTDP est un algorithme **anytime** qui :\n1. Ne met a jour que les etats **visites** par simulation\n2. Converge vers V* sur les **etats atteignables** depuis l'etat de depart\n3. Peut ignorer certains etats non pertinents pour la politique courante\n\n**Etat (0,2) tres sous-estime :**\n- Depuis (0,0), la politique optimale va vers l'est, pas vers le nord\n- L'etat (0,2) n'est jamais visite dans les simulations\n- Sa valeur reste proche de l'initialisation (ici negative a cause de mauvaises transitions simulees)\n\n> **Note technique** : RTDP garantit la convergence vers V* uniquement sur les etats visites infiniment souvent. Pour une convergence globale, on utilise des variantes comme LRTDP (Labeled RTDP) qui marquent les etats \"resolus\".\n\n**Quand utiliser RTDP ?**\n\n| Situation | Recommandation |\n|-----------|----------------|\n| Grands espaces d'etats | RTDP (focus sur etats pertinents) |\n| Solution exacte requise | Value Iteration classique |\n| Temps de calcul limite | RTDP (anytime) |\n| Tous les etats importants | Value/Policy Iteration |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reward Shaping\n",
    "\n",
    "### Le probleme des recompenses sparses\n",
    "\n",
    "Quand les recompenses sont rares (ex: +1 seulement au but), l'apprentissage est tres lent.\n",
    "\n",
    "### Solution : Ajouter une recompense de faconnage\n",
    "\n",
    "$$R'(s, a, s') = R(s, a, s') + F(s, a, s')$$\n",
    "\n",
    "### Theoreme de preservation de politique (Ng et al., 1999)\n",
    "\n",
    "> Si F a la forme d'une **fonction de potentiel** :\n",
    "> \n",
    "> $$F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)$$\n",
    "> \n",
    "> Alors **la politique optimale est preservee** !\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- Φ(s) represente \"a quel point s est proche du but\"\n",
    "- F recompense les transitions vers des etats meilleurs\n",
    "- La forme specifique garantit que les raccourcis ne sont pas crees"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Synthese : Comparaison des methodes de resolution MDP\n\nAvant de passer aux bandits, recapitulons les methodes vues pour resoudre les MDPs :\n\n| Methode | Principe | Complexite/iter | Convergence | Usage typique |\n|---------|----------|-----------------|-------------|---------------|\n| **Value Iteration** | Bellman updates iteratifs | O(\\|S\\|^2 \\|A\\|) | Asymptotique | Standard, simple |\n| **Policy Iteration** | Evaluation + Amelioration | O(\\|S\\|^3) | Exacte, finie | Petits MDPs |\n| **LP** | Programmation lineaire | Polynomial | Exacte | Structure particuliere |\n| **RTDP** | Updates sur trajectoires | Variable | Sur etats visites | Grands espaces |\n| **Expectimax** | Arbre de recherche | Exponentiel | Exacte (horizon fini) | Jeux, planification |\n\n**Arbre de decision pour choisir une methode :**\n\n```\nMDP a resoudre\n    |\n    +-- Horizon fini ?\n    |       |\n    |       +-- Oui --> Expectimax ou backward induction\n    |       +-- Non --> Methodes iteratives\n    |\n    +-- Grand espace d'etats (\\|S\\| > 10,000) ?\n    |       |\n    |       +-- Oui --> RTDP, LRTDP, ou approximation\n    |       +-- Non --> VI ou PI\n    |\n    +-- Besoin de precision exacte ?\n            |\n            +-- Oui --> Policy Iteration ou LP\n            +-- Non --> Value Iteration (plus rapide par iteration)\n```\n\n> **Note** : En pratique moderne, les grands MDPs sont souvent resolus par **Deep Reinforcement Learning** (DQN, PPO, etc.) qui approximent V ou Q avec des reseaux de neurones. Voir la serie `RL/`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reward Shaping avec Fonction de Potentiel ===\n",
      "\n",
      "But en (3, 2)\n",
      "Phi(s) = -distance(s, but)\n",
      "\n",
      "Fonction de potentiel Phi(s) :\n",
      "  -3,00   -2,00   -1,00   -0,00 \n",
      "  -3,16   ####    -1,41   -1,00 \n",
      "  -3,61   -2,83   -2,24   -2,00 \n",
      "\n",
      "Exemples de shaping reward F(s, a, s') = gamma*Phi(s') - Phi(s) :\n",
      "  F((0,0) -> (1,0)) = 1,060 (vers le but)\n",
      "  F((1,0) -> (0,0)) = -0,417 (loin du but)\n",
      "  F((2,2) -> (3,2)) = 1,000 (atteindre le but)\n",
      "\n",
      "=> Le shaping recompense les mouvements vers le but,\n",
      "   mais le theoreme garantit que la politique optimale est preservee.\n"
     ]
    }
   ],
   "source": [
    "// Demonstration du Reward Shaping\n",
    "\n",
    "public class ShapedGridMDP : GridMDP\n",
    "{\n",
    "    private Func<(int, int), double> _potential;\n",
    "    private double _gamma;\n",
    "    \n",
    "    public ShapedGridMDP(int width, int height, double gamma, Func<(int, int), double> potential) \n",
    "        : base(width, height, gamma)\n",
    "    {\n",
    "        _potential = potential;\n",
    "        _gamma = gamma;\n",
    "    }\n",
    "    \n",
    "    public double GetShapedReward((int, int) s, (int, int) sPrime)\n",
    "    {\n",
    "        double R = GetReward(s);\n",
    "        double F = _gamma * _potential(sPrime) - _potential(s);\n",
    "        return R + F;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Fonction de potentiel : distance negative au but\n",
    "(int, int) goal = (3, 2);\n",
    "Func<(int, int), double> Phi = s => -Math.Sqrt(Math.Pow(s.Item1 - goal.Item1, 2) + Math.Pow(s.Item2 - goal.Item2, 2));\n",
    "\n",
    "Console.WriteLine(\"=== Reward Shaping avec Fonction de Potentiel ===\\n\");\n",
    "Console.WriteLine($\"But en {goal}\");\n",
    "Console.WriteLine(\"Phi(s) = -distance(s, but)\\n\");\n",
    "\n",
    "Console.WriteLine(\"Fonction de potentiel Phi(s) :\");\n",
    "for (int y = 2; y >= 0; y--)\n",
    "{\n",
    "    for (int x = 0; x < 4; x++)\n",
    "    {\n",
    "        if (mdp.Walls.Contains((x, y)))\n",
    "            Console.Write(\"  ####  \");\n",
    "        else\n",
    "            Console.Write($\" {Phi((x, y)),6:F2} \");\n",
    "    }\n",
    "    Console.WriteLine();\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"\\nExemples de shaping reward F(s, a, s') = gamma*Phi(s') - Phi(s) :\");\n",
    "Console.WriteLine($\"  F((0,0) -> (1,0)) = {0.9 * Phi((1, 0)) - Phi((0, 0)):F3} (vers le but)\");\n",
    "Console.WriteLine($\"  F((1,0) -> (0,0)) = {0.9 * Phi((0, 0)) - Phi((1, 0)):F3} (loin du but)\");\n",
    "Console.WriteLine($\"  F((2,2) -> (3,2)) = {0.9 * Phi((3, 2)) - Phi((2, 2)):F3} (atteindre le but)\");\n",
    "\n",
    "Console.WriteLine();\n",
    "Console.WriteLine(\"=> Le shaping recompense les mouvements vers le but,\");\n",
    "Console.WriteLine(\"   mais le theoreme garantit que la politique optimale est preservee.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bandits Multi-Bras\n",
    "\n",
    "### Le probleme\n",
    "\n",
    "Vous avez K machines a sous (\"bras\"). Chaque bras i donne une recompense selon une distribution inconnue.\n",
    "\n",
    "**Dilemme exploration/exploitation** :\n",
    "- Explorer : essayer de nouveaux bras pour estimer leurs distributions\n",
    "- Exploiter : jouer le meilleur bras connu\n",
    "\n",
    "### Approches classiques\n",
    "\n",
    "| Methode | Description |\n",
    "|---------|-------------|\n",
    "| ε-greedy | Exploiter avec proba 1-ε, explorer avec ε |\n",
    "| UCB | Upper Confidence Bound : optimisme face a l'incertitude |\n",
    "| Thompson | Echantillonner selon la posterior des recompenses |\n",
    "| **Gittins** | Solution optimale pour bandits avec discount |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Calcul pratique de l'indice de Gittins\n\nL'indice de Gittins est **difficile a calculer** exactement car il necessite de resoudre un probleme d'arret optimal pour chaque etat possible du bras.\n\n**Methodes de calcul :**\n\n| Methode | Complexite | Precision |\n|---------|------------|-----------|\n| Calcul exact (DP) | Exponentielle en horizon | Exacte |\n| Approximation restart | Polynomiale | Borne inferieure |\n| Interpolation tabulee | O(1) lookup | Pre-calcule |\n\n**Cas particulier : Beta-Bernoulli**\n\nPour un bras avec prior Beta(α, β) et recompenses Bernoulli, l'indice peut etre pre-calcule et stocke dans une table.\n\n> **En pratique** : L'indice de Gittins est rarement utilise car UCB et Thompson Sampling sont plus simples a implementer et ont des garanties theoriques similaires pour de nombreux problemes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit avec 4 bras, moyennes vraies inconnues\n",
      "Meilleure moyenne : 0,7\n",
      "\n",
      "ε-greedy (ε=0,1):\n",
      "  Recompense totale : 678,8\n",
      "  Regret cumule : 21,2\n",
      "  Tirages par bras : 23, 93, 853, 31\n",
      "\n",
      "UCB1:\n",
      "  Recompense totale : 616,1\n",
      "  Regret cumule : 83,9\n",
      "  Tirages par bras : 54, 166, 719, 61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Bandit multi-bras avec differentes strategies\n",
    "\n",
    "public class MultiArmedBandit\n",
    "{\n",
    "    private double[] _trueMeans;\n",
    "    private Random _rng;\n",
    "    \n",
    "    public int K { get; }\n",
    "    \n",
    "    public MultiArmedBandit(double[] means, int seed = 42)\n",
    "    {\n",
    "        _trueMeans = means;\n",
    "        K = means.Length;\n",
    "        _rng = new Random(seed);\n",
    "    }\n",
    "    \n",
    "    public double Pull(int arm)\n",
    "    {\n",
    "        // Recompense gaussienne autour de la moyenne vraie\n",
    "        double u1 = _rng.NextDouble();\n",
    "        double u2 = _rng.NextDouble();\n",
    "        double z = Math.Sqrt(-2 * Math.Log(u1)) * Math.Cos(2 * Math.PI * u2);\n",
    "        return _trueMeans[arm] + 0.5 * z;\n",
    "    }\n",
    "    \n",
    "    public double OptimalMean => _trueMeans.Max();\n",
    "}\n",
    "\n",
    "// Strategies\n",
    "public interface IBanditStrategy\n",
    "{\n",
    "    int SelectArm(int[] counts, double[] sumRewards);\n",
    "    string Name { get; }\n",
    "}\n",
    "\n",
    "public class EpsilonGreedy : IBanditStrategy\n",
    "{\n",
    "    private double _epsilon;\n",
    "    private Random _rng = new Random();\n",
    "    public string Name => $\"ε-greedy (ε={_epsilon})\";\n",
    "    \n",
    "    public EpsilonGreedy(double epsilon) { _epsilon = epsilon; }\n",
    "    \n",
    "    public int SelectArm(int[] counts, double[] sumRewards)\n",
    "    {\n",
    "        if (_rng.NextDouble() < _epsilon)\n",
    "            return _rng.Next(counts.Length);\n",
    "        \n",
    "        int best = 0;\n",
    "        double bestMean = double.NegativeInfinity;\n",
    "        for (int i = 0; i < counts.Length; i++)\n",
    "        {\n",
    "            double mean = counts[i] > 0 ? sumRewards[i] / counts[i] : 0;\n",
    "            if (mean > bestMean) { bestMean = mean; best = i; }\n",
    "        }\n",
    "        return best;\n",
    "    }\n",
    "}\n",
    "\n",
    "public class UCB : IBanditStrategy\n",
    "{\n",
    "    public string Name => \"UCB1\";\n",
    "    \n",
    "    public int SelectArm(int[] counts, double[] sumRewards)\n",
    "    {\n",
    "        int totalPulls = counts.Sum();\n",
    "        if (totalPulls < counts.Length)\n",
    "            return totalPulls; // Essayer chaque bras une fois\n",
    "        \n",
    "        int best = 0;\n",
    "        double bestUCB = double.NegativeInfinity;\n",
    "        for (int i = 0; i < counts.Length; i++)\n",
    "        {\n",
    "            double mean = sumRewards[i] / counts[i];\n",
    "            double bonus = Math.Sqrt(2 * Math.Log(totalPulls) / counts[i]);\n",
    "            double ucb = mean + bonus;\n",
    "            if (ucb > bestUCB) { bestUCB = ucb; best = i; }\n",
    "        }\n",
    "        return best;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Simulation\n",
    "var bandit = new MultiArmedBandit(new[] { 0.3, 0.5, 0.7, 0.4 });\n",
    "var strategies = new IBanditStrategy[] { new EpsilonGreedy(0.1), new UCB() };\n",
    "\n",
    "Console.WriteLine($\"Bandit avec {bandit.K} bras, moyennes vraies inconnues\");\n",
    "Console.WriteLine($\"Meilleure moyenne : {bandit.OptimalMean}\\n\");\n",
    "\n",
    "int T = 1000;\n",
    "\n",
    "foreach (var strategy in strategies)\n",
    "{\n",
    "    var counts = new int[bandit.K];\n",
    "    var sumRewards = new double[bandit.K];\n",
    "    double totalReward = 0;\n",
    "    double totalRegret = 0;\n",
    "    \n",
    "    for (int t = 0; t < T; t++)\n",
    "    {\n",
    "        int arm = strategy.SelectArm(counts, sumRewards);\n",
    "        double reward = bandit.Pull(arm);\n",
    "        \n",
    "        counts[arm]++;\n",
    "        sumRewards[arm] += reward;\n",
    "        totalReward += reward;\n",
    "        totalRegret += bandit.OptimalMean - reward;\n",
    "    }\n",
    "    \n",
    "    Console.WriteLine($\"{strategy.Name}:\");\n",
    "    Console.WriteLine($\"  Recompense totale : {totalReward:F1}\");\n",
    "    Console.WriteLine($\"  Regret cumule : {totalRegret:F1}\");\n",
    "    Console.WriteLine($\"  Tirages par bras : {string.Join(\", \", counts)}\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation des strategies de bandits\n",
    "\n",
    "**Moyennes vraies (inconnues de l'agent) :** Bras 1=0.3, Bras 2=0.5, **Bras 3=0.7**, Bras 4=0.4\n",
    "\n",
    "**Comparaison des strategies :**\n",
    "\n",
    "| Strategie | Recompense | Regret | Tirages du meilleur bras |\n",
    "|-----------|------------|--------|--------------------------|\n",
    "| ε-greedy (ε=0.1) | 686 | 14 | 884/1000 (88%) |\n",
    "| UCB1 | 616 | 84 | 719/1000 (72%) |\n",
    "\n",
    "**Analyse :**\n",
    "- **ε-greedy** a mieux performe ici car il exploite plus (90% du temps)\n",
    "- **UCB** explore plus systematiquement (tous les bras 54-166 fois)\n",
    "- Le regret cumule est plus faible pour ε-greedy dans ce cas simple\n",
    "\n",
    "**Pourquoi UCB explore-t-il plus ?**\n",
    "- UCB ajoute un **bonus d'incertitude** aux bras peu explores\n",
    "- Un bras tire 54 fois a un gros bonus, meme si sa moyenne est faible\n",
    "- Cette exploration est **garantie optimale asymptotiquement**\n",
    "\n",
    "**Lecon pratique :**\n",
    "- ε-greedy est simple et souvent suffisant pour des problemes simples\n",
    "- UCB brille quand les differences entre bras sont subtiles\n",
    "- Le choix depend du compromis exploration/exploitation souhaite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Indice de Gittins\n",
    "\n",
    "### Le Theoreme Fondamental (Gittins, 1979)\n",
    "\n",
    "L'indice de Gittins est l'un des resultats les plus elegants de la theorie de la decision. Il resout le probleme du bandit multi-bras de maniere **optimale** et **decomposable**.\n",
    "\n",
    "> **Theoreme** : Pour un bandit multi-bras avec facteur de discount γ, la strategie optimale est de **toujours jouer le bras avec l'indice de Gittins le plus eleve**.\n",
    "\n",
    "### Pourquoi est-ce remarquable ?\n",
    "\n",
    "1. **Reduction de complexite** : Un probleme a K bras interdependants devient K problemes independants\n",
    "2. **Optimalite garantie** : Pas une heuristique, c'est la solution exacte\n",
    "3. **Index-ability** : Chaque bras a un \"prix\" intrinseque\n",
    "\n",
    "### Definition intuitive\n",
    "\n",
    "L'indice de Gittins d'un bras represente :\n",
    "\n",
    "> \"Le prix equivalent certain\" de ce bras - la recompense garantie pour laquelle on serait indifferent entre jouer ce bras et recevoir cette recompense fixe.\n",
    "\n",
    "Plus formellement : c'est le taux d'interet critique au-dessus duquel on prefererait \"encaisser\" plutot que jouer le bras.\n",
    "\n",
    "### Calcul formel\n",
    "\n",
    "L'indice est la solution d'un probleme d'arret optimal :\n",
    "\n",
    "$$G(s) = \\sup_{\\tau \\geq 1} \\frac{E\\left[\\sum_{t=0}^{\\tau-1} \\gamma^t R_t \\mid s_0 = s\\right]}{E\\left[\\sum_{t=0}^{\\tau-1} \\gamma^t \\mid s_0 = s\\right]}$$\n",
    "\n",
    "- Le numerateur est la recompense totale actualisee jusqu'au temps d'arret τ\n",
    "- Le denominateur est le \"temps effectif\" actualise\n",
    "- On maximise sur tous les temps d'arret possibles\n",
    "\n",
    "### Implications pratiques\n",
    "\n",
    "| Aspect | Consequence |\n",
    "|--------|-------------|\n",
    "| **Exploration** | Un bras incertain a un indice eleve (optimisme) |\n",
    "| **Exploitation** | Un bras connu avec haute moyenne a un indice eleve |\n",
    "| **Compromis** | L'indice equilibre automatiquement exploration/exploitation |\n",
    "\n",
    "### Limitation\n",
    "\n",
    "Le theoreme de Gittins s'applique sous des hypotheses specifiques :\n",
    "- Discount γ < 1 (pas pour horizon fini sans discount)\n",
    "- Les bras sont independants (pas d'interactions)\n",
    "- Une seule action par etape (pas de contraintes de ressources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. POMDPs : MDPs Partiellement Observables\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Dans un MDP standard, l'agent connait l'etat exact du monde a chaque instant. En pratique, c'est souvent **irrealiste** :\n",
    "\n",
    "- Un robot ne voit pas a travers les murs\n",
    "- Un medecin ne connait pas la vraie maladie, seulement les symptomes\n",
    "- Un joueur de poker ne voit pas les cartes adverses\n",
    "\n",
    "### Extension du MDP\n",
    "\n",
    "Dans un POMDP, l'agent **ne connait pas l'etat exact**. Il recoit des **observations** qui sont des indicateurs bruitees de l'etat reel.\n",
    "\n",
    "### Definition formelle\n",
    "\n",
    "Un POMDP est un tuple (S, A, P, R, O, Ω, γ) :\n",
    "\n",
    "| Element | Description |\n",
    "|---------|-------------|\n",
    "| S, A, P, R, γ | Comme MDP (etats, actions, transitions, recompenses, discount) |\n",
    "| **O** | Ensemble des observations possibles |\n",
    "| **Ω(o\\|s', a)** | Modele de capteur : P(observation \\| nouvel etat, action) |\n",
    "\n",
    "### Belief State : La Cle des POMDPs\n",
    "\n",
    "Puisque l'agent ne connait pas l'etat, il doit maintenir une **distribution de croyance** (belief) b(s) sur tous les etats possibles.\n",
    "\n",
    "> b(s) = P(etat = s | historique des observations et actions)\n",
    "\n",
    "Le belief state est une **statistique suffisante** : il resume toute l'information pertinente de l'historique.\n",
    "\n",
    "### Mise a jour du Belief State\n",
    "\n",
    "Apres avoir fait action a et observe o, le nouveau belief b' est :\n",
    "\n",
    "$$b'(s') = \\eta \\cdot \\Omega(o|s', a) \\sum_s P(s'|s, a) b(s)$$\n",
    "\n",
    "ou η est une constante de normalisation.\n",
    "\n",
    "C'est exactement une **mise a jour bayesienne** ! Le POMDP est donc naturellement lie a l'inference probabiliste.\n",
    "\n",
    "### Plans conditionnels\n",
    "\n",
    "La politique d'un POMDP n'est pas s → a comme dans un MDP, mais b → a ou equivalemment un **arbre de decision** :\n",
    "\n",
    "```\n",
    "        a1           <- Action initiale\n",
    "       /  \\\n",
    "     o1    o2        <- Observations possibles\n",
    "     |      |\n",
    "    a2     a3        <- Actions conditionnelles\n",
    "   / \\    / \\\n",
    "  o1 o2  o1 o2\n",
    "  ...\n",
    "```\n",
    "\n",
    "### Complexite\n",
    "\n",
    "Les POMDPs sont **PSPACE-complete** a resoudre exactement. En pratique, on utilise :\n",
    "- Point-Based Value Iteration (PBVI)\n",
    "- SARSOP\n",
    "- Monte Carlo Tree Search (MCTS)\n",
    "- Ou on approxime par des MDP avec belief states discrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POMDP : Probleme du Tigre ===\n",
      "\n",
      "Deux portes : gauche (G) et droite (D)\n",
      "Un tigre est cache derriere l'une des portes.\n",
      "Actions : Ouvrir G, Ouvrir D, Ecouter\n",
      "\n",
      "Belief initial : P(tigre gauche) = 50 %\n",
      "\n",
      "Utilites esperees :\n",
      "  E[U(ouvrir gauche) | b=50 %] = -45,0\n",
      "  E[U(ouvrir droite) | b=50 %] = -45,0\n",
      "  Ecouter : cout immediat = -1, mais reduit l'incertitude\n",
      "\n",
      "Simulation : 3 ecoutes donnent 'bruit gauche'\n",
      "\n",
      "Apres observation 1 : P(tigre gauche) = 85,0 %\n",
      "Apres observation 2 : P(tigre gauche) = 97,0 %\n",
      "Apres observation 3 : P(tigre gauche) = 99,5 %\n",
      "\n",
      "E[U(ouvrir gauche)] = -99,4\n",
      "E[U(ouvrir droite)] = 9,4\n",
      "\n",
      "=> Decision : OUVRIR DROITE (tigre probablement a gauche)\n"
     ]
    }
   ],
   "source": [
    "// POMDP simple : Tigre derriere une porte\n",
    "\n",
    "Console.WriteLine(\"=== POMDP : Probleme du Tigre ===\\n\");\n",
    "Console.WriteLine(\"Deux portes : gauche (G) et droite (D)\");\n",
    "Console.WriteLine(\"Un tigre est cache derriere l'une des portes.\");\n",
    "Console.WriteLine(\"Actions : Ouvrir G, Ouvrir D, Ecouter\\n\");\n",
    "\n",
    "// Etats : tigre gauche (TG), tigre droite (TD)\n",
    "// Actions : ouvrir_gauche, ouvrir_droite, ecouter\n",
    "// Observations (si ecouter) : bruit_gauche, bruit_droite\n",
    "\n",
    "double pCorrectHearing = 0.85; // P(bruit_gauche | tigre_gauche)\n",
    "double rewardTresor = 10;\n",
    "double rewardTigre = -100;\n",
    "double costEcoute = -1;\n",
    "\n",
    "// Belief state : b = P(tigre_gauche)\n",
    "double b = 0.5; // Prior uniforme\n",
    "\n",
    "Console.WriteLine($\"Belief initial : P(tigre gauche) = {b:P0}\\n\");\n",
    "\n",
    "// Fonction de valeur pour chaque action\n",
    "double EU_ouvrirGauche(double belief) => \n",
    "    belief * rewardTigre + (1 - belief) * rewardTresor;\n",
    "\n",
    "double EU_ouvrirDroite(double belief) => \n",
    "    belief * rewardTresor + (1 - belief) * rewardTigre;\n",
    "\n",
    "// Pour ecouter, on doit calculer la valeur esperee apres observation\n",
    "// (simplifie ici)\n",
    "\n",
    "Console.WriteLine(\"Utilites esperees :\");\n",
    "Console.WriteLine($\"  E[U(ouvrir gauche) | b={b:P0}] = {EU_ouvrirGauche(b):F1}\");\n",
    "Console.WriteLine($\"  E[U(ouvrir droite) | b={b:P0}] = {EU_ouvrirDroite(b):F1}\");\n",
    "Console.WriteLine($\"  Ecouter : cout immediat = {costEcoute}, mais reduit l'incertitude\\n\");\n",
    "\n",
    "// Simuler une sequence d'observations\n",
    "Console.WriteLine(\"Simulation : 3 ecoutes donnent 'bruit gauche'\\n\");\n",
    "\n",
    "for (int i = 0; i < 3; i++)\n",
    "{\n",
    "    // Mise a jour bayesienne apres observation \"bruit gauche\"\n",
    "    // P(TG|bruit_g) = P(bruit_g|TG) * P(TG) / P(bruit_g)\n",
    "    double pBruitGauche = b * pCorrectHearing + (1 - b) * (1 - pCorrectHearing);\n",
    "    b = (pCorrectHearing * b) / pBruitGauche;\n",
    "    \n",
    "    Console.WriteLine($\"Apres observation {i+1} : P(tigre gauche) = {b:P1}\");\n",
    "}\n",
    "\n",
    "Console.WriteLine();\n",
    "Console.WriteLine($\"E[U(ouvrir gauche)] = {EU_ouvrirGauche(b):F1}\");\n",
    "Console.WriteLine($\"E[U(ouvrir droite)] = {EU_ouvrirDroite(b):F1}\");\n",
    "Console.WriteLine();\n",
    "Console.WriteLine($\"=> Decision : OUVRIR DROITE (tigre probablement a gauche)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation du probleme du tigre (POMDP)\n",
    "\n",
    "**Evolution du belief state :**\n",
    "\n",
    "| Etape | P(tigre gauche) | Decision optimale |\n",
    "|-------|-----------------|-------------------|\n",
    "| Initial | 50% | Indecis (EU = -45 pour les deux portes) |\n",
    "| Apres ecoute 1 | 85% | Encore incertain |\n",
    "| Apres ecoute 2 | 97% | Tres confiant |\n",
    "| Apres ecoute 3 | 99.5% | Quasi-certain |\n",
    "\n",
    "**Pourquoi ecouter plusieurs fois ?**\n",
    "- Chaque ecoute coute -1 mais reduit l'incertitude\n",
    "- Apres 3 ecoutes, EU(ouvrir droite) = +9.4 >> EU(ouvrir gauche) = -99.4\n",
    "- Le cout total d'ecoute (3 x -1 = -3) est tres inferieur au gain de certitude\n",
    "\n",
    "**Mise a jour bayesienne :**\n",
    "- P(tigre gauche | bruit gauche) = P(bruit gauche | tigre gauche) x P(tigre gauche) / P(bruit gauche)\n",
    "- Avec P(bruit correct | position) = 85%, chaque observation \"pousse\" le belief\n",
    "\n",
    "**Point cle POMDP :**\n",
    "- L'agent ne connait pas l'etat exact (position du tigre)\n",
    "- Il maintient une **distribution de croyance** (belief)\n",
    "- La decision optimale depend du belief, pas de l'etat reel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10bis. Belief State Updates avec Infer.NET\n",
    "\n",
    "L'exemple precedent calculait les mises a jour du belief \"a la main\". Avec Infer.NET, on peut automatiser cette inference bayesienne, ce qui devient precieux pour des modeles plus complexes.\n",
    "\n",
    "### Avantages d'Infer.NET pour les POMDPs\n",
    "\n",
    "| Aspect | Calcul manuel | Infer.NET |\n",
    "|--------|---------------|-----------|\n",
    "| **Formule** | Ecrire Bayes explicitement | Automatique |\n",
    "| **Etats multiples** | Combinatoire | Gere par le moteur |\n",
    "| **Observations multiples** | Produits manuels | Modelisation naturelle |\n",
    "| **Incertitude sur parametres** | Tres complexe | Hyper-priors possibles |\n",
    "\n",
    "### Scenario : Maintenance predictive\n",
    "\n",
    "Un systeme peut etre dans 3 etats : {Bon, Degrade, Defaillant}.\n",
    "A chaque pas de temps :\n",
    "- L'etat peut se degrader (Markov)\n",
    "- On observe un signal de capteur (bruite)\n",
    "- On decide : continuer, maintenance preventive, ou remplacement"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Vision d'ensemble : De la decision one-shot aux systemes adaptatifs\n\nCe notebook conclut la serie Decision Theory en faisant le pont vers le **Reinforcement Learning** :\n\n```\nDecisions One-Shot (Notebooks 14-19)\n    |\n    +-- Utilite et aversion au risque (14-15)\n    +-- Attributs multiples (16)\n    +-- Diagrammes d'influence (17)\n    +-- Valeur de l'information (18)\n    +-- Robustesse et experts (19)\n    |\n    v\nDecisions Sequentielles (Notebook 20)\n    |\n    +-- MDPs : Etats connus, actions multiples\n    +-- Bellman : Decomposition recursive\n    +-- Bandits : Exploration/exploitation\n    +-- POMDPs : Etats partiellement observables\n    |\n    v\nReinforcement Learning (Serie RL/)\n    |\n    +-- Q-Learning : Apprendre sans modele\n    +-- Deep RL : Approximation neuronale\n    +-- Policy Gradient : Optimisation directe\n```\n\n**Message cle :**\n> La theorie de la decision fournit les **fondements mathematiques** (utilite, rationalite, Bellman) sur lesquels reposent les algorithmes modernes de RL. Comprendre ces concepts permet de mieux concevoir, debugger et ameliorer les systemes d'apprentissage.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Belief State Updates avec Infer.NET ===\n",
      "\n",
      "Scenario : Maintenance predictive d'un systeme\n",
      "\n",
      "Belief initial :\n",
      "  P(Bon) = 95,0 %\n",
      "  P(Degrade) = 4,0 %\n",
      "  P(Defaillant) = 1,0 %\n",
      "\n",
      "=== Pas de temps 1 ===\n",
      "Apres prediction (transition) :\n",
      "  P(Bon) = 85,5 %\n",
      "  P(Degrade) = 11,0 %\n",
      "  P(Defaillant) = 3,5 %\n",
      "Compiling model...done.\n",
      "Observation : Normal\n",
      "Apres correction (Bayes via Infer.NET) :\n",
      "  P(Bon       ) = 95,9 % ############################\n",
      "  P(Degrade   ) = 3,9 % #\n",
      "  P(Defaillant) = 0,2 % \n",
      "Utilites esperees :\n",
      "  E[U(Continuer   )] =    96,8\n",
      "  E[U(Maintenance )] =   -16,2\n",
      "  E[U(Remplacer   )] =   -99,7\n",
      "=> Decision : Continuer\n",
      "\n",
      "=== Pas de temps 2 ===\n",
      "Apres prediction (transition) :\n",
      "  P(Bon) = 86,3 %\n",
      "  P(Degrade) = 11,0 %\n",
      "  P(Defaillant) = 2,7 %\n",
      "Compiling model...done.\n",
      "Observation : Anormal\n",
      "Apres correction (Bayes via Infer.NET) :\n",
      "  P(Bon       ) = 29,6 % ########\n",
      "  P(Degrade   ) = 52,7 % ###############\n",
      "  P(Defaillant) = 17,7 % #####\n",
      "Utilites esperees :\n",
      "  E[U(Continuer   )] =   -32,3\n",
      "  E[U(Maintenance )] =    27,4\n",
      "  E[U(Remplacer   )] =   -73,5\n",
      "=> Decision : Maintenance\n",
      "\n",
      "=== Pas de temps 3 ===\n",
      "Apres prediction (transition) :\n",
      "  P(Bon) = 26,6 %\n",
      "  P(Degrade) = 47,2 %\n",
      "  P(Defaillant) = 26,2 %\n",
      "Compiling model...done.\n",
      "Observation : Anormal\n",
      "Apres correction (Bayes via Infer.NET) :\n",
      "  P(Bon       ) = 2,2 % \n",
      "  P(Degrade   ) = 55,8 % ################\n",
      "  P(Defaillant) = 42,0 % ############\n",
      "Utilites esperees :\n",
      "  E[U(Continuer   )] =  -179,7\n",
      "  E[U(Maintenance )] =    23,2\n",
      "  E[U(Remplacer   )] =   -37,1\n",
      "=> Decision : Maintenance\n",
      "\n",
      "=== Resume ===\n",
      "Les observations 'Anormal' successives ont fait augmenter P(Degrade),\n",
      "declenchant le passage de 'Continuer' a 'Maintenance' ou 'Remplacer'.\n",
      "\n",
      "C'est le principe de la maintenance predictive bayesienne !\n"
     ]
    }
   ],
   "source": [
    "// Belief State Updates avec Infer.NET : Maintenance Predictive\n",
    "\n",
    "using Microsoft.ML.Probabilistic.Math;\n",
    "\n",
    "Console.WriteLine(\"=== Belief State Updates avec Infer.NET ===\\n\");\n",
    "Console.WriteLine(\"Scenario : Maintenance predictive d'un systeme\\n\");\n",
    "\n",
    "// Definition des etats et observations\n",
    "int nEtats = 3;\n",
    "Range etatRange = new Range(nEtats).Named(\"etatRange\");\n",
    "string[] etatsNom = { \"Bon\", \"Degrade\", \"Defaillant\" };\n",
    "\n",
    "// Matrice de transition (degradation naturelle)\n",
    "double[,] transMatrix = {\n",
    "    // vers:    Bon    Degrade  Defaillant\n",
    "    /* Bon */    { 0.90,  0.08,    0.02 },\n",
    "    /* Deg */    { 0.00,  0.85,    0.15 },\n",
    "    /* Def */    { 0.00,  0.00,    1.00 }  // Absorbant\n",
    "};\n",
    "\n",
    "// Modele d'observation : capteur de vibration (0=normal, 1=anormal)\n",
    "double[,] obsMatrix = {\n",
    "    // P(obs | etat)  Normal  Anormal\n",
    "    /* Bon */       { 0.95,   0.05 },\n",
    "    /* Deg */       { 0.30,   0.70 },\n",
    "    /* Def */       { 0.05,   0.95 }\n",
    "};\n",
    "\n",
    "// Utilites des decisions\n",
    "double[,] utilities = {\n",
    "    //                  Bon     Degrade  Defaillant\n",
    "    /* Continuer */   { 100,    50,      -500 },  // Risque si defaillant\n",
    "    /* Maintenance */ { -20,    80,       -50 },  // Preventif, moins de gain si bon\n",
    "    /* Remplacer */   { -100,  -100,       50 }   // Couteux mais resout defaillance\n",
    "};\n",
    "string[] actionsNom = { \"Continuer\", \"Maintenance\", \"Remplacer\" };\n",
    "\n",
    "// === Simulation avec Infer.NET ===\n",
    "// Observations sequentielles : Normal, Anormal, Anormal\n",
    "int[] observations = { 0, 1, 1 };\n",
    "\n",
    "// Belief initial (le systeme vient d'etre installe => Bon)\n",
    "double[] belief = { 0.95, 0.04, 0.01 };\n",
    "\n",
    "Console.WriteLine(\"Belief initial :\");\n",
    "for (int e = 0; e < nEtats; e++)\n",
    "    Console.WriteLine($\"  P({etatsNom[e]}) = {belief[e]:P1}\");\n",
    "Console.WriteLine();\n",
    "\n",
    "// Boucle de mise a jour\n",
    "for (int t = 0; t < observations.Length; t++)\n",
    "{\n",
    "    Console.WriteLine($\"=== Pas de temps {t + 1} ===\");\n",
    "    \n",
    "    // 1. Prediction (transition)\n",
    "    double[] predicted = new double[nEtats];\n",
    "    for (int sPrime = 0; sPrime < nEtats; sPrime++)\n",
    "    {\n",
    "        for (int s = 0; s < nEtats; s++)\n",
    "            predicted[sPrime] += transMatrix[s, sPrime] * belief[s];\n",
    "    }\n",
    "    \n",
    "    Console.WriteLine(\"Apres prediction (transition) :\");\n",
    "    for (int e = 0; e < nEtats; e++)\n",
    "        Console.WriteLine($\"  P({etatsNom[e]}) = {predicted[e]:P1}\");\n",
    "    \n",
    "    // 2. Correction (observation) avec Infer.NET\n",
    "    Variable<int> etatVar = Variable.Discrete(predicted).Named(\"etat\");\n",
    "    etatVar.SetValueRange(etatRange);\n",
    "    \n",
    "    Variable<int> obsVar = Variable.New<int>().Named(\"observation\");\n",
    "    obsVar.SetValueRange(new Range(2));\n",
    "    \n",
    "    // Modele d'observation\n",
    "    using (Variable.Case(etatVar, 0))\n",
    "        obsVar.SetTo(Variable.Discrete(obsMatrix[0, 0], obsMatrix[0, 1]));\n",
    "    using (Variable.Case(etatVar, 1))\n",
    "        obsVar.SetTo(Variable.Discrete(obsMatrix[1, 0], obsMatrix[1, 1]));\n",
    "    using (Variable.Case(etatVar, 2))\n",
    "        obsVar.SetTo(Variable.Discrete(obsMatrix[2, 0], obsMatrix[2, 1]));\n",
    "    \n",
    "    // Observation\n",
    "    int obs = observations[t];\n",
    "    obsVar.ObservedValue = obs;\n",
    "    \n",
    "    // Inference\n",
    "    InferenceEngine engineBelief = new InferenceEngine();\n",
    "    engineBelief.Compiler.CompilerChoice = Microsoft.ML.Probabilistic.Compiler.CompilerChoice.Roslyn;\n",
    "    \n",
    "    var posteriorEtat = engineBelief.Infer<Discrete>(etatVar);\n",
    "    Vector probs = posteriorEtat.GetProbs();\n",
    "    \n",
    "    Console.WriteLine($\"Observation : {(obs == 0 ? \"Normal\" : \"Anormal\")}\");\n",
    "    Console.WriteLine(\"Apres correction (Bayes via Infer.NET) :\");\n",
    "    for (int e = 0; e < nEtats; e++)\n",
    "    {\n",
    "        string bar = new string('#', (int)(probs[e] * 30));\n",
    "        Console.WriteLine($\"  P({etatsNom[e],-10}) = {probs[e]:P1} {bar}\");\n",
    "    }\n",
    "    \n",
    "    // 3. Decision basee sur le belief\n",
    "    double[] EU = new double[3];\n",
    "    for (int a = 0; a < 3; a++)\n",
    "    {\n",
    "        for (int e = 0; e < nEtats; e++)\n",
    "            EU[a] += probs[e] * utilities[a, e];\n",
    "    }\n",
    "    \n",
    "    int bestAction = EU.Select((v, i) => (v, i)).OrderByDescending(x => x.v).First().i;\n",
    "    \n",
    "    Console.WriteLine(\"Utilites esperees :\");\n",
    "    for (int a = 0; a < 3; a++)\n",
    "        Console.WriteLine($\"  E[U({actionsNom[a],-12})] = {EU[a],7:F1}\");\n",
    "    Console.WriteLine($\"=> Decision : {actionsNom[bestAction]}\");\n",
    "    Console.WriteLine();\n",
    "    \n",
    "    // Mettre a jour le belief pour le prochain pas\n",
    "    // Note: GetProbs() retourne un Vector, on copie manuellement\n",
    "    for (int e = 0; e < nEtats; e++)\n",
    "        belief[e] = probs[e];\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"=== Resume ===\");\n",
    "Console.WriteLine(\"Les observations 'Anormal' successives ont fait augmenter P(Degrade),\");\n",
    "Console.WriteLine(\"declenchant le passage de 'Continuer' a 'Maintenance' ou 'Remplacer'.\");\n",
    "Console.WriteLine(\"\\nC'est le principe de la maintenance predictive bayesienne !\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation de la maintenance predictive bayesienne\n",
    "\n",
    "**Scenario simule :** Observations = [Normal, Anormal, Anormal]\n",
    "\n",
    "**Evolution du belief et des decisions :**\n",
    "\n",
    "| Pas | Observation | P(Bon) | P(Degrade) | P(Defaillant) | Decision |\n",
    "|-----|-------------|--------|------------|---------------|----------|\n",
    "| 1 | Normal | 96% | 4% | 0.2% | Continuer (EU=97) |\n",
    "| 2 | Anormal | 30% | 53% | 18% | Maintenance (EU=27) |\n",
    "| 3 | Anormal | 2% | 56% | 42% | Maintenance (EU=23) |\n",
    "\n",
    "**Ce que montre Infer.NET :**\n",
    "- Le moteur calcule automatiquement les posterieurs bayesiens\n",
    "- La boucle prediction-correction est le coeur des filtres bayesiens (Kalman, particule)\n",
    "\n",
    "**Declenchement de la maintenance :**\n",
    "- L'observation \"Normal\" au pas 1 **renforce** le belief que le systeme est bon\n",
    "- Les observations \"Anormal\" aux pas 2-3 **degradent** progressivement le belief\n",
    "- La decision passe de \"Continuer\" a \"Maintenance\" quand P(Degrade) + P(Defaillant) devient significatif\n",
    "\n",
    "**Application industrielle :**\n",
    "- Ce pattern est utilise en **maintenance predictive** (Industry 4.0)\n",
    "- Les capteurs IoT fournissent des observations en continu\n",
    "- Le systeme decide automatiquement quand intervenir, avant la panne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Lien avec la Serie RL\n",
    "\n",
    "### Ce notebook : Concepts fondamentaux\n",
    "\n",
    "- MDPs et equations de Bellman\n",
    "- Methodes tabulaires (Value Iteration, Policy Iteration)\n",
    "- Concepts theoriques (Gittins, POMDPs)\n",
    "\n",
    "### Serie RL (`MyIA.AI.Notebooks/RL/`) : Implementations avancees\n",
    "\n",
    "| Notebook | Contenu |\n",
    "|----------|--------|\n",
    "| RL-1-Introduction | Q-Learning tabulaire |\n",
    "| RL-2-DeepRL | DQN avec reseaux de neurones |\n",
    "| RL-3-PolicyGradient | REINFORCE, Actor-Critic |\n",
    "| RL-4-AdvancedMethods | PPO, A2C, SAC |\n",
    "| RL-5-Applications | Gym, Stable-Baselines3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resume\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **MDP** | (S, A, P, R, γ) - cadre formel pour decisions sequentielles |\n",
    "| **Bellman** | V*(s) = max_a [R + γ Σ P V*] |\n",
    "| **Value Iteration** | Mise a jour iterative de V jusqu'a convergence |\n",
    "| **Policy Iteration** | Evaluation + Amelioration alternees |\n",
    "| **Reward Shaping** | F = γΦ(s') - Φ(s) preserve la politique optimale |\n",
    "| **Bandits** | Exploration vs Exploitation |\n",
    "| **Gittins** | Indice optimal pour bandits avec discount |\n",
    "| **POMDP** | MDP avec observations bruitees, belief states |\n",
    "\n",
    "---\n",
    "\n",
    "## Pour aller plus loin\n",
    "\n",
    "| Si vous voulez... | Consultez... |\n",
    "|-------------------|-------------|\n",
    "| Deep Reinforcement Learning | Serie `MyIA.AI.Notebooks/RL/` |\n",
    "| Implementations PyTorch/TF | Stable-Baselines3 |\n",
    "| Theorie avancee | Sutton & Barto \"Reinforcement Learning\" |\n",
    "\n",
    "---\n",
    "\n",
    "## Fin de la Serie Decision Theory\n",
    "\n",
    "Felicitations ! Vous avez termine les 7 notebooks sur la Decision Theory.\n",
    "\n",
    "### Recapitulatif de la serie 14-20\n",
    "\n",
    "| # | Titre | Concepts cles |\n",
    "|---|-------|---------------|\n",
    "| 14 | Utility Foundations | Axiomes VNM, loteries, agent rationnel |\n",
    "| 15 | Utility Money | CARA/CRRA, aversion au risque, dominance |\n",
    "| 16 | Multi-Attribute | MAUT, independance, SMART |\n",
    "| 17 | Decision Networks | Influence diagrams, arcs informationnels |\n",
    "| 18 | Value of Information | EVPI, EVSI, droits de forage |\n",
    "| 19 | Expert Systems | Minimax, regret, robustesse |\n",
    "| 20 | Sequential | MDPs, bandits, POMDPs |\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Bellman (1957) : Dynamic Programming\n",
    "- Gittins (1979) : Bandit Processes and Dynamic Allocation Indices\n",
    "- Ng, Harada, Russell (1999) : Policy Invariance Under Reward Transformations\n",
    "- Kaelbling, Littman, Cassandra (1998) : Planning and Acting in Partially Observable Stochastic Domains\n",
    "- Sutton & Barto (2018) : Reinforcement Learning: An Introduction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "file_extension": ".cs",
   "mimetype": "text/x-csharp",
   "name": "C#",
   "pygments_lexer": "csharp",
   "version": "13.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}