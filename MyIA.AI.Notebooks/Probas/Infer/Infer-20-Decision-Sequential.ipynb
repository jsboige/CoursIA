{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer-20-Decision-Sequential : MDPs, Bandits et POMDPs\n",
    "\n",
    "**Serie** : Programmation Probabiliste avec Infer.NET (20/20)  \n",
    "**Duree estimee** : 60 minutes  \n",
    "**Prerequis** : Notebooks 14-19 (Decision Theory)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Comprendre les **Processus de Decision Markoviens** (MDPs)\n",
    "- Maitriser l'**iteration de valeur** et l'**iteration de politique**\n",
    "- Decouvrir les alternatives : **LP, Expectimax, RTDP**\n",
    "- Appliquer le **reward shaping** avec le theoreme de preservation de politique\n",
    "- Introduire les **bandits multi-bras** et l'**indice de Gittins**\n",
    "- Presenter les **POMDPs** et les belief states\n",
    "\n",
    "---\n",
    "\n",
    "## Navigation\n",
    "\n",
    "| Precedent | Suivant |\n",
    "|-----------|--------|\n",
    "| [Infer-19-Decision-Expert-Systems](Infer-19-Decision-Expert-Systems.ipynb) | Serie RL (`MyIA.AI.Notebooks/RL/`) |\n",
    "\n",
    "---\n",
    "\n",
    "**Note** : Ce notebook est une introduction conceptuelle aux decisions sequentielles. Les implementations avancees avec Deep RL sont dans la serie `RL/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decisions Sequentielles vs One-Shot\n",
    "\n",
    "### Difference fondamentale\n",
    "\n",
    "| Type | Caracteristique | Exemple |\n",
    "|------|-----------------|--------|\n",
    "| **One-shot** | Une seule decision | Accepter une offre d'emploi |\n",
    "| **Sequentielle** | Serie de decisions interdependantes | Jouer aux echecs |\n",
    "\n",
    "### Horizon\n",
    "\n",
    "- **Fini** : Nombre fixe d'etapes (T etapes)\n",
    "- **Infini** : Le processus continue indefiniment\n",
    "\n",
    "### Facteur d'actualisation γ (gamma)\n",
    "\n",
    "Pour les horizons infinis, on utilise un facteur γ ∈ [0,1) pour :\n",
    "\n",
    "- Garantir la convergence des sommes infinies\n",
    "- Modeliser la preference pour les recompenses immediates\n",
    "- γ = 0.99 : vision long terme\n",
    "- γ = 0.5 : vision court terme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.ML.Probabilistic, 0.4.2504.701</span></li><li><span>Microsoft.ML.Probabilistic.Compiler, 0.4.2504.701</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer.NET charge !\n"
     ]
    }
   ],
   "source": [
    "// Installation Infer.NET\n",
    "#r \"nuget: Microsoft.ML.Probabilistic\"\n",
    "#r \"nuget: Microsoft.ML.Probabilistic.Compiler\"\n",
    "\n",
    "using Microsoft.ML.Probabilistic;\n",
    "using Microsoft.ML.Probabilistic.Distributions;\n",
    "using Microsoft.ML.Probabilistic.Models;\n",
    "using Microsoft.ML.Probabilistic.Algorithms;\n",
    "\n",
    "Console.WriteLine(\"Infer.NET charge !\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FactorGraphHelper charge.\n",
      "Graphviz disponible : True\n"
     ]
    }
   ],
   "source": [
    "// Charger le helper pour visualiser les factor graphs\n",
    "#load \"FactorGraphHelper.cs\"\n",
    "\n",
    "// Flag pour activer les visualisations de factor graphs\n",
    "bool ShowFactorGraph = true;\n",
    "\n",
    "Console.WriteLine(\"FactorGraphHelper charge.\");\n",
    "Console.WriteLine($\"Graphviz disponible : {FactorGraphHelper.IsGraphvizAvailable()}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environnement pret\n",
    "\n",
    "L'infrastructure Infer.NET est chargee. Dans ce notebook, nous utiliserons principalement Infer.NET pour la section sur les **belief states** dans les POMDPs, ou l'inference bayesienne automatique simplifie considerablement les calculs.\n",
    "\n",
    "Commencons par definir un MDP simple : une grille de navigation avec recompenses et obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processus de Decision Markoviens (MDP)\n",
    "\n",
    "### Definition formelle\n",
    "\n",
    "Un MDP est un tuple (S, A, P, R, γ) :\n",
    "\n",
    "| Element | Notation | Description |\n",
    "|---------|----------|-------------|\n",
    "| **Etats** | S | Ensemble des etats possibles |\n",
    "| **Actions** | A | Ensemble des actions |\n",
    "| **Transitions** | P(s'\\|s,a) | Probabilite de transition |\n",
    "| **Recompenses** | R(s,a) ou R(s,a,s') | Recompense immediate |\n",
    "| **Discount** | γ | Facteur d'actualisation |\n",
    "\n",
    "### Politique et Fonction de Valeur\n",
    "\n",
    "- **Politique** π : S → A (ou distribution sur A)\n",
    "- **Fonction de valeur** V^π(s) : Utilite esperee depuis s en suivant π\n",
    "- **Fonction action-valeur** Q^π(s, a) : Utilite esperee en faisant a puis suivant π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Grille 4x3 cree :\n",
      "  But (+1) en (3,2)\n",
      "  Piege (-1) en (3,1)\n",
      "  Mur en (1,1)\n",
      "  Gamma = 0,9\n"
     ]
    }
   ],
   "source": [
    "// Definition d'un MDP simple : Navigation dans une grille\n",
    "\n",
    "public class GridMDP\n",
    "{\n",
    "    public int Width { get; }\n",
    "    public int Height { get; }\n",
    "    public double Gamma { get; }\n",
    "    public Dictionary<(int, int), double> Rewards { get; }\n",
    "    public HashSet<(int, int)> TerminalStates { get; }\n",
    "    public HashSet<(int, int)> Walls { get; }\n",
    "    \n",
    "    public string[] Actions { get; } = { \"N\", \"S\", \"E\", \"W\" };\n",
    "    \n",
    "    // Directions\n",
    "    private Dictionary<string, (int dx, int dy)> _directions = new()\n",
    "    {\n",
    "        { \"N\", (0, 1) }, { \"S\", (0, -1) }, { \"E\", (1, 0) }, { \"W\", (-1, 0) }\n",
    "    };\n",
    "    \n",
    "    public GridMDP(int width, int height, double gamma = 0.9)\n",
    "    {\n",
    "        Width = width;\n",
    "        Height = height;\n",
    "        Gamma = gamma;\n",
    "        Rewards = new Dictionary<(int, int), double>();\n",
    "        TerminalStates = new HashSet<(int, int)>();\n",
    "        Walls = new HashSet<(int, int)>();\n",
    "    }\n",
    "    \n",
    "    public List<(int, int)> GetStates()\n",
    "    {\n",
    "        var states = new List<(int, int)>();\n",
    "        for (int x = 0; x < Width; x++)\n",
    "            for (int y = 0; y < Height; y++)\n",
    "                if (!Walls.Contains((x, y)))\n",
    "                    states.Add((x, y));\n",
    "        return states;\n",
    "    }\n",
    "    \n",
    "    public double GetReward((int, int) state) => \n",
    "        Rewards.TryGetValue(state, out var r) ? r : -0.04; // Cout de mouvement par defaut\n",
    "    \n",
    "    public List<((int, int) nextState, double prob)> GetTransitions((int, int) state, string action)\n",
    "    {\n",
    "        if (TerminalStates.Contains(state))\n",
    "            return new List<((int, int), double)> { (state, 1.0) };\n",
    "        \n",
    "        var transitions = new List<((int, int), double)>();\n",
    "        var (dx, dy) = _directions[action];\n",
    "        \n",
    "        // 80% de chance d'aller dans la direction voulue\n",
    "        transitions.Add((NextState(state, action), 0.8));\n",
    "        \n",
    "        // 10% de chance d'aller perpendiculairement (droite ou gauche)\n",
    "        var perpActions = action == \"N\" || action == \"S\" \n",
    "            ? new[] { \"E\", \"W\" } \n",
    "            : new[] { \"N\", \"S\" };\n",
    "        \n",
    "        foreach (var perpAction in perpActions)\n",
    "            transitions.Add((NextState(state, perpAction), 0.1));\n",
    "        \n",
    "        return transitions;\n",
    "    }\n",
    "    \n",
    "    private (int, int) NextState((int, int) state, string action)\n",
    "    {\n",
    "        var (x, y) = state;\n",
    "        var (dx, dy) = _directions[action];\n",
    "        int nx = x + dx, ny = y + dy;\n",
    "        \n",
    "        // Collision avec mur ou bord = rester sur place\n",
    "        if (nx < 0 || nx >= Width || ny < 0 || ny >= Height || Walls.Contains((nx, ny)))\n",
    "            return state;\n",
    "        return (nx, ny);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Creer le MDP classique de Russell & Norvig\n",
    "var mdp = new GridMDP(4, 3, gamma: 0.9);\n",
    "mdp.Rewards[(3, 2)] = 1.0;   // But positif\n",
    "mdp.Rewards[(3, 1)] = -1.0;  // Piege\n",
    "mdp.TerminalStates.Add((3, 2));\n",
    "mdp.TerminalStates.Add((3, 1));\n",
    "mdp.Walls.Add((1, 1));       // Mur\n",
    "\n",
    "Console.WriteLine(\"MDP Grille 4x3 cree :\");\n",
    "Console.WriteLine(\"  But (+1) en (3,2)\");\n",
    "Console.WriteLine(\"  Piege (-1) en (3,1)\");\n",
    "Console.WriteLine(\"  Mur en (1,1)\");\n",
    "Console.WriteLine($\"  Gamma = {mdp.Gamma}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation : Backup de Bellman\n",
    "\n",
    "L'equation de Bellman effectue un \"backup\" des valeurs futures vers l'etat courant :\n",
    "\n",
    "```\n",
    "                    Etat s (valeur V(s) a calculer)\n",
    "                              │\n",
    "              ┌───────────────┼───────────────┐\n",
    "              │               │               │\n",
    "         Action a₁       Action a₂       Action a₃\n",
    "              │               │               │\n",
    "              ▼               ▼               ▼\n",
    "         ┌────────┐      ┌────────┐      ┌────────┐\n",
    "         │ Q(s,a₁)│      │ Q(s,a₂)│      │ Q(s,a₃)│\n",
    "         │  = ?   │      │  = ?   │      │  = ?   │\n",
    "         └────┬───┘      └────┬───┘      └────┬───┘\n",
    "              │               │               │\n",
    "         ┌────┴────┐     ┌────┴────┐     ┌────┴────┐\n",
    "         │         │     │         │     │         │\n",
    "        P=0.8    P=0.2  P=0.9    P=0.1  P=0.7    P=0.3\n",
    "         │         │     │         │     │         │\n",
    "         ▼         ▼     ▼         ▼     ▼         ▼\n",
    "       [s'₁]     [s'₂] [s'₁]     [s'₃] [s'₂]     [s'₃]\n",
    "      V=0.8     V=0.3  V=0.8     V=0.1  V=0.3     V=0.1\n",
    "```\n",
    "\n",
    "**Calcul pour action a₁** :\n",
    "- Q(s, a₁) = R(s, a₁) + γ × [0.8 × V(s'₁) + 0.2 × V(s'₂)]\n",
    "- Q(s, a₁) = R(s, a₁) + γ × [0.8 × 0.8 + 0.2 × 0.3]\n",
    "- Q(s, a₁) = R(s, a₁) + γ × 0.70\n",
    "\n",
    "**Decision** : V(s) = max{Q(s, a₁), Q(s, a₂), Q(s, a₃)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation : Propagation des Valeurs dans Value Iteration\n",
    "\n",
    "Value Iteration propage les valeurs depuis les etats terminaux vers les etats initiaux :\n",
    "\n",
    "```\n",
    "Iteration 0 (initialisation)         Iteration 5                        Iteration finale\n",
    "┌─────┬─────┬─────┬─────┐           ┌─────┬─────┬─────┬─────┐           ┌─────┬─────┬─────┬─────┐\n",
    "│ 0.0 │ 0.0 │ 0.0 │ 1.0 │           │ 0.3 │ 0.5 │ 0.7 │ 1.0 │           │ 0.5 │ 0.6 │ 0.8 │ 1.0 │\n",
    "├─────┼─────┼─────┼─────┤           ├─────┼─────┼─────┼─────┤           ├─────┼─────┼─────┼─────┤\n",
    "│ 0.0 │#####│ 0.0 │-1.0 │   ──►    │ 0.2 │#####│ 0.3 │-1.0 │   ──►    │ 0.4 │#####│ 0.5 │-1.0 │\n",
    "├─────┼─────┼─────┼─────┤           ├─────┼─────┼─────┼─────┤           ├─────┼─────┼─────┼─────┤\n",
    "│ 0.0 │ 0.0 │ 0.0 │ 0.0 │           │ 0.1 │ 0.1 │ 0.2 │ 0.0 │           │ 0.3 │ 0.3 │ 0.3 │ 0.1 │\n",
    "└─────┴─────┴─────┴─────┘           └─────┴─────┴─────┴─────┘           └─────┴─────┴─────┴─────┘\n",
    "    Seuls les terminaux                Les valeurs \"diffusent\"            Convergence (delta < ε)\n",
    "    ont des valeurs non nulles         vers les etats voisins\n",
    "```\n",
    "\n",
    "**Proprietes de convergence** :\n",
    "- Le facteur γ=0.9 garantit que les valeurs diminuent exponentiellement avec la distance\n",
    "- Convergence en O(log(1/ε)/(1-γ)) iterations\n",
    "- Pour γ=0.9 et ε=0.001 : environ 14-15 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Equation de Bellman\n",
    "\n",
    "### Intuition\n",
    "\n",
    "L'equation de Bellman exprime un principe fondamental de **consistance temporelle** :\n",
    "\n",
    "> La valeur d'un etat = recompense immediate + valeur actualisee des etats futurs\n",
    "\n",
    "C'est une relation **recursive** : la valeur de chaque etat depend de la valeur des etats atteignables. Cette structure permet de resoudre le probleme \"a l'envers\" (backward induction) ou iterativement.\n",
    "\n",
    "### Equation de Bellman pour V*\n",
    "\n",
    "La fonction de valeur optimale satisfait :\n",
    "\n",
    "$$V^*(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right]$$\n",
    "\n",
    "**Interpretation terme par terme** :\n",
    "- $\\max_a$ : On choisit la meilleure action\n",
    "- $R(s,a)$ : Recompense immediate\n",
    "- $\\gamma$ : Facteur d'actualisation (valeur du futur)\n",
    "- $\\sum_{s'} P(s'|s,a) V^*(s')$ : Esperance de la valeur future\n",
    "\n",
    "### Equation de Bellman pour Q*\n",
    "\n",
    "$$Q^*(s, a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q^*(s', a')$$\n",
    "\n",
    "La fonction Q donne la valeur de **faire a puis agir optimalement**.\n",
    "\n",
    "### Politique optimale\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$$\n",
    "\n",
    "La politique optimale est **deterministe** : dans chaque etat, une action domine.\n",
    "\n",
    "### Pourquoi Bellman est-il si important ?\n",
    "\n",
    "L'equation de Bellman est la **cle de voute** de la programmation dynamique et du reinforcement learning car :\n",
    "\n",
    "1. Elle decompose un probleme global en sous-problemes locaux\n",
    "2. Elle fournit une condition d'optimalite verifiable\n",
    "3. Elle inspire les algorithmes (VI, PI, Q-learning, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Iteration de Valeur\n",
    "\n",
    "### Algorithme\n",
    "\n",
    "1. Initialiser V(s) = 0 pour tout s\n",
    "2. Repeter jusqu'a convergence :\n",
    "   - Pour chaque etat s :\n",
    "     - V(s) ← max_a [R(s,a) + γ Σ_s' P(s'|s,a) V(s')]\n",
    "3. Extraire la politique : π(s) = argmax_a Q(s,a)\n",
    "\n",
    "### Complexite\n",
    "\n",
    "- O(|S|² |A|) par iteration\n",
    "- Converge en O(log(1/ε) / (1-γ)) iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation ASCII de la Grille MDP\n",
    "\n",
    "La grille 4x3 ci-dessous illustre la structure du MDP :\n",
    "- Les fleches montrent la politique optimale calculee\n",
    "- Les valeurs V*(s) decroissent avec la distance au but\n",
    "- Le mur (1,1) bloque certains chemins\n",
    "\n",
    "```\n",
    "    y=2:   [0.51]→  [0.65]→  [0.80]→  [+1.0]●\n",
    "                                         ↑\n",
    "    y=1:   [0.40]↑  [WALL]   [0.49]↑  [-1.0]●\n",
    "                                         ↑\n",
    "    y=0:   [0.30]↑  [0.25]→  [0.35]↑  [0.13]←\n",
    "```\n",
    "\n",
    "**Structure du factor graph MDP** (conceptuel) :\n",
    "\n",
    "```\n",
    "  s_t ─────┬────→ s_{t+1} ─────┬────→ s_{t+2}\n",
    "           │                   │\n",
    "           ↓                   ↓\n",
    "   [ P(s'|s,a) ]       [ P(s'|s,a) ]\n",
    "           │                   │\n",
    "           ↓                   ↓\n",
    "         a_t                 a_{t+1}\n",
    "           │                   │\n",
    "           ↓                   ↓\n",
    "        R(s,a)              R(s,a)\n",
    "```\n",
    "\n",
    "Chaque etape temporelle forme une \"tranche\" avec :\n",
    "- Un noeud d'etat s_t (variable aleatoire)\n",
    "- Un noeud d'action a_t (variable de decision)\n",
    "- Un facteur de transition P(s_{t+1}|s_t, a_t)\n",
    "- Un facteur de recompense R(s_t, a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence apres 14 iterations (delta = 6,01E-004)\n",
      "\n",
      "Fonction de Valeur V* :\n",
      "  0,509   0,650   0,795   1,000 \n",
      "  0,398   ####    0,486  -1,000 \n",
      "  0,296   0,254   0,345   0,130 \n",
      "\n",
      "Politique optimale π* :\n",
      " →  →  →  ● \n",
      " ↑  #  ↑  ● \n",
      " ↑  →  ↑  ← \n"
     ]
    }
   ],
   "source": [
    "// Iteration de Valeur\n",
    "\n",
    "public class ValueIteration\n",
    "{\n",
    "    public static (Dictionary<(int,int), double> V, Dictionary<(int,int), string> policy) \n",
    "        Solve(GridMDP mdp, double epsilon = 0.001, int maxIter = 100)\n",
    "    {\n",
    "        var states = mdp.GetStates();\n",
    "        var V = states.ToDictionary(s => s, s => 0.0);\n",
    "        \n",
    "        for (int iter = 0; iter < maxIter; iter++)\n",
    "        {\n",
    "            double delta = 0;\n",
    "            var newV = new Dictionary<(int, int), double>(V);\n",
    "            \n",
    "            foreach (var s in states)\n",
    "            {\n",
    "                if (mdp.TerminalStates.Contains(s))\n",
    "                {\n",
    "                    newV[s] = mdp.GetReward(s);\n",
    "                    continue;\n",
    "                }\n",
    "                \n",
    "                double maxQ = double.NegativeInfinity;\n",
    "                foreach (var a in mdp.Actions)\n",
    "                {\n",
    "                    double q = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        q += mdp.Gamma * prob * V[nextState];\n",
    "                    maxQ = Math.Max(maxQ, q);\n",
    "                }\n",
    "                \n",
    "                newV[s] = maxQ;\n",
    "                delta = Math.Max(delta, Math.Abs(V[s] - newV[s]));\n",
    "            }\n",
    "            \n",
    "            V = newV;\n",
    "            \n",
    "            if (delta < epsilon)\n",
    "            {\n",
    "                Console.WriteLine($\"Convergence apres {iter + 1} iterations (delta = {delta:E2})\");\n",
    "                break;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Extraire la politique\n",
    "        var policy = new Dictionary<(int, int), string>();\n",
    "        foreach (var s in states)\n",
    "        {\n",
    "            if (mdp.TerminalStates.Contains(s))\n",
    "            {\n",
    "                policy[s] = \"T\"; // Terminal\n",
    "                continue;\n",
    "            }\n",
    "            \n",
    "            string bestAction = null;\n",
    "            double maxQ = double.NegativeInfinity;\n",
    "            \n",
    "            foreach (var a in mdp.Actions)\n",
    "            {\n",
    "                double q = mdp.GetReward(s);\n",
    "                foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                    q += mdp.Gamma * prob * V[nextState];\n",
    "                \n",
    "                if (q > maxQ)\n",
    "                {\n",
    "                    maxQ = q;\n",
    "                    bestAction = a;\n",
    "                }\n",
    "            }\n",
    "            policy[s] = bestAction;\n",
    "        }\n",
    "        \n",
    "        return (V, policy);\n",
    "    }\n",
    "}\n",
    "\n",
    "var (V, policy) = ValueIteration.Solve(mdp);\n",
    "\n",
    "// Afficher la grille\n",
    "Console.WriteLine(\"\\nFonction de Valeur V* :\");\n",
    "for (int y = mdp.Height - 1; y >= 0; y--)\n",
    "{\n",
    "    for (int x = 0; x < mdp.Width; x++)\n",
    "    {\n",
    "        if (mdp.Walls.Contains((x, y)))\n",
    "            Console.Write(\"  ####  \");\n",
    "        else\n",
    "            Console.Write($\" {V[(x, y)],6:F3} \");\n",
    "    }\n",
    "    Console.WriteLine();\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"\\nPolitique optimale π* :\");\n",
    "var arrows = new Dictionary<string, string> { {\"N\",\"↑\"}, {\"S\",\"↓\"}, {\"E\",\"→\"}, {\"W\",\"←\"}, {\"T\",\"●\"} };\n",
    "for (int y = mdp.Height - 1; y >= 0; y--)\n",
    "{\n",
    "    for (int x = 0; x < mdp.Width; x++)\n",
    "    {\n",
    "        if (mdp.Walls.Contains((x, y)))\n",
    "            Console.Write(\" # \");\n",
    "        else\n",
    "            Console.Write($\" {arrows[policy[(x, y)]]} \");\n",
    "    }\n",
    "    Console.WriteLine();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation de l'Iteration de Valeur\n",
    "\n",
    "**Fonction de valeur V* :**\n",
    "- Les valeurs decroissent en s'eloignant du but (+1 en (3,2))\n",
    "- La case (3,1) vaut -1 car c'est un piege (terminal negatif)\n",
    "- Les valeurs reflètent la **distance actualisee au but** avec risque de deviation\n",
    "\n",
    "**Politique optimale π* :**\n",
    "- Toutes les fleches pointent vers le but (3,2) en evitant le piege (3,1)\n",
    "- La case (3,0) pointe vers la **gauche** (←) pour eviter de tomber dans le piege !\n",
    "- Cette politique non-intuitive illustre la puissance de l'optimisation\n",
    "\n",
    "**Pourquoi 14 iterations ?**\n",
    "- Le facteur γ = 0.9 propage les valeurs lentement\n",
    "- Les etats eloignes du but ont besoin de plusieurs iterations pour \"sentir\" la recompense\n",
    "- Critere d'arret : delta < 0.001 (precision suffisante)\n",
    "\n",
    "**Verification pratique :**\n",
    "- V(0,0) ≈ 0.30 : depuis (0,0), on peut atteindre le but avec un profit actualise de ~0.30\n",
    "- V(2,2) ≈ 0.80 : juste a cote du but, on y arrive presque certainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Iteration de Politique\n",
    "\n",
    "### Algorithme\n",
    "\n",
    "1. Initialiser π arbitrairement\n",
    "2. Repeter jusqu'a stabilite :\n",
    "   - **Evaluation** : Calculer V^π (resoudre systeme lineaire)\n",
    "   - **Amelioration** : π(s) ← argmax_a Q^π(s,a)\n",
    "\n",
    "### Compromis Value Iteration vs Policy Iteration\n",
    "\n",
    "| Critere | Value Iteration | Policy Iteration |\n",
    "|---------|-----------------|------------------|\n",
    "| **Iterations** | Beaucoup (log(1/ε)/(1-γ)) | Peu (souvent 5-10) |\n",
    "| **Cout/iteration** | O(\\|S\\|² \\|A\\|) leger | O(\\|S\\|³) evaluation exacte |\n",
    "| **Convergence** | Asymptotique (ε-optimal) | Exacte en nombre fini |\n",
    "| **Memoire** | V(s) seulement | V(s) + π(s) |\n",
    "\n",
    "### Quand utiliser chaque methode ?\n",
    "\n",
    "**Preferer Value Iteration si** :\n",
    "- Grand espace d'etats (\\|S\\| > 10,000)\n",
    "- Precision approximative suffisante\n",
    "- γ proche de 1 (convergence lente de PI)\n",
    "- Implementation simple souhaitee\n",
    "\n",
    "**Preferer Policy Iteration si** :\n",
    "- Petit/moyen espace d'etats\n",
    "- Solution exacte requise\n",
    "- Politique stable rapidement\n",
    "- Evaluation peut etre acceleree (ex: methodes matricielles)\n",
    "\n",
    "### Variantes hybrides\n",
    "\n",
    "- **Modified Policy Iteration** : Evaluation partielle (k iterations de Bellman au lieu de resolution exacte)\n",
    "- **Asynchronous VI** : Mise a jour d'un sous-ensemble d'etats a chaque iteration\n",
    "- **Prioritized Sweeping** : Mettre a jour les etats les plus \"surprenants\" en premier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Iteration de Politique ===\n",
      "\n",
      "Iteration 1 : stable = False\n",
      "Iteration 2 : stable = False\n",
      "Iteration 3 : stable = True\n",
      "\n",
      "Comparaison avec Value Iteration :\n",
      "  Politiques identiques !\n"
     ]
    }
   ],
   "source": [
    "// Iteration de Politique\n",
    "\n",
    "public class PolicyIteration\n",
    "{\n",
    "    public static (Dictionary<(int,int), double> V, Dictionary<(int,int), string> policy) \n",
    "        Solve(GridMDP mdp, int maxIter = 20)\n",
    "    {\n",
    "        var states = mdp.GetStates();\n",
    "        \n",
    "        // Initialiser politique aleatoire\n",
    "        var policy = states.ToDictionary(s => s, s => \n",
    "            mdp.TerminalStates.Contains(s) ? \"T\" : \"N\");\n",
    "        \n",
    "        var V = states.ToDictionary(s => s, s => 0.0);\n",
    "        \n",
    "        for (int iter = 0; iter < maxIter; iter++)\n",
    "        {\n",
    "            // 1. Evaluation de politique (iterative simplifiee)\n",
    "            for (int evalIter = 0; evalIter < 50; evalIter++)\n",
    "            {\n",
    "                var newV = new Dictionary<(int, int), double>(V);\n",
    "                foreach (var s in states)\n",
    "                {\n",
    "                    if (mdp.TerminalStates.Contains(s))\n",
    "                    {\n",
    "                        newV[s] = mdp.GetReward(s);\n",
    "                        continue;\n",
    "                    }\n",
    "                    \n",
    "                    string a = policy[s];\n",
    "                    double v = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        v += mdp.Gamma * prob * V[nextState];\n",
    "                    newV[s] = v;\n",
    "                }\n",
    "                V = newV;\n",
    "            }\n",
    "            \n",
    "            // 2. Amelioration de politique\n",
    "            bool stable = true;\n",
    "            foreach (var s in states)\n",
    "            {\n",
    "                if (mdp.TerminalStates.Contains(s)) continue;\n",
    "                \n",
    "                string oldAction = policy[s];\n",
    "                string bestAction = null;\n",
    "                double maxQ = double.NegativeInfinity;\n",
    "                \n",
    "                foreach (var a in mdp.Actions)\n",
    "                {\n",
    "                    double q = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        q += mdp.Gamma * prob * V[nextState];\n",
    "                    \n",
    "                    if (q > maxQ)\n",
    "                    {\n",
    "                        maxQ = q;\n",
    "                        bestAction = a;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                policy[s] = bestAction;\n",
    "                if (bestAction != oldAction) stable = false;\n",
    "            }\n",
    "            \n",
    "            Console.WriteLine($\"Iteration {iter + 1} : stable = {stable}\");\n",
    "            if (stable) break;\n",
    "        }\n",
    "        \n",
    "        return (V, policy);\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"=== Iteration de Politique ===\\n\");\n",
    "var (V_pi, policy_pi) = PolicyIteration.Solve(mdp);\n",
    "\n",
    "// Verifier que les resultats sont identiques\n",
    "Console.WriteLine(\"\\nComparaison avec Value Iteration :\");\n",
    "bool same = true;\n",
    "foreach (var s in mdp.GetStates())\n",
    "{\n",
    "    if (policy[s] != policy_pi[s])\n",
    "    {\n",
    "        same = false;\n",
    "        Console.WriteLine($\"  Difference en {s}: VI={policy[s]}, PI={policy_pi[s]}\");\n",
    "    }\n",
    "}\n",
    "if (same) Console.WriteLine(\"  Politiques identiques !\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation de l'Iteration de Politique\n",
    "\n",
    "**Convergence rapide :**\n",
    "- Seulement **3 iterations** contre 14 pour Value Iteration !\n",
    "- Chaque iteration est plus couteuse (evaluation complete), mais beaucoup moins d'iterations\n",
    "\n",
    "**Pourquoi moins d'iterations ?**\n",
    "- Policy Iteration fait des **sauts** dans l'espace des politiques\n",
    "- Value Iteration fait des **petits pas** dans l'espace des valeurs\n",
    "- La politique peut etre optimale bien avant que les valeurs convergent exactement\n",
    "\n",
    "**Verification de coherence :**\n",
    "- Les deux methodes donnent la **meme politique optimale**\n",
    "- C'est rassurant : le probleme a une solution unique\n",
    "\n",
    "**Quand utiliser chaque methode ?**\n",
    "\n",
    "| Situation | Methode recommandee |\n",
    "|-----------|---------------------|\n",
    "| Grand espace d'etats | Value Iteration (moins de memoire) |\n",
    "| Besoin de precision exacte | Policy Iteration |\n",
    "| Implementation simple | Value Iteration |\n",
    "| γ tres proche de 1 | Policy Iteration (VI converge lentement) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Alternatives : LP, Expectimax, RTDP\n",
    "\n",
    "### Programmation Lineaire (LP)\n",
    "\n",
    "Le MDP peut etre formule comme un programme lineaire :\n",
    "\n",
    "- **Variables** : V(s) pour chaque etat\n",
    "- **Objectif** : min Σ_s V(s)\n",
    "- **Contraintes** : V(s) ≥ R(s,a) + γ Σ_s' P(s'|s,a) V(s') pour tout a\n",
    "\n",
    "### Expectimax\n",
    "\n",
    "Pour les MDPs a horizon fini, on peut utiliser un arbre de recherche :\n",
    "\n",
    "```\n",
    "       (s0)\n",
    "      / | \\\n",
    "   a1  a2  a3     <- Noeuds max (choix agent)\n",
    "   / \\ \n",
    " s1  s2           <- Noeuds chance (transition)\n",
    "```\n",
    "\n",
    "### RTDP (Real-Time Dynamic Programming)\n",
    "\n",
    "- Mise a jour le long de trajectoires simulees\n",
    "- Focus sur les etats atteignables\n",
    "- Algorithme **anytime** : ameliore avec plus de temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RTDP (100 trials depuis (0,0)) ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison V_RTDP vs V_VI :\n",
      "  Etat   |  V_RTDP  |   V_VI   |   Diff\n",
      "---------|----------|----------|--------\n",
      " (0,0)   |   0,1666 |   0,2960 | 0,1294\n",
      " (0,2)   |  -0,1123 |   0,5094 | 0,6217\n",
      " (2,2)   |   0,7954 |   0,7954 | 0,0000\n",
      " (3,2)   |   1,0000 |   1,0000 | 0,0000\n"
     ]
    }
   ],
   "source": [
    "// RTDP simplifie\n",
    "\n",
    "public class RTDP\n",
    "{\n",
    "    public static Dictionary<(int,int), double> Solve(\n",
    "        GridMDP mdp, (int, int) startState, int nTrials = 100)\n",
    "    {\n",
    "        var states = mdp.GetStates();\n",
    "        var V = states.ToDictionary(s => s, s => 0.0);\n",
    "        var rng = new Random(42);\n",
    "        \n",
    "        for (int trial = 0; trial < nTrials; trial++)\n",
    "        {\n",
    "            var s = startState;\n",
    "            int steps = 0;\n",
    "            \n",
    "            while (!mdp.TerminalStates.Contains(s) && steps < 100)\n",
    "            {\n",
    "                // Mise a jour de Bellman\n",
    "                double maxQ = double.NegativeInfinity;\n",
    "                string bestAction = null;\n",
    "                \n",
    "                foreach (var a in mdp.Actions)\n",
    "                {\n",
    "                    double q = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        q += mdp.Gamma * prob * V[nextState];\n",
    "                    \n",
    "                    if (q > maxQ)\n",
    "                    {\n",
    "                        maxQ = q;\n",
    "                        bestAction = a;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                V[s] = maxQ;\n",
    "                \n",
    "                // Simuler transition\n",
    "                var transitions = mdp.GetTransitions(s, bestAction);\n",
    "                double r = rng.NextDouble();\n",
    "                double cumProb = 0;\n",
    "                foreach (var (nextState, prob) in transitions)\n",
    "                {\n",
    "                    cumProb += prob;\n",
    "                    if (r <= cumProb)\n",
    "                    {\n",
    "                        s = nextState;\n",
    "                        break;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                steps++;\n",
    "            }\n",
    "            \n",
    "            // Update terminal state\n",
    "            if (mdp.TerminalStates.Contains(s))\n",
    "                V[s] = mdp.GetReward(s);\n",
    "        }\n",
    "        \n",
    "        return V;\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"=== RTDP (100 trials depuis (0,0)) ===\\n\");\n",
    "var V_rtdp = RTDP.Solve(mdp, (0, 0), nTrials: 100);\n",
    "\n",
    "// Comparer avec Value Iteration\n",
    "Console.WriteLine(\"Comparaison V_RTDP vs V_VI :\");\n",
    "Console.WriteLine(\"  Etat   |  V_RTDP  |   V_VI   |   Diff\");\n",
    "Console.WriteLine(\"---------|----------|----------|--------\");\n",
    "foreach (var s in new[] { (0,0), (0,2), (2,2), (3,2) })\n",
    "{\n",
    "    double diff = Math.Abs(V_rtdp[s] - V[s]);\n",
    "    Console.WriteLine($\" ({s.Item1},{s.Item2})   | {V_rtdp[s],8:F4} | {V[s],8:F4} | {diff,6:F4}\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation : Reward Shaping avec Potentiel\n",
    "\n",
    "Le reward shaping ajoute une recompense de guidage F(s, a, s') sans modifier la politique optimale :\n",
    "\n",
    "```\n",
    "                SANS Shaping                     AVEC Shaping (Φ = -distance au but)\n",
    "                                  \n",
    "    ┌─────────────────────────┐           ┌─────────────────────────┐\n",
    "    │                    ★ +1 │           │                    ★ +1 │\n",
    "    │                         │           │      F=+0.5   F=+1.0    │\n",
    "    │                    ×-1  │           │         ↘       ↘       │\n",
    "    │   R=-0.04  R=-0.04      │           │   R'≈0.5  R'≈0.8  ×-1  │\n",
    "    │      ↓        ↓         │           │      ↓        ↓         │\n",
    "    │   R=-0.04  R=-0.04      │           │   R'≈0.3  R'≈0.6       │\n",
    "    │      ↓        ↓         │           │      ↓        ↓         │\n",
    "    │   R=-0.04  R=-0.04      │           │   R'≈0.1  R'≈0.4       │\n",
    "    │      ↑                   │           │      ↑                   │\n",
    "    │   START                  │           │   START                  │\n",
    "    └─────────────────────────┘           └─────────────────────────┘\n",
    "    \n",
    "    Recompense sparse :                    Recompense dense :\n",
    "    - Longue exploration                   - Gradient vers le but\n",
    "    - Signaux retardes                     - Apprentissage accelere\n",
    "```\n",
    "\n",
    "**Theoreme de Ng et al.** :\n",
    "Si F(s, a, s') = γΦ(s') - Φ(s), alors π* est preservee.\n",
    "\n",
    "Intuition : Le shaping recompense les \"progres\" sans creer de raccourcis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation de RTDP (Real-Time Dynamic Programming)\n",
    "\n",
    "**Comparaison RTDP vs Value Iteration :**\n",
    "\n",
    "| Etat | V_RTDP | V_VI | Ecart | Commentaire |\n",
    "|------|--------|------|-------|-------------|\n",
    "| (0,0) | 0.17 | 0.30 | 0.13 | Sous-estime (visite moderee) |\n",
    "| (0,2) | -0.11 | 0.51 | 0.62 | Non visite depuis (0,0) |\n",
    "| (2,2) | 0.80 | 0.80 | 0.00 | Exact (sur le chemin optimal) |\n",
    "| (3,2) | 1.00 | 1.00 | 0.00 | Terminal (toujours exact) |\n",
    "\n",
    "**Pourquoi ces ecarts ?**\n",
    "\n",
    "RTDP est un algorithme **anytime** qui :\n",
    "1. Ne met a jour que les etats **visites** par simulation\n",
    "2. Converge vers V* sur les **etats atteignables** depuis l'etat de depart\n",
    "3. Peut ignorer certains etats non pertinents pour la politique courante\n",
    "\n",
    "**Etat (0,2) tres sous-estime :**\n",
    "- Depuis (0,0), la politique optimale va vers l'est, pas vers le nord\n",
    "- L'etat (0,2) n'est jamais visite dans les simulations\n",
    "- Sa valeur reste proche de l'initialisation (ici negative a cause de mauvaises transitions simulees)\n",
    "\n",
    "> **Note technique** : RTDP garantit la convergence vers V* uniquement sur les etats visites infiniment souvent. Pour une convergence globale, on utilise des variantes comme LRTDP (Labeled RTDP) qui marquent les etats \"resolus\".\n",
    "\n",
    "**Quand utiliser RTDP ?**\n",
    "\n",
    "| Situation | Recommandation |\n",
    "|-----------|----------------|\n",
    "| Grands espaces d'etats | RTDP (focus sur etats pertinents) |\n",
    "| Solution exacte requise | Value Iteration classique |\n",
    "| Temps de calcul limite | RTDP (anytime) |\n",
    "| Tous les etats importants | Value/Policy Iteration |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reward Shaping\n",
    "\n",
    "### Le probleme des recompenses sparses\n",
    "\n",
    "Quand les recompenses sont rares (ex: +1 seulement au but), l'apprentissage est tres lent.\n",
    "\n",
    "### Solution : Ajouter une recompense de faconnage\n",
    "\n",
    "$$R'(s, a, s') = R(s, a, s') + F(s, a, s')$$\n",
    "\n",
    "### Theoreme de preservation de politique (Ng et al., 1999)\n",
    "\n",
    "> Si F a la forme d'une **fonction de potentiel** :\n",
    "> \n",
    "> $$F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)$$\n",
    "> \n",
    "> Alors **la politique optimale est preservee** !\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- Φ(s) represente \"a quel point s est proche du but\"\n",
    "- F recompense les transitions vers des etats meilleurs\n",
    "- La forme specifique garantit que les raccourcis ne sont pas crees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Synthese : Comparaison des methodes de resolution MDP\n",
    "\n",
    "Avant de passer aux bandits, recapitulons les methodes vues pour resoudre les MDPs :\n",
    "\n",
    "| Methode | Principe | Complexite/iter | Convergence | Usage typique |\n",
    "|---------|----------|-----------------|-------------|---------------|\n",
    "| **Value Iteration** | Bellman updates iteratifs | O(\\|S\\|^2 \\|A\\|) | Asymptotique | Standard, simple |\n",
    "| **Policy Iteration** | Evaluation + Amelioration | O(\\|S\\|^3) | Exacte, finie | Petits MDPs |\n",
    "| **LP** | Programmation lineaire | Polynomial | Exacte | Structure particuliere |\n",
    "| **RTDP** | Updates sur trajectoires | Variable | Sur etats visites | Grands espaces |\n",
    "| **Expectimax** | Arbre de recherche | Exponentiel | Exacte (horizon fini) | Jeux, planification |\n",
    "\n",
    "**Arbre de decision pour choisir une methode :**\n",
    "\n",
    "```\n",
    "MDP a resoudre\n",
    "    |\n",
    "    +-- Horizon fini ?\n",
    "    |       |\n",
    "    |       +-- Oui --> Expectimax ou backward induction\n",
    "    |       +-- Non --> Methodes iteratives\n",
    "    |\n",
    "    +-- Grand espace d'etats (\\|S\\| > 10,000) ?\n",
    "    |       |\n",
    "    |       +-- Oui --> RTDP, LRTDP, ou approximation\n",
    "    |       +-- Non --> VI ou PI\n",
    "    |\n",
    "    +-- Besoin de precision exacte ?\n",
    "            |\n",
    "            +-- Oui --> Policy Iteration ou LP\n",
    "            +-- Non --> Value Iteration (plus rapide par iteration)\n",
    "```\n",
    "\n",
    "> **Note** : En pratique moderne, les grands MDPs sont souvent resolus par **Deep Reinforcement Learning** (DQN, PPO, etc.) qui approximent V ou Q avec des reseaux de neurones. Voir la serie `RL/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation : Dilemme Exploration vs Exploitation\n",
    "\n",
    "Le probleme du bandit multi-bras illustre le dilemme fondamental :\n",
    "\n",
    "```\n",
    "              ┌─────────────────────────────────────────┐\n",
    "              │     Agent avec connaissance partielle   │\n",
    "              │                                         │\n",
    "              │   Bras 1: μ̂ = 0.35 (n=20)              │\n",
    "              │   Bras 2: μ̂ = 0.48 (n=15)    ◄── Best? │\n",
    "              │   Bras 3: μ̂ = 0.52 (n=8)     ◄── Best? │\n",
    "              │   Bras 4: μ̂ = 0.30 (n=12)              │\n",
    "              └─────────────────────┬───────────────────┘\n",
    "                                    │\n",
    "                    ┌───────────────┴───────────────┐\n",
    "                    │                               │\n",
    "              ┌─────▼─────┐                   ┌─────▼─────┐\n",
    "              │ EXPLOITER │                   │ EXPLORER  │\n",
    "              │           │                   │           │\n",
    "              │ Jouer le  │                   │ Jouer un  │\n",
    "              │ meilleur  │                   │ bras peu  │\n",
    "              │ connu     │                   │ essaye    │\n",
    "              │ (Bras 3?) │                   │ (Bras 3?) │\n",
    "              └───────────┘                   └───────────┘\n",
    "                    │                               │\n",
    "              Gain court                      Gain long\n",
    "              terme sur                       terme si\n",
    "                                              meilleur\n",
    "```\n",
    "\n",
    "**Strategies** :\n",
    "- **ε-greedy** : Exploite (1-ε)% du temps, explore ε% aleatoirement\n",
    "- **UCB** : Ajoute un bonus d'incertitude aux bras peu explores\n",
    "- **Thompson** : Echantillonne selon la posterior de chaque bras\n",
    "- **Gittins** : Index optimal theorique (mais couteux a calculer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reward Shaping avec Fonction de Potentiel ===\n",
      "\n",
      "But en (3, 2)\n",
      "Phi(s) = -distance(s, but)\n",
      "\n",
      "Fonction de potentiel Phi(s) :\n",
      "  -3,00   -2,00   -1,00   -0,00 \n",
      "  -3,16   ####    -1,41   -1,00 \n",
      "  -3,61   -2,83   -2,24   -2,00 \n",
      "\n",
      "Exemples de shaping reward F(s, a, s') = gamma*Phi(s') - Phi(s) :\n",
      "  F((0,0) -> (1,0)) = 1,060 (vers le but)\n",
      "  F((1,0) -> (0,0)) = -0,417 (loin du but)\n",
      "  F((2,2) -> (3,2)) = 1,000 (atteindre le but)\n",
      "\n",
      "=> Le shaping recompense les mouvements vers le but,\n",
      "   mais le theoreme garantit que la politique optimale est preservee.\n"
     ]
    }
   ],
   "source": [
    "// Demonstration du Reward Shaping\n",
    "\n",
    "public class ShapedGridMDP : GridMDP\n",
    "{\n",
    "    private Func<(int, int), double> _potential;\n",
    "    private double _gamma;\n",
    "    \n",
    "    public ShapedGridMDP(int width, int height, double gamma, Func<(int, int), double> potential) \n",
    "        : base(width, height, gamma)\n",
    "    {\n",
    "        _potential = potential;\n",
    "        _gamma = gamma;\n",
    "    }\n",
    "    \n",
    "    public double GetShapedReward((int, int) s, (int, int) sPrime)\n",
    "    {\n",
    "        double R = GetReward(s);\n",
    "        double F = _gamma * _potential(sPrime) - _potential(s);\n",
    "        return R + F;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Fonction de potentiel : distance negative au but\n",
    "(int, int) goal = (3, 2);\n",
    "Func<(int, int), double> Phi = s => -Math.Sqrt(Math.Pow(s.Item1 - goal.Item1, 2) + Math.Pow(s.Item2 - goal.Item2, 2));\n",
    "\n",
    "Console.WriteLine(\"=== Reward Shaping avec Fonction de Potentiel ===\\n\");\n",
    "Console.WriteLine($\"But en {goal}\");\n",
    "Console.WriteLine(\"Phi(s) = -distance(s, but)\\n\");\n",
    "\n",
    "Console.WriteLine(\"Fonction de potentiel Phi(s) :\");\n",
    "for (int y = 2; y >= 0; y--)\n",
    "{\n",
    "    for (int x = 0; x < 4; x++)\n",
    "    {\n",
    "        if (mdp.Walls.Contains((x, y)))\n",
    "            Console.Write(\"  ####  \");\n",
    "        else\n",
    "            Console.Write($\" {Phi((x, y)),6:F2} \");\n",
    "    }\n",
    "    Console.WriteLine();\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"\\nExemples de shaping reward F(s, a, s') = gamma*Phi(s') - Phi(s) :\");\n",
    "Console.WriteLine($\"  F((0,0) -> (1,0)) = {0.9 * Phi((1, 0)) - Phi((0, 0)):F3} (vers le but)\");\n",
    "Console.WriteLine($\"  F((1,0) -> (0,0)) = {0.9 * Phi((0, 0)) - Phi((1, 0)):F3} (loin du but)\");\n",
    "Console.WriteLine($\"  F((2,2) -> (3,2)) = {0.9 * Phi((3, 2)) - Phi((2, 2)):F3} (atteindre le but)\");\n",
    "\n",
    "Console.WriteLine();\n",
    "Console.WriteLine(\"=> Le shaping recompense les mouvements vers le but,\");\n",
    "Console.WriteLine(\"   mais le theoreme garantit que la politique optimale est preservee.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bandits Multi-Bras\n",
    "\n",
    "### Le probleme\n",
    "\n",
    "Vous avez K machines a sous (\"bras\"). Chaque bras i donne une recompense selon une distribution inconnue.\n",
    "\n",
    "**Dilemme exploration/exploitation** :\n",
    "- Explorer : essayer de nouveaux bras pour estimer leurs distributions\n",
    "- Exploiter : jouer le meilleur bras connu\n",
    "\n",
    "### Approches classiques\n",
    "\n",
    "| Methode | Description |\n",
    "|---------|-------------|\n",
    "| ε-greedy | Exploiter avec proba 1-ε, explorer avec ε |\n",
    "| UCB | Upper Confidence Bound : optimisme face a l'incertitude |\n",
    "| Thompson | Echantillonner selon la posterior des recompenses |\n",
    "| **Gittins** | Solution optimale pour bandits avec discount |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul pratique de l'indice de Gittins\n",
    "\n",
    "L'indice de Gittins est **difficile a calculer** exactement car il necessite de resoudre un probleme d'arret optimal pour chaque etat possible du bras.\n",
    "\n",
    "**Methodes de calcul :**\n",
    "\n",
    "| Methode | Complexite | Precision |\n",
    "|---------|------------|-----------|\n",
    "| Calcul exact (DP) | Exponentielle en horizon | Exacte |\n",
    "| Approximation restart | Polynomiale | Borne inferieure |\n",
    "| Interpolation tabulee | O(1) lookup | Pre-calcule |\n",
    "\n",
    "**Cas particulier : Beta-Bernoulli**\n",
    "\n",
    "Pour un bras avec prior Beta(α, β) et recompenses Bernoulli, l'indice peut etre pre-calcule et stocke dans une table.\n",
    "\n",
    "> **En pratique** : L'indice de Gittins est rarement utilise car UCB et Thompson Sampling sont plus simples a implementer et ont des garanties theoriques similaires pour de nombreux problemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit avec 4 bras, moyennes vraies inconnues\n",
      "Meilleure moyenne : 0,7\n",
      "\n",
      "ε-greedy (ε=0,1):\n",
      "  Recompense totale : 673,6\n",
      "  Regret cumule : 26,4\n",
      "  Tirages par bras : 24, 114, 829, 33\n",
      "\n",
      "UCB1:\n",
      "  Recompense totale : 616,1\n",
      "  Regret cumule : 83,9\n",
      "  Tirages par bras : 54, 166, 719, 61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Bandit multi-bras avec differentes strategies\n",
    "\n",
    "public class MultiArmedBandit\n",
    "{\n",
    "    private double[] _trueMeans;\n",
    "    private Random _rng;\n",
    "    \n",
    "    public int K { get; }\n",
    "    \n",
    "    public MultiArmedBandit(double[] means, int seed = 42)\n",
    "    {\n",
    "        _trueMeans = means;\n",
    "        K = means.Length;\n",
    "        _rng = new Random(seed);\n",
    "    }\n",
    "    \n",
    "    public double Pull(int arm)\n",
    "    {\n",
    "        // Recompense gaussienne autour de la moyenne vraie\n",
    "        double u1 = _rng.NextDouble();\n",
    "        double u2 = _rng.NextDouble();\n",
    "        double z = Math.Sqrt(-2 * Math.Log(u1)) * Math.Cos(2 * Math.PI * u2);\n",
    "        return _trueMeans[arm] + 0.5 * z;\n",
    "    }\n",
    "    \n",
    "    public double OptimalMean => _trueMeans.Max();\n",
    "}\n",
    "\n",
    "// Strategies\n",
    "public interface IBanditStrategy\n",
    "{\n",
    "    int SelectArm(int[] counts, double[] sumRewards);\n",
    "    string Name { get; }\n",
    "}\n",
    "\n",
    "public class EpsilonGreedy : IBanditStrategy\n",
    "{\n",
    "    private double _epsilon;\n",
    "    private Random _rng = new Random();\n",
    "    public string Name => $\"ε-greedy (ε={_epsilon})\";\n",
    "    \n",
    "    public EpsilonGreedy(double epsilon) { _epsilon = epsilon; }\n",
    "    \n",
    "    public int SelectArm(int[] counts, double[] sumRewards)\n",
    "    {\n",
    "        if (_rng.NextDouble() < _epsilon)\n",
    "            return _rng.Next(counts.Length);\n",
    "        \n",
    "        int best = 0;\n",
    "        double bestMean = double.NegativeInfinity;\n",
    "        for (int i = 0; i < counts.Length; i++)\n",
    "        {\n",
    "            double mean = counts[i] > 0 ? sumRewards[i] / counts[i] : 0;\n",
    "            if (mean > bestMean) { bestMean = mean; best = i; }\n",
    "        }\n",
    "        return best;\n",
    "    }\n",
    "}\n",
    "\n",
    "public class UCB : IBanditStrategy\n",
    "{\n",
    "    public string Name => \"UCB1\";\n",
    "    \n",
    "    public int SelectArm(int[] counts, double[] sumRewards)\n",
    "    {\n",
    "        int totalPulls = counts.Sum();\n",
    "        if (totalPulls < counts.Length)\n",
    "            return totalPulls; // Essayer chaque bras une fois\n",
    "        \n",
    "        int best = 0;\n",
    "        double bestUCB = double.NegativeInfinity;\n",
    "        for (int i = 0; i < counts.Length; i++)\n",
    "        {\n",
    "            double mean = sumRewards[i] / counts[i];\n",
    "            double bonus = Math.Sqrt(2 * Math.Log(totalPulls) / counts[i]);\n",
    "            double ucb = mean + bonus;\n",
    "            if (ucb > bestUCB) { bestUCB = ucb; best = i; }\n",
    "        }\n",
    "        return best;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Simulation\n",
    "var bandit = new MultiArmedBandit(new[] { 0.3, 0.5, 0.7, 0.4 });\n",
    "var strategies = new IBanditStrategy[] { new EpsilonGreedy(0.1), new UCB() };\n",
    "\n",
    "Console.WriteLine($\"Bandit avec {bandit.K} bras, moyennes vraies inconnues\");\n",
    "Console.WriteLine($\"Meilleure moyenne : {bandit.OptimalMean}\\n\");\n",
    "\n",
    "int T = 1000;\n",
    "\n",
    "foreach (var strategy in strategies)\n",
    "{\n",
    "    var counts = new int[bandit.K];\n",
    "    var sumRewards = new double[bandit.K];\n",
    "    double totalReward = 0;\n",
    "    double totalRegret = 0;\n",
    "    \n",
    "    for (int t = 0; t < T; t++)\n",
    "    {\n",
    "        int arm = strategy.SelectArm(counts, sumRewards);\n",
    "        double reward = bandit.Pull(arm);\n",
    "        \n",
    "        counts[arm]++;\n",
    "        sumRewards[arm] += reward;\n",
    "        totalReward += reward;\n",
    "        totalRegret += bandit.OptimalMean - reward;\n",
    "    }\n",
    "    \n",
    "    Console.WriteLine($\"{strategy.Name}:\");\n",
    "    Console.WriteLine($\"  Recompense totale : {totalReward:F1}\");\n",
    "    Console.WriteLine($\"  Regret cumule : {totalRegret:F1}\");\n",
    "    Console.WriteLine($\"  Tirages par bras : {string.Join(\", \", counts)}\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation des strategies de bandits\n",
    "\n",
    "**Moyennes vraies (inconnues de l'agent) :** Bras 1=0.3, Bras 2=0.5, **Bras 3=0.7**, Bras 4=0.4\n",
    "\n",
    "**Comparaison des strategies :**\n",
    "\n",
    "| Strategie | Recompense | Regret | Tirages du meilleur bras |\n",
    "|-----------|------------|--------|--------------------------|\n",
    "| ε-greedy (ε=0.1) | 686 | 14 | 884/1000 (88%) |\n",
    "| UCB1 | 616 | 84 | 719/1000 (72%) |\n",
    "\n",
    "**Analyse :**\n",
    "- **ε-greedy** a mieux performe ici car il exploite plus (90% du temps)\n",
    "- **UCB** explore plus systematiquement (tous les bras 54-166 fois)\n",
    "- Le regret cumule est plus faible pour ε-greedy dans ce cas simple\n",
    "\n",
    "**Pourquoi UCB explore-t-il plus ?**\n",
    "- UCB ajoute un **bonus d'incertitude** aux bras peu explores\n",
    "- Un bras tire 54 fois a un gros bonus, meme si sa moyenne est faible\n",
    "- Cette exploration est **garantie optimale asymptotiquement**\n",
    "\n",
    "**Lecon pratique :**\n",
    "- ε-greedy est simple et souvent suffisant pour des problemes simples\n",
    "- UCB brille quand les differences entre bras sont subtiles\n",
    "- Le choix depend du compromis exploration/exploitation souhaite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation : Structure du POMDP\n",
    "\n",
    "**Difference MDP vs POMDP** :\n",
    "\n",
    "```\n",
    "MDP (etat observable):\n",
    "  s_0 ──→ s_1 ──→ s_2 ──→ ...\n",
    "   ↑       ↑       ↑\n",
    "  a_0     a_1     a_2\n",
    "\n",
    "POMDP (etat cache):\n",
    "  s_0 ──→ s_1 ──→ s_2 ──→ ...     (etats caches)\n",
    "   │       │       │\n",
    "   ↓       ↓       ↓\n",
    "  o_0     o_1     o_2              (observations)\n",
    "   ↑       ↑       ↑\n",
    "  a_0 ←── a_1 ←── a_2              (actions basees sur observations)\n",
    "```\n",
    "\n",
    "**Probleme du Tigre - Structure graphique** :\n",
    "\n",
    "```\n",
    "                 ┌─────────────────────────────────────┐\n",
    "                 │      BELIEF STATE b(s)             │\n",
    "                 │  P(tigre_gauche) = 0.5 (initial)   │\n",
    "                 └────────────┬────────────────────────┘\n",
    "                              │\n",
    "              ┌───────────────┼───────────────┐\n",
    "              │               │               │\n",
    "         ┌────▼────┐    ┌─────▼─────┐   ┌─────▼─────┐\n",
    "         │ Ouvrir  │    │  Ouvrir   │   │  Ecouter  │\n",
    "         │ Gauche  │    │  Droite   │   │           │\n",
    "         └────┬────┘    └─────┬─────┘   └─────┬─────┘\n",
    "              │               │               │\n",
    "         EU = -45         EU = -45         EU = -1 + VoI\n",
    "              │               │               │\n",
    "         [terminal]      [terminal]       [continue]\n",
    "                                               │\n",
    "                                    ┌──────────┴──────────┐\n",
    "                                    │                     │\n",
    "                              ┌─────▼─────┐         ┌─────▼─────┐\n",
    "                              │ Bruit_G   │         │ Bruit_D   │\n",
    "                              │ (P=0.85   │         │ (P=0.15   │\n",
    "                              │ si TG)    │         │ si TG)    │\n",
    "                              └─────┬─────┘         └─────┬─────┘\n",
    "                                    │                     │\n",
    "                              b' = 0.85              b' = 0.15\n",
    "```\n",
    "\n",
    "La **valeur d'ecouter** vient de la reduction d'incertitude : apres observation, on peut prendre une meilleure decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Indice de Gittins\n",
    "\n",
    "### Le Theoreme Fondamental (Gittins, 1979)\n",
    "\n",
    "L'indice de Gittins est l'un des resultats les plus elegants de la theorie de la decision. Il resout le probleme du bandit multi-bras de maniere **optimale** et **decomposable**.\n",
    "\n",
    "> **Theoreme** : Pour un bandit multi-bras avec facteur de discount γ, la strategie optimale est de **toujours jouer le bras avec l'indice de Gittins le plus eleve**.\n",
    "\n",
    "### Pourquoi est-ce remarquable ?\n",
    "\n",
    "1. **Reduction de complexite** : Un probleme a K bras interdependants devient K problemes independants\n",
    "2. **Optimalite garantie** : Pas une heuristique, c'est la solution exacte\n",
    "3. **Index-ability** : Chaque bras a un \"prix\" intrinseque\n",
    "\n",
    "### Definition intuitive\n",
    "\n",
    "L'indice de Gittins d'un bras represente :\n",
    "\n",
    "> \"Le prix equivalent certain\" de ce bras - la recompense garantie pour laquelle on serait indifferent entre jouer ce bras et recevoir cette recompense fixe.\n",
    "\n",
    "Plus formellement : c'est le taux d'interet critique au-dessus duquel on prefererait \"encaisser\" plutot que jouer le bras.\n",
    "\n",
    "### Calcul formel\n",
    "\n",
    "L'indice est la solution d'un probleme d'arret optimal :\n",
    "\n",
    "$$G(s) = \\sup_{\\tau \\geq 1} \\frac{E\\left[\\sum_{t=0}^{\\tau-1} \\gamma^t R_t \\mid s_0 = s\\right]}{E\\left[\\sum_{t=0}^{\\tau-1} \\gamma^t \\mid s_0 = s\\right]}$$\n",
    "\n",
    "- Le numerateur est la recompense totale actualisee jusqu'au temps d'arret τ\n",
    "- Le denominateur est le \"temps effectif\" actualise\n",
    "- On maximise sur tous les temps d'arret possibles\n",
    "\n",
    "### Implications pratiques\n",
    "\n",
    "| Aspect | Consequence |\n",
    "|--------|-------------|\n",
    "| **Exploration** | Un bras incertain a un indice eleve (optimisme) |\n",
    "| **Exploitation** | Un bras connu avec haute moyenne a un indice eleve |\n",
    "| **Compromis** | L'indice equilibre automatiquement exploration/exploitation |\n",
    "\n",
    "### Limitation\n",
    "\n",
    "Le theoreme de Gittins s'applique sous des hypotheses specifiques :\n",
    "- Discount γ < 1 (pas pour horizon fini sans discount)\n",
    "- Les bras sont independants (pas d'interactions)\n",
    "- Une seule action par etape (pas de contraintes de ressources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. POMDPs : MDPs Partiellement Observables\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Dans un MDP standard, l'agent connait l'etat exact du monde a chaque instant. En pratique, c'est souvent **irrealiste** :\n",
    "\n",
    "- Un robot ne voit pas a travers les murs\n",
    "- Un medecin ne connait pas la vraie maladie, seulement les symptomes\n",
    "- Un joueur de poker ne voit pas les cartes adverses\n",
    "\n",
    "### Extension du MDP\n",
    "\n",
    "Dans un POMDP, l'agent **ne connait pas l'etat exact**. Il recoit des **observations** qui sont des indicateurs bruitees de l'etat reel.\n",
    "\n",
    "### Definition formelle\n",
    "\n",
    "Un POMDP est un tuple (S, A, P, R, O, Ω, γ) :\n",
    "\n",
    "| Element | Description |\n",
    "|---------|-------------|\n",
    "| S, A, P, R, γ | Comme MDP (etats, actions, transitions, recompenses, discount) |\n",
    "| **O** | Ensemble des observations possibles |\n",
    "| **Ω(o\\|s', a)** | Modele de capteur : P(observation \\| nouvel etat, action) |\n",
    "\n",
    "### Belief State : La Cle des POMDPs\n",
    "\n",
    "Puisque l'agent ne connait pas l'etat, il doit maintenir une **distribution de croyance** (belief) b(s) sur tous les etats possibles.\n",
    "\n",
    "> b(s) = P(etat = s | historique des observations et actions)\n",
    "\n",
    "Le belief state est une **statistique suffisante** : il resume toute l'information pertinente de l'historique.\n",
    "\n",
    "### Mise a jour du Belief State\n",
    "\n",
    "Apres avoir fait action a et observe o, le nouveau belief b' est :\n",
    "\n",
    "$$b'(s') = \\eta \\cdot \\Omega(o|s', a) \\sum_s P(s'|s, a) b(s)$$\n",
    "\n",
    "ou η est une constante de normalisation.\n",
    "\n",
    "C'est exactement une **mise a jour bayesienne** ! Le POMDP est donc naturellement lie a l'inference probabiliste.\n",
    "\n",
    "### Plans conditionnels\n",
    "\n",
    "La politique d'un POMDP n'est pas s → a comme dans un MDP, mais b → a ou equivalemment un **arbre de decision** :\n",
    "\n",
    "```\n",
    "        a1           <- Action initiale\n",
    "       /  \\\n",
    "     o1    o2        <- Observations possibles\n",
    "     |      |\n",
    "    a2     a3        <- Actions conditionnelles\n",
    "   / \\    / \\\n",
    "  o1 o2  o1 o2\n",
    "  ...\n",
    "```\n",
    "\n",
    "### Complexite\n",
    "\n",
    "Les POMDPs sont **PSPACE-complete** a resoudre exactement. En pratique, on utilise :\n",
    "- Point-Based Value Iteration (PBVI)\n",
    "- SARSOP\n",
    "- Monte Carlo Tree Search (MCTS)\n",
    "- Ou on approxime par des MDP avec belief states discrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation : Evolution du Belief State dans le Scenario de Maintenance\n",
    "\n",
    "Le graphique ci-dessous montre comment les observations \"anormales\" successives modifient la distribution de croyance :\n",
    "\n",
    "```\n",
    "                          Evolution du Belief State\n",
    "    \n",
    "    P(Bon)         ███████████████████████████████████░░░░░  95%\n",
    "    t=0 (initial)  Degrade ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  4%\n",
    "                   Defaill █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  1%\n",
    "                          \n",
    "    P(Bon)         ███████████████████████████████████████░░  96% (obs: Normal)\n",
    "    t=1            Degrade ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  4%\n",
    "                   Defaill ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  0%\n",
    "                          \n",
    "    P(Bon)         ████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  30% (obs: Anormal)\n",
    "    t=2            Degrade █████████████████████░░░░░░░░░░░░░░  53%\n",
    "                   Defaill ███████░░░░░░░░░░░░░░░░░░░░░░░░░░░  18%\n",
    "                          \n",
    "    P(Bon)         █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   2% (obs: Anormal)\n",
    "    t=3            Degrade ██████████████████████░░░░░░░░░░░░░  56%\n",
    "                   Defaill █████████████████░░░░░░░░░░░░░░░░░░  42%\n",
    "    \n",
    "                   ─────────────────────────────────────────►\n",
    "                   Meilleure action :\n",
    "                   t=0-1: Continuer (EU≈96)\n",
    "                   t=2-3: Maintenance (EU≈23-27)\n",
    "```\n",
    "\n",
    "**Observations cles** :\n",
    "1. Une observation \"Normal\" renforce P(Bon) (capteur fiable si systeme ok)\n",
    "2. Deux observations \"Anormal\" font basculer vers P(Degrade) majoritaire\n",
    "3. La decision optimale passe de \"Continuer\" a \"Maintenance\" quand l'incertitude sur l'etat est resolue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POMDP : Probleme du Tigre ===\n",
      "\n",
      "Deux portes : gauche (G) et droite (D)\n",
      "Un tigre est cache derriere l'une des portes.\n",
      "Actions : Ouvrir G, Ouvrir D, Ecouter\n",
      "\n",
      "Belief initial : P(tigre gauche) = 50 %\n",
      "\n",
      "Utilites esperees :\n",
      "  E[U(ouvrir gauche) | b=50 %] = -45,0\n",
      "  E[U(ouvrir droite) | b=50 %] = -45,0\n",
      "  Ecouter : cout immediat = -1, mais reduit l'incertitude\n",
      "\n",
      "Simulation : 3 ecoutes donnent 'bruit gauche'\n",
      "\n",
      "Apres observation 1 : P(tigre gauche) = 85,0 %\n",
      "Apres observation 2 : P(tigre gauche) = 97,0 %\n",
      "Apres observation 3 : P(tigre gauche) = 99,5 %\n",
      "\n",
      "E[U(ouvrir gauche)] = -99,4\n",
      "E[U(ouvrir droite)] = 9,4\n",
      "\n",
      "=> Decision : OUVRIR DROITE (tigre probablement a gauche)\n"
     ]
    }
   ],
   "source": [
    "// POMDP simple : Tigre derriere une porte\n",
    "\n",
    "Console.WriteLine(\"=== POMDP : Probleme du Tigre ===\\n\");\n",
    "Console.WriteLine(\"Deux portes : gauche (G) et droite (D)\");\n",
    "Console.WriteLine(\"Un tigre est cache derriere l'une des portes.\");\n",
    "Console.WriteLine(\"Actions : Ouvrir G, Ouvrir D, Ecouter\\n\");\n",
    "\n",
    "// Etats : tigre gauche (TG), tigre droite (TD)\n",
    "// Actions : ouvrir_gauche, ouvrir_droite, ecouter\n",
    "// Observations (si ecouter) : bruit_gauche, bruit_droite\n",
    "\n",
    "double pCorrectHearing = 0.85; // P(bruit_gauche | tigre_gauche)\n",
    "double rewardTresor = 10;\n",
    "double rewardTigre = -100;\n",
    "double costEcoute = -1;\n",
    "\n",
    "// Belief state : b = P(tigre_gauche)\n",
    "double b = 0.5; // Prior uniforme\n",
    "\n",
    "Console.WriteLine($\"Belief initial : P(tigre gauche) = {b:P0}\\n\");\n",
    "\n",
    "// Fonction de valeur pour chaque action\n",
    "double EU_ouvrirGauche(double belief) => \n",
    "    belief * rewardTigre + (1 - belief) * rewardTresor;\n",
    "\n",
    "double EU_ouvrirDroite(double belief) => \n",
    "    belief * rewardTresor + (1 - belief) * rewardTigre;\n",
    "\n",
    "// Pour ecouter, on doit calculer la valeur esperee apres observation\n",
    "// (simplifie ici)\n",
    "\n",
    "Console.WriteLine(\"Utilites esperees :\");\n",
    "Console.WriteLine($\"  E[U(ouvrir gauche) | b={b:P0}] = {EU_ouvrirGauche(b):F1}\");\n",
    "Console.WriteLine($\"  E[U(ouvrir droite) | b={b:P0}] = {EU_ouvrirDroite(b):F1}\");\n",
    "Console.WriteLine($\"  Ecouter : cout immediat = {costEcoute}, mais reduit l'incertitude\\n\");\n",
    "\n",
    "// Simuler une sequence d'observations\n",
    "Console.WriteLine(\"Simulation : 3 ecoutes donnent 'bruit gauche'\\n\");\n",
    "\n",
    "for (int i = 0; i < 3; i++)\n",
    "{\n",
    "    // Mise a jour bayesienne apres observation \"bruit gauche\"\n",
    "    // P(TG|bruit_g) = P(bruit_g|TG) * P(TG) / P(bruit_g)\n",
    "    double pBruitGauche = b * pCorrectHearing + (1 - b) * (1 - pCorrectHearing);\n",
    "    b = (pCorrectHearing * b) / pBruitGauche;\n",
    "    \n",
    "    Console.WriteLine($\"Apres observation {i+1} : P(tigre gauche) = {b:P1}\");\n",
    "}\n",
    "\n",
    "Console.WriteLine();\n",
    "Console.WriteLine($\"E[U(ouvrir gauche)] = {EU_ouvrirGauche(b):F1}\");\n",
    "Console.WriteLine($\"E[U(ouvrir droite)] = {EU_ouvrirDroite(b):F1}\");\n",
    "Console.WriteLine();\n",
    "Console.WriteLine($\"=> Decision : OUVRIR DROITE (tigre probablement a gauche)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation du probleme du tigre (POMDP)\n",
    "\n",
    "**Evolution du belief state :**\n",
    "\n",
    "| Etape | P(tigre gauche) | Decision optimale |\n",
    "|-------|-----------------|-------------------|\n",
    "| Initial | 50% | Indecis (EU = -45 pour les deux portes) |\n",
    "| Apres ecoute 1 | 85% | Encore incertain |\n",
    "| Apres ecoute 2 | 97% | Tres confiant |\n",
    "| Apres ecoute 3 | 99.5% | Quasi-certain |\n",
    "\n",
    "**Pourquoi ecouter plusieurs fois ?**\n",
    "- Chaque ecoute coute -1 mais reduit l'incertitude\n",
    "- Apres 3 ecoutes, EU(ouvrir droite) = +9.4 >> EU(ouvrir gauche) = -99.4\n",
    "- Le cout total d'ecoute (3 x -1 = -3) est tres inferieur au gain de certitude\n",
    "\n",
    "**Mise a jour bayesienne :**\n",
    "- P(tigre gauche | bruit gauche) = P(bruit gauche | tigre gauche) x P(tigre gauche) / P(bruit gauche)\n",
    "- Avec P(bruit correct | position) = 85%, chaque observation \"pousse\" le belief\n",
    "\n",
    "**Point cle POMDP :**\n",
    "- L'agent ne connait pas l'etat exact (position du tigre)\n",
    "- Il maintient une **distribution de croyance** (belief)\n",
    "- La decision optimale depend du belief, pas de l'etat reel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Factor Graph du Modele de Maintenance ===\n",
      "\n",
      "Compiling model...done.\n",
      "Posterior apres observation 'anormal': Discrete(0,1296 0,5185 0,3519)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"margin: 10px 0; padding: 10px; border: 1px solid #ddd; border-radius: 5px; background: #fafafa;\">\n",
       "    <div style=\"font-weight: bold; margin-bottom: 8px; color: #333;\">Model_MaintenancePOMDP_01_31_26_20_19_06_55.svg</div>\n",
       "    <div style=\"max-width: 800px; overflow: auto;\">\n",
       "        <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 14.1.2 (20260124.0452)\r\n",
       " -->\r\n",
       "<!-- Title: Model Pages: 1 -->\r\n",
       "<svg width=\"295pt\" height=\"289pt\"\r\n",
       " viewBox=\"0.00 0.00 295.00 289.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 285.25)\">\r\n",
       "<title>Model</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-285.25 290.75,-285.25 290.75,4 -4,4\"/>\r\n",
       "<!-- node0 -->\r\n",
       "<g id=\"node1\" class=\"node\">\r\n",
       "<title>node0</title>\r\n",
       "<path fill=\"none\" stroke=\"none\" d=\"M46.75,-281.25C46.75,-281.25 12,-281.25 12,-281.25 6,-281.25 0,-275.25 0,-269.25 0,-269.25 0,-257.25 0,-257.25 0,-251.25 6,-245.25 12,-245.25 12,-245.25 46.75,-245.25 46.75,-245.25 52.75,-245.25 58.75,-251.25 58.75,-257.25 58.75,-257.25 58.75,-269.25 58.75,-269.25 58.75,-275.25 52.75,-281.25 46.75,-281.25\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"29.38\" y=\"-259.95\" font-family=\"Times New Roman,serif\" font-size=\"9.00\" fill=\"#000000\">vDiscrete24</text>\r\n",
       "</g>\r\n",
       "<!-- node1 -->\r\n",
       "<g id=\"node2\" class=\"node\">\r\n",
       "<title>node1</title>\r\n",
       "<path fill=\"#000000\" stroke=\"black\" d=\"M44.38,-199.5C44.38,-199.5 14.38,-199.5 14.38,-199.5 8.38,-199.5 2.38,-193.5 2.38,-187.5 2.38,-187.5 2.38,-175.5 2.38,-175.5 2.38,-169.5 8.38,-163.5 14.38,-163.5 14.38,-163.5 44.38,-163.5 44.38,-163.5 50.38,-163.5 56.38,-169.5 56.38,-175.5 56.38,-175.5 56.38,-187.5 56.38,-187.5 56.38,-193.5 50.38,-199.5 44.38,-199.5\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"29.38\" y=\"-178.78\" font-family=\"Times New Roman,serif\" font-size=\"8.00\" fill=\"#ffffff\">Random</text>\r\n",
       "</g>\r\n",
       "<!-- node0&#45;&gt;node1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\">\r\n",
       "<title>node0&#45;&gt;node1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M29.38,-245.34C29.38,-235.43 29.38,-222.65 29.38,-211.21\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"32.88,-211.45 29.38,-201.45 25.88,-211.45 32.88,-211.45\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"35\" y=\"-219.65\" font-family=\"Times New Roman,serif\" font-size=\"8.00\" fill=\"#d3d3d3\">dist</text>\r\n",
       "</g>\r\n",
       "<!-- node2 -->\r\n",
       "<g id=\"node3\" class=\"node\">\r\n",
       "<title>node2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.38,-117.75C48.38,-117.75 18.38,-117.75 18.38,-117.75 12.38,-117.75 6.38,-111.75 6.38,-105.75 6.38,-105.75 6.38,-93.75 6.38,-93.75 6.38,-87.75 12.38,-81.75 18.38,-81.75 18.38,-81.75 48.38,-81.75 48.38,-81.75 54.38,-81.75 60.38,-87.75 60.38,-93.75 60.38,-93.75 60.38,-105.75 60.38,-105.75 60.38,-111.75 54.38,-117.75 48.38,-117.75\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"33.38\" y=\"-95.88\" font-family=\"Times New Roman,serif\" font-size=\"10.00\" fill=\"#0000ff\">etatPrior</text>\r\n",
       "</g>\r\n",
       "<!-- node1&#45;&gt;node2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\">\r\n",
       "<title>node1&#45;&gt;node2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M30.24,-163.2C30.74,-153.27 31.38,-140.56 31.95,-129.2\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"35.43,-129.7 32.43,-119.53 28.44,-129.34 35.43,-129.7\"/>\r\n",
       "</g>\r\n",
       "<!-- node5 -->\r\n",
       "<g id=\"node6\" class=\"node\">\r\n",
       "<title>node5</title>\r\n",
       "<path fill=\"none\" stroke=\"none\" d=\"M157.38,-36C157.38,-36 127.38,-36 127.38,-36 121.38,-36 115.38,-30 115.38,-24 115.38,-24 115.38,-12 115.38,-12 115.38,-6 121.38,0 127.38,0 127.38,0 157.38,0 157.38,0 163.38,0 169.38,-6 169.38,-12 169.38,-12 169.38,-24 169.38,-24 169.38,-30 163.38,-36 157.38,-36\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"142.38\" y=\"-14.12\" font-family=\"Times New Roman,serif\" font-size=\"10.00\" fill=\"#0000ff\">1</text>\r\n",
       "</g>\r\n",
       "<!-- node2&#45;&gt;node5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\">\r\n",
       "<title>node2&#45;&gt;node5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.19,-81.27C64.51,-72.8 77.28,-62.62 89.12,-54 94.51,-50.08 100.31,-46.07 106.03,-42.22\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"107.83,-45.23 114.23,-36.79 103.96,-39.4 107.83,-45.23\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"103.75\" y=\"-56.15\" font-family=\"Times New Roman,serif\" font-size=\"8.00\" fill=\"#d3d3d3\">condition</text>\r\n",
       "</g>\r\n",
       "<!-- node3 -->\r\n",
       "<g id=\"node4\" class=\"node\">\r\n",
       "<title>node3</title>\r\n",
       "<path fill=\"none\" stroke=\"none\" d=\"M120.75,-199.5C120.75,-199.5 86,-199.5 86,-199.5 80,-199.5 74,-193.5 74,-187.5 74,-187.5 74,-175.5 74,-175.5 74,-169.5 80,-163.5 86,-163.5 86,-163.5 120.75,-163.5 120.75,-163.5 126.75,-163.5 132.75,-169.5 132.75,-175.5 132.75,-175.5 132.75,-187.5 132.75,-187.5 132.75,-193.5 126.75,-199.5 120.75,-199.5\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"103.38\" y=\"-178.2\" font-family=\"Times New Roman,serif\" font-size=\"9.00\" fill=\"#000000\">vDiscrete25</text>\r\n",
       "</g>\r\n",
       "<!-- node4 -->\r\n",
       "<g id=\"node5\" class=\"node\">\r\n",
       "<title>node4</title>\r\n",
       "<path fill=\"#000000\" stroke=\"black\" d=\"M121.38,-117.75C121.38,-117.75 91.38,-117.75 91.38,-117.75 85.38,-117.75 79.38,-111.75 79.38,-105.75 79.38,-105.75 79.38,-93.75 79.38,-93.75 79.38,-87.75 85.38,-81.75 91.38,-81.75 91.38,-81.75 121.38,-81.75 121.38,-81.75 127.38,-81.75 133.38,-87.75 133.38,-93.75 133.38,-93.75 133.38,-105.75 133.38,-105.75 133.38,-111.75 127.38,-117.75 121.38,-117.75\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"106.38\" y=\"-97.03\" font-family=\"Times New Roman,serif\" font-size=\"8.00\" fill=\"#ffffff\">Random</text>\r\n",
       "</g>\r\n",
       "<!-- node3&#45;&gt;node4 -->\r\n",
       "<g id=\"edge3\" class=\"edge\">\r\n",
       "<title>node3&#45;&gt;node4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M104.01,-163.59C104.38,-153.68 104.86,-140.9 105.3,-129.46\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"108.78,-129.83 105.66,-119.7 101.79,-129.56 108.78,-129.83\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"110.68\" y=\"-137.9\" font-family=\"Times New Roman,serif\" font-size=\"8.00\" fill=\"#d3d3d3\">dist</text>\r\n",
       "</g>\r\n",
       "<!-- node4&#45;&gt;node5 -->\r\n",
       "<g id=\"edge4\" class=\"edge\">\r\n",
       "<title>node4&#45;&gt;node5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M114.19,-81.45C118.84,-71.13 124.85,-57.82 130.13,-46.14\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"133.16,-47.93 134.08,-37.38 126.78,-45.05 133.16,-47.93\"/>\r\n",
       "</g>\r\n",
       "<!-- node6 -->\r\n",
       "<g id=\"node7\" class=\"node\">\r\n",
       "<title>node6</title>\r\n",
       "<path fill=\"none\" stroke=\"none\" d=\"M197.75,-199.5C197.75,-199.5 163,-199.5 163,-199.5 157,-199.5 151,-193.5 151,-187.5 151,-187.5 151,-175.5 151,-175.5 151,-169.5 157,-163.5 163,-163.5 163,-163.5 197.75,-163.5 197.75,-163.5 203.75,-163.5 209.75,-169.5 209.75,-175.5 209.75,-175.5 209.75,-187.5 209.75,-187.5 209.75,-193.5 203.75,-199.5 197.75,-199.5\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"180.38\" y=\"-178.2\" font-family=\"Times New Roman,serif\" font-size=\"9.00\" fill=\"#000000\">vDiscrete26</text>\r\n",
       "</g>\r\n",
       "<!-- node7 -->\r\n",
       "<g id=\"node8\" class=\"node\">\r\n",
       "<title>node7</title>\r\n",
       "<path fill=\"#000000\" stroke=\"black\" d=\"M193.38,-117.75C193.38,-117.75 163.38,-117.75 163.38,-117.75 157.38,-117.75 151.38,-111.75 151.38,-105.75 151.38,-105.75 151.38,-93.75 151.38,-93.75 151.38,-87.75 157.38,-81.75 163.38,-81.75 163.38,-81.75 193.38,-81.75 193.38,-81.75 199.38,-81.75 205.38,-87.75 205.38,-93.75 205.38,-93.75 205.38,-105.75 205.38,-105.75 205.38,-111.75 199.38,-117.75 193.38,-117.75\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"178.38\" y=\"-97.03\" font-family=\"Times New Roman,serif\" font-size=\"8.00\" fill=\"#ffffff\">Random</text>\r\n",
       "</g>\r\n",
       "<!-- node6&#45;&gt;node7 -->\r\n",
       "<g id=\"edge6\" class=\"edge\">\r\n",
       "<title>node6&#45;&gt;node7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M179.95,-163.59C179.7,-153.68 179.38,-140.9 179.09,-129.46\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"182.6,-129.61 178.85,-119.7 175.6,-129.79 182.6,-129.61\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"185.12\" y=\"-137.9\" font-family=\"Times New Roman,serif\" font-size=\"8.00\" fill=\"#d3d3d3\">dist</text>\r\n",
       "</g>\r\n",
       "<!-- node7&#45;&gt;node5 -->\r\n",
       "<g id=\"edge7\" class=\"edge\">\r\n",
       "<title>node7&#45;&gt;node5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.56,-81.45C165.91,-71.13 159.9,-57.82 154.62,-46.14\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"157.97,-45.05 150.67,-37.38 151.59,-47.93 157.97,-45.05\"/>\r\n",
       "</g>\r\n",
       "<!-- node8 -->\r\n",
       "<g id=\"node9\" class=\"node\">\r\n",
       "<title>node8</title>\r\n",
       "<path fill=\"none\" stroke=\"none\" d=\"M274.75,-199.5C274.75,-199.5 240,-199.5 240,-199.5 234,-199.5 228,-193.5 228,-187.5 228,-187.5 228,-175.5 228,-175.5 228,-169.5 234,-163.5 240,-163.5 240,-163.5 274.75,-163.5 274.75,-163.5 280.75,-163.5 286.75,-169.5 286.75,-175.5 286.75,-175.5 286.75,-187.5 286.75,-187.5 286.75,-193.5 280.75,-199.5 274.75,-199.5\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"257.38\" y=\"-178.2\" font-family=\"Times New Roman,serif\" font-size=\"9.00\" fill=\"#000000\">vDiscrete27</text>\r\n",
       "</g>\r\n",
       "<!-- node9 -->\r\n",
       "<g id=\"node10\" class=\"node\">\r\n",
       "<title>node9</title>\r\n",
       "<path fill=\"#000000\" stroke=\"black\" d=\"M269.38,-117.75C269.38,-117.75 239.38,-117.75 239.38,-117.75 233.38,-117.75 227.38,-111.75 227.38,-105.75 227.38,-105.75 227.38,-93.75 227.38,-93.75 227.38,-87.75 233.38,-81.75 239.38,-81.75 239.38,-81.75 269.38,-81.75 269.38,-81.75 275.38,-81.75 281.38,-87.75 281.38,-93.75 281.38,-93.75 281.38,-105.75 281.38,-105.75 281.38,-111.75 275.38,-117.75 269.38,-117.75\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"254.38\" y=\"-97.03\" font-family=\"Times New Roman,serif\" font-size=\"8.00\" fill=\"#ffffff\">Random</text>\r\n",
       "</g>\r\n",
       "<!-- node8&#45;&gt;node9 -->\r\n",
       "<g id=\"edge8\" class=\"edge\">\r\n",
       "<title>node8&#45;&gt;node9</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M256.74,-163.59C256.37,-153.68 255.89,-140.9 255.45,-129.46\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"258.96,-129.56 255.09,-119.7 251.97,-129.83 258.96,-129.56\"/>\r\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"261.68\" y=\"-137.9\" font-family=\"Times New Roman,serif\" font-size=\"8.00\" fill=\"#d3d3d3\">dist</text>\r\n",
       "</g>\r\n",
       "<!-- node9&#45;&gt;node5 -->\r\n",
       "<g id=\"edge9\" class=\"edge\">\r\n",
       "<title>node9&#45;&gt;node5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.07,-81.45C214.11,-70.08 193.03,-55.07 175.54,-42.61\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"177.65,-39.82 167.48,-36.87 173.59,-45.53 177.65,-39.82\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n",
       "\n",
       "    </div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "warning CS1701: En supposant que la référence d'assembly 'Microsoft.AspNetCore.Html.Abstractions, Version=2.3.0.0, Culture=neutral, PublicKeyToken=adb9793829ddae60' utilisée par 'Microsoft.DotNet.Interactive' correspond à l'identité 'Microsoft.AspNetCore.Html.Abstractions, Version=10.0.0.0, Culture=neutral, PublicKeyToken=adb9793829ddae60' de 'Microsoft.AspNetCore.Html.Abstractions', il se peut que vous deviez fournir une stratégie runtime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Visualisation du factor graph pour le modele de maintenance predictive\n",
    "// En activant ShowFactorGraph, Infer.NET genere un fichier .gv\n",
    "\n",
    "Console.WriteLine(\"=== Factor Graph du Modele de Maintenance ===\\n\");\n",
    "\n",
    "// Creer un modele simple pour visualiser la structure\n",
    "var engineViz = new InferenceEngine();\n",
    "engineViz.Compiler.CompilerChoice = Microsoft.ML.Probabilistic.Compiler.CompilerChoice.Roslyn;\n",
    "\n",
    "if (ShowFactorGraph)\n",
    "{\n",
    "    engineViz.ShowFactorGraph = true;\n",
    "    engineViz.ModelName = \"Model_MaintenancePOMDP\";\n",
    "}\n",
    "\n",
    "// Modele : Etat -> Observation\n",
    "Range vizEtatRange = new Range(3).Named(\"vizEtatRange\");\n",
    "var vizPrior = Variable.Discrete(new double[] { 0.7, 0.2, 0.1 }).Named(\"etatPrior\");\n",
    "vizPrior.SetValueRange(vizEtatRange);\n",
    "\n",
    "var vizObs = Variable.New<int>().Named(\"capteur\");\n",
    "vizObs.SetValueRange(new Range(2));\n",
    "\n",
    "// Structure conditionnelle pour le modele d'observation\n",
    "using (Variable.Case(vizPrior, 0))\n",
    "    vizObs.SetTo(Variable.Discrete(0.95, 0.05));\n",
    "using (Variable.Case(vizPrior, 1))\n",
    "    vizObs.SetTo(Variable.Discrete(0.30, 0.70));\n",
    "using (Variable.Case(vizPrior, 2))\n",
    "    vizObs.SetTo(Variable.Discrete(0.05, 0.95));\n",
    "\n",
    "vizObs.ObservedValue = 1; // Observer \"anormal\"\n",
    "\n",
    "var vizPosterior = engineViz.Infer<Discrete>(vizPrior);\n",
    "Console.WriteLine($\"Posterior apres observation 'anormal': {vizPosterior}\");\n",
    "\n",
    "// Afficher le factor graph si disponible\n",
    "if (ShowFactorGraph && FactorGraphHelper.IsGraphvizAvailable())\n",
    "{\n",
    "    display(HTML(FactorGraphHelper.GetLatestFactorGraphHtml()));\n",
    "}\n",
    "else if (ShowFactorGraph)\n",
    "{\n",
    "    Console.WriteLine(\"\\nNote: Installez Graphviz pour visualiser le factor graph.\");\n",
    "    Console.WriteLine(\"Fichier .gv genere dans le repertoire courant.\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10bis. Belief State Updates avec Infer.NET\n",
    "\n",
    "L'exemple precedent calculait les mises a jour du belief \"a la main\". Avec Infer.NET, on peut automatiser cette inference bayesienne, ce qui devient precieux pour des modeles plus complexes.\n",
    "\n",
    "### Avantages d'Infer.NET pour les POMDPs\n",
    "\n",
    "| Aspect | Calcul manuel | Infer.NET |\n",
    "|--------|---------------|-----------|\n",
    "| **Formule** | Ecrire Bayes explicitement | Automatique |\n",
    "| **Etats multiples** | Combinatoire | Gere par le moteur |\n",
    "| **Observations multiples** | Produits manuels | Modelisation naturelle |\n",
    "| **Incertitude sur parametres** | Tres complexe | Hyper-priors possibles |\n",
    "\n",
    "### Scenario : Maintenance predictive\n",
    "\n",
    "Un systeme peut etre dans 3 etats : {Bon, Degrade, Defaillant}.\n",
    "A chaque pas de temps :\n",
    "- L'etat peut se degrader (Markov)\n",
    "- On observe un signal de capteur (bruite)\n",
    "- On decide : continuer, maintenance preventive, ou remplacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vision d'ensemble : De la decision one-shot aux systemes adaptatifs\n",
    "\n",
    "Ce notebook conclut la serie Decision Theory en faisant le pont vers le **Reinforcement Learning** :\n",
    "\n",
    "```\n",
    "Decisions One-Shot (Notebooks 14-19)\n",
    "    |\n",
    "    +-- Utilite et aversion au risque (14-15)\n",
    "    +-- Attributs multiples (16)\n",
    "    +-- Diagrammes d'influence (17)\n",
    "    +-- Valeur de l'information (18)\n",
    "    +-- Robustesse et experts (19)\n",
    "    |\n",
    "    v\n",
    "Decisions Sequentielles (Notebook 20)\n",
    "    |\n",
    "    +-- MDPs : Etats connus, actions multiples\n",
    "    +-- Bellman : Decomposition recursive\n",
    "    +-- Bandits : Exploration/exploitation\n",
    "    +-- POMDPs : Etats partiellement observables\n",
    "    |\n",
    "    v\n",
    "Reinforcement Learning (Serie RL/)\n",
    "    |\n",
    "    +-- Q-Learning : Apprendre sans modele\n",
    "    +-- Deep RL : Approximation neuronale\n",
    "    +-- Policy Gradient : Optimisation directe\n",
    "```\n",
    "\n",
    "**Message cle :**\n",
    "> La theorie de la decision fournit les **fondements mathematiques** (utilite, rationalite, Bellman) sur lesquels reposent les algorithmes modernes de RL. Comprendre ces concepts permet de mieux concevoir, debugger et ameliorer les systemes d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Belief State Updates avec Infer.NET ===\n",
      "\n",
      "Scenario : Maintenance predictive d'un systeme\n",
      "\n",
      "Belief initial :\n",
      "  P(Bon) = 95,0 %\n",
      "  P(Degrade) = 4,0 %\n",
      "  P(Defaillant) = 1,0 %\n",
      "\n",
      "=== Pas de temps 1 ===\n",
      "Apres prediction (transition) :\n",
      "  P(Bon) = 85,5 %\n",
      "  P(Degrade) = 11,0 %\n",
      "  P(Defaillant) = 3,5 %\n",
      "Compiling model...done.\n",
      "Observation : Normal\n",
      "Apres correction (Bayes via Infer.NET) :\n",
      "  P(Bon       ) = 95,9 % ############################\n",
      "  P(Degrade   ) = 3,9 % #\n",
      "  P(Defaillant) = 0,2 % \n",
      "Utilites esperees :\n",
      "  E[U(Continuer   )] =    96,8\n",
      "  E[U(Maintenance )] =   -16,2\n",
      "  E[U(Remplacer   )] =   -99,7\n",
      "=> Decision : Continuer\n",
      "\n",
      "=== Pas de temps 2 ===\n",
      "Apres prediction (transition) :\n",
      "  P(Bon) = 86,3 %\n",
      "  P(Degrade) = 11,0 %\n",
      "  P(Defaillant) = 2,7 %\n",
      "Compiling model...done.\n",
      "Observation : Anormal\n",
      "Apres correction (Bayes via Infer.NET) :\n",
      "  P(Bon       ) = 29,6 % ########\n",
      "  P(Degrade   ) = 52,7 % ###############\n",
      "  P(Defaillant) = 17,7 % #####\n",
      "Utilites esperees :\n",
      "  E[U(Continuer   )] =   -32,3\n",
      "  E[U(Maintenance )] =    27,4\n",
      "  E[U(Remplacer   )] =   -73,5\n",
      "=> Decision : Maintenance\n",
      "\n",
      "=== Pas de temps 3 ===\n",
      "Apres prediction (transition) :\n",
      "  P(Bon) = 26,6 %\n",
      "  P(Degrade) = 47,2 %\n",
      "  P(Defaillant) = 26,2 %\n",
      "Compiling model...done.\n",
      "Observation : Anormal\n",
      "Apres correction (Bayes via Infer.NET) :\n",
      "  P(Bon       ) = 2,2 % \n",
      "  P(Degrade   ) = 55,8 % ################\n",
      "  P(Defaillant) = 42,0 % ############\n",
      "Utilites esperees :\n",
      "  E[U(Continuer   )] =  -179,7\n",
      "  E[U(Maintenance )] =    23,2\n",
      "  E[U(Remplacer   )] =   -37,1\n",
      "=> Decision : Maintenance\n",
      "\n",
      "=== Resume ===\n",
      "Les observations 'Anormal' successives ont fait augmenter P(Degrade),\n",
      "declenchant le passage de 'Continuer' a 'Maintenance' ou 'Remplacer'.\n",
      "\n",
      "C'est le principe de la maintenance predictive bayesienne !\n"
     ]
    }
   ],
   "source": [
    "// Belief State Updates avec Infer.NET : Maintenance Predictive\n",
    "\n",
    "using Microsoft.ML.Probabilistic.Math;\n",
    "\n",
    "Console.WriteLine(\"=== Belief State Updates avec Infer.NET ===\\n\");\n",
    "Console.WriteLine(\"Scenario : Maintenance predictive d'un systeme\\n\");\n",
    "\n",
    "// Definition des etats et observations\n",
    "int nEtats = 3;\n",
    "Range etatRange = new Range(nEtats).Named(\"etatRange\");\n",
    "string[] etatsNom = { \"Bon\", \"Degrade\", \"Defaillant\" };\n",
    "\n",
    "// Matrice de transition (degradation naturelle)\n",
    "double[,] transMatrix = {\n",
    "    // vers:    Bon    Degrade  Defaillant\n",
    "    /* Bon */    { 0.90,  0.08,    0.02 },\n",
    "    /* Deg */    { 0.00,  0.85,    0.15 },\n",
    "    /* Def */    { 0.00,  0.00,    1.00 }  // Absorbant\n",
    "};\n",
    "\n",
    "// Modele d'observation : capteur de vibration (0=normal, 1=anormal)\n",
    "double[,] obsMatrix = {\n",
    "    // P(obs | etat)  Normal  Anormal\n",
    "    /* Bon */       { 0.95,   0.05 },\n",
    "    /* Deg */       { 0.30,   0.70 },\n",
    "    /* Def */       { 0.05,   0.95 }\n",
    "};\n",
    "\n",
    "// Utilites des decisions\n",
    "double[,] utilities = {\n",
    "    //                  Bon     Degrade  Defaillant\n",
    "    /* Continuer */   { 100,    50,      -500 },  // Risque si defaillant\n",
    "    /* Maintenance */ { -20,    80,       -50 },  // Preventif, moins de gain si bon\n",
    "    /* Remplacer */   { -100,  -100,       50 }   // Couteux mais resout defaillance\n",
    "};\n",
    "string[] actionsNom = { \"Continuer\", \"Maintenance\", \"Remplacer\" };\n",
    "\n",
    "// === Simulation avec Infer.NET ===\n",
    "// Observations sequentielles : Normal, Anormal, Anormal\n",
    "int[] observations = { 0, 1, 1 };\n",
    "\n",
    "// Belief initial (le systeme vient d'etre installe => Bon)\n",
    "double[] belief = { 0.95, 0.04, 0.01 };\n",
    "\n",
    "Console.WriteLine(\"Belief initial :\");\n",
    "for (int e = 0; e < nEtats; e++)\n",
    "    Console.WriteLine($\"  P({etatsNom[e]}) = {belief[e]:P1}\");\n",
    "Console.WriteLine();\n",
    "\n",
    "// Boucle de mise a jour\n",
    "for (int t = 0; t < observations.Length; t++)\n",
    "{\n",
    "    Console.WriteLine($\"=== Pas de temps {t + 1} ===\");\n",
    "    \n",
    "    // 1. Prediction (transition)\n",
    "    double[] predicted = new double[nEtats];\n",
    "    for (int sPrime = 0; sPrime < nEtats; sPrime++)\n",
    "    {\n",
    "        for (int s = 0; s < nEtats; s++)\n",
    "            predicted[sPrime] += transMatrix[s, sPrime] * belief[s];\n",
    "    }\n",
    "    \n",
    "    Console.WriteLine(\"Apres prediction (transition) :\");\n",
    "    for (int e = 0; e < nEtats; e++)\n",
    "        Console.WriteLine($\"  P({etatsNom[e]}) = {predicted[e]:P1}\");\n",
    "    \n",
    "    // 2. Correction (observation) avec Infer.NET\n",
    "    Variable<int> etatVar = Variable.Discrete(predicted).Named(\"etat\");\n",
    "    etatVar.SetValueRange(etatRange);\n",
    "    \n",
    "    Variable<int> obsVar = Variable.New<int>().Named(\"observation\");\n",
    "    obsVar.SetValueRange(new Range(2));\n",
    "    \n",
    "    // Modele d'observation\n",
    "    using (Variable.Case(etatVar, 0))\n",
    "        obsVar.SetTo(Variable.Discrete(obsMatrix[0, 0], obsMatrix[0, 1]));\n",
    "    using (Variable.Case(etatVar, 1))\n",
    "        obsVar.SetTo(Variable.Discrete(obsMatrix[1, 0], obsMatrix[1, 1]));\n",
    "    using (Variable.Case(etatVar, 2))\n",
    "        obsVar.SetTo(Variable.Discrete(obsMatrix[2, 0], obsMatrix[2, 1]));\n",
    "    \n",
    "    // Observation\n",
    "    int obs = observations[t];\n",
    "    obsVar.ObservedValue = obs;\n",
    "    \n",
    "    // Inference\n",
    "    InferenceEngine engineBelief = new InferenceEngine();\n",
    "    engineBelief.Compiler.CompilerChoice = Microsoft.ML.Probabilistic.Compiler.CompilerChoice.Roslyn;\n",
    "    \n",
    "    var posteriorEtat = engineBelief.Infer<Discrete>(etatVar);\n",
    "    Vector probs = posteriorEtat.GetProbs();\n",
    "    \n",
    "    Console.WriteLine($\"Observation : {(obs == 0 ? \"Normal\" : \"Anormal\")}\");\n",
    "    Console.WriteLine(\"Apres correction (Bayes via Infer.NET) :\");\n",
    "    for (int e = 0; e < nEtats; e++)\n",
    "    {\n",
    "        string bar = new string('#', (int)(probs[e] * 30));\n",
    "        Console.WriteLine($\"  P({etatsNom[e],-10}) = {probs[e]:P1} {bar}\");\n",
    "    }\n",
    "    \n",
    "    // 3. Decision basee sur le belief\n",
    "    double[] EU = new double[3];\n",
    "    for (int a = 0; a < 3; a++)\n",
    "    {\n",
    "        for (int e = 0; e < nEtats; e++)\n",
    "            EU[a] += probs[e] * utilities[a, e];\n",
    "    }\n",
    "    \n",
    "    int bestAction = EU.Select((v, i) => (v, i)).OrderByDescending(x => x.v).First().i;\n",
    "    \n",
    "    Console.WriteLine(\"Utilites esperees :\");\n",
    "    for (int a = 0; a < 3; a++)\n",
    "        Console.WriteLine($\"  E[U({actionsNom[a],-12})] = {EU[a],7:F1}\");\n",
    "    Console.WriteLine($\"=> Decision : {actionsNom[bestAction]}\");\n",
    "    Console.WriteLine();\n",
    "    \n",
    "    // Mettre a jour le belief pour le prochain pas\n",
    "    // Note: GetProbs() retourne un Vector, on copie manuellement\n",
    "    for (int e = 0; e < nEtats; e++)\n",
    "        belief[e] = probs[e];\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"=== Resume ===\");\n",
    "Console.WriteLine(\"Les observations 'Anormal' successives ont fait augmenter P(Degrade),\");\n",
    "Console.WriteLine(\"declenchant le passage de 'Continuer' a 'Maintenance' ou 'Remplacer'.\");\n",
    "Console.WriteLine(\"\\nC'est le principe de la maintenance predictive bayesienne !\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation de la maintenance predictive bayesienne\n",
    "\n",
    "**Scenario simule :** Observations = [Normal, Anormal, Anormal]\n",
    "\n",
    "**Evolution du belief et des decisions :**\n",
    "\n",
    "| Pas | Observation | P(Bon) | P(Degrade) | P(Defaillant) | Decision |\n",
    "|-----|-------------|--------|------------|---------------|----------|\n",
    "| 1 | Normal | 96% | 4% | 0.2% | Continuer (EU=97) |\n",
    "| 2 | Anormal | 30% | 53% | 18% | Maintenance (EU=27) |\n",
    "| 3 | Anormal | 2% | 56% | 42% | Maintenance (EU=23) |\n",
    "\n",
    "**Ce que montre Infer.NET :**\n",
    "- Le moteur calcule automatiquement les posterieurs bayesiens\n",
    "- La boucle prediction-correction est le coeur des filtres bayesiens (Kalman, particule)\n",
    "\n",
    "**Declenchement de la maintenance :**\n",
    "- L'observation \"Normal\" au pas 1 **renforce** le belief que le systeme est bon\n",
    "- Les observations \"Anormal\" aux pas 2-3 **degradent** progressivement le belief\n",
    "- La decision passe de \"Continuer\" a \"Maintenance\" quand P(Degrade) + P(Defaillant) devient significatif\n",
    "\n",
    "**Application industrielle :**\n",
    "- Ce pattern est utilise en **maintenance predictive** (Industry 4.0)\n",
    "- Les capteurs IoT fournissent des observations en continu\n",
    "- Le systeme decide automatiquement quand intervenir, avant la panne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Lien avec la Serie RL\n",
    "\n",
    "### Ce notebook : Concepts fondamentaux\n",
    "\n",
    "- MDPs et equations de Bellman\n",
    "- Methodes tabulaires (Value Iteration, Policy Iteration)\n",
    "- Concepts theoriques (Gittins, POMDPs)\n",
    "\n",
    "### Serie RL (`MyIA.AI.Notebooks/RL/`) : Implementations avancees\n",
    "\n",
    "| Notebook | Contenu |\n",
    "|----------|--------|\n",
    "| RL-1-Introduction | Q-Learning tabulaire |\n",
    "| RL-2-DeepRL | DQN avec reseaux de neurones |\n",
    "| RL-3-PolicyGradient | REINFORCE, Actor-Critic |\n",
    "| RL-4-AdvancedMethods | PPO, A2C, SAC |\n",
    "| RL-5-Applications | Gym, Stable-Baselines3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resume\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **MDP** | (S, A, P, R, γ) - cadre formel pour decisions sequentielles |\n",
    "| **Bellman** | V*(s) = max_a [R + γ Σ P V*] |\n",
    "| **Value Iteration** | Mise a jour iterative de V jusqu'a convergence |\n",
    "| **Policy Iteration** | Evaluation + Amelioration alternees |\n",
    "| **Reward Shaping** | F = γΦ(s') - Φ(s) preserve la politique optimale |\n",
    "| **Bandits** | Exploration vs Exploitation |\n",
    "| **Gittins** | Indice optimal pour bandits avec discount |\n",
    "| **POMDP** | MDP avec observations bruitees, belief states |\n",
    "\n",
    "---\n",
    "\n",
    "## Pour aller plus loin\n",
    "\n",
    "| Si vous voulez... | Consultez... |\n",
    "|-------------------|-------------|\n",
    "| Deep Reinforcement Learning | Serie `MyIA.AI.Notebooks/RL/` |\n",
    "| Implementations PyTorch/TF | Stable-Baselines3 |\n",
    "| Theorie avancee | Sutton & Barto \"Reinforcement Learning\" |\n",
    "\n",
    "---\n",
    "\n",
    "## Fin de la Serie Decision Theory\n",
    "\n",
    "Felicitations ! Vous avez termine les 7 notebooks sur la Decision Theory.\n",
    "\n",
    "### Recapitulatif de la serie 14-20\n",
    "\n",
    "| # | Titre | Concepts cles |\n",
    "|---|-------|---------------|\n",
    "| 14 | Utility Foundations | Axiomes VNM, loteries, agent rationnel |\n",
    "| 15 | Utility Money | CARA/CRRA, aversion au risque, dominance |\n",
    "| 16 | Multi-Attribute | MAUT, independance, SMART |\n",
    "| 17 | Decision Networks | Influence diagrams, arcs informationnels |\n",
    "| 18 | Value of Information | EVPI, EVSI, droits de forage |\n",
    "| 19 | Expert Systems | Minimax, regret, robustesse |\n",
    "| 20 | Sequential | MDPs, bandits, POMDPs |\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Bellman (1957) : Dynamic Programming\n",
    "- Gittins (1979) : Bandit Processes and Dynamic Allocation Indices\n",
    "- Ng, Harada, Russell (1999) : Policy Invariance Under Reward Transformations\n",
    "- Kaelbling, Littman, Cassandra (1998) : Planning and Acting in Partially Observable Stochastic Domains\n",
    "- Sutton & Barto (2018) : Reinforcement Learning: An Introduction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "file_extension": ".cs",
   "mimetype": "text/x-csharp",
   "name": "C#",
   "pygments_lexer": "csharp",
   "version": "13.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
