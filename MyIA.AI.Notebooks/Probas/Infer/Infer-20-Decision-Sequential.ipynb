{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer-20-Decision-Sequential : MDPs, Bandits et POMDPs\n",
    "\n",
    "**Serie** : Programmation Probabiliste avec Infer.NET (20/20)  \n",
    "**Duree estimee** : 60 minutes  \n",
    "**Prerequis** : Notebooks 14-19 (Decision Theory)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Comprendre les **Processus de Decision Markoviens** (MDPs)\n",
    "- Maitriser l'**iteration de valeur** et l'**iteration de politique**\n",
    "- Decouvrir les alternatives : **LP, Expectimax, RTDP**\n",
    "- Appliquer le **reward shaping** avec le theoreme de preservation de politique\n",
    "- Introduire les **bandits multi-bras** et l'**indice de Gittins**\n",
    "- Presenter les **POMDPs** et les belief states\n",
    "\n",
    "---\n",
    "\n",
    "## Navigation\n",
    "\n",
    "| Precedent | Suivant |\n",
    "|-----------|--------|\n",
    "| [Infer-19-Decision-Expert-Systems](Infer-19-Decision-Expert-Systems.ipynb) | Serie RL (`MyIA.AI.Notebooks/RL/`) |\n",
    "\n",
    "---\n",
    "\n",
    "**Note** : Ce notebook est une introduction conceptuelle aux decisions sequentielles. Les implementations avancees avec Deep RL sont dans la serie `RL/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decisions Sequentielles vs One-Shot\n",
    "\n",
    "### Difference fondamentale\n",
    "\n",
    "| Type | Caracteristique | Exemple |\n",
    "|------|-----------------|--------|\n",
    "| **One-shot** | Une seule decision | Accepter une offre d'emploi |\n",
    "| **Sequentielle** | Serie de decisions interdependantes | Jouer aux echecs |\n",
    "\n",
    "### Horizon\n",
    "\n",
    "- **Fini** : Nombre fixe d'etapes (T etapes)\n",
    "- **Infini** : Le processus continue indefiniment\n",
    "\n",
    "### Facteur d'actualisation γ (gamma)\n",
    "\n",
    "Pour les horizons infinis, on utilise un facteur γ ∈ [0,1) pour :\n",
    "\n",
    "- Garantir la convergence des sommes infinies\n",
    "- Modeliser la preference pour les recompenses immediates\n",
    "- γ = 0.99 : vision long terme\n",
    "- γ = 0.5 : vision court terme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.ML.Probabilistic, 0.4.2504.701</span></li><li><span>Microsoft.ML.Probabilistic.Compiler, 0.4.2504.701</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer.NET charge !\n"
     ]
    }
   ],
   "source": [
    "// Installation Infer.NET\n",
    "#r \"nuget: Microsoft.ML.Probabilistic\"\n",
    "#r \"nuget: Microsoft.ML.Probabilistic.Compiler\"\n",
    "\n",
    "using Microsoft.ML.Probabilistic;\n",
    "using Microsoft.ML.Probabilistic.Distributions;\n",
    "using Microsoft.ML.Probabilistic.Models;\n",
    "using Microsoft.ML.Probabilistic.Algorithms;\n",
    "\n",
    "Console.WriteLine(\"Infer.NET charge !\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processus de Decision Markoviens (MDP)\n",
    "\n",
    "### Definition formelle\n",
    "\n",
    "Un MDP est un tuple (S, A, P, R, γ) :\n",
    "\n",
    "| Element | Notation | Description |\n",
    "|---------|----------|-------------|\n",
    "| **Etats** | S | Ensemble des etats possibles |\n",
    "| **Actions** | A | Ensemble des actions |\n",
    "| **Transitions** | P(s'\\|s,a) | Probabilite de transition |\n",
    "| **Recompenses** | R(s,a) ou R(s,a,s') | Recompense immediate |\n",
    "| **Discount** | γ | Facteur d'actualisation |\n",
    "\n",
    "### Politique et Fonction de Valeur\n",
    "\n",
    "- **Politique** π : S → A (ou distribution sur A)\n",
    "- **Fonction de valeur** V^π(s) : Utilite esperee depuis s en suivant π\n",
    "- **Fonction action-valeur** Q^π(s, a) : Utilite esperee en faisant a puis suivant π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Grille 4x3 cree :\n",
      "  But (+1) en (3,2)\n",
      "  Piege (-1) en (3,1)\n",
      "  Mur en (1,1)\n",
      "  Gamma = 0,9\n"
     ]
    }
   ],
   "source": [
    "// Definition d'un MDP simple : Navigation dans une grille\n",
    "\n",
    "public class GridMDP\n",
    "{\n",
    "    public int Width { get; }\n",
    "    public int Height { get; }\n",
    "    public double Gamma { get; }\n",
    "    public Dictionary<(int, int), double> Rewards { get; }\n",
    "    public HashSet<(int, int)> TerminalStates { get; }\n",
    "    public HashSet<(int, int)> Walls { get; }\n",
    "    \n",
    "    public string[] Actions { get; } = { \"N\", \"S\", \"E\", \"W\" };\n",
    "    \n",
    "    // Directions\n",
    "    private Dictionary<string, (int dx, int dy)> _directions = new()\n",
    "    {\n",
    "        { \"N\", (0, 1) }, { \"S\", (0, -1) }, { \"E\", (1, 0) }, { \"W\", (-1, 0) }\n",
    "    };\n",
    "    \n",
    "    public GridMDP(int width, int height, double gamma = 0.9)\n",
    "    {\n",
    "        Width = width;\n",
    "        Height = height;\n",
    "        Gamma = gamma;\n",
    "        Rewards = new Dictionary<(int, int), double>();\n",
    "        TerminalStates = new HashSet<(int, int)>();\n",
    "        Walls = new HashSet<(int, int)>();\n",
    "    }\n",
    "    \n",
    "    public List<(int, int)> GetStates()\n",
    "    {\n",
    "        var states = new List<(int, int)>();\n",
    "        for (int x = 0; x < Width; x++)\n",
    "            for (int y = 0; y < Height; y++)\n",
    "                if (!Walls.Contains((x, y)))\n",
    "                    states.Add((x, y));\n",
    "        return states;\n",
    "    }\n",
    "    \n",
    "    public double GetReward((int, int) state) => \n",
    "        Rewards.TryGetValue(state, out var r) ? r : -0.04; // Cout de mouvement par defaut\n",
    "    \n",
    "    public List<((int, int) nextState, double prob)> GetTransitions((int, int) state, string action)\n",
    "    {\n",
    "        if (TerminalStates.Contains(state))\n",
    "            return new List<((int, int), double)> { (state, 1.0) };\n",
    "        \n",
    "        var transitions = new List<((int, int), double)>();\n",
    "        var (dx, dy) = _directions[action];\n",
    "        \n",
    "        // 80% de chance d'aller dans la direction voulue\n",
    "        transitions.Add((NextState(state, action), 0.8));\n",
    "        \n",
    "        // 10% de chance d'aller perpendiculairement (droite ou gauche)\n",
    "        var perpActions = action == \"N\" || action == \"S\" \n",
    "            ? new[] { \"E\", \"W\" } \n",
    "            : new[] { \"N\", \"S\" };\n",
    "        \n",
    "        foreach (var perpAction in perpActions)\n",
    "            transitions.Add((NextState(state, perpAction), 0.1));\n",
    "        \n",
    "        return transitions;\n",
    "    }\n",
    "    \n",
    "    private (int, int) NextState((int, int) state, string action)\n",
    "    {\n",
    "        var (x, y) = state;\n",
    "        var (dx, dy) = _directions[action];\n",
    "        int nx = x + dx, ny = y + dy;\n",
    "        \n",
    "        // Collision avec mur ou bord = rester sur place\n",
    "        if (nx < 0 || nx >= Width || ny < 0 || ny >= Height || Walls.Contains((nx, ny)))\n",
    "            return state;\n",
    "        return (nx, ny);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Creer le MDP classique de Russell & Norvig\n",
    "var mdp = new GridMDP(4, 3, gamma: 0.9);\n",
    "mdp.Rewards[(3, 2)] = 1.0;   // But positif\n",
    "mdp.Rewards[(3, 1)] = -1.0;  // Piege\n",
    "mdp.TerminalStates.Add((3, 2));\n",
    "mdp.TerminalStates.Add((3, 1));\n",
    "mdp.Walls.Add((1, 1));       // Mur\n",
    "\n",
    "Console.WriteLine(\"MDP Grille 4x3 cree :\");\n",
    "Console.WriteLine(\"  But (+1) en (3,2)\");\n",
    "Console.WriteLine(\"  Piege (-1) en (3,1)\");\n",
    "Console.WriteLine(\"  Mur en (1,1)\");\n",
    "Console.WriteLine($\"  Gamma = {mdp.Gamma}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Equation de Bellman\n\n### Intuition\n\nL'equation de Bellman exprime un principe fondamental de **consistance temporelle** :\n\n> La valeur d'un etat = recompense immediate + valeur actualisee des etats futurs\n\nC'est une relation **recursive** : la valeur de chaque etat depend de la valeur des etats atteignables. Cette structure permet de resoudre le probleme \"a l'envers\" (backward induction) ou iterativement.\n\n### Equation de Bellman pour V*\n\nLa fonction de valeur optimale satisfait :\n\n$$V^*(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right]$$\n\n**Interpretation terme par terme** :\n- $\\max_a$ : On choisit la meilleure action\n- $R(s,a)$ : Recompense immediate\n- $\\gamma$ : Facteur d'actualisation (valeur du futur)\n- $\\sum_{s'} P(s'|s,a) V^*(s')$ : Esperance de la valeur future\n\n### Equation de Bellman pour Q*\n\n$$Q^*(s, a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q^*(s', a')$$\n\nLa fonction Q donne la valeur de **faire a puis agir optimalement**.\n\n### Politique optimale\n\n$$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$$\n\nLa politique optimale est **deterministe** : dans chaque etat, une action domine.\n\n### Pourquoi Bellman est-il si important ?\n\nL'equation de Bellman est la **cle de voute** de la programmation dynamique et du reinforcement learning car :\n\n1. Elle decompose un probleme global en sous-problemes locaux\n2. Elle fournit une condition d'optimalite verifiable\n3. Elle inspire les algorithmes (VI, PI, Q-learning, etc.)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Iteration de Valeur\n",
    "\n",
    "### Algorithme\n",
    "\n",
    "1. Initialiser V(s) = 0 pour tout s\n",
    "2. Repeter jusqu'a convergence :\n",
    "   - Pour chaque etat s :\n",
    "     - V(s) ← max_a [R(s,a) + γ Σ_s' P(s'|s,a) V(s')]\n",
    "3. Extraire la politique : π(s) = argmax_a Q(s,a)\n",
    "\n",
    "### Complexite\n",
    "\n",
    "- O(|S|² |A|) par iteration\n",
    "- Converge en O(log(1/ε) / (1-γ)) iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence apres 14 iterations (delta = 6,01E-004)\n",
      "\n",
      "Fonction de Valeur V* :\n",
      "  0,509   0,650   0,795   1,000 \n",
      "  0,398   ####    0,486  -1,000 \n",
      "  0,296   0,254   0,345   0,130 \n",
      "\n",
      "Politique optimale π* :\n",
      " →  →  →  ● \n",
      " ↑  #  ↑  ● \n",
      " ↑  →  ↑  ← \n"
     ]
    }
   ],
   "source": [
    "// Iteration de Valeur\n",
    "\n",
    "public class ValueIteration\n",
    "{\n",
    "    public static (Dictionary<(int,int), double> V, Dictionary<(int,int), string> policy) \n",
    "        Solve(GridMDP mdp, double epsilon = 0.001, int maxIter = 100)\n",
    "    {\n",
    "        var states = mdp.GetStates();\n",
    "        var V = states.ToDictionary(s => s, s => 0.0);\n",
    "        \n",
    "        for (int iter = 0; iter < maxIter; iter++)\n",
    "        {\n",
    "            double delta = 0;\n",
    "            var newV = new Dictionary<(int, int), double>(V);\n",
    "            \n",
    "            foreach (var s in states)\n",
    "            {\n",
    "                if (mdp.TerminalStates.Contains(s))\n",
    "                {\n",
    "                    newV[s] = mdp.GetReward(s);\n",
    "                    continue;\n",
    "                }\n",
    "                \n",
    "                double maxQ = double.NegativeInfinity;\n",
    "                foreach (var a in mdp.Actions)\n",
    "                {\n",
    "                    double q = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        q += mdp.Gamma * prob * V[nextState];\n",
    "                    maxQ = Math.Max(maxQ, q);\n",
    "                }\n",
    "                \n",
    "                newV[s] = maxQ;\n",
    "                delta = Math.Max(delta, Math.Abs(V[s] - newV[s]));\n",
    "            }\n",
    "            \n",
    "            V = newV;\n",
    "            \n",
    "            if (delta < epsilon)\n",
    "            {\n",
    "                Console.WriteLine($\"Convergence apres {iter + 1} iterations (delta = {delta:E2})\");\n",
    "                break;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Extraire la politique\n",
    "        var policy = new Dictionary<(int, int), string>();\n",
    "        foreach (var s in states)\n",
    "        {\n",
    "            if (mdp.TerminalStates.Contains(s))\n",
    "            {\n",
    "                policy[s] = \"T\"; // Terminal\n",
    "                continue;\n",
    "            }\n",
    "            \n",
    "            string bestAction = null;\n",
    "            double maxQ = double.NegativeInfinity;\n",
    "            \n",
    "            foreach (var a in mdp.Actions)\n",
    "            {\n",
    "                double q = mdp.GetReward(s);\n",
    "                foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                    q += mdp.Gamma * prob * V[nextState];\n",
    "                \n",
    "                if (q > maxQ)\n",
    "                {\n",
    "                    maxQ = q;\n",
    "                    bestAction = a;\n",
    "                }\n",
    "            }\n",
    "            policy[s] = bestAction;\n",
    "        }\n",
    "        \n",
    "        return (V, policy);\n",
    "    }\n",
    "}\n",
    "\n",
    "var (V, policy) = ValueIteration.Solve(mdp);\n",
    "\n",
    "// Afficher la grille\n",
    "Console.WriteLine(\"\\nFonction de Valeur V* :\");\n",
    "for (int y = mdp.Height - 1; y >= 0; y--)\n",
    "{\n",
    "    for (int x = 0; x < mdp.Width; x++)\n",
    "    {\n",
    "        if (mdp.Walls.Contains((x, y)))\n",
    "            Console.Write(\"  ####  \");\n",
    "        else\n",
    "            Console.Write($\" {V[(x, y)],6:F3} \");\n",
    "    }\n",
    "    Console.WriteLine();\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"\\nPolitique optimale π* :\");\n",
    "var arrows = new Dictionary<string, string> { {\"N\",\"↑\"}, {\"S\",\"↓\"}, {\"E\",\"→\"}, {\"W\",\"←\"}, {\"T\",\"●\"} };\n",
    "for (int y = mdp.Height - 1; y >= 0; y--)\n",
    "{\n",
    "    for (int x = 0; x < mdp.Width; x++)\n",
    "    {\n",
    "        if (mdp.Walls.Contains((x, y)))\n",
    "            Console.Write(\" # \");\n",
    "        else\n",
    "            Console.Write($\" {arrows[policy[(x, y)]]} \");\n",
    "    }\n",
    "    Console.WriteLine();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Iteration de Politique\n\n### Algorithme\n\n1. Initialiser π arbitrairement\n2. Repeter jusqu'a stabilite :\n   - **Evaluation** : Calculer V^π (resoudre systeme lineaire)\n   - **Amelioration** : π(s) ← argmax_a Q^π(s,a)\n\n### Compromis Value Iteration vs Policy Iteration\n\n| Critere | Value Iteration | Policy Iteration |\n|---------|-----------------|------------------|\n| **Iterations** | Beaucoup (log(1/ε)/(1-γ)) | Peu (souvent 5-10) |\n| **Cout/iteration** | O(\\|S\\|² \\|A\\|) leger | O(\\|S\\|³) evaluation exacte |\n| **Convergence** | Asymptotique (ε-optimal) | Exacte en nombre fini |\n| **Memoire** | V(s) seulement | V(s) + π(s) |\n\n### Quand utiliser chaque methode ?\n\n**Preferer Value Iteration si** :\n- Grand espace d'etats (\\|S\\| > 10,000)\n- Precision approximative suffisante\n- γ proche de 1 (convergence lente de PI)\n- Implementation simple souhaitee\n\n**Preferer Policy Iteration si** :\n- Petit/moyen espace d'etats\n- Solution exacte requise\n- Politique stable rapidement\n- Evaluation peut etre acceleree (ex: methodes matricielles)\n\n### Variantes hybrides\n\n- **Modified Policy Iteration** : Evaluation partielle (k iterations de Bellman au lieu de resolution exacte)\n- **Asynchronous VI** : Mise a jour d'un sous-ensemble d'etats a chaque iteration\n- **Prioritized Sweeping** : Mettre a jour les etats les plus \"surprenants\" en premier"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Iteration de Politique ===\n",
      "\n",
      "Iteration 1 : stable = False\n",
      "Iteration 2 : stable = False\n",
      "Iteration 3 : stable = True\n",
      "\n",
      "Comparaison avec Value Iteration :\n",
      "  Politiques identiques !\n"
     ]
    }
   ],
   "source": [
    "// Iteration de Politique\n",
    "\n",
    "public class PolicyIteration\n",
    "{\n",
    "    public static (Dictionary<(int,int), double> V, Dictionary<(int,int), string> policy) \n",
    "        Solve(GridMDP mdp, int maxIter = 20)\n",
    "    {\n",
    "        var states = mdp.GetStates();\n",
    "        \n",
    "        // Initialiser politique aleatoire\n",
    "        var policy = states.ToDictionary(s => s, s => \n",
    "            mdp.TerminalStates.Contains(s) ? \"T\" : \"N\");\n",
    "        \n",
    "        var V = states.ToDictionary(s => s, s => 0.0);\n",
    "        \n",
    "        for (int iter = 0; iter < maxIter; iter++)\n",
    "        {\n",
    "            // 1. Evaluation de politique (iterative simplifiee)\n",
    "            for (int evalIter = 0; evalIter < 50; evalIter++)\n",
    "            {\n",
    "                var newV = new Dictionary<(int, int), double>(V);\n",
    "                foreach (var s in states)\n",
    "                {\n",
    "                    if (mdp.TerminalStates.Contains(s))\n",
    "                    {\n",
    "                        newV[s] = mdp.GetReward(s);\n",
    "                        continue;\n",
    "                    }\n",
    "                    \n",
    "                    string a = policy[s];\n",
    "                    double v = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        v += mdp.Gamma * prob * V[nextState];\n",
    "                    newV[s] = v;\n",
    "                }\n",
    "                V = newV;\n",
    "            }\n",
    "            \n",
    "            // 2. Amelioration de politique\n",
    "            bool stable = true;\n",
    "            foreach (var s in states)\n",
    "            {\n",
    "                if (mdp.TerminalStates.Contains(s)) continue;\n",
    "                \n",
    "                string oldAction = policy[s];\n",
    "                string bestAction = null;\n",
    "                double maxQ = double.NegativeInfinity;\n",
    "                \n",
    "                foreach (var a in mdp.Actions)\n",
    "                {\n",
    "                    double q = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        q += mdp.Gamma * prob * V[nextState];\n",
    "                    \n",
    "                    if (q > maxQ)\n",
    "                    {\n",
    "                        maxQ = q;\n",
    "                        bestAction = a;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                policy[s] = bestAction;\n",
    "                if (bestAction != oldAction) stable = false;\n",
    "            }\n",
    "            \n",
    "            Console.WriteLine($\"Iteration {iter + 1} : stable = {stable}\");\n",
    "            if (stable) break;\n",
    "        }\n",
    "        \n",
    "        return (V, policy);\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"=== Iteration de Politique ===\\n\");\n",
    "var (V_pi, policy_pi) = PolicyIteration.Solve(mdp);\n",
    "\n",
    "// Verifier que les resultats sont identiques\n",
    "Console.WriteLine(\"\\nComparaison avec Value Iteration :\");\n",
    "bool same = true;\n",
    "foreach (var s in mdp.GetStates())\n",
    "{\n",
    "    if (policy[s] != policy_pi[s])\n",
    "    {\n",
    "        same = false;\n",
    "        Console.WriteLine($\"  Difference en {s}: VI={policy[s]}, PI={policy_pi[s]}\");\n",
    "    }\n",
    "}\n",
    "if (same) Console.WriteLine(\"  Politiques identiques !\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Alternatives : LP, Expectimax, RTDP\n",
    "\n",
    "### Programmation Lineaire (LP)\n",
    "\n",
    "Le MDP peut etre formule comme un programme lineaire :\n",
    "\n",
    "- **Variables** : V(s) pour chaque etat\n",
    "- **Objectif** : min Σ_s V(s)\n",
    "- **Contraintes** : V(s) ≥ R(s,a) + γ Σ_s' P(s'|s,a) V(s') pour tout a\n",
    "\n",
    "### Expectimax\n",
    "\n",
    "Pour les MDPs a horizon fini, on peut utiliser un arbre de recherche :\n",
    "\n",
    "```\n",
    "       (s0)\n",
    "      / | \\\n",
    "   a1  a2  a3     <- Noeuds max (choix agent)\n",
    "   / \\ \n",
    " s1  s2           <- Noeuds chance (transition)\n",
    "```\n",
    "\n",
    "### RTDP (Real-Time Dynamic Programming)\n",
    "\n",
    "- Mise a jour le long de trajectoires simulees\n",
    "- Focus sur les etats atteignables\n",
    "- Algorithme **anytime** : ameliore avec plus de temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RTDP (100 trials depuis (0,0)) ===\n",
      "\n",
      "Comparaison V_RTDP vs V_VI :\n",
      "  Etat   |  V_RTDP  |   V_VI   |   Diff\n",
      "---------|----------|----------|--------\n",
      " (0,0)   |   0,1666 |   0,2960 | 0,1294\n",
      " (0,2)   |  -0,1123 |   0,5094 | 0,6217\n",
      " (2,2)   |   0,7954 |   0,7954 | 0,0000\n",
      " (3,2)   |   1,0000 |   1,0000 | 0,0000\n"
     ]
    }
   ],
   "source": [
    "// RTDP simplifie\n",
    "\n",
    "public class RTDP\n",
    "{\n",
    "    public static Dictionary<(int,int), double> Solve(\n",
    "        GridMDP mdp, (int, int) startState, int nTrials = 100)\n",
    "    {\n",
    "        var states = mdp.GetStates();\n",
    "        var V = states.ToDictionary(s => s, s => 0.0);\n",
    "        var rng = new Random(42);\n",
    "        \n",
    "        for (int trial = 0; trial < nTrials; trial++)\n",
    "        {\n",
    "            var s = startState;\n",
    "            int steps = 0;\n",
    "            \n",
    "            while (!mdp.TerminalStates.Contains(s) && steps < 100)\n",
    "            {\n",
    "                // Mise a jour de Bellman\n",
    "                double maxQ = double.NegativeInfinity;\n",
    "                string bestAction = null;\n",
    "                \n",
    "                foreach (var a in mdp.Actions)\n",
    "                {\n",
    "                    double q = mdp.GetReward(s);\n",
    "                    foreach (var (nextState, prob) in mdp.GetTransitions(s, a))\n",
    "                        q += mdp.Gamma * prob * V[nextState];\n",
    "                    \n",
    "                    if (q > maxQ)\n",
    "                    {\n",
    "                        maxQ = q;\n",
    "                        bestAction = a;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                V[s] = maxQ;\n",
    "                \n",
    "                // Simuler transition\n",
    "                var transitions = mdp.GetTransitions(s, bestAction);\n",
    "                double r = rng.NextDouble();\n",
    "                double cumProb = 0;\n",
    "                foreach (var (nextState, prob) in transitions)\n",
    "                {\n",
    "                    cumProb += prob;\n",
    "                    if (r <= cumProb)\n",
    "                    {\n",
    "                        s = nextState;\n",
    "                        break;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                steps++;\n",
    "            }\n",
    "            \n",
    "            // Update terminal state\n",
    "            if (mdp.TerminalStates.Contains(s))\n",
    "                V[s] = mdp.GetReward(s);\n",
    "        }\n",
    "        \n",
    "        return V;\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"=== RTDP (100 trials depuis (0,0)) ===\\n\");\n",
    "var V_rtdp = RTDP.Solve(mdp, (0, 0), nTrials: 100);\n",
    "\n",
    "// Comparer avec Value Iteration\n",
    "Console.WriteLine(\"Comparaison V_RTDP vs V_VI :\");\n",
    "Console.WriteLine(\"  Etat   |  V_RTDP  |   V_VI   |   Diff\");\n",
    "Console.WriteLine(\"---------|----------|----------|--------\");\n",
    "foreach (var s in new[] { (0,0), (0,2), (2,2), (3,2) })\n",
    "{\n",
    "    double diff = Math.Abs(V_rtdp[s] - V[s]);\n",
    "    Console.WriteLine($\" ({s.Item1},{s.Item2})   | {V_rtdp[s],8:F4} | {V[s],8:F4} | {diff,6:F4}\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reward Shaping\n",
    "\n",
    "### Le probleme des recompenses sparses\n",
    "\n",
    "Quand les recompenses sont rares (ex: +1 seulement au but), l'apprentissage est tres lent.\n",
    "\n",
    "### Solution : Ajouter une recompense de faconnage\n",
    "\n",
    "$$R'(s, a, s') = R(s, a, s') + F(s, a, s')$$\n",
    "\n",
    "### Theoreme de preservation de politique (Ng et al., 1999)\n",
    "\n",
    "> Si F a la forme d'une **fonction de potentiel** :\n",
    "> \n",
    "> $$F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)$$\n",
    "> \n",
    "> Alors **la politique optimale est preservee** !\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- Φ(s) represente \"a quel point s est proche du but\"\n",
    "- F recompense les transitions vers des etats meilleurs\n",
    "- La forme specifique garantit que les raccourcis ne sont pas crees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reward Shaping avec Fonction de Potentiel ===\n",
      "\n",
      "But en (3, 2)\n",
      "Phi(s) = -distance(s, but)\n",
      "\n",
      "Fonction de potentiel Phi(s) :\n",
      "  -3,00   -2,00   -1,00   -0,00 \n",
      "  -3,16   ####    -1,41   -1,00 \n",
      "  -3,61   -2,83   -2,24   -2,00 \n",
      "\n",
      "Exemples de shaping reward F(s, a, s') = gamma*Phi(s') - Phi(s) :\n",
      "  F((0,0) -> (1,0)) = 1,060 (vers le but)\n",
      "  F((1,0) -> (0,0)) = -0,417 (loin du but)\n",
      "  F((2,2) -> (3,2)) = 1,000 (atteindre le but)\n",
      "\n",
      "=> Le shaping recompense les mouvements vers le but,\n",
      "   mais le theoreme garantit que la politique optimale est preservee.\n"
     ]
    }
   ],
   "source": [
    "// Demonstration du Reward Shaping\n",
    "\n",
    "public class ShapedGridMDP : GridMDP\n",
    "{\n",
    "    private Func<(int, int), double> _potential;\n",
    "    private double _gamma;\n",
    "    \n",
    "    public ShapedGridMDP(int width, int height, double gamma, Func<(int, int), double> potential) \n",
    "        : base(width, height, gamma)\n",
    "    {\n",
    "        _potential = potential;\n",
    "        _gamma = gamma;\n",
    "    }\n",
    "    \n",
    "    public double GetShapedReward((int, int) s, (int, int) sPrime)\n",
    "    {\n",
    "        double R = GetReward(s);\n",
    "        double F = _gamma * _potential(sPrime) - _potential(s);\n",
    "        return R + F;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Fonction de potentiel : distance negative au but\n",
    "(int, int) goal = (3, 2);\n",
    "Func<(int, int), double> Phi = s => -Math.Sqrt(Math.Pow(s.Item1 - goal.Item1, 2) + Math.Pow(s.Item2 - goal.Item2, 2));\n",
    "\n",
    "Console.WriteLine(\"=== Reward Shaping avec Fonction de Potentiel ===\\n\");\n",
    "Console.WriteLine($\"But en {goal}\");\n",
    "Console.WriteLine(\"Phi(s) = -distance(s, but)\\n\");\n",
    "\n",
    "Console.WriteLine(\"Fonction de potentiel Phi(s) :\");\n",
    "for (int y = 2; y >= 0; y--)\n",
    "{\n",
    "    for (int x = 0; x < 4; x++)\n",
    "    {\n",
    "        if (mdp.Walls.Contains((x, y)))\n",
    "            Console.Write(\"  ####  \");\n",
    "        else\n",
    "            Console.Write($\" {Phi((x, y)),6:F2} \");\n",
    "    }\n",
    "    Console.WriteLine();\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"\\nExemples de shaping reward F(s, a, s') = gamma*Phi(s') - Phi(s) :\");\n",
    "Console.WriteLine($\"  F((0,0) -> (1,0)) = {0.9 * Phi((1, 0)) - Phi((0, 0)):F3} (vers le but)\");\n",
    "Console.WriteLine($\"  F((1,0) -> (0,0)) = {0.9 * Phi((0, 0)) - Phi((1, 0)):F3} (loin du but)\");\n",
    "Console.WriteLine($\"  F((2,2) -> (3,2)) = {0.9 * Phi((3, 2)) - Phi((2, 2)):F3} (atteindre le but)\");\n",
    "\n",
    "Console.WriteLine();\n",
    "Console.WriteLine(\"=> Le shaping recompense les mouvements vers le but,\");\n",
    "Console.WriteLine(\"   mais le theoreme garantit que la politique optimale est preservee.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bandits Multi-Bras\n",
    "\n",
    "### Le probleme\n",
    "\n",
    "Vous avez K machines a sous (\"bras\"). Chaque bras i donne une recompense selon une distribution inconnue.\n",
    "\n",
    "**Dilemme exploration/exploitation** :\n",
    "- Explorer : essayer de nouveaux bras pour estimer leurs distributions\n",
    "- Exploiter : jouer le meilleur bras connu\n",
    "\n",
    "### Approches classiques\n",
    "\n",
    "| Methode | Description |\n",
    "|---------|-------------|\n",
    "| ε-greedy | Exploiter avec proba 1-ε, explorer avec ε |\n",
    "| UCB | Upper Confidence Bound : optimisme face a l'incertitude |\n",
    "| Thompson | Echantillonner selon la posterior des recompenses |\n",
    "| **Gittins** | Solution optimale pour bandits avec discount |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit avec 4 bras, moyennes vraies inconnues\n",
      "Meilleure moyenne : 0,7\n",
      "\n",
      "ε-greedy (ε=0,1):\n",
      "  Recompense totale : 684,6\n",
      "  Regret cumule : 15,4\n",
      "  Tirages par bras : 28, 69, 882, 21\n",
      "\n",
      "UCB1:\n",
      "  Recompense totale : 616,1\n",
      "  Regret cumule : 83,9\n",
      "  Tirages par bras : 54, 166, 719, 61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Bandit multi-bras avec differentes strategies\n",
    "\n",
    "public class MultiArmedBandit\n",
    "{\n",
    "    private double[] _trueMeans;\n",
    "    private Random _rng;\n",
    "    \n",
    "    public int K { get; }\n",
    "    \n",
    "    public MultiArmedBandit(double[] means, int seed = 42)\n",
    "    {\n",
    "        _trueMeans = means;\n",
    "        K = means.Length;\n",
    "        _rng = new Random(seed);\n",
    "    }\n",
    "    \n",
    "    public double Pull(int arm)\n",
    "    {\n",
    "        // Recompense gaussienne autour de la moyenne vraie\n",
    "        double u1 = _rng.NextDouble();\n",
    "        double u2 = _rng.NextDouble();\n",
    "        double z = Math.Sqrt(-2 * Math.Log(u1)) * Math.Cos(2 * Math.PI * u2);\n",
    "        return _trueMeans[arm] + 0.5 * z;\n",
    "    }\n",
    "    \n",
    "    public double OptimalMean => _trueMeans.Max();\n",
    "}\n",
    "\n",
    "// Strategies\n",
    "public interface IBanditStrategy\n",
    "{\n",
    "    int SelectArm(int[] counts, double[] sumRewards);\n",
    "    string Name { get; }\n",
    "}\n",
    "\n",
    "public class EpsilonGreedy : IBanditStrategy\n",
    "{\n",
    "    private double _epsilon;\n",
    "    private Random _rng = new Random();\n",
    "    public string Name => $\"ε-greedy (ε={_epsilon})\";\n",
    "    \n",
    "    public EpsilonGreedy(double epsilon) { _epsilon = epsilon; }\n",
    "    \n",
    "    public int SelectArm(int[] counts, double[] sumRewards)\n",
    "    {\n",
    "        if (_rng.NextDouble() < _epsilon)\n",
    "            return _rng.Next(counts.Length);\n",
    "        \n",
    "        int best = 0;\n",
    "        double bestMean = double.NegativeInfinity;\n",
    "        for (int i = 0; i < counts.Length; i++)\n",
    "        {\n",
    "            double mean = counts[i] > 0 ? sumRewards[i] / counts[i] : 0;\n",
    "            if (mean > bestMean) { bestMean = mean; best = i; }\n",
    "        }\n",
    "        return best;\n",
    "    }\n",
    "}\n",
    "\n",
    "public class UCB : IBanditStrategy\n",
    "{\n",
    "    public string Name => \"UCB1\";\n",
    "    \n",
    "    public int SelectArm(int[] counts, double[] sumRewards)\n",
    "    {\n",
    "        int totalPulls = counts.Sum();\n",
    "        if (totalPulls < counts.Length)\n",
    "            return totalPulls; // Essayer chaque bras une fois\n",
    "        \n",
    "        int best = 0;\n",
    "        double bestUCB = double.NegativeInfinity;\n",
    "        for (int i = 0; i < counts.Length; i++)\n",
    "        {\n",
    "            double mean = sumRewards[i] / counts[i];\n",
    "            double bonus = Math.Sqrt(2 * Math.Log(totalPulls) / counts[i]);\n",
    "            double ucb = mean + bonus;\n",
    "            if (ucb > bestUCB) { bestUCB = ucb; best = i; }\n",
    "        }\n",
    "        return best;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Simulation\n",
    "var bandit = new MultiArmedBandit(new[] { 0.3, 0.5, 0.7, 0.4 });\n",
    "var strategies = new IBanditStrategy[] { new EpsilonGreedy(0.1), new UCB() };\n",
    "\n",
    "Console.WriteLine($\"Bandit avec {bandit.K} bras, moyennes vraies inconnues\");\n",
    "Console.WriteLine($\"Meilleure moyenne : {bandit.OptimalMean}\\n\");\n",
    "\n",
    "int T = 1000;\n",
    "\n",
    "foreach (var strategy in strategies)\n",
    "{\n",
    "    var counts = new int[bandit.K];\n",
    "    var sumRewards = new double[bandit.K];\n",
    "    double totalReward = 0;\n",
    "    double totalRegret = 0;\n",
    "    \n",
    "    for (int t = 0; t < T; t++)\n",
    "    {\n",
    "        int arm = strategy.SelectArm(counts, sumRewards);\n",
    "        double reward = bandit.Pull(arm);\n",
    "        \n",
    "        counts[arm]++;\n",
    "        sumRewards[arm] += reward;\n",
    "        totalReward += reward;\n",
    "        totalRegret += bandit.OptimalMean - reward;\n",
    "    }\n",
    "    \n",
    "    Console.WriteLine($\"{strategy.Name}:\");\n",
    "    Console.WriteLine($\"  Recompense totale : {totalReward:F1}\");\n",
    "    Console.WriteLine($\"  Regret cumule : {totalRegret:F1}\");\n",
    "    Console.WriteLine($\"  Tirages par bras : {string.Join(\", \", counts)}\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Indice de Gittins\n\n### Le Theoreme Fondamental (Gittins, 1979)\n\nL'indice de Gittins est l'un des resultats les plus elegants de la theorie de la decision. Il resout le probleme du bandit multi-bras de maniere **optimale** et **decomposable**.\n\n> **Theoreme** : Pour un bandit multi-bras avec facteur de discount γ, la strategie optimale est de **toujours jouer le bras avec l'indice de Gittins le plus eleve**.\n\n### Pourquoi est-ce remarquable ?\n\n1. **Reduction de complexite** : Un probleme a K bras interdependants devient K problemes independants\n2. **Optimalite garantie** : Pas une heuristique, c'est la solution exacte\n3. **Index-ability** : Chaque bras a un \"prix\" intrinseque\n\n### Definition intuitive\n\nL'indice de Gittins d'un bras represente :\n\n> \"Le prix equivalent certain\" de ce bras - la recompense garantie pour laquelle on serait indifferent entre jouer ce bras et recevoir cette recompense fixe.\n\nPlus formellement : c'est le taux d'interet critique au-dessus duquel on prefererait \"encaisser\" plutot que jouer le bras.\n\n### Calcul formel\n\nL'indice est la solution d'un probleme d'arret optimal :\n\n$$G(s) = \\sup_{\\tau \\geq 1} \\frac{E\\left[\\sum_{t=0}^{\\tau-1} \\gamma^t R_t \\mid s_0 = s\\right]}{E\\left[\\sum_{t=0}^{\\tau-1} \\gamma^t \\mid s_0 = s\\right]}$$\n\n- Le numerateur est la recompense totale actualisee jusqu'au temps d'arret τ\n- Le denominateur est le \"temps effectif\" actualise\n- On maximise sur tous les temps d'arret possibles\n\n### Implications pratiques\n\n| Aspect | Consequence |\n|--------|-------------|\n| **Exploration** | Un bras incertain a un indice eleve (optimisme) |\n| **Exploitation** | Un bras connu avec haute moyenne a un indice eleve |\n| **Compromis** | L'indice equilibre automatiquement exploration/exploitation |\n\n### Limitation\n\nLe theoreme de Gittins s'applique sous des hypotheses specifiques :\n- Discount γ < 1 (pas pour horizon fini sans discount)\n- Les bras sont independants (pas d'interactions)\n- Une seule action par etape (pas de contraintes de ressources)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. POMDPs : MDPs Partiellement Observables\n\n### Motivation\n\nDans un MDP standard, l'agent connait l'etat exact du monde a chaque instant. En pratique, c'est souvent **irrealiste** :\n\n- Un robot ne voit pas a travers les murs\n- Un medecin ne connait pas la vraie maladie, seulement les symptomes\n- Un joueur de poker ne voit pas les cartes adverses\n\n### Extension du MDP\n\nDans un POMDP, l'agent **ne connait pas l'etat exact**. Il recoit des **observations** qui sont des indicateurs bruitees de l'etat reel.\n\n### Definition formelle\n\nUn POMDP est un tuple (S, A, P, R, O, Ω, γ) :\n\n| Element | Description |\n|---------|-------------|\n| S, A, P, R, γ | Comme MDP (etats, actions, transitions, recompenses, discount) |\n| **O** | Ensemble des observations possibles |\n| **Ω(o\\|s', a)** | Modele de capteur : P(observation \\| nouvel etat, action) |\n\n### Belief State : La Cle des POMDPs\n\nPuisque l'agent ne connait pas l'etat, il doit maintenir une **distribution de croyance** (belief) b(s) sur tous les etats possibles.\n\n> b(s) = P(etat = s | historique des observations et actions)\n\nLe belief state est une **statistique suffisante** : il resume toute l'information pertinente de l'historique.\n\n### Mise a jour du Belief State\n\nApres avoir fait action a et observe o, le nouveau belief b' est :\n\n$$b'(s') = \\eta \\cdot \\Omega(o|s', a) \\sum_s P(s'|s, a) b(s)$$\n\nou η est une constante de normalisation.\n\nC'est exactement une **mise a jour bayesienne** ! Le POMDP est donc naturellement lie a l'inference probabiliste.\n\n### Plans conditionnels\n\nLa politique d'un POMDP n'est pas s → a comme dans un MDP, mais b → a ou equivalemment un **arbre de decision** :\n\n```\n        a1           <- Action initiale\n       /  \\\n     o1    o2        <- Observations possibles\n     |      |\n    a2     a3        <- Actions conditionnelles\n   / \\    / \\\n  o1 o2  o1 o2\n  ...\n```\n\n### Complexite\n\nLes POMDPs sont **PSPACE-complete** a resoudre exactement. En pratique, on utilise :\n- Point-Based Value Iteration (PBVI)\n- SARSOP\n- Monte Carlo Tree Search (MCTS)\n- Ou on approxime par des MDP avec belief states discrets"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POMDP : Probleme du Tigre ===\n",
      "\n",
      "Deux portes : gauche (G) et droite (D)\n",
      "Un tigre est cache derriere l'une des portes.\n",
      "Actions : Ouvrir G, Ouvrir D, Ecouter\n",
      "\n",
      "Belief initial : P(tigre gauche) = 50 %\n",
      "\n",
      "Utilites esperees :\n",
      "  E[U(ouvrir gauche) | b=50 %] = -45,0\n",
      "  E[U(ouvrir droite) | b=50 %] = -45,0\n",
      "  Ecouter : cout immediat = -1, mais reduit l'incertitude\n",
      "\n",
      "Simulation : 3 ecoutes donnent 'bruit gauche'\n",
      "\n",
      "Apres observation 1 : P(tigre gauche) = 85,0 %\n",
      "Apres observation 2 : P(tigre gauche) = 97,0 %\n",
      "Apres observation 3 : P(tigre gauche) = 99,5 %\n",
      "\n",
      "E[U(ouvrir gauche)] = -99,4\n",
      "E[U(ouvrir droite)] = 9,4\n",
      "\n",
      "=> Decision : OUVRIR DROITE (tigre probablement a gauche)\n"
     ]
    }
   ],
   "source": [
    "// POMDP simple : Tigre derriere une porte\n",
    "\n",
    "Console.WriteLine(\"=== POMDP : Probleme du Tigre ===\\n\");\n",
    "Console.WriteLine(\"Deux portes : gauche (G) et droite (D)\");\n",
    "Console.WriteLine(\"Un tigre est cache derriere l'une des portes.\");\n",
    "Console.WriteLine(\"Actions : Ouvrir G, Ouvrir D, Ecouter\\n\");\n",
    "\n",
    "// Etats : tigre gauche (TG), tigre droite (TD)\n",
    "// Actions : ouvrir_gauche, ouvrir_droite, ecouter\n",
    "// Observations (si ecouter) : bruit_gauche, bruit_droite\n",
    "\n",
    "double pCorrectHearing = 0.85; // P(bruit_gauche | tigre_gauche)\n",
    "double rewardTresor = 10;\n",
    "double rewardTigre = -100;\n",
    "double costEcoute = -1;\n",
    "\n",
    "// Belief state : b = P(tigre_gauche)\n",
    "double b = 0.5; // Prior uniforme\n",
    "\n",
    "Console.WriteLine($\"Belief initial : P(tigre gauche) = {b:P0}\\n\");\n",
    "\n",
    "// Fonction de valeur pour chaque action\n",
    "double EU_ouvrirGauche(double belief) => \n",
    "    belief * rewardTigre + (1 - belief) * rewardTresor;\n",
    "\n",
    "double EU_ouvrirDroite(double belief) => \n",
    "    belief * rewardTresor + (1 - belief) * rewardTigre;\n",
    "\n",
    "// Pour ecouter, on doit calculer la valeur esperee apres observation\n",
    "// (simplifie ici)\n",
    "\n",
    "Console.WriteLine(\"Utilites esperees :\");\n",
    "Console.WriteLine($\"  E[U(ouvrir gauche) | b={b:P0}] = {EU_ouvrirGauche(b):F1}\");\n",
    "Console.WriteLine($\"  E[U(ouvrir droite) | b={b:P0}] = {EU_ouvrirDroite(b):F1}\");\n",
    "Console.WriteLine($\"  Ecouter : cout immediat = {costEcoute}, mais reduit l'incertitude\\n\");\n",
    "\n",
    "// Simuler une sequence d'observations\n",
    "Console.WriteLine(\"Simulation : 3 ecoutes donnent 'bruit gauche'\\n\");\n",
    "\n",
    "for (int i = 0; i < 3; i++)\n",
    "{\n",
    "    // Mise a jour bayesienne apres observation \"bruit gauche\"\n",
    "    // P(TG|bruit_g) = P(bruit_g|TG) * P(TG) / P(bruit_g)\n",
    "    double pBruitGauche = b * pCorrectHearing + (1 - b) * (1 - pCorrectHearing);\n",
    "    b = (pCorrectHearing * b) / pBruitGauche;\n",
    "    \n",
    "    Console.WriteLine($\"Apres observation {i+1} : P(tigre gauche) = {b:P1}\");\n",
    "}\n",
    "\n",
    "Console.WriteLine();\n",
    "Console.WriteLine($\"E[U(ouvrir gauche)] = {EU_ouvrirGauche(b):F1}\");\n",
    "Console.WriteLine($\"E[U(ouvrir droite)] = {EU_ouvrirDroite(b):F1}\");\n",
    "Console.WriteLine();\n",
    "Console.WriteLine($\"=> Decision : OUVRIR DROITE (tigre probablement a gauche)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 10bis. Belief State Updates avec Infer.NET\n\nL'exemple precedent calculait les mises a jour du belief \"a la main\". Avec Infer.NET, on peut automatiser cette inference bayesienne, ce qui devient precieux pour des modeles plus complexes.\n\n### Avantages d'Infer.NET pour les POMDPs\n\n| Aspect | Calcul manuel | Infer.NET |\n|--------|---------------|-----------|\n| **Formule** | Ecrire Bayes explicitement | Automatique |\n| **Etats multiples** | Combinatoire | Gere par le moteur |\n| **Observations multiples** | Produits manuels | Modelisation naturelle |\n| **Incertitude sur parametres** | Tres complexe | Hyper-priors possibles |\n\n### Scenario : Maintenance predictive\n\nUn systeme peut etre dans 3 etats : {Bon, Degrade, Defaillant}.\nA chaque pas de temps :\n- L'etat peut se degrader (Markov)\n- On observe un signal de capteur (bruite)\n- On decide : continuer, maintenance preventive, ou remplacement",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "// Belief State Updates avec Infer.NET : Maintenance Predictive\n\nusing Microsoft.ML.Probabilistic.Math;\n\nConsole.WriteLine(\"=== Belief State Updates avec Infer.NET ===\\n\");\nConsole.WriteLine(\"Scenario : Maintenance predictive d'un systeme\\n\");\n\n// Definition des etats et observations\nint nEtats = 3;\nRange etatRange = new Range(nEtats).Named(\"etatRange\");\nstring[] etatsNom = { \"Bon\", \"Degrade\", \"Defaillant\" };\n\n// Matrice de transition (degradation naturelle)\ndouble[,] transMatrix = {\n    // vers:    Bon    Degrade  Defaillant\n    /* Bon */    { 0.90,  0.08,    0.02 },\n    /* Deg */    { 0.00,  0.85,    0.15 },\n    /* Def */    { 0.00,  0.00,    1.00 }  // Absorbant\n};\n\n// Modele d'observation : capteur de vibration (0=normal, 1=anormal)\ndouble[,] obsMatrix = {\n    // P(obs | etat)  Normal  Anormal\n    /* Bon */       { 0.95,   0.05 },\n    /* Deg */       { 0.30,   0.70 },\n    /* Def */       { 0.05,   0.95 }\n};\n\n// Utilites des decisions\ndouble[,] utilities = {\n    //                  Bon     Degrade  Defaillant\n    /* Continuer */   { 100,    50,      -500 },  // Risque si defaillant\n    /* Maintenance */ { -20,    80,       -50 },  // Preventif, moins de gain si bon\n    /* Remplacer */   { -100,  -100,       50 }   // Couteux mais resout defaillance\n};\nstring[] actionsNom = { \"Continuer\", \"Maintenance\", \"Remplacer\" };\n\n// === Simulation avec Infer.NET ===\n// Observations sequentielles : Normal, Anormal, Anormal\nint[] observations = { 0, 1, 1 };\n\n// Belief initial (le systeme vient d'etre installe => Bon)\ndouble[] belief = { 0.95, 0.04, 0.01 };\n\nConsole.WriteLine(\"Belief initial :\");\nfor (int e = 0; e < nEtats; e++)\n    Console.WriteLine($\"  P({etatsNom[e]}) = {belief[e]:P1}\");\nConsole.WriteLine();\n\n// Boucle de mise a jour\nfor (int t = 0; t < observations.Length; t++)\n{\n    Console.WriteLine($\"=== Pas de temps {t + 1} ===\");\n    \n    // 1. Prediction (transition)\n    double[] predicted = new double[nEtats];\n    for (int sPrime = 0; sPrime < nEtats; sPrime++)\n    {\n        for (int s = 0; s < nEtats; s++)\n            predicted[sPrime] += transMatrix[s, sPrime] * belief[s];\n    }\n    \n    Console.WriteLine(\"Apres prediction (transition) :\");\n    for (int e = 0; e < nEtats; e++)\n        Console.WriteLine($\"  P({etatsNom[e]}) = {predicted[e]:P1}\");\n    \n    // 2. Correction (observation) avec Infer.NET\n    Variable<int> etatVar = Variable.Discrete(predicted).Named(\"etat\");\n    etatVar.SetValueRange(etatRange);\n    \n    Variable<int> obsVar = Variable.New<int>().Named(\"observation\");\n    obsVar.SetValueRange(new Range(2));\n    \n    // Modele d'observation\n    using (Variable.Case(etatVar, 0))\n        obsVar.SetTo(Variable.Discrete(obsMatrix[0, 0], obsMatrix[0, 1]));\n    using (Variable.Case(etatVar, 1))\n        obsVar.SetTo(Variable.Discrete(obsMatrix[1, 0], obsMatrix[1, 1]));\n    using (Variable.Case(etatVar, 2))\n        obsVar.SetTo(Variable.Discrete(obsMatrix[2, 0], obsMatrix[2, 1]));\n    \n    // Observation\n    int obs = observations[t];\n    obsVar.ObservedValue = obs;\n    \n    // Inference\n    InferenceEngine engineBelief = new InferenceEngine();\n    engineBelief.Compiler.CompilerChoice = Microsoft.ML.Probabilistic.Compiler.CompilerChoice.Roslyn;\n    \n    var posteriorEtat = engineBelief.Infer<Discrete>(etatVar);\n    \n    Console.WriteLine($\"Observation : {(obs == 0 ? \"Normal\" : \"Anormal\")}\");\n    Console.WriteLine(\"Apres correction (Bayes via Infer.NET) :\");\n    for (int e = 0; e < nEtats; e++)\n    {\n        string bar = new string('#', (int)(posteriorEtat.GetProbs()[e] * 30));\n        Console.WriteLine($\"  P({etatsNom[e],-10}) = {posteriorEtat.GetProbs()[e]:P1} {bar}\");\n    }\n    \n    // 3. Decision basee sur le belief\n    double[] EU = new double[3];\n    for (int a = 0; a < 3; a++)\n    {\n        for (int e = 0; e < nEtats; e++)\n            EU[a] += posteriorEtat.GetProbs()[e] * utilities[a, e];\n    }\n    \n    int bestAction = EU.Select((v, i) => (v, i)).OrderByDescending(x => x.v).First().i;\n    \n    Console.WriteLine(\"Utilites esperees :\");\n    for (int a = 0; a < 3; a++)\n        Console.WriteLine($\"  E[U({actionsNom[a],-12})] = {EU[a],7:F1}\");\n    Console.WriteLine($\"=> Decision : {actionsNom[bestAction]}\");\n    Console.WriteLine();\n    \n    // Mettre a jour le belief pour le prochain pas\n    belief = posteriorEtat.GetProbs();\n}\n\nConsole.WriteLine(\"=== Resume ===\");\nConsole.WriteLine(\"Les observations 'Anormal' successives ont fait augmenter P(Degrade),\");\nConsole.WriteLine(\"declenchant le passage de 'Continuer' a 'Maintenance' ou 'Remplacer'.\");\nConsole.WriteLine(\"\\nC'est le principe de la maintenance predictive bayesienne !\");",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Lien avec la Serie RL\n",
    "\n",
    "### Ce notebook : Concepts fondamentaux\n",
    "\n",
    "- MDPs et equations de Bellman\n",
    "- Methodes tabulaires (Value Iteration, Policy Iteration)\n",
    "- Concepts theoriques (Gittins, POMDPs)\n",
    "\n",
    "### Serie RL (`MyIA.AI.Notebooks/RL/`) : Implementations avancees\n",
    "\n",
    "| Notebook | Contenu |\n",
    "|----------|--------|\n",
    "| RL-1-Introduction | Q-Learning tabulaire |\n",
    "| RL-2-DeepRL | DQN avec reseaux de neurones |\n",
    "| RL-3-PolicyGradient | REINFORCE, Actor-Critic |\n",
    "| RL-4-AdvancedMethods | PPO, A2C, SAC |\n",
    "| RL-5-Applications | Gym, Stable-Baselines3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resume\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **MDP** | (S, A, P, R, γ) - cadre formel pour decisions sequentielles |\n",
    "| **Bellman** | V*(s) = max_a [R + γ Σ P V*] |\n",
    "| **Value Iteration** | Mise a jour iterative de V jusqu'a convergence |\n",
    "| **Policy Iteration** | Evaluation + Amelioration alternees |\n",
    "| **Reward Shaping** | F = γΦ(s') - Φ(s) preserve la politique optimale |\n",
    "| **Bandits** | Exploration vs Exploitation |\n",
    "| **Gittins** | Indice optimal pour bandits avec discount |\n",
    "| **POMDP** | MDP avec observations bruitees, belief states |\n",
    "\n",
    "---\n",
    "\n",
    "## Pour aller plus loin\n",
    "\n",
    "| Si vous voulez... | Consultez... |\n",
    "|-------------------|-------------|\n",
    "| Deep Reinforcement Learning | Serie `MyIA.AI.Notebooks/RL/` |\n",
    "| Implementations PyTorch/TF | Stable-Baselines3 |\n",
    "| Theorie avancee | Sutton & Barto \"Reinforcement Learning\" |\n",
    "\n",
    "---\n",
    "\n",
    "## Fin de la Serie Decision Theory\n",
    "\n",
    "Felicitations ! Vous avez termine les 7 notebooks sur la Decision Theory.\n",
    "\n",
    "### Recapitulatif de la serie 14-20\n",
    "\n",
    "| # | Titre | Concepts cles |\n",
    "|---|-------|---------------|\n",
    "| 14 | Utility Foundations | Axiomes VNM, loteries, agent rationnel |\n",
    "| 15 | Utility Money | CARA/CRRA, aversion au risque, dominance |\n",
    "| 16 | Multi-Attribute | MAUT, independance, SMART |\n",
    "| 17 | Decision Networks | Influence diagrams, arcs informationnels |\n",
    "| 18 | Value of Information | EVPI, EVSI, droits de forage |\n",
    "| 19 | Expert Systems | Minimax, regret, robustesse |\n",
    "| 20 | Sequential | MDPs, bandits, POMDPs |\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Bellman (1957) : Dynamic Programming\n",
    "- Gittins (1979) : Bandit Processes and Dynamic Allocation Indices\n",
    "- Ng, Harada, Russell (1999) : Policy Invariance Under Reward Transformations\n",
    "- Kaelbling, Littman, Cassandra (1998) : Planning and Acting in Partially Observable Stochastic Domains\n",
    "- Sutton & Barto (2018) : Reinforcement Learning: An Introduction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "file_extension": ".cs",
   "mimetype": "text/x-csharp",
   "name": "C#",
   "pygments_lexer": "csharp",
   "version": "13.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}