{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72aac374",
   "metadata": {
    "papermill": {
     "duration": 0.004578,
     "end_time": "2026-01-27T02:02:29.114414",
     "exception": false,
     "start_time": "2026-01-27T02:02:29.109836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer-8-Model-Selection : Selection et Comparaison de Modeles\n",
    "\n",
    "**Serie** : Programmation Probabiliste avec Infer.NET (8/13)  \n",
    "**Duree estimee** : 45 minutes  \n",
    "**Prerequis** : Infer-7-Classification\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Comprendre le probleme du surapprentissage\n",
    "- Calculer l'evidence du modele (marginal likelihood)\n",
    "- Utiliser le facteur de Bayes pour comparer des modeles\n",
    "- Implementer l'Automatic Relevance Determination (ARD)\n",
    "\n",
    "---\n",
    "\n",
    "## Navigation\n",
    "\n",
    "| Precedent | Suivant |\n",
    "|-----------|--------|\n",
    "| [Infer-7-Classification](Infer-7-Classification.ipynb) | [Infer-9-Topic-Models](Infer-9-Topic-Models.ipynb) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528c2c1",
   "metadata": {
    "papermill": {
     "duration": 0.002108,
     "end_time": "2026-01-27T02:02:29.119208",
     "exception": false,
     "start_time": "2026-01-27T02:02:29.117100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Nous preparons l'environnement pour explorer la selection de modeles bayesienne. Cette approche permet de comparer objectivement differents modeles en calculant leur evidence marginale, implementant ainsi le rasoir d'Occam de maniere mathematique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6538772",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:29.133621Z",
     "iopub.status.busy": "2026-01-27T02:02:29.126898Z",
     "iopub.status.idle": "2026-01-27T02:02:31.973980Z",
     "shell.execute_reply": "2026-01-27T02:02:31.971127Z"
    },
    "papermill": {
     "duration": 2.852842,
     "end_time": "2026-01-27T02:02:31.974126",
     "exception": false,
     "start_time": "2026-01-27T02:02:29.121284",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.ML.Probabilistic, 0.4.2504.701</span></li><li><span>Microsoft.ML.Probabilistic.Compiler, 0.4.2504.701</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer.NET pret !\r\n"
     ]
    }
   ],
   "source": [
    "#r \"nuget: Microsoft.ML.Probabilistic\"\n",
    "#r \"nuget: Microsoft.ML.Probabilistic.Compiler\"\n",
    "\n",
    "using Microsoft.ML.Probabilistic;\n",
    "using Microsoft.ML.Probabilistic.Distributions;\n",
    "using Microsoft.ML.Probabilistic.Utilities;\n",
    "using Microsoft.ML.Probabilistic.Math;\n",
    "using Microsoft.ML.Probabilistic.Models;\n",
    "using Microsoft.ML.Probabilistic.Algorithms;\n",
    "using Microsoft.ML.Probabilistic.Compiler;\n",
    "\n",
    "Console.WriteLine(\"Infer.NET pret !\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dh7jodvycpp",
   "source": "### Note technique : Calcul de l'evidence dans Infer.NET\n\nInfer.NET calcule l'evidence du modele via une astuce elegante :\n\n1. On introduit une variable indicatrice `evidence = Bernoulli(0.5)`\n2. Le modele est conditionne par `Variable.If(evidence)`\n3. Apres inference, `evidence.LogOdds` donne le log de l'evidence\n\n> **Pourquoi ca fonctionne ?** Par le theoreme de Bayes :\n> $$P(\\text{evidence}=\\text{true}|D) \\propto P(D|\\text{evidence}=\\text{true}) \\times P(\\text{evidence}=\\text{true})$$\n> \n> Comme $P(\\text{evidence}=\\text{true}) = 0.5$, le log-odds posterieur est directement le log de l'evidence (a une constante pres).\n\nCette methode est specifique a Infer.NET et permet d'obtenir l'evidence sans calcul integral explicite.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "f1fdd2fb",
   "metadata": {
    "papermill": {
     "duration": 0.002418,
     "end_time": "2026-01-27T02:02:31.980268",
     "exception": false,
     "start_time": "2026-01-27T02:02:31.977850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Le Probleme du Surapprentissage\n",
    "\n",
    "### Observation\n",
    "\n",
    "Un modele complexe peut parfaitement ajuster les donnees d'entrainement mais mal generaliser.\n",
    "\n",
    "### Exemple\n",
    "\n",
    "Ajuster un polynome de degre n-1 a n points : ajustement parfait mais prediction catastrophique.\n",
    "\n",
    "### Solution bayesienne\n",
    "\n",
    "- Les priors penalisent les modeles complexes\n",
    "- L'evidence du modele equilibre ajustement et complexite\n",
    "- C'est le **rasoir d'Occam bayesien**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cacf89",
   "metadata": {
    "papermill": {
     "duration": 0.00247,
     "end_time": "2026-01-27T02:02:31.985573",
     "exception": false,
     "start_time": "2026-01-27T02:02:31.983103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Evidence du Modele (Marginal Likelihood)\n",
    "\n",
    "### Definition\n",
    "\n",
    "$$P(D|M) = \\int P(D|\\theta, M) P(\\theta|M) d\\theta$$\n",
    "\n",
    "L'evidence est la probabilite des donnees sous le modele, marginalisee sur les parametres.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- Un modele simple fait des predictions moins precises mais moins dispersees\n",
    "- Un modele complexe fait des predictions plus precises mais plus dispersees\n",
    "- L'evidence favorise le bon equilibre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15263494",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:31.992296Z",
     "iopub.status.busy": "2026-01-27T02:02:31.992051Z",
     "iopub.status.idle": "2026-01-27T02:02:34.095452Z",
     "shell.execute_reply": "2026-01-27T02:02:34.095241Z"
    },
    "papermill": {
     "duration": 2.107371,
     "end_time": "2026-01-27T02:02:34.095523",
     "exception": false,
     "start_time": "2026-01-27T02:02:31.988152",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "=== Evidence du Modele ===\n",
      "\n",
      "Modele 1 (1 gaussienne) : log evidence = -16,98\n"
     ]
    }
   ],
   "source": [
    "// Calcul de l'evidence avec Infer.NET\n",
    "\n",
    "// Donnees\n",
    "double[] observations = { 13, 15, 17, 14, 16, 15, 18 };\n",
    "int n = observations.Length;\n",
    "\n",
    "// MODELE 1 : Une seule gaussienne\n",
    "Variable<bool> evidence1 = Variable.Bernoulli(0.5).Named(\"evidence1\");\n",
    "\n",
    "using (Variable.If(evidence1))\n",
    "{\n",
    "    Variable<double> moyenne1 = Variable.GaussianFromMeanAndPrecision(15, 0.01);\n",
    "    Variable<double> precision1 = Variable.GammaFromShapeAndScale(2, 0.5);\n",
    "    \n",
    "    for (int i = 0; i < n; i++)\n",
    "    {\n",
    "        Variable<double> obs1 = Variable.GaussianFromMeanAndPrecision(moyenne1, precision1);\n",
    "        obs1.ObservedValue = observations[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "InferenceEngine moteur1 = new InferenceEngine();\n",
    "moteur1.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "\n",
    "double logEvidence1 = moteur1.Infer<Bernoulli>(evidence1).LogOdds;\n",
    "\n",
    "Console.WriteLine(\"=== Evidence du Modele ===\");\n",
    "Console.WriteLine($\"\\nModele 1 (1 gaussienne) : log evidence = {logEvidence1:F2}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kxqr4eukslk",
   "source": "### Lecture du resultat\n\n**Log evidence = -16.98** pour le modele a une gaussienne.\n\nCette valeur negative est normale : c'est un logarithme de probabilite, donc toujours negatif (ou nul). Plus la valeur est proche de 0, meilleure est l'evidence.\n\nEn termes absolus, $e^{-16.98} \\approx 4.2 \\times 10^{-8}$ semble tres petit, mais c'est la **comparaison relative** entre modeles qui importe, pas la valeur absolue.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "tg7grrjd7g",
   "source": "### Implementation : Modele 2 - Melange de deux gaussiennes\n\nLe **modele de melange** (mixture model) suppose que chaque observation provient de l'une ou l'autre de deux gaussiennes, avec une probabilite $\\pi$ pour la premiere et $1-\\pi$ pour la seconde.\n\n**Parametres du modele** :\n- $\\mu_1, \\mu_2$ : moyennes des deux composantes\n- $\\tau$ : precision commune (simplification)\n- $\\pi$ : proportion du melange (poids de la premiere composante)\n\n**Code** : Pour chaque observation, on tire d'abord `composante ~ Bernoulli(pi)`, puis on observe depuis la gaussienne correspondante.\n\n> **Note algorithmique** : Nous utilisons `VariationalMessagePassing` car les melanges de gaussiennes sont plus stables avec VMP qu'avec EP pour le calcul d'evidence.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad48f2",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:34.104125Z",
     "iopub.status.busy": "2026-01-27T02:02:34.103903Z",
     "iopub.status.idle": "2026-01-27T02:02:37.074901Z",
     "shell.execute_reply": "2026-01-27T02:02:37.067777Z"
    },
    "papermill": {
     "duration": 2.975549,
     "end_time": "2026-01-27T02:02:37.074986",
     "exception": false,
     "start_time": "2026-01-27T02:02:34.099437",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Modele 2 (melange 2 gaussiennes) : log evidence = -20,12\n"
     ]
    }
   ],
   "source": [
    "// MODELE 2 : Melange de deux gaussiennes\n",
    "Variable<bool> evidence2 = Variable.Bernoulli(0.5).Named(\"evidence2\");\n",
    "\n",
    "using (Variable.If(evidence2))\n",
    "{\n",
    "    Variable<double> moyenne2a = Variable.GaussianFromMeanAndPrecision(10, 0.01);\n",
    "    Variable<double> moyenne2b = Variable.GaussianFromMeanAndPrecision(20, 0.01);\n",
    "    Variable<double> precision2 = Variable.GammaFromShapeAndScale(2, 0.5);\n",
    "    Variable<double> poidsMixte = Variable.Beta(1, 1);\n",
    "    \n",
    "    for (int i = 0; i < n; i++)\n",
    "    {\n",
    "        Variable<bool> composante = Variable.Bernoulli(poidsMixte);\n",
    "        Variable<double> obs2 = Variable.New<double>();\n",
    "        using (Variable.If(composante))\n",
    "        {\n",
    "            obs2.SetTo(Variable.GaussianFromMeanAndPrecision(moyenne2a, precision2));\n",
    "        }\n",
    "        using (Variable.IfNot(composante))\n",
    "        {\n",
    "            obs2.SetTo(Variable.GaussianFromMeanAndPrecision(moyenne2b, precision2));\n",
    "        }\n",
    "        obs2.ObservedValue = observations[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "InferenceEngine moteur2 = new InferenceEngine(new VariationalMessagePassing());\n",
    "moteur2.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "\n",
    "double logEvidence2 = moteur2.Infer<Bernoulli>(evidence2).LogOdds;\n",
    "\n",
    "Console.WriteLine($\"Modele 2 (melange 2 gaussiennes) : log evidence = {logEvidence2:F2}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35652b62",
   "metadata": {
    "papermill": {
     "duration": 0.00547,
     "end_time": "2026-01-27T02:02:37.088312",
     "exception": false,
     "start_time": "2026-01-27T02:02:37.082842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Facteur de Bayes\n",
    "\n",
    "### Definition\n",
    "\n",
    "$$BF_{12} = \\frac{P(D|M_1)}{P(D|M_2)} = \\exp(\\log E_1 - \\log E_2)$$\n",
    "\n",
    "### Interpretation (echelle de Jeffreys)\n",
    "\n",
    "| log(BF) | BF | Evidence pour M1 |\n",
    "|---------|----|-----------------|\n",
    "| 0-1 | 1-3 | Negligeable |\n",
    "| 1-2 | 3-10 | Substantielle |\n",
    "| 2-3 | 10-30 | Forte |\n",
    "| 3-5 | 30-150 | Tres forte |\n",
    "| >5 | >150 | Decisive |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918883d",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:37.100399Z",
     "iopub.status.busy": "2026-01-27T02:02:37.100150Z",
     "iopub.status.idle": "2026-01-27T02:02:37.158796Z",
     "shell.execute_reply": "2026-01-27T02:02:37.158641Z"
    },
    "papermill": {
     "duration": 0.065118,
     "end_time": "2026-01-27T02:02:37.158861",
     "exception": false,
     "start_time": "2026-01-27T02:02:37.093743",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Facteur de Bayes ===\n",
      "\n",
      "log(BF) = 3,14\n",
      "BF = 23,03\n",
      "\n",
      "Evidence tres forte/decisive en faveur de : Modele 1 (1 gaussienne)\n"
     ]
    }
   ],
   "source": [
    "// Facteur de Bayes\n",
    "double logBF = logEvidence1 - logEvidence2;\n",
    "double BF = Math.Exp(logBF);\n",
    "\n",
    "Console.WriteLine(\"=== Facteur de Bayes ===\");\n",
    "Console.WriteLine($\"\\nlog(BF) = {logBF:F2}\");\n",
    "Console.WriteLine($\"BF = {BF:F2}\");\n",
    "\n",
    "string interpretation;\n",
    "if (Math.Abs(logBF) < 1) interpretation = \"Evidence negligeable\";\n",
    "else if (Math.Abs(logBF) < 2) interpretation = \"Evidence substantielle\";\n",
    "else if (Math.Abs(logBF) < 3) interpretation = \"Evidence forte\";\n",
    "else interpretation = \"Evidence tres forte/decisive\";\n",
    "\n",
    "string favori = logBF > 0 ? \"Modele 1 (1 gaussienne)\" : \"Modele 2 (melange)\";\n",
    "Console.WriteLine($\"\\n{interpretation} en faveur de : {favori}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2zz6zocoea7",
   "metadata": {},
   "source": [
    "### Interprétation du facteur de Bayes\n",
    "\n",
    "**Résultat** : log(BF) = 3.14, BF ≈ 23\n",
    "\n",
    "Selon l'échelle de Jeffreys, un BF de 23 représente une **evidence forte** (entre 10 et 30) en faveur du modèle 1.\n",
    "\n",
    "**Pourquoi le modèle simple gagne-t-il ?**\n",
    "\n",
    "Les données {13, 15, 17, 14, 16, 15, 18} sont **unimodales** avec moyenne ~15.3. Le modèle à 2 gaussiennes :\n",
    "1. Introduit des paramètres inutiles (2 moyennes, poids du mélange)\n",
    "2. Ces paramètres doivent être \"expliqués\" par le prior\n",
    "3. Le prior \"dilue\" la vraisemblance sur un espace plus grand\n",
    "\n",
    "C'est le **rasoir d'Occam bayésien** en action : à ajustement égal, le modèle simple est préféré car il fait des prédictions plus \"concentrées\".\n",
    "\n",
    "> **Formule intuitive** : Evidence ≈ (vraisemblance) × (volume du prior utilisé) / (volume total du prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ddb475",
   "metadata": {
    "papermill": {
     "duration": 0.005049,
     "end_time": "2026-01-27T02:02:37.169051",
     "exception": false,
     "start_time": "2026-01-27T02:02:37.164002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Selection du Nombre de Composantes\n",
    "\n",
    "### Application\n",
    "\n",
    "Determiner le nombre optimal de composantes dans un modele de melange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uv9cebrgw1",
   "source": "### Donnees bimodales : quand le modele complexe gagne\n\nLes donnees precedentes etaient unimodales (autour de 15). Testons maintenant avec des donnees **clairement bimodales** : deux groupes bien separes autour de 6 et 15.\n\n**Question** : Le modele a 2 composantes va-t-il maintenant etre prefere ?\n\nL'algorithme compare systematiquement 1 vs 2 composantes en calculant l'evidence de chaque modele.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda97cef",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:37.179992Z",
     "iopub.status.busy": "2026-01-27T02:02:37.179731Z",
     "iopub.status.idle": "2026-01-27T02:02:42.493509Z",
     "shell.execute_reply": "2026-01-27T02:02:42.485604Z"
    },
    "papermill": {
     "duration": 5.319582,
     "end_time": "2026-01-27T02:02:42.493580",
     "exception": false,
     "start_time": "2026-01-27T02:02:37.173998",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Selection du nombre de composantes ===\n",
      "Donnees : 5, 6, 7, 5,5, 6,5, 15, 16, 17, 14, 15,5, 16,5, 6, 15\n",
      "\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "1 composante : log evidence = -45,89\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "2 composantes : log evidence = -31,35\n",
      "\n",
      "=> Le modele a 2 composante(s) est prefere\n"
     ]
    }
   ],
   "source": [
    "// Donnees bimodales\n",
    "double[] dataBimodal = { 5, 6, 7, 5.5, 6.5, 15, 16, 17, 14, 15.5, 16.5, 6, 15 };\n",
    "int nBi = dataBimodal.Length;\n",
    "\n",
    "Console.WriteLine(\"=== Selection du nombre de composantes ===\");\n",
    "Console.WriteLine($\"Donnees : {string.Join(\", \", dataBimodal)}\\n\");\n",
    "\n",
    "// Test avec 1, 2, 3 composantes\n",
    "double[] logEvidences = new double[3];\n",
    "\n",
    "// 1 composante\n",
    "{\n",
    "    Variable<bool> ev = Variable.Bernoulli(0.5);\n",
    "    using (Variable.If(ev))\n",
    "    {\n",
    "        Variable<double> m = Variable.GaussianFromMeanAndPrecision(10, 0.01);\n",
    "        Variable<double> p = Variable.GammaFromShapeAndScale(2, 0.5);\n",
    "        foreach (var d in dataBimodal)\n",
    "        {\n",
    "            Variable<double> o = Variable.GaussianFromMeanAndPrecision(m, p);\n",
    "            o.ObservedValue = d;\n",
    "        }\n",
    "    }\n",
    "    var eng = new InferenceEngine();\n",
    "    eng.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "    logEvidences[0] = eng.Infer<Bernoulli>(ev).LogOdds;\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"1 composante : log evidence = {logEvidences[0]:F2}\");\n",
    "\n",
    "// 2 composantes - simplifie\n",
    "{\n",
    "    Variable<bool> ev = Variable.Bernoulli(0.5);\n",
    "    using (Variable.If(ev))\n",
    "    {\n",
    "        Variable<double> m1 = Variable.GaussianFromMeanAndPrecision(6, 0.1);\n",
    "        Variable<double> m2 = Variable.GaussianFromMeanAndPrecision(15, 0.1);\n",
    "        Variable<double> p = Variable.GammaFromShapeAndScale(2, 1);\n",
    "        Variable<double> w = Variable.Beta(1, 1);\n",
    "        \n",
    "        foreach (var d in dataBimodal)\n",
    "        {\n",
    "            Variable<bool> c = Variable.Bernoulli(w);\n",
    "            Variable<double> o = Variable.New<double>();\n",
    "            using (Variable.If(c)) { o.SetTo(Variable.GaussianFromMeanAndPrecision(m1, p)); }\n",
    "            using (Variable.IfNot(c)) { o.SetTo(Variable.GaussianFromMeanAndPrecision(m2, p)); }\n",
    "            o.ObservedValue = d;\n",
    "        }\n",
    "    }\n",
    "    var eng = new InferenceEngine(new VariationalMessagePassing());\n",
    "    eng.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "    logEvidences[1] = eng.Infer<Bernoulli>(ev).LogOdds;\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"2 composantes : log evidence = {logEvidences[1]:F2}\");\n",
    "\n",
    "// Meilleur modele\n",
    "int meilleur = logEvidences[0] > logEvidences[1] ? 1 : 2;\n",
    "Console.WriteLine($\"\\n=> Le modele a {meilleur} composante(s) est prefere\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8z4v08m86qg",
   "source": "### Analyse des resultats : donnees bimodales\n\n**Donnees** : deux groupes distincts autour de 6 et 15\n\n| Modele | Log evidence | Interpretation |\n|--------|--------------|----------------|\n| 1 composante | -45.89 | Mal adapte (moyenne ~10.5 ne represente aucun groupe) |\n| 2 composantes | -31.35 | Bien adapte (capture les deux modes) |\n\n**Difference** : log(BF) = -31.35 - (-45.89) = 14.54\n\nUn facteur de Bayes $e^{14.54} \\approx 2 \\times 10^6$ en faveur du modele a 2 composantes constitue une evidence **decisive**.\n\n> **Lecon cle** : Le facteur de Bayes detecte automatiquement la structure des donnees. Il prefere le modele simple quand les donnees sont simples (section 3-4) et le modele complexe quand les donnees l'exigent (ici).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "4abedeba",
   "metadata": {
    "papermill": {
     "duration": 0.007711,
     "end_time": "2026-01-27T02:02:42.509717",
     "exception": false,
     "start_time": "2026-01-27T02:02:42.502006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Automatic Relevance Determination (ARD)\n",
    "\n",
    "### Principe\n",
    "\n",
    "ARD utilise des priors hierarchiques pour determiner automatiquement quelles features sont pertinentes.\n",
    "\n",
    "### Modele\n",
    "\n",
    "$$\\alpha_f \\sim \\text{Gamma}(a, b)$$\n",
    "$$w_f \\sim \\mathcal{N}(0, \\alpha_f^{-1})$$\n",
    "\n",
    "Si $\\alpha_f$ devient grand, le poids $w_f$ est contraint pres de 0 -> feature non pertinente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "byqokwwe3av",
   "source": "### Transition : de la comparaison de modeles a la selection de features\n\nJusqu'ici, nous avons compare des **modeles entiers** (1 vs 2 gaussiennes, lineaire vs quadratique). Mais en pratique, on veut souvent repondre a une question plus fine :\n\n**Quelles features sont vraiment utiles dans mon modele ?**\n\nC'est le probleme de la **selection de variables**. L'approche bayesienne offre une solution elegante : l'**Automatic Relevance Determination** (ARD).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "99fn0bnvoyc",
   "source": "### Generation des donnees synthetiques\n\nNous creons un probleme de regression ou :\n- **Feature 1** : coefficient reel = 2.0 (pertinente)\n- **Feature 2** : coefficient reel = 0.0 (non pertinente)\n- **Feature 3** : coefficient reel = 3.0 (pertinente)\n\nLe modele ARD devra \"decouvrir\" automatiquement que la feature 2 n'apporte aucune information predictive.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4467883",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:42.526072Z",
     "iopub.status.busy": "2026-01-27T02:02:42.525853Z",
     "iopub.status.idle": "2026-01-27T02:02:42.614347Z",
     "shell.execute_reply": "2026-01-27T02:02:42.614197Z"
    },
    "papermill": {
     "duration": 0.097134,
     "end_time": "2026-01-27T02:02:42.614415",
     "exception": false,
     "start_time": "2026-01-27T02:02:42.517281",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ARD : Automatic Relevance Determination ===\n",
      "\n",
      "Vrais poids : w1=2, w2=0 (non pertinent), w3=3\n"
     ]
    }
   ],
   "source": [
    "// ARD pour regression\n",
    "\n",
    "// Donnees : y = 2*x1 + 0*x2 + 3*x3 + bruit\n",
    "// x2 est une feature non pertinente\n",
    "int nSamples = 20;\n",
    "int nFeatures = 3;\n",
    "Random rng = new Random(42);\n",
    "\n",
    "double[,] X = new double[nSamples, nFeatures];\n",
    "double[] y = new double[nSamples];\n",
    "double[] vraisPoids = { 2.0, 0.0, 3.0 };  // x2 a poids 0\n",
    "\n",
    "for (int i = 0; i < nSamples; i++)\n",
    "{\n",
    "    for (int f = 0; f < nFeatures; f++)\n",
    "    {\n",
    "        X[i, f] = rng.NextDouble() * 2 - 1;  // [-1, 1]\n",
    "    }\n",
    "    y[i] = vraisPoids[0] * X[i, 0] + vraisPoids[1] * X[i, 1] + vraisPoids[2] * X[i, 2]\n",
    "           + rng.NextDouble() * 0.5 - 0.25;  // Bruit\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"=== ARD : Automatic Relevance Determination ===\");\n",
    "Console.WriteLine($\"\\nVrais poids : w1={vraisPoids[0]}, w2={vraisPoids[1]} (non pertinent), w3={vraisPoids[2]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h9z45ebt0u",
   "source": "### Construction du modele ARD hierarchique\n\nLe modele ARD introduit un **hyperparametre de precision** $\\alpha_f$ pour chaque feature $f$ :\n\n$$w_f \\sim \\mathcal{N}(0, \\alpha_f^{-1})$$\n\n**Interpretation** :\n- Si $\\alpha_f$ est petit (ex: 0.3), la variance $1/\\alpha_f$ est grande, donc $w_f$ peut prendre des valeurs significatives\n- Si $\\alpha_f$ devient grand (ex: 10), la variance est petite, $w_f$ est \"pousse\" vers 0\n\nLes $\\alpha_f$ sont eux-memes des variables aleatoires avec prior `Gamma(1, 1)`. L'inference determine simultanement les poids et leur pertinence.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5337822",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:42.633063Z",
     "iopub.status.busy": "2026-01-27T02:02:42.632802Z",
     "iopub.status.idle": "2026-01-27T02:02:43.058320Z",
     "shell.execute_reply": "2026-01-27T02:02:43.058188Z"
    },
    "papermill": {
     "duration": 0.43579,
     "end_time": "2026-01-27T02:02:43.058384",
     "exception": false,
     "start_time": "2026-01-27T02:02:42.622594",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "\n",
      "Resultats ARD :\n",
      "  Feature 1 : poids = 2,04 +/- 0,12, alpha = 0,49 (pertinence haute)\n",
      "  Feature 2 : poids = 0,02 +/- 0,11, alpha = 1,49 (pertinence moyenne)\n",
      "  Feature 3 : poids = 2,97 +/- 0,15, alpha = 0,28 (pertinence haute)\n",
      "\n",
      "=> Les features avec alpha eleve sont considerees non pertinentes\n"
     ]
    }
   ],
   "source": [
    "// Modele ARD\n",
    "Range sampleRange = new Range(nSamples).Named(\"sample\");\n",
    "Range featureRange = new Range(nFeatures).Named(\"feature\");\n",
    "\n",
    "// Precisions par feature (ARD)\n",
    "VariableArray<double> alpha = Variable.Array<double>(featureRange).Named(\"alpha\");\n",
    "alpha[featureRange] = Variable.GammaFromShapeAndScale(1, 1).ForEach(featureRange);\n",
    "\n",
    "// Poids avec prior dependant de alpha\n",
    "VariableArray<double> poids = Variable.Array<double>(featureRange).Named(\"poids\");\n",
    "using (Variable.ForEach(featureRange))\n",
    "{\n",
    "    poids[featureRange] = Variable.GaussianFromMeanAndPrecision(0, alpha[featureRange]);\n",
    "}\n",
    "\n",
    "// Bruit de l'observation\n",
    "Variable<double> noisePrecision = Variable.GammaFromShapeAndScale(2, 1).Named(\"noise\");\n",
    "\n",
    "// Donnees\n",
    "VariableArray2D<double> xVar = Variable.Array<double>(sampleRange, featureRange).Named(\"x\");\n",
    "VariableArray<double> yVar = Variable.Array<double>(sampleRange).Named(\"y\");\n",
    "\n",
    "using (Variable.ForEach(sampleRange))\n",
    "{\n",
    "    Variable<double> prediction = Variable.Constant(0.0);\n",
    "    for (int f = 0; f < nFeatures; f++)\n",
    "    {\n",
    "        prediction = prediction + poids[f] * xVar[sampleRange, f];\n",
    "    }\n",
    "    yVar[sampleRange] = Variable.GaussianFromMeanAndPrecision(prediction, noisePrecision);\n",
    "}\n",
    "\n",
    "xVar.ObservedValue = X;\n",
    "yVar.ObservedValue = y;\n",
    "\n",
    "InferenceEngine moteurARD = new InferenceEngine(new ExpectationPropagation());\n",
    "moteurARD.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "\n",
    "Gaussian[] poidsPost = moteurARD.Infer<Gaussian[]>(poids);\n",
    "Gamma[] alphaPost = moteurARD.Infer<Gamma[]>(alpha);\n",
    "\n",
    "Console.WriteLine(\"\\nResultats ARD :\");\n",
    "for (int f = 0; f < nFeatures; f++)\n",
    "{\n",
    "    double wMean = poidsPost[f].GetMean();\n",
    "    double wStd = Math.Sqrt(poidsPost[f].GetVariance());\n",
    "    double alphaMean = alphaPost[f].GetMean();\n",
    "    string relevance = alphaMean > 5 ? \"faible\" : alphaMean > 1 ? \"moyenne\" : \"haute\";\n",
    "    Console.WriteLine($\"  Feature {f+1} : poids = {wMean:F2} +/- {wStd:F2}, alpha = {alphaMean:F2} (pertinence {relevance})\");\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"\\n=> Les features avec alpha eleve sont considerees non pertinentes\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xdkmdixf5b",
   "metadata": {},
   "source": [
    "### Analyse des résultats ARD\n",
    "\n",
    "**Comparaison vrais poids vs estimations** :\n",
    "\n",
    "| Feature | Vrai poids | Estimé | α estimé | Pertinence |\n",
    "|---------|------------|--------|----------|------------|\n",
    "| 1 | **2.0** | 2.04 | 0.49 | Haute ✓ |\n",
    "| 2 | **0.0** | 0.02 | 1.49 | Moyenne |\n",
    "| 3 | **3.0** | 2.97 | 0.28 | Haute ✓ |\n",
    "\n",
    "**Interprétation des hyperparamètres α** :\n",
    "- **α petit** (< 1) → le prior sur w est large → w peut prendre des valeurs significatives → feature pertinente\n",
    "- **α grand** (> 1) → le prior sur w est concentré autour de 0 → w contraint à ~0 → feature non pertinente\n",
    "\n",
    "**Pourquoi Feature 2 n'a pas α très élevé ?**\n",
    "\n",
    "Avec seulement 20 échantillons, le modèle reste incertain. α = 1.49 indique une pertinence \"moyenne\" plutôt que clairement nulle. Plus de données augmenteraient α pour cette feature.\n",
    "\n",
    "> **Application pratique** : ARD est utilisé en machine learning pour la **sélection automatique de features** sans validation croisée explicite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b71176",
   "metadata": {
    "papermill": {
     "duration": 0.00924,
     "end_time": "2026-01-27T02:02:43.077040",
     "exception": false,
     "start_time": "2026-01-27T02:02:43.067800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Validation Croisee Bayesienne\n",
    "\n",
    "### Principe\n",
    "\n",
    "Au lieu de diviser les donnees, utiliser la predictive posterieure pour evaluer le modele.\n",
    "\n",
    "### Leave-One-Out (LOO)\n",
    "\n",
    "$$\\text{LOO-CV} = \\sum_{i=1}^n \\log P(y_i | y_{-i}, M)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vra9u39ies8",
   "source": "### Algorithme Leave-One-Out bayesien\n\nPour chaque point $i$ :\n1. **Entrainer** le modele sur tous les points sauf $i$\n2. **Calculer** la distribution predictive posterieure\n3. **Evaluer** la log-probabilite du point $i$ sous cette predictive\n\nLa **variance predictive** combine deux sources d'incertitude :\n- L'incertitude sur la moyenne ($\\text{Var}(\\mu)$)\n- Le bruit inherent des observations ($1/\\tau$)\n\n$$\\sigma^2_{\\text{pred}} = \\text{Var}(\\mu) + \\frac{1}{\\mathbb{E}[\\tau]}$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f2af1",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:43.096299Z",
     "iopub.status.busy": "2026-01-27T02:02:43.096128Z",
     "iopub.status.idle": "2026-01-27T02:02:45.734916Z",
     "shell.execute_reply": "2026-01-27T02:02:45.734773Z"
    },
    "papermill": {
     "duration": 2.64901,
     "end_time": "2026-01-27T02:02:45.734994",
     "exception": false,
     "start_time": "2026-01-27T02:02:43.085984",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "=== Validation Leave-One-Out ===\n",
      "\n",
      "Log predictive totale : -18,13\n",
      "Log predictive moyenne : -1,81\n"
     ]
    }
   ],
   "source": [
    "// Validation LOO simplifiee\n",
    "\n",
    "double[] dataLOO = { 10, 12, 11, 13, 12, 11, 14, 10, 12, 11 };\n",
    "int nLOO = dataLOO.Length;\n",
    "\n",
    "double totalLogPred = 0;\n",
    "\n",
    "for (int i = 0; i < nLOO; i++)\n",
    "{\n",
    "    // Entrainer sur toutes les donnees sauf i\n",
    "    Variable<double> mu = Variable.GaussianFromMeanAndPrecision(10, 0.01);\n",
    "    Variable<double> prec = Variable.GammaFromShapeAndScale(2, 0.5);\n",
    "    \n",
    "    for (int j = 0; j < nLOO; j++)\n",
    "    {\n",
    "        if (j != i)\n",
    "        {\n",
    "            Variable<double> obs = Variable.GaussianFromMeanAndPrecision(mu, prec);\n",
    "            obs.ObservedValue = dataLOO[j];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    InferenceEngine eng = new InferenceEngine();\n",
    "    eng.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "    \n",
    "    Gaussian muPost = eng.Infer<Gaussian>(mu);\n",
    "    Gamma precPost = eng.Infer<Gamma>(prec);\n",
    "    \n",
    "    // Probabilite predictive pour le point i\n",
    "    double predMean = muPost.GetMean();\n",
    "    double predVar = muPost.GetVariance() + 1.0 / precPost.GetMean();\n",
    "    \n",
    "    double logProb = Gaussian.FromMeanAndVariance(predMean, predVar).GetLogProb(dataLOO[i]);\n",
    "    totalLogPred += logProb;\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"=== Validation Leave-One-Out ===\");\n",
    "Console.WriteLine($\"\\nLog predictive totale : {totalLogPred:F2}\");\n",
    "Console.WriteLine($\"Log predictive moyenne : {totalLogPred / nLOO:F2}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r1j8wn479sc",
   "source": "### Interpretation de la validation LOO\n\n**Log predictive moyenne = -1.81**\n\nCette metrique mesure la capacite du modele a predire des observations non vues. Comparons avec des references :\n\n| Log predictive moyenne | Qualite du modele |\n|------------------------|-------------------|\n| > -1.0 | Excellente |\n| -1.0 a -2.0 | Bonne |\n| -2.0 a -3.0 | Acceptable |\n| < -3.0 | Mauvaise |\n\nNotre valeur de -1.81 indique une **bonne** capacite predictive.\n\n> **Avantage du LOO bayesien** : Contrairement au LOO classique, cette methode :\n> - Utilise l'incertitude complete (pas juste une estimation ponctuelle)\n> - Tient compte de l'incertitude sur les parametres via la variance predictive\n> - Ne necessite pas de reentrainer completement le modele (en theorie, via des approximations)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "d90f5f02",
   "metadata": {
    "papermill": {
     "duration": 0.026055,
     "end_time": "2026-01-27T02:02:45.788833",
     "exception": false,
     "start_time": "2026-01-27T02:02:45.762778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Exercice : Comparer Polynomes\n",
    "\n",
    "### Enonce\n",
    "\n",
    "Comparez trois modeles de regression :\n",
    "- Lineaire : y = a*x + b\n",
    "- Quadratique : y = a*x^2 + b*x + c\n",
    "- Cubique : y = a*x^3 + b*x^2 + c*x + d\n",
    "\n",
    "Sur des donnees lineaires avec bruit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h3k6kjvamho",
   "source": "### Mise en pratique : comparaison lineaire vs quadratique\n\nLes donnees sont generees selon $y = 2x + 1 + \\epsilon$ (relation lineaire).\n\n**Modeles a comparer** :\n- **Lineaire** : $y = ax + b$ (2 parametres)\n- **Quadratique** : $y = ax^2 + bx + c$ (3 parametres)\n\nLe modele quadratique peut representer la relation lineaire (avec $a \\approx 0$), mais paie un \"cout de complexite\" pour ce parametre inutile.\n\n> **Hint** : Pour ajouter un modele cubique, il suffirait d'ajouter un terme $d \\times x^3$. Le meme raisonnement s'applique : plus de parametres = plus de penalite si ils ne sont pas necessaires.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5bf6cf",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:45.841972Z",
     "iopub.status.busy": "2026-01-27T02:02:45.841691Z",
     "iopub.status.idle": "2026-01-27T02:02:53.439247Z",
     "shell.execute_reply": "2026-01-27T02:02:53.439096Z"
    },
    "papermill": {
     "duration": 7.622729,
     "end_time": "2026-01-27T02:02:53.439324",
     "exception": false,
     "start_time": "2026-01-27T02:02:45.816595",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparaison de Modeles Polynomiaux ===\n",
      "Vraie relation : y = 2*x + 1\n",
      "\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Modele lineaire : log evidence = -14,03\n",
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "Modele quadratique : log evidence = -20,28\n",
      "\n",
      "=> Le modele Lineaire est prefere (rasoir d'Occam)\n"
     ]
    }
   ],
   "source": [
    "// EXERCICE : Comparaison de modeles polynomiaux\n",
    "\n",
    "// Donnees lineaires : y = 2*x + 1 + bruit\n",
    "double[] xPoly = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n",
    "double[] yPoly = { 1.2, 3.1, 4.8, 7.2, 8.9, 11.1, 13.0, 14.8, 17.2, 19.1 };\n",
    "int nPoly = xPoly.Length;\n",
    "\n",
    "Console.WriteLine(\"=== Comparaison de Modeles Polynomiaux ===\");\n",
    "Console.WriteLine(\"Vraie relation : y = 2*x + 1\\n\");\n",
    "\n",
    "// Modele lineaire\n",
    "Variable<bool> evLin = Variable.Bernoulli(0.5);\n",
    "using (Variable.If(evLin))\n",
    "{\n",
    "    Variable<double> a = Variable.GaussianFromMeanAndVariance(0, 10);\n",
    "    Variable<double> b = Variable.GaussianFromMeanAndVariance(0, 10);\n",
    "    Variable<double> noise = Variable.GammaFromShapeAndScale(2, 0.5);\n",
    "    \n",
    "    for (int i = 0; i < nPoly; i++)\n",
    "    {\n",
    "        Variable<double> pred = a * xPoly[i] + b;\n",
    "        Variable<double> obs = Variable.GaussianFromMeanAndPrecision(pred, noise);\n",
    "        obs.ObservedValue = yPoly[i];\n",
    "    }\n",
    "}\n",
    "var engLin = new InferenceEngine();\n",
    "engLin.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "double logEvLin = engLin.Infer<Bernoulli>(evLin).LogOdds;\n",
    "Console.WriteLine($\"Modele lineaire : log evidence = {logEvLin:F2}\");\n",
    "\n",
    "// Modele quadratique\n",
    "Variable<bool> evQuad = Variable.Bernoulli(0.5);\n",
    "using (Variable.If(evQuad))\n",
    "{\n",
    "    Variable<double> a = Variable.GaussianFromMeanAndVariance(0, 10);\n",
    "    Variable<double> b = Variable.GaussianFromMeanAndVariance(0, 10);\n",
    "    Variable<double> c = Variable.GaussianFromMeanAndVariance(0, 10);\n",
    "    Variable<double> noise = Variable.GammaFromShapeAndScale(2, 0.5);\n",
    "    \n",
    "    for (int i = 0; i < nPoly; i++)\n",
    "    {\n",
    "        Variable<double> pred = a * xPoly[i] * xPoly[i] + b * xPoly[i] + c;\n",
    "        Variable<double> obs = Variable.GaussianFromMeanAndPrecision(pred, noise);\n",
    "        obs.ObservedValue = yPoly[i];\n",
    "    }\n",
    "}\n",
    "var engQuad = new InferenceEngine();\n",
    "engQuad.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "double logEvQuad = engQuad.Infer<Bernoulli>(evQuad).LogOdds;\n",
    "Console.WriteLine($\"Modele quadratique : log evidence = {logEvQuad:F2}\");\n",
    "\n",
    "// Meilleur modele\n",
    "string meilleurMod = logEvLin > logEvQuad ? \"Lineaire\" : \"Quadratique\";\n",
    "Console.WriteLine($\"\\n=> Le modele {meilleurMod} est prefere (rasoir d'Occam)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h3advals0x",
   "source": "### Analyse de l'exercice polynomes\n\n**Resultats** :\n\n| Modele | Parametres | Log evidence |\n|--------|------------|--------------|\n| Lineaire | a, b (2) | -14.03 |\n| Quadratique | a, b, c (3) | -20.28 |\n\n**Facteur de Bayes** : $\\exp(-14.03 - (-20.28)) = \\exp(6.25) \\approx 518$\n\nLe modele lineaire est **decisement** prefere (BF > 150).\n\n**Pourquoi le modele quadratique perd-il ?**\n\n1. **Donnees generees lineairement** : y = 2x + 1 + bruit\n2. **Coefficient quadratique inutile** : le parametre $a$ (coefficient de $x^2$) n'apporte rien\n3. **Penalite de complexite** : le prior sur $a$ \"dilue\" la vraisemblance\n\n> **Exercice supplementaire** : Que se passerait-il si les donnees etaient generees par y = 0.1*x^2 + 2*x + 1 ? Le terme quadratique est present mais faible. Le modele lineaire pourrait encore gagner si le signal quadratique est noye dans le bruit - c'est la balance ajustement vs complexite en action.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ff1f079d",
   "metadata": {
    "papermill": {
     "duration": 0.025279,
     "end_time": "2026-01-27T02:02:53.498487",
     "exception": false,
     "start_time": "2026-01-27T02:02:53.473208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Resume\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Evidence** | P(D\\|M) - vraisemblance marginale |\n",
    "| **Facteur de Bayes** | Ratio d'evidences pour comparer modeles |\n",
    "| **Rasoir d'Occam** | Preference automatique pour modeles simples |\n",
    "| **ARD** | Selection automatique de features |\n",
    "| **LOO-CV** | Validation sans diviser les donnees |\n",
    "\n",
    "---\n",
    "\n",
    "## Prochaine etape\n",
    "\n",
    "Dans [Infer-9-Topic-Models](Infer-9-Topic-Models.ipynb), nous explorerons :\n",
    "\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "- Modelisation de topics dans les documents\n",
    "- Inference sur structures hierarchiques complexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emyx91kfzua",
   "source": "---\n\n## Annexe : Distributions et concepts\n\n### Distributions utilisees dans ce notebook\n\n| Distribution | Role | Parametres typiques |\n|--------------|------|---------------------|\n| `Bernoulli(0.5)` | Variable indicatrice pour calcul d'evidence | p=0.5 (prior non informatif) |\n| `GaussianFromMeanAndPrecision` | Modele d'observation | mean~15, precision~1 |\n| `GammaFromShapeAndScale` | Prior sur precision | shape=2, scale=0.5 |\n| `Beta(1,1)` | Prior sur proportion de melange | Prior uniforme sur [0,1] |\n\n### Concepts probabilistes illustres\n\n| Concept | Section | Application |\n|---------|---------|-------------|\n| Evidence marginale $P(D\\|M)$ | 3, 5 | Comparer modeles sans validation croisee |\n| Facteur de Bayes | 4 | Quantifier la preference pour un modele |\n| Rasoir d'Occam bayesien | 3-5 | Penalisation automatique de la complexite |\n| Priors hierarchiques | 6 | ARD pour selection de features |\n| Distribution predictive | 7 | LOO-CV bayesien |\n\n### Applications pratiques\n\n- **Selection du nombre de composantes** : Clustering avec nombre de clusters inconnu\n- **Selection de features** : Regression avec beaucoup de covariables\n- **Choix d'architecture** : Reseaux bayesiens avec structure variable\n- **Comparaison de familles** : Lineaire vs non-lineaire, parametrique vs non-parametrique",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "lzrf27g1xck",
   "source": "### Points cles a retenir\n\n**1. Le rasoir d'Occam est automatique**\n\nLa selection de modeles bayesienne penalise naturellement la complexite. Pas besoin de criteres ad hoc comme l'AIC ou le BIC - l'evidence marginale fait le travail.\n\n**2. L'echelle compte**\n\n| Methode | Quand l'utiliser |\n|---------|------------------|\n| Facteur de Bayes | Comparer 2-3 modeles distincts |\n| ARD | Selectionner parmi de nombreuses features |\n| LOO-CV | Evaluer la capacite predictive |\n\n**3. Les priors sont importants**\n\nLes priors vagues ($\\sigma^2 = 10$ sur les poids) penalisent moins que des priors informatifs. Un mauvais choix de prior peut fausser la comparaison.\n\n> **Conseil pratique** : Pour une comparaison equitable, utilisez des priors de meme \"force\" (meme variance) sur les parametres comparables entre modeles.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "file_extension": ".cs",
   "mimetype": "text/x-csharp",
   "name": "C#",
   "pygments_lexer": "csharp",
   "version": "13.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.894072,
   "end_time": "2026-01-27T02:02:53.767864",
   "environment_variables": {},
   "exception": null,
   "input_path": "c:\\dev\\CoursIA\\MyIA.AI.Notebooks\\Probas\\Infer\\Infer-8-Model-Selection.ipynb",
   "output_path": "c:\\dev\\CoursIA\\MyIA.AI.Notebooks\\Probas\\Infer\\test_outputs\\Infer-8-Model-Selection_output.ipynb",
   "parameters": {},
   "start_time": "2026-01-27T02:02:26.873792",
   "version": "2.6.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}