{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a615707",
   "metadata": {
    "papermill": {
     "duration": 0.0088,
     "end_time": "2026-01-27T02:02:15.887378",
     "exception": false,
     "start_time": "2026-01-27T02:02:15.878578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer-7-Classification : Classification Bayesienne\n",
    "\n",
    "**Serie** : Programmation Probabiliste avec Infer.NET (7/13)  \n",
    "**Duree estimee** : 50 minutes  \n",
    "**Prerequis** : Infer-6-TrueSkill\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Implementer la regression logistique bayesienne\n",
    "- Comprendre le Bayes Point Machine (BPM)\n",
    "- Appliquer l'inference bayesienne aux tests cliniques (A/B testing)\n",
    "- Gerer l'incertitude dans les predictions\n",
    "\n",
    "---\n",
    "\n",
    "## Navigation\n",
    "\n",
    "| Precedent | Suivant |\n",
    "|-----------|--------|\n",
    "| [Infer-6-TrueSkill](Infer-6-TrueSkill.ipynb) | [Infer-8-Model-Selection](Infer-8-Model-Selection.ipynb) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba6ecc",
   "metadata": {
    "papermill": {
     "duration": 0.010652,
     "end_time": "2026-01-27T02:02:15.910800",
     "exception": false,
     "start_time": "2026-01-27T02:02:15.900148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Cette section prepare l'environnement pour les modeles de classification bayesienne. Contrairement aux classifieurs deterministes, l'approche bayesienne fournit non seulement une prediction mais aussi une mesure de confiance via des distributions de probabilite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b0d07",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:15.944117Z",
     "iopub.status.busy": "2026-01-27T02:02:15.932556Z",
     "iopub.status.idle": "2026-01-27T02:02:18.692733Z",
     "shell.execute_reply": "2026-01-27T02:02:18.690385Z"
    },
    "papermill": {
     "duration": 2.773075,
     "end_time": "2026-01-27T02:02:18.692840",
     "exception": false,
     "start_time": "2026-01-27T02:02:15.919765",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.ML.Probabilistic, 0.4.2504.701</span></li><li><span>Microsoft.ML.Probabilistic.Compiler, 0.4.2504.701</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer.NET pret !\r\n"
     ]
    }
   ],
   "source": [
    "#r \"nuget: Microsoft.ML.Probabilistic\"\n",
    "#r \"nuget: Microsoft.ML.Probabilistic.Compiler\"\n",
    "\n",
    "using Microsoft.ML.Probabilistic;\n",
    "using Microsoft.ML.Probabilistic.Distributions;\n",
    "using Microsoft.ML.Probabilistic.Utilities;\n",
    "using Microsoft.ML.Probabilistic.Math;\n",
    "using Microsoft.ML.Probabilistic.Models;\n",
    "using Microsoft.ML.Probabilistic.Algorithms;\n",
    "using Microsoft.ML.Probabilistic.Compiler;\n",
    "\n",
    "Console.WriteLine(\"Infer.NET pret !\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q8n6lx7eupr",
   "source": "### Environnement pret\n\nLes packages Infer.NET charges incluent :\n- **Microsoft.ML.Probabilistic** : Structures de donnees probabilistes (distributions, variables)\n- **Microsoft.ML.Probabilistic.Compiler** : Compilation des modeles en code executable\n- **Microsoft.ML.Probabilistic.Math** : Fonctions mathematiques (MMath.NormalCdf pour le probit)\n\nLes algorithmes d'inference disponibles pour la classification :\n| Algorithme | Usage | Precision |\n|------------|-------|-----------|\n| **ExpectationPropagation (EP)** | Modeles probit, BPM | Haute |\n| **VariationalMessagePassing (VMP)** | Modeles gaussiens mixtes | Moderee |\n| **GibbsSampling** | Modeles complexes | Tres haute (mais lent) |\n\n> **Note** : Pour la classification, EP est generalement prefere car il gere bien les facteurs de troncature (comparaisons avec seuils) inherents aux modeles probit.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "7d75b1ca",
   "metadata": {
    "papermill": {
     "duration": 0.002768,
     "end_time": "2026-01-27T02:02:18.698440",
     "exception": false,
     "start_time": "2026-01-27T02:02:18.695672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Classification Probabiliste\n",
    "\n",
    "### Difference avec la classification classique\n",
    "\n",
    "| Approche | Sortie | Incertitude |\n",
    "|----------|--------|-------------|\n",
    "| **Classique** | Classe predite | Non |\n",
    "| **Probabiliste** | P(classe) | Oui |\n",
    "| **Bayesienne** | Distribution sur P(classe) | Oui + incertitude sur le modele |\n",
    "\n",
    "### Avantages bayesiens\n",
    "\n",
    "- Quantification de l'incertitude\n",
    "- Regularisation naturelle (priors)\n",
    "- Mise a jour incrementale\n",
    "- Pas de surapprentissage si bon prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b78841",
   "metadata": {
    "papermill": {
     "duration": 0.002157,
     "end_time": "2026-01-27T02:02:18.703173",
     "exception": false,
     "start_time": "2026-01-27T02:02:18.701016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Regression Logistique Bayesienne (1 feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4jp4i20uv3y",
   "source": "### Architecture du modele probit\n\nLe modele de regression logistique bayesienne utilise une variable latente gaussienne :\n\n$$y_i = \\mathbb{1}[w \\cdot x_i - b + \\epsilon_i > 0]$$\n\nou :\n- $w$ est le poids (prior Gaussien)\n- $b$ est le seuil (prior Gaussien)\n- $\\epsilon_i \\sim \\mathcal{N}(0, 1/\\tau)$ est le bruit (precision $\\tau$ avec prior Gamma)\n\n**Structure du graphe de facteurs** :\n\n```\npoids ~ Gaussian(0, 10)     seuil ~ Gaussian(0, 10)     bruitPrecision ~ Gamma(2, 0.5)\n        \\                         /                              /\n         \\                       /                              /\n          [score = poids*x - seuil]                            /\n                    \\                                         /\n                     \\                                       /\n                      [scoreBruite ~ Gaussian(score, bruitPrecision)]\n                                      |\n                                      v\n                              [y = scoreBruite > 0]\n```\n\nCette formulation permet d'utiliser l'Expectation Propagation pour inferer les posterieurs sur `poids` et `seuil`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c8e76",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:18.708766Z",
     "iopub.status.busy": "2026-01-27T02:02:18.708597Z",
     "iopub.status.idle": "2026-01-27T02:02:20.593853Z",
     "shell.execute_reply": "2026-01-27T02:02:20.593698Z"
    },
    "papermill": {
     "duration": 1.888459,
     "end_time": "2026-01-27T02:02:20.593928",
     "exception": false,
     "start_time": "2026-01-27T02:02:18.705469",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "=== Regression Logistique Bayesienne ===\n",
      "\n",
      "Poids : Gaussian(0,8144, 0,03683)\n",
      "Seuil : Gaussian(3,378, 0,7493)\n",
      "\n",
      "Interpretation : classe 1 si feature > 4,15\n"
     ]
    }
   ],
   "source": [
    "// Donnees : classification binaire avec 1 feature\n",
    "double[] features = { 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0 };\n",
    "bool[] labels = { false, false, false, true, false, true, true, true };\n",
    "int n = features.Length;\n",
    "\n",
    "// Modele : y = sigmoid(w * x - b)\n",
    "// Equivalent probit : y = I(w * x - b + noise > 0)\n",
    "\n",
    "Variable<double> poids = Variable.GaussianFromMeanAndVariance(0, 10).Named(\"poids\");\n",
    "Variable<double> seuil = Variable.GaussianFromMeanAndVariance(0, 10).Named(\"seuil\");\n",
    "Variable<double> bruitPrecision = Variable.GammaFromShapeAndScale(2, 0.5).Named(\"bruit\");\n",
    "\n",
    "Range dataRange = new Range(n);\n",
    "VariableArray<double> xObs = Variable.Array<double>(dataRange).Named(\"x\");\n",
    "VariableArray<bool> yObs = Variable.Array<bool>(dataRange).Named(\"y\");\n",
    "\n",
    "using (Variable.ForEach(dataRange))\n",
    "{\n",
    "    Variable<double> score = poids * xObs[dataRange] - seuil;\n",
    "    Variable<double> scoreBruite = Variable.GaussianFromMeanAndPrecision(score, bruitPrecision);\n",
    "    yObs[dataRange] = (scoreBruite > 0);\n",
    "}\n",
    "\n",
    "xObs.ObservedValue = features;\n",
    "yObs.ObservedValue = labels;\n",
    "\n",
    "InferenceEngine moteur = new InferenceEngine(new ExpectationPropagation());\n",
    "moteur.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "\n",
    "Gaussian poidsPost = moteur.Infer<Gaussian>(poids);\n",
    "Gaussian seuilPost = moteur.Infer<Gaussian>(seuil);\n",
    "\n",
    "Console.WriteLine(\"=== Regression Logistique Bayesienne ===\");\n",
    "Console.WriteLine($\"\\nPoids : {poidsPost}\");\n",
    "Console.WriteLine($\"Seuil : {seuilPost}\");\n",
    "Console.WriteLine($\"\\nInterpretation : classe 1 si feature > {seuilPost.GetMean() / poidsPost.GetMean():F2}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u8lbgb6u8ei",
   "metadata": {},
   "source": [
    "### Analyse du modèle de régression logistique\n",
    "\n",
    "**Résultats** :\n",
    "- Poids ≈ 0.81 (positif → classe 1 augmente avec la feature)\n",
    "- Seuil ≈ 3.38\n",
    "- Point de décision : feature > 4.15 → classe 1\n",
    "\n",
    "**Interprétation géométrique** :\n",
    "Le modèle probit définit une frontière de décision à `x ≈ 4.15`. Les données montrent effectivement une transition entre les classes autour de x=4-5.\n",
    "\n",
    "**Incertitude sur les paramètres** :\n",
    "- σ(poids) ≈ 0.19 → relativement certain\n",
    "- σ(seuil) ≈ 0.87 → plus incertain\n",
    "\n",
    "> **Avantage bayésien** : Contrairement à la régression logistique classique qui donne des estimations ponctuelles, nous obtenons des **distributions complètes** sur les paramètres, permettant de propager l'incertitude aux prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4008e7",
   "metadata": {
    "papermill": {
     "duration": 0.003935,
     "end_time": "2026-01-27T02:02:20.602194",
     "exception": false,
     "start_time": "2026-01-27T02:02:20.598259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Prediction avec Incertitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198be0d",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:20.611254Z",
     "iopub.status.busy": "2026-01-27T02:02:20.611061Z",
     "iopub.status.idle": "2026-01-27T02:02:21.781074Z",
     "shell.execute_reply": "2026-01-27T02:02:21.780883Z"
    },
    "papermill": {
     "duration": 1.175433,
     "end_time": "2026-01-27T02:02:21.781148",
     "exception": false,
     "start_time": "2026-01-27T02:02:20.605715",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Predictions avec incertitude ===\n",
      "\n",
      "Compiling model...done.\n",
      "x = 2,5 : P(classe=1) = 0,218\n",
      "Compiling model...done.\n",
      "x = 4,5 : P(classe=1) = 0,561\n",
      "Compiling model...done.\n",
      "x = 6,5 : P(classe=1) = 0,822\n",
      "Compiling model...done.\n",
      "x = 9,0 : P(classe=1) = 0,951\n"
     ]
    }
   ],
   "source": [
    "// Prediction pour de nouvelles valeurs\n",
    "double[] nouveauxX = { 2.5, 4.5, 6.5, 9.0 };\n",
    "\n",
    "Console.WriteLine(\"=== Predictions avec incertitude ===\");\n",
    "Console.WriteLine();\n",
    "\n",
    "foreach (double x in nouveauxX)\n",
    "{\n",
    "    // Modele de prediction\n",
    "    Variable<Gaussian> poidsPrior = Variable.Observed(poidsPost);\n",
    "    Variable<Gaussian> seuilPrior = Variable.Observed(seuilPost);\n",
    "    \n",
    "    Variable<double> poidsPred = Variable.Random<double, Gaussian>(poidsPrior);\n",
    "    Variable<double> seuilPred = Variable.Random<double, Gaussian>(seuilPrior);\n",
    "    Variable<double> bruitPred = Variable.GammaFromShapeAndScale(2, 0.5);\n",
    "    \n",
    "    Variable<double> scorePred = poidsPred * x - seuilPred;\n",
    "    Variable<double> scoreBruitePred = Variable.GaussianFromMeanAndPrecision(scorePred, bruitPred);\n",
    "    Variable<bool> predLabel = (scoreBruitePred > 0);\n",
    "    \n",
    "    InferenceEngine moteurPred = new InferenceEngine(new ExpectationPropagation());\n",
    "    moteurPred.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "    \n",
    "    Bernoulli prediction = moteurPred.Infer<Bernoulli>(predLabel);\n",
    "    Console.WriteLine($\"x = {x:F1} : P(classe=1) = {prediction.GetProbTrue():F3}\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6za36pa8yso",
   "source": "### Interpretation des predictions probabilistes\n\n| Feature (x) | P(classe=1) | Interpretation |\n|-------------|-------------|----------------|\n| 2.5 | 0.218 | Tres probablement classe 0 |\n| 4.5 | 0.561 | Zone d'incertitude (proche de 0.5) |\n| 6.5 | 0.822 | Probablement classe 1 |\n| 9.0 | 0.951 | Tres probablement classe 1 |\n\n**Observations cles** :\n\n1. **Gradient de confiance** : La probabilite augmente de maniere monotone avec x, coherent avec le poids positif appris.\n\n2. **Zone de transition** : Autour de x=4.5, le modele est incertain (P proche de 0.5). Cela correspond au point de decision calcule (x=4.15).\n\n3. **Propagation de l'incertitude** : Ces probabilites ne sont pas juste `sigmoid(w*x - b)` avec des parametres fixes. Elles integrent l'incertitude sur w et b apprise pendant l'entrainement.\n\n> **Difference avec la classification deterministe** : Un classifieur classique donnerait une prediction binaire (0 ou 1). Ici, nous obtenons une probabilite calibree qui reflette notre confiance reelle basee sur les donnees disponibles.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "9bc0d33b",
   "metadata": {
    "papermill": {
     "duration": 0.004177,
     "end_time": "2026-01-27T02:02:21.789809",
     "exception": false,
     "start_time": "2026-01-27T02:02:21.785632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Classification Multi-Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ll7ewym2zac",
   "source": "### Extension aux dimensions superieures\n\nPasser d'une seule feature a plusieurs features generalise le modele :\n\n| Aspect | 1 feature | p features |\n|--------|-----------|------------|\n| Parametre | $w$ (scalaire) | $\\mathbf{w}$ (vecteur) |\n| Score | $w \\cdot x - b$ | $\\mathbf{w}^T \\mathbf{x} - b$ |\n| Frontiere | Point sur $\\mathbb{R}$ | Hyperplan dans $\\mathbb{R}^p$ |\n| Priors | 2 Gaussiennes | p+1 Gaussiennes |\n\nLe graphe de facteurs s'etend naturellement avec un **produit scalaire** entre le vecteur de poids et le vecteur de features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77037ea0",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:21.799366Z",
     "iopub.status.busy": "2026-01-27T02:02:21.799191Z",
     "iopub.status.idle": "2026-01-27T02:02:22.236770Z",
     "shell.execute_reply": "2026-01-27T02:02:22.234833Z"
    },
    "papermill": {
     "duration": 0.443116,
     "end_time": "2026-01-27T02:02:22.236837",
     "exception": false,
     "start_time": "2026-01-27T02:02:21.793721",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "=== Classification Multi-Features ===\n",
      "Poids feature 1 : Gaussian(0,5454, 0,06499)\n",
      "Poids feature 2 : Gaussian(1,254, 0,07999)\n",
      "Seuil : Gaussian(5,058, 0,6716)\n"
     ]
    }
   ],
   "source": [
    "// Donnees avec 2 features\n",
    "double[,] featuresMulti = {\n",
    "    { 1.0, 2.0 },\n",
    "    { 2.0, 1.5 },\n",
    "    { 1.5, 3.0 },\n",
    "    { 3.0, 3.5 },\n",
    "    { 4.0, 2.0 },\n",
    "    { 3.5, 4.0 },\n",
    "    { 5.0, 3.0 },\n",
    "    { 4.5, 5.0 }\n",
    "};\n",
    "bool[] labelsMulti = { false, false, false, true, false, true, true, true };\n",
    "\n",
    "int nSamples = labelsMulti.Length;\n",
    "int nFeatures = 2;\n",
    "\n",
    "Range sampleRange = new Range(nSamples).Named(\"sample\");\n",
    "Range featureRange = new Range(nFeatures).Named(\"feature\");\n",
    "\n",
    "// Poids pour chaque feature\n",
    "VariableArray<double> poidsMulti = Variable.Array<double>(featureRange).Named(\"poids\");\n",
    "poidsMulti[featureRange] = Variable.GaussianFromMeanAndVariance(0, 10).ForEach(featureRange);\n",
    "\n",
    "Variable<double> seuilMulti = Variable.GaussianFromMeanAndVariance(0, 10).Named(\"seuil\");\n",
    "\n",
    "VariableArray2D<double> xMulti = Variable.Array<double>(sampleRange, featureRange).Named(\"x\");\n",
    "VariableArray<bool> yMulti = Variable.Array<bool>(sampleRange).Named(\"y\");\n",
    "\n",
    "using (Variable.ForEach(sampleRange))\n",
    "{\n",
    "    // Score = somme(poids[f] * x[f]) - seuil\n",
    "    Variable<double> scoreMulti = Variable.Sum(\n",
    "        Variable.Array<double>(featureRange).Named(\"produit\"));\n",
    "    \n",
    "    // Alternative plus simple : calculer explicitement\n",
    "    Variable<double> score0 = poidsMulti[0] * xMulti[sampleRange, 0];\n",
    "    Variable<double> score1 = poidsMulti[1] * xMulti[sampleRange, 1];\n",
    "    Variable<double> scoreTot = score0 + score1 - seuilMulti;\n",
    "    Variable<double> scoreNoise = Variable.GaussianFromMeanAndVariance(scoreTot, 1);\n",
    "    yMulti[sampleRange] = (scoreNoise > 0);\n",
    "}\n",
    "\n",
    "xMulti.ObservedValue = featuresMulti;\n",
    "yMulti.ObservedValue = labelsMulti;\n",
    "\n",
    "InferenceEngine moteurMulti = new InferenceEngine(new ExpectationPropagation());\n",
    "moteurMulti.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "\n",
    "Gaussian[] poidsPostMulti = moteurMulti.Infer<Gaussian[]>(poidsMulti);\n",
    "Gaussian seuilPostMulti = moteurMulti.Infer<Gaussian>(seuilMulti);\n",
    "\n",
    "Console.WriteLine(\"=== Classification Multi-Features ===\");\n",
    "for (int f = 0; f < nFeatures; f++)\n",
    "{\n",
    "    Console.WriteLine($\"Poids feature {f+1} : {poidsPostMulti[f]}\");\n",
    "}\n",
    "Console.WriteLine($\"Seuil : {seuilPostMulti}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uf82v4btvd",
   "source": "### Analyse des poids multi-features\n\n**Resultats obtenus** :\n- Poids feature 1 (x1) : 0.55 ± 0.25\n- Poids feature 2 (x2) : 1.25 ± 0.28\n- Seuil : 5.06 ± 0.82\n\n**Interpretation geometrique** :\n\nLa frontiere de decision est definie par l'equation :\n$$0.55 \\cdot x_1 + 1.25 \\cdot x_2 - 5.06 = 0$$\n\nSoit : $x_2 = -0.44 \\cdot x_1 + 4.05$\n\n| Aspect | Valeur | Signification |\n|--------|--------|---------------|\n| Pente frontiere | -0.44 | Frontiere quasi-horizontale |\n| Intercept | 4.05 | Coupe l'axe x2 vers 4 |\n| Ratio poids | 2.3 | Feature 2 a 2.3x plus d'influence que feature 1 |\n\n> **Importance relative** : Le poids de la feature 2 est plus de deux fois celui de la feature 1, indiquant que x2 est plus discriminant pour la classification. Cela pourrait orienter la collecte de donnees futures.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "a4c0bc0f",
   "metadata": {
    "papermill": {
     "duration": 0.00542,
     "end_time": "2026-01-27T02:02:22.247993",
     "exception": false,
     "start_time": "2026-01-27T02:02:22.242573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Bayes Point Machine (BPM)\n",
    "\n",
    "### Principe\n",
    "\n",
    "Le BPM est une methode de classification bayesienne qui :\n",
    "- Marginalise sur tous les hyperplans separateurs possibles\n",
    "- Donne des probabilites calibrees\n",
    "- Utilise EP pour l'inference\n",
    "\n",
    "### Formulation\n",
    "\n",
    "$$P(y=1|x) = \\int P(y=1|x,w) P(w|D) dw$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72blgkubd",
   "source": "### Note technique : Modele probit vs logit\n\nLe code precedent utilise un **modele probit** (comparaison avec bruit gaussien) plutot qu'un modele logit (fonction sigmoid). Voici la difference :\n\n| Aspect | Probit | Logit |\n|--------|--------|-------|\n| Lien | $\\Phi^{-1}(p) = w^T x$ | $\\log\\frac{p}{1-p} = w^T x$ |\n| Distribution latente | Gaussienne | Logistique |\n| Inference bayesienne | Plus facile (EP) | Plus difficile |\n| Equivalence pratique | Quasi-identique pour la plupart des cas | |\n\nEn Infer.NET, le modele probit est prefere car il se prete naturellement a l'Expectation Propagation avec des distributions gaussiennes.\n\n$$P(y=1|x) = \\Phi(w^T x) = \\int_{-\\infty}^{w^T x} \\mathcal{N}(z|0,1) dz$$\n\nou $\\Phi$ est la fonction de repartition de la loi normale standard.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bcf40",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:22.260408Z",
     "iopub.status.busy": "2026-01-27T02:02:22.260170Z",
     "iopub.status.idle": "2026-01-27T02:02:22.344376Z",
     "shell.execute_reply": "2026-01-27T02:02:22.344239Z"
    },
    "papermill": {
     "duration": 0.090797,
     "end_time": "2026-01-27T02:02:22.344444",
     "exception": false,
     "start_time": "2026-01-27T02:02:22.253647",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe SimpleBPM definie.\r\n"
     ]
    }
   ],
   "source": [
    "// Bayes Point Machine simplifie\n",
    "\n",
    "public class SimpleBPM\n",
    "{\n",
    "    private int nFeatures;\n",
    "    private Gaussian[] poidsPosteriors;\n",
    "    private Gaussian seuilPosterior;\n",
    "    private InferenceEngine moteur;\n",
    "    \n",
    "    public SimpleBPM(int nFeatures)\n",
    "    {\n",
    "        this.nFeatures = nFeatures;\n",
    "        this.moteur = new InferenceEngine(new ExpectationPropagation());\n",
    "        this.moteur.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "    }\n",
    "    \n",
    "    public void Entrainer(double[,] X, bool[] y)\n",
    "    {\n",
    "        int n = y.Length;\n",
    "        Range sampleRange = new Range(n);\n",
    "        Range featureRange = new Range(nFeatures);\n",
    "        \n",
    "        VariableArray<double> poids = Variable.Array<double>(featureRange);\n",
    "        poids[featureRange] = Variable.GaussianFromMeanAndVariance(0, 1).ForEach(featureRange);\n",
    "        \n",
    "        Variable<double> seuil = Variable.GaussianFromMeanAndVariance(0, 1);\n",
    "        \n",
    "        VariableArray2D<double> xVar = Variable.Array<double>(sampleRange, featureRange);\n",
    "        VariableArray<bool> yVar = Variable.Array<bool>(sampleRange);\n",
    "        \n",
    "        using (Variable.ForEach(sampleRange))\n",
    "        {\n",
    "            Variable<double> score = Variable.Constant(0.0);\n",
    "            for (int f = 0; f < nFeatures; f++)\n",
    "            {\n",
    "                score = score + poids[f] * xVar[sampleRange, f];\n",
    "            }\n",
    "            score = score - seuil;\n",
    "            Variable<double> scoreNoise = Variable.GaussianFromMeanAndVariance(score, 1);\n",
    "            yVar[sampleRange] = (scoreNoise > 0);\n",
    "        }\n",
    "        \n",
    "        xVar.ObservedValue = X;\n",
    "        yVar.ObservedValue = y;\n",
    "        \n",
    "        poidsPosteriors = moteur.Infer<Gaussian[]>(poids);\n",
    "        seuilPosterior = moteur.Infer<Gaussian>(seuil);\n",
    "    }\n",
    "    \n",
    "    public double Predire(double[] x)\n",
    "    {\n",
    "        // Score moyen\n",
    "        double scoreMoyen = 0;\n",
    "        double scoreVariance = 0;\n",
    "        \n",
    "        for (int f = 0; f < nFeatures; f++)\n",
    "        {\n",
    "            scoreMoyen += poidsPosteriors[f].GetMean() * x[f];\n",
    "            scoreVariance += poidsPosteriors[f].GetVariance() * x[f] * x[f];\n",
    "        }\n",
    "        scoreMoyen -= seuilPosterior.GetMean();\n",
    "        scoreVariance += seuilPosterior.GetVariance() + 1;  // +1 pour le bruit\n",
    "        \n",
    "        // Probit : P(score + noise > 0) - utiliser MMath.NormalCdf\n",
    "        return MMath.NormalCdf(scoreMoyen / Math.Sqrt(scoreVariance));\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"Classe SimpleBPM definie.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8iiyytzrwrs",
   "source": "### Implementation du Bayes Point Machine\n\nLa classe `SimpleBPM` encapsule le workflow complet :\n\n1. **Entrainement** (`Entrainer`) :\n   - Definit les priors Gaussiens sur les poids et le seuil\n   - Construit le graphe de facteurs avec le modele probit\n   - Execute l'inference EP pour obtenir les posterieurs\n\n2. **Prediction** (`Predire`) :\n   - Calcule le score moyen : $\\mu_{score} = \\mathbf{w}_{post}^T \\mathbf{x} - b_{post}$\n   - Calcule la variance totale : $\\sigma^2_{score} = \\sum_i \\text{Var}(w_i) x_i^2 + \\text{Var}(b) + 1$\n   - Retourne $\\Phi(\\mu_{score} / \\sigma_{score})$ via `MMath.NormalCdf`\n\n**Formule de prediction avec incertitude** :\n\n$$P(y=1|\\mathbf{x}) = \\Phi\\left(\\frac{\\mathbf{\\mu}_w^T \\mathbf{x} - \\mu_b}{\\sqrt{\\mathbf{x}^T \\Sigma_w \\mathbf{x} + \\sigma_b^2 + 1}}\\right)$$\n\nou $\\Phi$ est la CDF de la loi normale standard.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "rrt7vjmte7o",
   "source": "### Validation sur les donnees 2D\n\nTestons maintenant notre classe `SimpleBPM` sur les donnees multi-features definies precedemment pour valider l'implementation. Le BPM devrait donner des predictions coherentes avec l'inference directe, mais avec une API plus simple pour les predictions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63524cf",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:22.357217Z",
     "iopub.status.busy": "2026-01-27T02:02:22.356966Z",
     "iopub.status.idle": "2026-01-27T02:02:22.841970Z",
     "shell.execute_reply": "2026-01-27T02:02:22.834989Z"
    },
    "papermill": {
     "duration": 0.491735,
     "end_time": "2026-01-27T02:02:22.842043",
     "exception": false,
     "start_time": "2026-01-27T02:02:22.350308",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "=== Predictions BPM ===\n",
      "(1, 1) : P(classe=1) = 0,268\n",
      "(3, 3) : P(classe=1) = 0,603\n",
      "(5, 4) : P(classe=1) = 0,751\n",
      "(2, 4) : P(classe=1) = 0,633\n"
     ]
    }
   ],
   "source": [
    "// Utilisation du BPM\n",
    "var bpm = new SimpleBPM(2);\n",
    "bpm.Entrainer(featuresMulti, labelsMulti);\n",
    "\n",
    "Console.WriteLine(\"=== Predictions BPM ===\");\n",
    "double[][] testPoints = {\n",
    "    new[] { 1.0, 1.0 },\n",
    "    new[] { 3.0, 3.0 },\n",
    "    new[] { 5.0, 4.0 },\n",
    "    new[] { 2.0, 4.0 }\n",
    "};\n",
    "\n",
    "foreach (var point in testPoints)\n",
    "{\n",
    "    double prob = bpm.Predire(point);\n",
    "    Console.WriteLine($\"({point[0]}, {point[1]}) : P(classe=1) = {prob:F3}\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8odjsoa9nju",
   "source": "### Analyse des predictions BPM\n\n| Point (x1, x2) | P(classe=1) | Position relative | Verdict |\n|----------------|-------------|-------------------|---------|\n| (1, 1) | 0.268 | Loin sous la frontiere | Classe 0 |\n| (3, 3) | 0.603 | Proche de la frontiere | Incertain |\n| (5, 4) | 0.751 | Au-dessus de la frontiere | Classe 1 |\n| (2, 4) | 0.633 | Legèrement au-dessus | Classe 1 (incertain) |\n\n**Caracteristiques du Bayes Point Machine** :\n\n1. **Calibration** : Les probabilites proches de 0.5 indiquent correctement les zones d'incertitude pres de la frontiere.\n\n2. **Marginalisation** : Le BPM considere tous les hyperplans plausibles ponderes par leur probabilite, pas un seul hyperplan optimal.\n\n3. **Regularisation implicite** : Le prior Gaussien sur les poids (variance = 1) empeche le surapprentissage en penalisant les poids extremes.\n\n> **Comparaison SVM vs BPM** : Un SVM donnerait une frontiere \"dure\" et une classification binaire. Le BPM fournit des probabilites calibrees, particulierement utiles quand le cout d'erreur est asymetrique ou quand il faut prioriser les cas incertains pour une revue humaine.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ff6c157e",
   "metadata": {
    "papermill": {
     "duration": 0.007008,
     "end_time": "2026-01-27T02:02:22.856432",
     "exception": false,
     "start_time": "2026-01-27T02:02:22.849424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Test Clinique Bayesien (A/B Testing)\n",
    "\n",
    "### Contexte\n",
    "\n",
    "Comparer l'efficacite d'un nouveau traitement vs placebo.\n",
    "\n",
    "### Approche bayesienne\n",
    "\n",
    "- Prior sur l'efficacite de chaque traitement\n",
    "- Mise a jour avec les observations\n",
    "- Probabilite que le traitement soit meilleur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad057266",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:22.871478Z",
     "iopub.status.busy": "2026-01-27T02:02:22.871194Z",
     "iopub.status.idle": "2026-01-27T02:02:23.206529Z",
     "shell.execute_reply": "2026-01-27T02:02:23.206399Z"
    },
    "papermill": {
     "duration": 0.342914,
     "end_time": "2026-01-27T02:02:23.206595",
     "exception": false,
     "start_time": "2026-01-27T02:02:22.863681",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...compilation had 1 warning(s).\n",
      "  [1] DifferenceBetaOp.DifferenceAverageConditional(tauxTraitement_uses_F[1], tauxPlacebo_uses_F[1]) has quality band Experimental which is less than the recommended quality band (Preview)\n",
      "done.\n",
      "=== Test Clinique Bayesien ===\n",
      "\n",
      "Placebo : 30/100 guerisons\n",
      "Traitement : 45/100 guerisons\n",
      "\n",
      "Taux placebo : Beta(31,71)[mean=0,3039]\n",
      "  Moyenne : 0,304\n",
      "  IC 95% : [0,213, 0,395]\n",
      "\n",
      "Taux traitement : Beta(46,56)[mean=0,451]\n",
      "  Moyenne : 0,451\n",
      "\n",
      "P(traitement meilleur) = 0,986\n"
     ]
    }
   ],
   "source": [
    "// Test clinique A/B bayesien\n",
    "\n",
    "// Donnees observees\n",
    "int nPlacebo = 100;\n",
    "int guerisPlacebo = 30;  // 30% guerison\n",
    "\n",
    "int nTraitement = 100;\n",
    "int guerisTraitement = 45;  // 45% guerison\n",
    "\n",
    "// Modele : taux de guerison pour chaque groupe\n",
    "Variable<double> tauxPlacebo = Variable.Beta(1, 1).Named(\"tauxPlacebo\");  // Prior uniforme\n",
    "Variable<double> tauxTraitement = Variable.Beta(1, 1).Named(\"tauxTraitement\");\n",
    "\n",
    "// Observations (distribution binomiale)\n",
    "Variable<int> obsPlacebo = Variable.Binomial(nPlacebo, tauxPlacebo);\n",
    "Variable<int> obsTraitement = Variable.Binomial(nTraitement, tauxTraitement);\n",
    "\n",
    "obsPlacebo.ObservedValue = guerisPlacebo;\n",
    "obsTraitement.ObservedValue = guerisTraitement;\n",
    "\n",
    "// Traitement est meilleur ?\n",
    "Variable<bool> traitementMeilleur = (tauxTraitement > tauxPlacebo);\n",
    "\n",
    "InferenceEngine moteurClinique = new InferenceEngine();\n",
    "moteurClinique.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "\n",
    "Beta tauxPlaceboPost = moteurClinique.Infer<Beta>(tauxPlacebo);\n",
    "Beta tauxTraitementPost = moteurClinique.Infer<Beta>(tauxTraitement);\n",
    "Bernoulli traitementMeilleurPost = moteurClinique.Infer<Bernoulli>(traitementMeilleur);\n",
    "\n",
    "Console.WriteLine(\"=== Test Clinique Bayesien ===\");\n",
    "Console.WriteLine($\"\\nPlacebo : {guerisPlacebo}/{nPlacebo} guerisons\");\n",
    "Console.WriteLine($\"Traitement : {guerisTraitement}/{nTraitement} guerisons\");\n",
    "\n",
    "Console.WriteLine($\"\\nTaux placebo : {tauxPlaceboPost}\");\n",
    "Console.WriteLine($\"  Moyenne : {tauxPlaceboPost.GetMean():F3}\");\n",
    "Console.WriteLine($\"  IC 95% : [{tauxPlaceboPost.GetMean() - 2*Math.Sqrt(tauxPlaceboPost.GetVariance()):F3}, {tauxPlaceboPost.GetMean() + 2*Math.Sqrt(tauxPlaceboPost.GetVariance()):F3}]\");\n",
    "\n",
    "Console.WriteLine($\"\\nTaux traitement : {tauxTraitementPost}\");\n",
    "Console.WriteLine($\"  Moyenne : {tauxTraitementPost.GetMean():F3}\");\n",
    "\n",
    "Console.WriteLine($\"\\nP(traitement meilleur) = {traitementMeilleurPost.GetProbTrue():F3}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10634vpndmv",
   "source": "### Modele conjugue Beta-Binomial\n\nLe test clinique utilise le modele classique Beta-Binomial :\n\n**Prior** : $\\theta \\sim \\text{Beta}(1, 1)$ (uniforme sur [0,1])\n\n**Vraisemblance** : $k | \\theta \\sim \\text{Binomial}(n, \\theta)$\n\n**Posterior** : $\\theta | k \\sim \\text{Beta}(1 + k, 1 + n - k)$\n\nCette conjugaison permet une inference exacte. La question \"le traitement est-il meilleur ?\" se traduit par :\n$$P(\\theta_{traitement} > \\theta_{placebo} | \\text{donnees})$$\n\nInfer.NET calcule cette probabilite via l'operateur `DifferenceBetaOp`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "d26dff6a",
   "metadata": {
    "papermill": {
     "duration": 0.007181,
     "end_time": "2026-01-27T02:02:23.221769",
     "exception": false,
     "start_time": "2026-01-27T02:02:23.214588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analyse du test clinique\n",
    "\n",
    "**Résultats** :\n",
    "- Placebo : 30% guérison (IC 95% : 21%-39%)\n",
    "- Traitement : 45% guérison (IC 95% : 35%-55%)\n",
    "- **P(traitement meilleur) = 0.986**\n",
    "\n",
    "**Comparaison avec l'approche fréquentiste** :\n",
    "\n",
    "| Aspect | Fréquentiste | Bayésien |\n",
    "|--------|--------------|----------|\n",
    "| Question | \"Peut-on rejeter H0 ?\" | \"Quelle est P(traitement meilleur) ?\" |\n",
    "| Réponse | p-value < 0.05 → significatif | P = 98.6% → très probable |\n",
    "| Interprétation | Difficile à communiquer | Directe et intuitive |\n",
    "\n",
    "**Note importante** : Le warning \"quality band Experimental\" indique que l'opérateur `DifferenceBetaOp` est encore en développement dans Infer.NET. Les résultats restent fiables mais cette fonctionnalité pourrait évoluer.\n",
    "\n",
    "> **Application clinique** : Avec 98.6% de probabilité que le traitement soit meilleur, un comité d'éthique pourrait recommander de proposer le traitement au groupe placebo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52c9a4",
   "metadata": {
    "papermill": {
     "duration": 0.006979,
     "end_time": "2026-01-27T02:02:23.235625",
     "exception": false,
     "start_time": "2026-01-27T02:02:23.228646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Effet de la Taille d'Echantillon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g0lw2yo4b9t",
   "source": "### Puissance statistique bayesienne\n\nUne question naturelle est : \"Combien de patients faut-il recruter pour detecter une difference ?\" L'analyse suivante montre comment la certitude evolue avec la taille de l'echantillon, a effet constant (30% vs 45%).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b6c38",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:23.252288Z",
     "iopub.status.busy": "2026-01-27T02:02:23.252057Z",
     "iopub.status.idle": "2026-01-27T02:02:24.471677Z",
     "shell.execute_reply": "2026-01-27T02:02:24.471545Z"
    },
    "papermill": {
     "duration": 1.227569,
     "end_time": "2026-01-27T02:02:24.471743",
     "exception": false,
     "start_time": "2026-01-27T02:02:23.244174",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Impact de la taille d'echantillon ===\n",
      "\n",
      "Meme ratio (30% vs 45%), differentes tailles :\n",
      "\n",
      "Compiling model...compilation had 1 warning(s).\n",
      "  [1] DifferenceBetaOp.DifferenceAverageConditional(vdouble105_uses_F[1], vdouble104_uses_F[1]) has quality band Experimental which is less than the recommended quality band (Preview)\n",
      "done.\n",
      "n =   10 : P(traitement meilleur) = 0,6702\n",
      "Compiling model...compilation had 1 warning(s).\n",
      "  [1] DifferenceBetaOp.DifferenceAverageConditional(vdouble108_uses_F[1], vdouble107_uses_F[1]) has quality band Experimental which is less than the recommended quality band (Preview)\n",
      "done.\n",
      "n =   50 : P(traitement meilleur) = 0,9258\n",
      "Compiling model...compilation had 1 warning(s).\n",
      "  [1] DifferenceBetaOp.DifferenceAverageConditional(vdouble111_uses_F[1], vdouble110_uses_F[1]) has quality band Experimental which is less than the recommended quality band (Preview)\n",
      "done.\n",
      "n =  100 : P(traitement meilleur) = 0,9862\n",
      "Compiling model...compilation had 1 warning(s).\n",
      "  [1] DifferenceBetaOp.DifferenceAverageConditional(vdouble114_uses_F[1], vdouble113_uses_F[1]) has quality band Experimental which is less than the recommended quality band (Preview)\n",
      "done.\n",
      "n =  500 : P(traitement meilleur) = 1,0000\n",
      "Compiling model...compilation had 1 warning(s).\n",
      "  [1] DifferenceBetaOp.DifferenceAverageConditional(vdouble117_uses_F[1], vdouble116_uses_F[1]) has quality band Experimental which is less than the recommended quality band (Preview)\n",
      "done.\n",
      "n = 1000 : P(traitement meilleur) = 1,0000\n",
      "\n",
      "=> Plus de donnees = plus de certitude\n"
     ]
    }
   ],
   "source": [
    "// Impact de la taille d'echantillon\n",
    "\n",
    "Console.WriteLine(\"=== Impact de la taille d'echantillon ===\");\n",
    "Console.WriteLine(\"\\nMeme ratio (30% vs 45%), differentes tailles :\\n\");\n",
    "\n",
    "int[] tailles = { 10, 50, 100, 500, 1000 };\n",
    "\n",
    "foreach (int n in tailles)\n",
    "{\n",
    "    int gP = (int)(n * 0.30);\n",
    "    int gT = (int)(n * 0.45);\n",
    "    \n",
    "    Variable<double> tP = Variable.Beta(1, 1);\n",
    "    Variable<double> tT = Variable.Beta(1, 1);\n",
    "    \n",
    "    Variable.ConstrainEqual(Variable.Binomial(n, tP), gP);\n",
    "    Variable.ConstrainEqual(Variable.Binomial(n, tT), gT);\n",
    "    \n",
    "    Variable<bool> meilleur = (tT > tP);\n",
    "    \n",
    "    InferenceEngine m = new InferenceEngine();\n",
    "    m.Compiler.CompilerChoice = CompilerChoice.Roslyn;\n",
    "    \n",
    "    double prob = m.Infer<Bernoulli>(meilleur).GetProbTrue();\n",
    "    Console.WriteLine($\"n = {n,4} : P(traitement meilleur) = {prob:F4}\");\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"\\n=> Plus de donnees = plus de certitude\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "my479h57vh",
   "source": "### Convergence de la certitude avec la taille d'echantillon\n\n| Taille (n) | P(traitement meilleur) | Interpretation |\n|------------|------------------------|----------------|\n| 10 | 0.670 | Faible evidence (proche du hasard) |\n| 50 | 0.926 | Evidence moderee |\n| 100 | 0.986 | Evidence forte |\n| 500 | ~1.000 | Evidence tres forte |\n| 1000 | ~1.000 | Evidence quasi-certaine |\n\n**Loi de convergence** :\n\nLa certitude croit approximativement comme $\\sqrt{n}$ :\n- Doubler la confiance necessite quadrupler l'echantillon\n- Passer de 67% a 99% necessite ~100x plus de donnees\n\n**Implications pratiques** :\n\n| Contexte | Taille recommandee | Justification |\n|----------|-------------------|---------------|\n| Test pilote | n = 50 | Detection effet important (>90%) |\n| Essai clinique | n = 100-500 | Standard reglementaire |\n| Decision critique | n > 500 | Minimiser risque d'erreur |\n\n> **Avantage bayesien** : Contrairement aux tests frequentistes qui donnent une reponse binaire (significatif/non-significatif), l'approche bayesienne permet un **arret adaptatif** : on peut arreter l'essai des que P(traitement meilleur) depasse un seuil predetermine (ex: 95%), ou au contraire continuer si l'evidence est insuffisante.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "10acf573",
   "metadata": {
    "papermill": {
     "duration": 0.008213,
     "end_time": "2026-01-27T02:02:24.488199",
     "exception": false,
     "start_time": "2026-01-27T02:02:24.479986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Exercice : Classification Spam\n",
    "\n",
    "### Enonce\n",
    "\n",
    "Construisez un classificateur bayesien pour detecter les spams bases sur 3 features :\n",
    "- Nombre de mots en majuscules\n",
    "- Presence du mot \"gratuit\"\n",
    "- Longueur du message (en centaines de caracteres)\n",
    "\n",
    "### Donnees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4vz1cpy6yhq",
   "source": "### Objectif de l'exercice\n\nConstruire un classificateur de spam en utilisant le Bayes Point Machine implemente precedemment. Les features choisies representent des caracteristiques typiques des spams :\n\n| Feature | Description | Valeur typique spam |\n|---------|-------------|---------------------|\n| Majuscules | Nombre de mots en majuscules | Eleve (>15) |\n| Gratuit | Presence du mot \"gratuit\" (0/1) | 1 |\n| Longueur | Taille du message (en centaines de caracteres) | Faible (<2) |\n\nCes features sont simples mais illustrent les principes d'un filtre anti-spam reel.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30425d08",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T02:02:24.529755Z",
     "iopub.status.busy": "2026-01-27T02:02:24.529492Z",
     "iopub.status.idle": "2026-01-27T02:02:25.059703Z",
     "shell.execute_reply": "2026-01-27T02:02:25.052464Z"
    },
    "papermill": {
     "duration": 0.563737,
     "end_time": "2026-01-27T02:02:25.059773",
     "exception": false,
     "start_time": "2026-01-27T02:02:24.496036",
     "status": "completed"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "tags": [],
    "vscode": {
     "languageId": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...done.\n",
      "Iterating: \n",
      ".........|.........|.........|.........|.........| 50\n",
      "=== Classificateur Spam ===\n",
      "\n",
      "Predictions :\n",
      "  Email normal                   : P(spam)=0,053 -> OK\n",
      "  OFFRE GRATUITE!!!              : P(spam)=0,952 -> SPAM\n",
      "  Email avec quelques majuscules : P(spam)=0,334 -> OK\n",
      "  CLIQUEZ ICI GRATUIT            : P(spam)=0,984 -> SPAM\n"
     ]
    }
   ],
   "source": [
    "// EXERCICE : Classification spam\n",
    "\n",
    "// Donnees d'entrainement\n",
    "// [nbMajuscules, presenceGratuit (0/1), longueur]\n",
    "double[,] spamFeatures = {\n",
    "    { 5, 0, 2.5 },   // Non spam\n",
    "    { 3, 0, 3.0 },   // Non spam\n",
    "    { 15, 1, 1.0 },  // Spam\n",
    "    { 20, 1, 0.5 },  // Spam\n",
    "    { 2, 0, 4.0 },   // Non spam\n",
    "    { 25, 1, 1.5 },  // Spam\n",
    "    { 8, 0, 2.0 },   // Non spam\n",
    "    { 18, 1, 0.8 }   // Spam\n",
    "};\n",
    "bool[] spamLabels = { false, false, true, true, false, true, false, true };\n",
    "\n",
    "// Entrainement\n",
    "var spamClassifier = new SimpleBPM(3);\n",
    "spamClassifier.Entrainer(spamFeatures, spamLabels);\n",
    "\n",
    "Console.WriteLine(\"=== Classificateur Spam ===\");\n",
    "\n",
    "// Test sur nouveaux emails\n",
    "var testEmails = new (double[], string)[] {\n",
    "    (new[] { 4.0, 0.0, 3.0 }, \"Email normal\"),\n",
    "    (new[] { 22.0, 1.0, 1.0 }, \"OFFRE GRATUITE!!!\"),\n",
    "    (new[] { 10.0, 0.0, 2.5 }, \"Email avec quelques majuscules\"),\n",
    "    (new[] { 30.0, 1.0, 0.5 }, \"CLIQUEZ ICI GRATUIT\")\n",
    "};\n",
    "\n",
    "Console.WriteLine(\"\\nPredictions :\");\n",
    "foreach (var (features, desc) in testEmails)\n",
    "{\n",
    "    double probSpam = spamClassifier.Predire(features);\n",
    "    string verdict = probSpam > 0.5 ? \"SPAM\" : \"OK\";\n",
    "    Console.WriteLine($\"  {desc,-30} : P(spam)={probSpam:F3} -> {verdict}\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0lks9jhso2xf",
   "source": "### Analyse du classificateur spam\n\n**Resultats de prediction** :\n\n| Email | Majuscules | Gratuit | Longueur | P(spam) | Verdict |\n|-------|------------|---------|----------|---------|---------|\n| Email normal | 4 | Non | 3.0 | 0.053 | OK |\n| OFFRE GRATUITE!!! | 22 | Oui | 1.0 | 0.952 | SPAM |\n| Quelques majuscules | 10 | Non | 2.5 | 0.334 | OK |\n| CLIQUEZ ICI GRATUIT | 30 | Oui | 0.5 | 0.984 | SPAM |\n\n**Facteurs discriminants appris** :\n\nLe modele a appris que le spam est caracterise par :\n1. **Nombre eleve de majuscules** (poids positif)\n2. **Presence du mot \"gratuit\"** (poids positif fort)\n3. **Messages courts** (poids negatif sur la longueur)\n\n**Cas interessants** :\n\n- \"Email avec quelques majuscules\" (P=0.334) : Malgre 10 majuscules, l'absence de \"gratuit\" et la longueur normale le sauvent du classement spam.\n- Les deux emails avec \"gratuit\" ont P(spam) > 95%, montrant l'importance de cette feature.\n\n> **Application industrielle** : Ce type de classificateur bayesien est utilise dans les filtres anti-spam reels. L'avantage de l'approche probabiliste est de pouvoir definir un seuil de decision adapte au contexte : seuil bas (0.3) pour un filtre agressif, seuil haut (0.8) pour eviter les faux positifs dans un contexte professionnel.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "c23fa2ba",
   "metadata": {
    "papermill": {
     "duration": 0.009864,
     "end_time": "2026-01-27T02:02:25.080131",
     "exception": false,
     "start_time": "2026-01-27T02:02:25.070267",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## 10. Resume et Synthese\n\n### Concepts cles\n\n| Concept | Description |\n|---------|-------------|\n| **Regression logistique bayesienne** | Priors sur poids, posterieurs apres donnees |\n| **Bayes Point Machine** | Marginalisation sur hyperplans |\n| **Test A/B bayesien** | P(traitement meilleur) directement |\n| **Incertitude** | Quantifiee a chaque etape |\n| **Calibration** | Probabilites refletent la vraie incertitude |\n\n### Distributions utilisees\n\n| Distribution | Role | Parametres |\n|--------------|------|------------|\n| **Gaussian** | Prior/Posterior sur poids | (moyenne, variance) |\n| **Gamma** | Prior sur precision du bruit | (shape, scale) |\n| **Beta** | Prior/Posterior sur probabilites | (alpha, beta) |\n| **Bernoulli** | Prediction de classe | (probTrue) |\n| **Binomial** | Comptage de succes | (n, p) |\n\n### Comparaison des approches\n\n| Aspect | Classification classique | Classification bayesienne |\n|--------|-------------------------|---------------------------|\n| Sortie | Classe predite | Distribution sur les classes |\n| Incertitude modele | Non | Oui (distributions sur parametres) |\n| Incertitude prediction | Non | Oui (probabilites calibrees) |\n| Regularisation | Explicite (L1/L2) | Implicite (priors) |\n| Surapprentissage | Risque eleve | Reduit naturellement |\n| Interpretabilite | Limitee | Elevee (intervalles de credibilite) |\n\n---\n\n## Prochaine etape\n\nDans [Infer-8-Model-Selection](Infer-8-Model-Selection.ipynb), nous explorerons :\n\n- La selection et comparaison de modeles\n- L'evidence bayesienne (marginal likelihood)\n- Le facteur de Bayes\n- L'Automatic Relevance Determination (ARD)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "file_extension": ".cs",
   "mimetype": "text/x-csharp",
   "name": "C#",
   "pygments_lexer": "csharp",
   "version": "13.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.647646,
   "end_time": "2026-01-27T02:02:25.323296",
   "environment_variables": {},
   "exception": null,
   "input_path": "c:\\dev\\CoursIA\\MyIA.AI.Notebooks\\Probas\\Infer\\Infer-7-Classification.ipynb",
   "output_path": "c:\\dev\\CoursIA\\MyIA.AI.Notebooks\\Probas\\Infer\\test_outputs\\Infer-7-Classification_output.ipynb",
   "parameters": {},
   "start_time": "2026-01-27T02:02:13.675650",
   "version": "2.6.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}