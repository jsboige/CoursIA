{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nav-header",
   "metadata": {},
   "source": [
    "# App-7 : Wordle Solver -- CSP et theorie de l'information\n",
    "\n",
    "**Navigation** : [<< App-6 Minesweeper](App-6-Minesweeper.ipynb) | [Index](../README.md) | [App-8 MiniZinc >>](App-8-MiniZinc.ipynb)\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. **Modeliser** le jeu Wordle comme un probleme de filtrage sous contraintes\n",
    "2. **Implementer** un solveur par filtrage simple (elimination de mots incompatibles)\n",
    "3. **Formaliser** les indices Wordle comme des contraintes CSP sur les positions de lettres\n",
    "4. **Appliquer** la theorie de l'information (entropie de Shannon) pour choisir le meilleur mot\n",
    "5. **Comparer** les trois approches en termes de performance moyenne\n",
    "\n",
    "### Prerequis\n",
    "- Search-6 : Fondamentaux des CSP (variables, domaines, contraintes)\n",
    "- Search-7 : Consistance et propagation de contraintes\n",
    "- Python : collections, comprehensions, bases de numpy/matplotlib\n",
    "\n",
    "### Duree estimee : 45 minutes\n",
    "\n",
    "### Source\n",
    "\n",
    "Adapte du projet etudiant EPITA 2025 : **CSP-Wordle-Solver** (jsboigeEpita/2025-PPC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction (~5 min)\n",
    "\n",
    "### Le jeu Wordle\n",
    "\n",
    "**Wordle** est un jeu de devinette de mots :\n",
    "- Le joueur doit deviner un **mot secret de 5 lettres** en au plus **6 tentatives**\n",
    "- Apres chaque tentative, le jeu renvoie un **feedback** pour chaque lettre :\n",
    "\n",
    "| Couleur | Code | Signification |\n",
    "|---------|------|---------------|\n",
    "| Vert | 2 | Lettre correcte a la bonne position |\n",
    "| Jaune | 1 | Lettre presente dans le mot mais a une autre position |\n",
    "| Gris | 0 | Lettre absente du mot |\n",
    "\n",
    "### Pourquoi Wordle est un probleme de recherche\n",
    "\n",
    "On peut voir Wordle comme un probleme de **recherche d'information** :\n",
    "- L'espace de recherche est l'ensemble des mots possibles\n",
    "- Chaque tentative apporte de l'information qui **reduit** cet espace\n",
    "- L'objectif est de reduire l'espace a un seul mot en 6 tentatives ou moins\n",
    "\n",
    "### Trois approches\n",
    "\n",
    "Nous allons implementer et comparer trois strategies :\n",
    "\n",
    "| # | Approche | Principe | Performance attendue |\n",
    "|---|----------|----------|---------------------|\n",
    "| 1 | Filtrage simple | Eliminer les mots incompatibles, choisir au hasard | ~4-5 tentatives |\n",
    "| 2 | CSP | Modeliser les contraintes sur les positions de lettres | ~4-5 tentatives (plus structure) |\n",
    "| 3 | Entropie | Maximiser l'information gagnee par tentative | ~3.5 tentatives |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Helpers partages de la serie Search\n",
    "sys.path.insert(0, '..')\n",
    "from search_helpers import benchmark_table\n",
    "\n",
    "# Reproductibilite\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Liste de mots et systeme de feedback (~5 min)\n",
    "\n",
    "### Liste de mots\n",
    "\n",
    "Pour ce notebook, nous utilisons une liste de mots francais courants de 5 lettres. Dans un vrai Wordle, la liste officielle contient environ 2 300 mots (anglais) ; nous utilisons ici une liste reduite pour que les demonstrations restent rapides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "word-list",
   "metadata": {},
   "outputs": [],
   "source": "# Liste de mots francais de 5 lettres (sans accents, majuscules)\n# Mots courants selectionnes pour couvrir une bonne variete de lettres\nWORD_LIST = [\n    \"ABORD\", \"ACIER\", \"ADORE\", \"AGENT\", \"AIDER\", \"AIMER\", \"AINSI\",\n    \"ALBUM\", \"ALGUE\", \"ALLEE\", \"ALLER\", \"AMOUR\", \"ANCRE\", \"ANGLE\",\n    \"ANNEE\", \"APPEL\", \"ARBRE\", \"ARENE\", \"ARGOT\", \"ASTRE\", \"ATLAS\",\n    \"AVENT\", \"AVION\", \"AVOIR\", \"BADGE\", \"BAGUE\", \"BAIES\", \"BALLE\",\n    \"BANDE\", \"BARBE\", \"BARGE\", \"BASER\", \"BATON\", \"BIBLE\", \"BIDON\",\n    \"BILAN\", \"BLANC\", \"BLASE\", \"BOIRE\", \"BONUS\", \"BOTTE\", \"BOURG\",\n    \"BRAVE\", \"BRISE\", \"BRUME\", \"BULLE\", \"CABLE\", \"CADRE\", \"CALME\",\n    \"CANNE\", \"CARTE\", \"CAUSE\", \"CEDER\", \"CHAMP\", \"CHANT\", \"CHASE\",\n    \"CHOIX\", \"CHUTE\", \"CIBLE\", \"CLAIR\", \"CLONE\", \"COEUR\", \"COMTE\",\n    \"CONTE\", \"CORDE\", \"COUDE\", \"COUPE\", \"CRIER\", \"CRIME\", \"CRISE\",\n    \"CREUX\", \"CROIX\", \"CRUEL\", \"CYCLE\", \"DANSE\", \"DEBUT\", \"DELTA\",\n    \"DENSE\", \"DEPOT\", \"DOIGT\", \"DOUER\", \"DRAME\", \"DROIT\", \"DUPER\",\n    \"ECLAT\", \"ECRAN\", \"ELEVE\", \"EMAIL\", \"ENVIE\", \"EPAVE\", \"EPICE\",\n    \"ESSAI\", \"ETAGE\", \"EXACT\", \"EXILE\", \"EXTRA\", \"FABLE\", \"FARCE\",\n    \"FAUTE\", \"FERME\", \"FIBRE\", \"FIGER\", \"FILER\", \"FINAL\", \"FLEUR\",\n    \"FOLIE\", \"FORCE\", \"FORME\", \"FORUM\", \"FOSSE\", \"FRAIS\", \"FRANC\",\n    \"FREIN\", \"FRONT\", \"FRUIT\", \"FUMER\", \"GAMIN\", \"GARDE\", \"GELER\",\n    \"GENRE\", \"GLACE\", \"GLOBE\", \"GRADE\", \"GRAIN\", \"GRAND\", \"GRISE\",\n    \"GUIDE\", \"HABIT\", \"HAINE\", \"HERBE\", \"HEURE\", \"HUILE\", \"HYPER\",\n    \"IMAGE\", \"INDEX\", \"ISSUE\", \"JAMBE\", \"JEUNE\", \"JOUER\", \"JUGER\",\n    \"JUSTE\", \"LACET\", \"LAINE\", \"LANCE\", \"LARGE\", \"LASER\", \"LEVER\",\n    \"LIBRE\", \"LIGUE\", \"LINER\", \"LISSE\", \"LITRE\", \"LIVRE\", \"LOGIS\",\n    \"LOUER\", \"LOURD\", \"LUCRE\", \"LUEUR\", \"LYCEE\", \"MACON\", \"MALLE\",\n    \"MARGE\", \"MARIN\", \"MASSE\", \"MENER\", \"MERCI\", \"MEULE\", \"MINCE\",\n    \"MIXTE\", \"MODEM\", \"MONDE\", \"MORAL\", \"MORSE\", \"MOULE\", \"NAPPE\",\n    \"NEIGE\", \"NOBLE\", \"NUAGE\", \"OASIS\", \"OCEAN\", \"OFFRE\", \"OLIVE\",\n    \"OMBRE\", \"ONCLE\", \"OPERA\", \"ORAGE\", \"ORDRE\", \"ORGUE\", \"OTAGE\",\n    \"PAIRE\", \"PANNE\", \"PARIE\", \"PAROI\", \"PARTI\", \"PATIN\", \"PAUSE\",\n    \"PEAGE\", \"PEINE\", \"PELER\", \"PENTE\", \"PERLE\", \"PHARE\", \"PIECE\",\n    \"PINCE\", \"PISTE\", \"PLACE\", \"PLAGE\", \"PLEIN\", \"PLUIE\", \"POCHE\",\n    \"POEME\", \"POINT\", \"POMPE\", \"PORTE\", \"POSER\", \"POSTE\", \"POUCE\",\n    \"PRIME\", \"PRISE\", \"PROSE\", \"PRUNE\", \"QUAND\", \"RAIDE\", \"RAMPE\",\n    \"RANGE\", \"RAVIN\", \"REGAL\", \"REGLE\", \"REINE\", \"REPAS\", \"RESTE\",\n    \"RICHE\", \"RIDER\", \"RIVAL\", \"RONCE\", \"SABLE\", \"SALON\", \"SAUGE\",\n    \"SAUVE\", \"SCENE\", \"SEMER\", \"SERIE\", \"SIGNE\", \"SIROP\", \"SOBRE\",\n    \"SOCLE\", \"SOLDE\", \"SOMME\", \"SONDE\", \"SOUCI\", \"STADE\", \"STYLE\",\n    \"SUITE\", \"SUPER\", \"TABLE\", \"TACHE\", \"TALON\", \"TASSE\", \"TEMPS\",\n    \"TENTE\", \"TERRE\", \"THEME\", \"TIGRE\", \"TITRE\", \"TOILE\", \"TONNE\",\n    \"TOTAL\", \"TRACE\", \"TRAIN\", \"TRAIT\", \"TREVE\", \"TRIBU\", \"TRONC\",\n    \"TUILE\", \"ULTRA\", \"UNION\", \"UNITE\", \"USAGE\", \"USINE\", \"VAGUE\",\n    \"VALSE\", \"VASTE\", \"VEINE\", \"VENTE\", \"VERRE\", \"VERTU\", \"VIDER\",\n    \"VIGNE\", \"VITRE\", \"VIVRE\", \"VOILE\", \"VOLER\", \"VOTER\", \"YACHT\",\n    \"ZEBRA\", \"ZONES\",\n]\n\n# Filtrer les mots qui ne font pas exactement 5 lettres (securite)\nWORD_LIST = [w.upper().strip() for w in WORD_LIST if len(w.strip()) == 5]\nWORD_LIST = sorted(set(WORD_LIST))\n\nprint(f\"Liste de mots : {len(WORD_LIST)} mots de 5 lettres\")\nprint(f\"Exemples : {WORD_LIST[:10]}\")\nprint(f\"Derniers : {WORD_LIST[-5:]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "feedback-intro",
   "metadata": {},
   "source": [
    "### Systeme de feedback\n",
    "\n",
    "Le coeur du jeu Wordle est la fonction de feedback. Pour une tentative (*guess*) et un mot secret (*answer*), elle produit un tuple de 5 valeurs :\n",
    "\n",
    "$$\\text{feedback}(g, a) = (f_0, f_1, f_2, f_3, f_4) \\quad \\text{ou } f_i \\in \\{0, 1, 2\\}$$\n",
    "\n",
    "L'algorithme doit gerer correctement les **lettres repetees**. Par exemple, si le mot secret est `AIMER` et la tentative est `ANNEE`, le premier `A` est vert, le `N` en position 1 est gris (pas de `N` dans le secret), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedback-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feedback(guess: str, answer: str) -> Tuple[int, ...]:\n",
    "    \"\"\"Calcule le feedback Wordle pour une tentative.\n",
    "\n",
    "    Args:\n",
    "        guess: mot propose (5 lettres)\n",
    "        answer: mot secret (5 lettres)\n",
    "\n",
    "    Returns:\n",
    "        Tuple de 5 entiers : 0=gris, 1=jaune, 2=vert\n",
    "    \"\"\"\n",
    "    feedback = [0] * 5\n",
    "    answer_chars = list(answer)\n",
    "    guess_chars = list(guess)\n",
    "\n",
    "    # Passe 1 : identifier les verts (lettre correcte, bonne position)\n",
    "    for i in range(5):\n",
    "        if guess_chars[i] == answer_chars[i]:\n",
    "            feedback[i] = 2\n",
    "            answer_chars[i] = None  # Marquer comme utilisee\n",
    "            guess_chars[i] = None\n",
    "\n",
    "    # Passe 2 : identifier les jaunes (lettre presente, mauvaise position)\n",
    "    for i in range(5):\n",
    "        if guess_chars[i] is not None:\n",
    "            if guess_chars[i] in answer_chars:\n",
    "                feedback[i] = 1\n",
    "                # Marquer la premiere occurrence comme utilisee\n",
    "                idx = answer_chars.index(guess_chars[i])\n",
    "                answer_chars[idx] = None\n",
    "\n",
    "    return tuple(feedback)\n",
    "\n",
    "\n",
    "# Tests de la fonction de feedback\n",
    "test_cases = [\n",
    "    (\"CRANE\", \"CRANE\", \"Mot identique -> tout vert\"),\n",
    "    (\"AIMER\", \"TABLE\", \"Aucune lettre commune en bonne position\"),\n",
    "    (\"TRACE\", \"CRANE\", \"Lettres communes melangees\"),\n",
    "    (\"ANNEE\", \"AIMER\", \"Lettre repetee dans la tentative\"),\n",
    "    (\"ARBRE\", \"BRAVE\", \"Plusieurs lettres communes\"),\n",
    "]\n",
    "\n",
    "print(\"Tests du systeme de feedback\")\n",
    "print(\"=\" * 65)\n",
    "for guess, answer, desc in test_cases:\n",
    "    fb = compute_feedback(guess, answer)\n",
    "    fb_str = \"\".join([\"_YG\"[v] for v in fb])  # _ = gris, Y = jaune, G = vert\n",
    "    print(f\"  {guess} vs {answer} -> {fb} ({fb_str})  [{desc}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feedback-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : systeme de feedback\n",
    "\n",
    "**Sortie obtenue** : Les cinq cas de test valident le comportement de la fonction de feedback.\n",
    "\n",
    "| Cas | Tentative | Secret | Feedback | Verification |\n",
    "|-----|-----------|--------|----------|-------------|\n",
    "| Identique | CRANE | CRANE | GGGGG | Tout vert, correct |\n",
    "| Disjoint | AIMER | TABLE | Certains gris/jaunes | Lettres partagees detectees |\n",
    "| Melange | TRACE | CRANE | Mix Y/G | Positions correctes vs incorrectes |\n",
    "| Repetition | ANNEE | AIMER | Gestion correcte | Une seule occurrence consommee |\n",
    "\n",
    "**Points cles** :\n",
    "1. L'algorithme en **deux passes** (verts d'abord, puis jaunes) evite les faux positifs sur les lettres repetees\n",
    "2. Le marquage `None` garantit qu'une lettre du secret n'est comptee qu'une fois\n",
    "3. Le feedback encode $3^5 = 243$ patterns possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-feedback-intro",
   "metadata": {},
   "source": [
    "### Visualisation d'une grille Wordle\n",
    "\n",
    "Pour rendre les resultats plus lisibles, implementons un affichage graphique similaire au jeu original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-wordle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_wordle_grid(guesses: List[str], feedbacks: List[Tuple[int, ...]],\n",
    "                     answer: str = None, title: str = \"Partie Wordle\"):\n",
    "    \"\"\"Affiche une grille Wordle avec les couleurs de feedback.\"\"\"\n",
    "    n_rows = max(len(guesses), 6)\n",
    "    fig, ax = plt.subplots(figsize=(5, n_rows * 0.9 + 1))\n",
    "\n",
    "    colors_map = {\n",
    "        0: '#787C7E',  # Gris\n",
    "        1: '#C9B458',  # Jaune\n",
    "        2: '#6AAA64',  # Vert\n",
    "    }\n",
    "    empty_color = '#D3D6DA'\n",
    "\n",
    "    for row in range(n_rows):\n",
    "        for col in range(5):\n",
    "            if row < len(guesses):\n",
    "                color = colors_map[feedbacks[row][col]]\n",
    "                letter = guesses[row][col]\n",
    "            else:\n",
    "                color = empty_color\n",
    "                letter = ''\n",
    "\n",
    "            rect = mpatches.FancyBboxPatch(\n",
    "                (col * 1.1, (n_rows - 1 - row) * 1.1), 1.0, 1.0,\n",
    "                boxstyle='round,pad=0.02',\n",
    "                facecolor=color, edgecolor='#D3D6DA', linewidth=2\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            if letter:\n",
    "                ax.text(col * 1.1 + 0.5, (n_rows - 1 - row) * 1.1 + 0.5,\n",
    "                        letter, ha='center', va='center',\n",
    "                        fontsize=18, fontweight='bold', color='white')\n",
    "\n",
    "    ax.set_xlim(-0.2, 5.5)\n",
    "    ax.set_ylim(-0.2, n_rows * 1.1 + 0.2)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "    title_text = title\n",
    "    if answer:\n",
    "        title_text += f\"  (secret : {answer})\"\n",
    "    ax.set_title(title_text, fontsize=13, fontweight='bold', pad=10)\n",
    "\n",
    "    # Legende\n",
    "    legend_patches = [\n",
    "        mpatches.Patch(color='#6AAA64', label='Vert : bonne position'),\n",
    "        mpatches.Patch(color='#C9B458', label='Jaune : mauvaise position'),\n",
    "        mpatches.Patch(color='#787C7E', label='Gris : absent'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_patches, loc='lower center', fontsize=8,\n",
    "              ncol=3, bbox_to_anchor=(0.45, -0.05))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Demonstration avec un exemple\n",
    "demo_answer = \"CRANE\"\n",
    "demo_guesses = [\"TRACE\", \"CRISE\", \"CRANE\"]\n",
    "demo_feedbacks = [compute_feedback(g, demo_answer) for g in demo_guesses]\n",
    "\n",
    "draw_wordle_grid(demo_guesses, demo_feedbacks, answer=demo_answer,\n",
    "                 title=\"Exemple de partie\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Partie resolue en 3 tentatives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Approche 1 : Filtrage simple (~8 min)\n",
    "\n",
    "### Principe\n",
    "\n",
    "L'approche la plus intuitive consiste a :\n",
    "1. Maintenir une liste de **mots candidats** (initialement tous les mots)\n",
    "2. Apres chaque tentative, **filtrer** les mots incompatibles avec le feedback recu\n",
    "3. Choisir le prochain mot **au hasard** parmi les candidats restants\n",
    "\n",
    "Un mot candidat $w$ est **compatible** avec un feedback $f$ pour une tentative $g$ si et seulement si :\n",
    "\n",
    "$$\\text{compute\\_feedback}(g, w) = f$$\n",
    "\n",
    "En d'autres termes, si $w$ etait le mot secret, le feedback serait identique a celui observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(candidates: List[str], guess: str,\n",
    "                 feedback: Tuple[int, ...]) -> List[str]:\n",
    "    \"\"\"Filtre les mots candidats selon le feedback recu.\n",
    "\n",
    "    Garde uniquement les mots qui produiraient le meme feedback\n",
    "    si on les utilisait comme mot secret.\n",
    "    \"\"\"\n",
    "    return [w for w in candidates if compute_feedback(guess, w) == feedback]\n",
    "\n",
    "\n",
    "class SimpleFilterSolver:\n",
    "    \"\"\"Solveur Wordle par filtrage simple.\n",
    "\n",
    "    Strategie : filtrer les mots incompatibles, choisir au hasard parmi les restants.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_list: List[str]):\n",
    "        self.word_list = word_list\n",
    "\n",
    "    def solve(self, answer: str, verbose: bool = False) -> List[str]:\n",
    "        \"\"\"Resout une partie Wordle, retourne la liste des tentatives.\"\"\"\n",
    "        candidates = list(self.word_list)\n",
    "        guesses = []\n",
    "\n",
    "        for attempt in range(6):\n",
    "            # Choisir un mot au hasard parmi les candidats\n",
    "            guess = random.choice(candidates)\n",
    "            guesses.append(guess)\n",
    "            feedback = compute_feedback(guess, answer)\n",
    "\n",
    "            if verbose:\n",
    "                fb_str = \"\".join([\"_YG\"[v] for v in feedback])\n",
    "                print(f\"  Tentative {attempt + 1}: {guess} -> {fb_str}\"\n",
    "                      f\"  ({len(candidates)} candidats)\")\n",
    "\n",
    "            if guess == answer:\n",
    "                return guesses\n",
    "\n",
    "            candidates = filter_words(candidates, guess, feedback)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"    -> {len(candidates)} candidats restants\")\n",
    "\n",
    "            if not candidates:\n",
    "                break\n",
    "\n",
    "        return guesses\n",
    "\n",
    "\n",
    "print(\"SimpleFilterSolver defini.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-test-intro",
   "metadata": {},
   "source": [
    "Testons le solveur par filtrage simple sur quelques mots secrets et observons la reduction du nombre de candidats a chaque etape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du solveur par filtrage simple\n",
    "solver_simple = SimpleFilterSolver(WORD_LIST)\n",
    "\n",
    "# Fixer la graine pour la reproductibilite\n",
    "random.seed(42)\n",
    "\n",
    "test_words = [\"CRANE\", \"FLEUR\", \"MONDE\", \"PISTE\", \"VAGUE\"]\n",
    "\n",
    "print(\"Test du solveur par filtrage simple\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "results_simple = []\n",
    "for word in test_words:\n",
    "    random.seed(hash(word) % 2**31)  # Graine deterministe par mot\n",
    "    guesses = solver_simple.solve(word, verbose=True)\n",
    "    won = guesses[-1] == word\n",
    "    results_simple.append({\n",
    "        'word': word,\n",
    "        'guesses': len(guesses),\n",
    "        'won': won\n",
    "    })\n",
    "    status = \"Gagne\" if won else \"Perdu\"\n",
    "    print(f\"  => {status} en {len(guesses)} tentative(s)\\n\")\n",
    "\n",
    "avg = np.mean([r['guesses'] for r in results_simple if r['won']])\n",
    "print(f\"Moyenne (parties gagnees) : {avg:.1f} tentatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : filtrage simple\n",
    "\n",
    "**Sortie obtenue** : Le solveur par filtrage simple parvient generalement a trouver le mot en 3 a 5 tentatives.\n",
    "\n",
    "| Observation | Detail |\n",
    "|-------------|--------|\n",
    "| Reduction des candidats | Chaque tentative divise typiquement l'espace par 5-20 |\n",
    "| Variabilite | Le choix aleatoire rend les performances imprevisibles |\n",
    "| Pire cas | Un mauvais premier mot peut laisser beaucoup de candidats |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le filtrage est **correct** : il ne supprime jamais le mot secret des candidats\n",
    "2. Le choix aleatoire ne garantit pas l'optimalite de la reduction d'espace\n",
    "3. L'approche fonctionne mais peut etre amelioree en choisissant mieux le prochain mot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-reduction-intro",
   "metadata": {},
   "source": [
    "### Visualisation : reduction de l'espace\n",
    "\n",
    "Observons graphiquement comment le nombre de candidats diminue a chaque tentative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_reduction(solver, answer: str, seed: int = 42):\n",
    "    \"\"\"Trace la reduction du nombre de candidats pour une partie.\"\"\"\n",
    "    random.seed(seed)\n",
    "    candidates = list(solver.word_list)\n",
    "    counts = [len(candidates)]\n",
    "    guesses = []\n",
    "    feedbacks = []\n",
    "\n",
    "    for attempt in range(6):\n",
    "        guess = random.choice(candidates)\n",
    "        guesses.append(guess)\n",
    "        feedback = compute_feedback(guess, answer)\n",
    "        feedbacks.append(feedback)\n",
    "\n",
    "        if guess == answer:\n",
    "            counts.append(1)\n",
    "            break\n",
    "\n",
    "        candidates = filter_words(candidates, guess, feedback)\n",
    "        counts.append(len(candidates))\n",
    "\n",
    "    return guesses, feedbacks, counts\n",
    "\n",
    "\n",
    "# Tracer la reduction pour un exemple\n",
    "example_answer = \"CRANE\"\n",
    "guesses_ex, feedbacks_ex, counts_ex = trace_reduction(\n",
    "    solver_simple, example_answer, seed=123\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graphique 1 : Reduction des candidats\n",
    "steps = range(len(counts_ex))\n",
    "ax1.bar(steps, counts_ex, color='#42A5F5', edgecolor='#1565C0')\n",
    "ax1.set_xlabel('Etape (0 = initial)', fontsize=11)\n",
    "ax1.set_ylabel('Nombre de candidats', fontsize=11)\n",
    "ax1.set_title(f'Reduction des candidats (secret = {example_answer})',\n",
    "              fontsize=12, fontweight='bold')\n",
    "for i, v in enumerate(counts_ex):\n",
    "    ax1.text(i, v + 1, str(v), ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.set_xticks(list(steps))\n",
    "labels = ['Initial'] + [guesses_ex[i] for i in range(len(guesses_ex))]\n",
    "ax1.set_xticklabels(labels[:len(counts_ex)], rotation=45, ha='right')\n",
    "\n",
    "# Graphique 2 : Ratio de reduction\n",
    "ratios = []\n",
    "for i in range(1, len(counts_ex)):\n",
    "    if counts_ex[i - 1] > 0:\n",
    "        ratios.append(counts_ex[i - 1] / max(counts_ex[i], 1))\n",
    "    else:\n",
    "        ratios.append(1)\n",
    "ax2.bar(range(len(ratios)), ratios, color='#66BB6A', edgecolor='#2E7D32')\n",
    "ax2.set_xlabel('Tentative', fontsize=11)\n",
    "ax2.set_ylabel('Facteur de reduction', fontsize=11)\n",
    "ax2.set_title('Facteur de reduction par tentative', fontsize=12, fontweight='bold')\n",
    "for i, v in enumerate(ratios):\n",
    "    ax2.text(i, v + 0.1, f'{v:.1f}x', ha='center', fontsize=10)\n",
    "ax2.set_xticks(range(len(ratios)))\n",
    "ax2.set_xticklabels([guesses_ex[i] for i in range(len(ratios))])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Partie resolue en {len(guesses_ex)} tentatives.\")\n",
    "print(f\"Reduction totale : {counts_ex[0]} -> 1 mot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Approche 2 : Filtrage CSP (~10 min)\n",
    "\n",
    "### Modelisation CSP du probleme Wordle\n",
    "\n",
    "On peut voir chaque tentative Wordle comme un ensemble de **contraintes** sur le mot secret. Formalisons cela comme un CSP :\n",
    "\n",
    "| Composant | Definition |\n",
    "|-----------|------------|\n",
    "| **Variables** | $L_0, L_1, L_2, L_3, L_4$ (les 5 lettres du mot secret) |\n",
    "| **Domaines** | $D_i \\subseteq \\{A, B, \\ldots, Z\\}$ (lettres possibles a chaque position) |\n",
    "| **Contraintes** | Deduites du feedback de chaque tentative |\n",
    "\n",
    "### Contraintes deduites du feedback\n",
    "\n",
    "Pour une tentative $g = g_0 g_1 g_2 g_3 g_4$ avec un feedback $f = (f_0, f_1, f_2, f_3, f_4)$ :\n",
    "\n",
    "| Feedback | Contrainte |\n",
    "|----------|------------|\n",
    "| $f_i = 2$ (vert) | $L_i = g_i$ (fixer la lettre a cette position) |\n",
    "| $f_i = 1$ (jaune) | $L_i \\neq g_i$ ET $g_i \\in \\{L_0, \\ldots, L_4\\}$ (la lettre est dans le mot mais pas ici) |\n",
    "| $f_i = 0$ (gris) | $g_i \\notin \\{L_j : f_j \\neq 2 \\text{ et } g_j \\neq g_i\\}$ (la lettre n'est pas dans les positions non-vertes) |\n",
    "\n",
    "**Attention** : le cas gris est subtil quand la meme lettre apparait plusieurs fois dans la tentative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csp-solver",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSPWordleSolver:\n",
    "    \"\"\"Solveur Wordle par propagation de contraintes.\n",
    "\n",
    "    Maintient les domaines de chaque position et les contraintes\n",
    "    globales (lettres requises, compteurs de lettres).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_list: List[str]):\n",
    "        self.word_list = word_list\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinitialise l'etat du solveur.\"\"\"\n",
    "        all_letters = set('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "        # Domaine de chaque position : toutes les lettres possibles\n",
    "        self.domains = [set(all_letters) for _ in range(5)]\n",
    "        # Lettres qui doivent etre presentes dans le mot\n",
    "        self.required_letters = set()\n",
    "        # Nombre minimum d'occurrences de chaque lettre\n",
    "        self.min_counts = Counter()\n",
    "        # Nombre maximum d'occurrences de chaque lettre\n",
    "        self.max_counts = {c: 5 for c in all_letters}\n",
    "        # Positions fixees (vertes)\n",
    "        self.fixed = {}\n",
    "\n",
    "    def add_constraints(self, guess: str, feedback: Tuple[int, ...]):\n",
    "        \"\"\"Ajoute les contraintes deduites d'une tentative.\"\"\"\n",
    "        # Compter les verts et jaunes par lettre\n",
    "        letter_green_yellow = Counter()\n",
    "        letter_gray = set()\n",
    "\n",
    "        for i, (g, f) in enumerate(zip(guess, feedback)):\n",
    "            if f == 2:  # Vert : fixer la lettre\n",
    "                self.domains[i] = {g}\n",
    "                self.fixed[i] = g\n",
    "                letter_green_yellow[g] += 1\n",
    "            elif f == 1:  # Jaune : la lettre est dans le mot, pas ici\n",
    "                self.domains[i].discard(g)\n",
    "                self.required_letters.add(g)\n",
    "                letter_green_yellow[g] += 1\n",
    "            else:  # Gris\n",
    "                letter_gray.add(g)\n",
    "\n",
    "        # Mettre a jour les compteurs min/max\n",
    "        for letter, count in letter_green_yellow.items():\n",
    "            self.min_counts[letter] = max(self.min_counts[letter], count)\n",
    "\n",
    "        # Les lettres grises sans vert ni jaune sont completement absentes\n",
    "        for letter in letter_gray:\n",
    "            if letter not in letter_green_yellow:\n",
    "                # Lettre totalement absente\n",
    "                self.max_counts[letter] = 0\n",
    "                for i in range(5):\n",
    "                    if i not in self.fixed or self.fixed[i] != letter:\n",
    "                        self.domains[i].discard(letter)\n",
    "            else:\n",
    "                # Lettre presente mais pas plus que le nombre de verts+jaunes\n",
    "                self.max_counts[letter] = letter_green_yellow[letter]\n",
    "\n",
    "    def is_compatible(self, word: str) -> bool:\n",
    "        \"\"\"Verifie si un mot est compatible avec toutes les contraintes.\"\"\"\n",
    "        # Verifier les domaines de chaque position\n",
    "        for i, letter in enumerate(word):\n",
    "            if letter not in self.domains[i]:\n",
    "                return False\n",
    "\n",
    "        # Verifier les lettres requises\n",
    "        word_counter = Counter(word)\n",
    "        for letter in self.required_letters:\n",
    "            if letter not in word_counter:\n",
    "                return False\n",
    "\n",
    "        # Verifier les compteurs min/max\n",
    "        for letter, min_c in self.min_counts.items():\n",
    "            if word_counter.get(letter, 0) < min_c:\n",
    "                return False\n",
    "        for letter, max_c in self.max_counts.items():\n",
    "            if word_counter.get(letter, 0) > max_c:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def get_candidates(self) -> List[str]:\n",
    "        \"\"\"Retourne les mots compatibles avec les contraintes actuelles.\"\"\"\n",
    "        return [w for w in self.word_list if self.is_compatible(w)]\n",
    "\n",
    "    def solve(self, answer: str, verbose: bool = False) -> List[str]:\n",
    "        \"\"\"Resout une partie Wordle avec le solveur CSP.\"\"\"\n",
    "        self.reset()\n",
    "        guesses = []\n",
    "\n",
    "        for attempt in range(6):\n",
    "            candidates = self.get_candidates()\n",
    "            guess = random.choice(candidates) if candidates else None\n",
    "\n",
    "            if guess is None:\n",
    "                break\n",
    "\n",
    "            guesses.append(guess)\n",
    "            feedback = compute_feedback(guess, answer)\n",
    "\n",
    "            if verbose:\n",
    "                fb_str = \"\".join([\"_YG\"[v] for v in feedback])\n",
    "                print(f\"  Tentative {attempt + 1}: {guess} -> {fb_str}\"\n",
    "                      f\"  ({len(candidates)} candidats)\")\n",
    "                # Afficher les domaines reduits\n",
    "                self.add_constraints(guess, feedback)\n",
    "                domains_str = [f\"  Position {i}: \"\n",
    "                              f\"{sorted(d) if len(d) <= 8 else f'|D|={len(d)}'}\"\n",
    "                              for i, d in enumerate(self.domains)]\n",
    "                for ds in domains_str:\n",
    "                    print(f\"    {ds}\")\n",
    "            else:\n",
    "                self.add_constraints(guess, feedback)\n",
    "\n",
    "            if guess == answer:\n",
    "                return guesses\n",
    "\n",
    "        return guesses\n",
    "\n",
    "\n",
    "print(\"CSPWordleSolver defini.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csp-test-intro",
   "metadata": {},
   "source": [
    "Testons le solveur CSP et observons la reduction des domaines a chaque etape. La visualisation des domaines montre comment les contraintes eliminent progressivement les lettres impossibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csp-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du solveur CSP avec trace detaillee\n",
    "solver_csp = CSPWordleSolver(WORD_LIST)\n",
    "\n",
    "print(\"Test du solveur CSP avec reduction des domaines\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Exemple detaille\n",
    "random.seed(42)\n",
    "answer_csp = \"CRANE\"\n",
    "guesses_csp = solver_csp.solve(answer_csp, verbose=True)\n",
    "\n",
    "won = guesses_csp[-1] == answer_csp\n",
    "print(f\"\\n=> {'Gagne' if won else 'Perdu'} en {len(guesses_csp)} tentative(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csp-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : solveur CSP\n",
    "\n",
    "**Sortie obtenue** : Le solveur CSP montre la reduction progressive des domaines de chaque position.\n",
    "\n",
    "| Etape | Effet sur les domaines |\n",
    "|-------|------------------------|\n",
    "| Apres tentative 1 | Les lettres vertes fixent leur position ; les grises sont eliminees partout |\n",
    "| Apres tentative 2 | Les domaines se reduisent davantage ; les jaunes contraignent les positions |\n",
    "| Convergence | Chaque position n'a plus qu'une ou deux lettres possibles |\n",
    "\n",
    "**Avantages du CSP par rapport au filtrage simple** :\n",
    "1. **Structuration explicite** : on voit exactement quelles lettres restent possibles a chaque position\n",
    "2. **Propagation** : les contraintes se combinent (un vert en position 0 reduit aussi les options des autres positions via min/max counts)\n",
    "3. **Extensibilite** : on peut ajouter de l'arc-consistance ou du forward checking\n",
    "\n",
    "**Limitation** : avec un choix aleatoire du prochain mot, les performances sont similaires au filtrage simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csp-viz-intro",
   "metadata": {},
   "source": [
    "### Visualisation : domaines CSP etape par etape\n",
    "\n",
    "Visualisons comment les domaines de chaque position se reduisent au fil des tentatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csp-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_csp_domains(solver_cls, word_list, answer, seed=42):\n",
    "    \"\"\"Trace l'evolution des tailles de domaines pour chaque position.\"\"\"\n",
    "    random.seed(seed)\n",
    "    solver = solver_cls(word_list)\n",
    "    solver.reset()\n",
    "\n",
    "    domain_sizes = [[len(d) for d in solver.domains]]  # Etat initial\n",
    "    guesses = []\n",
    "    n_candidates = [len(word_list)]\n",
    "\n",
    "    for attempt in range(6):\n",
    "        candidates = solver.get_candidates()\n",
    "        guess = random.choice(candidates)\n",
    "        guesses.append(guess)\n",
    "        feedback = compute_feedback(guess, answer)\n",
    "        solver.add_constraints(guess, feedback)\n",
    "\n",
    "        domain_sizes.append([len(d) for d in solver.domains])\n",
    "        new_candidates = solver.get_candidates()\n",
    "        n_candidates.append(len(new_candidates))\n",
    "\n",
    "        if guess == answer:\n",
    "            break\n",
    "\n",
    "    return guesses, domain_sizes, n_candidates\n",
    "\n",
    "\n",
    "guesses_trace, domain_sizes, n_cand = trace_csp_domains(\n",
    "    CSPWordleSolver, WORD_LIST, \"CRANE\", seed=42\n",
    ")\n",
    "\n",
    "# Graphique de la reduction des domaines\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap des tailles de domaines\n",
    "data = np.array(domain_sizes)\n",
    "im = ax1.imshow(data.T, cmap='YlOrRd_r', aspect='auto', vmin=1, vmax=26)\n",
    "ax1.set_xlabel('Etape', fontsize=11)\n",
    "ax1.set_ylabel('Position', fontsize=11)\n",
    "ax1.set_yticks(range(5))\n",
    "ax1.set_yticklabels([f'Position {i}' for i in range(5)])\n",
    "labels = ['Initial'] + guesses_trace\n",
    "ax1.set_xticks(range(len(labels)))\n",
    "ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax1.set_title('Taille des domaines par position', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Annoter les valeurs\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(data.shape[1]):\n",
    "        ax1.text(i, j, str(data[i, j]), ha='center', va='center',\n",
    "                fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax1, shrink=0.8, label='Taille du domaine')\n",
    "\n",
    "# Nombre de candidats\n",
    "ax2.plot(range(len(n_cand)), n_cand, 'o-', color='#1565C0',\n",
    "         linewidth=2, markersize=8)\n",
    "ax2.fill_between(range(len(n_cand)), n_cand, alpha=0.2, color='#42A5F5')\n",
    "ax2.set_xlabel('Etape', fontsize=11)\n",
    "ax2.set_ylabel('Mots candidats', fontsize=11)\n",
    "ax2.set_title('Reduction de l\\'espace de recherche', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticks(range(len(labels)))\n",
    "ax2.set_xticklabels(labels, rotation=45, ha='right')\n",
    "for i, v in enumerate(n_cand):\n",
    "    ax2.text(i, v + max(n_cand) * 0.02, str(v), ha='center', fontsize=9)\n",
    "ax2.set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csp-viz-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : evolution des domaines CSP\n",
    "\n",
    "**Sortie obtenue** : La heatmap montre la taille des domaines (nombre de lettres possibles) pour chaque position a chaque etape.\n",
    "\n",
    "| Observation | Detail |\n",
    "|-------------|--------|\n",
    "| Etat initial | Chaque position a 26 lettres possibles |\n",
    "| Verts | Le domaine tombe a 1 immediatement (case foncee) |\n",
    "| Gris | Les lettres sont eliminees de toutes les positions non fixees |\n",
    "| Convergence | Apres 2-3 tentatives, les domaines sont fortement reduits |\n",
    "\n",
    "**Points cles** :\n",
    "1. La heatmap permet de visualiser la **propagation des contraintes** position par position\n",
    "2. Les positions avec un vert convergent immediatement (domaine = 1)\n",
    "3. Les lettres grises ont un effet global : elles sont eliminees de toutes les positions\n",
    "\n",
    "> **Lien avec Search-7** : cette reduction de domaines est une forme de forward checking, etudiee dans le notebook sur la consistance CSP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Approche 3 : Theorie de l'information (~12 min)\n",
    "\n",
    "### Le probleme du choix optimal\n",
    "\n",
    "Les deux approches precedentes choisissent le prochain mot **au hasard** parmi les candidats. Mais certains mots apportent **plus d'information** que d'autres.\n",
    "\n",
    "L'idee cle est d'utiliser l'**entropie de Shannon** pour mesurer combien d'information un mot apportera en moyenne.\n",
    "\n",
    "### Entropie de Shannon\n",
    "\n",
    "Pour une variable aleatoire discrete $X$ prenant les valeurs $x_1, \\ldots, x_n$ avec probabilites $p_1, \\ldots, p_n$ :\n",
    "\n",
    "$$H(X) = -\\sum_{i=1}^{n} p(x_i) \\log_2 p(x_i)$$\n",
    "\n",
    "L'entropie mesure l'**incertitude** : plus elle est elevee, plus le resultat est imprevisible et donc plus on apprend du resultat.\n",
    "\n",
    "### Application a Wordle\n",
    "\n",
    "Pour un mot candidat $g$ (guess), on peut calculer la **distribution des feedbacks** qu'il produirait :\n",
    "\n",
    "1. Pour chaque mot secret possible $w$ dans les candidats, calculer $\\text{feedback}(g, w)$\n",
    "2. Compter la frequence de chaque pattern de feedback\n",
    "3. Calculer l'entropie de cette distribution\n",
    "\n",
    "$$H(g) = -\\sum_{f \\in \\text{feedbacks}} \\frac{|\\{w : \\text{feedback}(g, w) = f\\}|}{|\\text{candidats}|} \\log_2 \\frac{|\\{w : \\text{feedback}(g, w) = f\\}|}{|\\text{candidats}|}$$\n",
    "\n",
    "Le meilleur mot est celui qui **maximise l'entropie** : il repartit les candidats le plus uniformement possible entre les differents patterns de feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entropy-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(guess: str, candidates: List[str]) -> float:\n",
    "    \"\"\"Calcule l'entropie du feedback pour un mot candidat.\n",
    "\n",
    "    Plus l'entropie est elevee, plus le mot est informatif.\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return 0.0\n",
    "\n",
    "    # Compter les patterns de feedback\n",
    "    pattern_counts = Counter()\n",
    "    for word in candidates:\n",
    "        fb = compute_feedback(guess, word)\n",
    "        pattern_counts[fb] += 1\n",
    "\n",
    "    # Calculer l'entropie\n",
    "    total = len(candidates)\n",
    "    entropy = 0.0\n",
    "    for count in pattern_counts.values():\n",
    "        p = count / total\n",
    "        if p > 0:\n",
    "            entropy -= p * math.log2(p)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def rank_by_entropy(candidates: List[str],\n",
    "                    word_pool: List[str] = None,\n",
    "                    top_n: int = 10) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Classe les mots par entropie decroissante.\n",
    "\n",
    "    Args:\n",
    "        candidates: mots encore possibles comme mot secret\n",
    "        word_pool: mots a evaluer (defaut = candidates)\n",
    "        top_n: nombre de resultats a retourner\n",
    "    \"\"\"\n",
    "    if word_pool is None:\n",
    "        word_pool = candidates\n",
    "\n",
    "    scored = []\n",
    "    for word in word_pool:\n",
    "        h = compute_entropy(word, candidates)\n",
    "        scored.append((word, h))\n",
    "\n",
    "    scored.sort(key=lambda x: -x[1])\n",
    "    return scored[:top_n]\n",
    "\n",
    "\n",
    "# Calculer l'entropie de quelques mots sur la liste complete\n",
    "print(\"Entropie de quelques mots (sur la liste complete)\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "sample_words = [\"CRANE\", \"ADORE\", \"AIMER\", \"TABLE\", \"EXTRA\",\n",
    "                \"FLEUR\", \"PISTE\", \"REGLE\"]\n",
    "\n",
    "for word in sample_words:\n",
    "    if word in WORD_LIST:\n",
    "        h = compute_entropy(word, WORD_LIST)\n",
    "        print(f\"  {word} : H = {h:.3f} bits\")\n",
    "\n",
    "# Entropie maximale theorique\n",
    "h_max = math.log2(len(WORD_LIST))\n",
    "print(f\"\\nEntropie maximale theorique : log2({len(WORD_LIST)}) = {h_max:.3f} bits\")\n",
    "print(f\"(une seule tentative suffirait si H = {h_max:.1f} bits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entropy-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : entropie des mots\n",
    "\n",
    "**Sortie obtenue** : L'entropie varie significativement selon le mot choisi.\n",
    "\n",
    "| Observation | Detail |\n",
    "|-------------|--------|\n",
    "| Mots a haute entropie | Contiennent des lettres frequentes et bien reparties |\n",
    "| Mots a basse entropie | Contiennent des lettres rares ou trop communes |\n",
    "| Entropie max theorique | $\\log_2(N)$ bits, atteinte si chaque candidat donne un feedback unique |\n",
    "\n",
    "**Points cles** :\n",
    "1. Un mot avec une entropie elevee **divise mieux** l'espace des candidats\n",
    "2. L'entropie maximale theorique n'est pas atteignable car il n'y a que $3^5 = 243$ patterns distincts\n",
    "3. Le meilleur premier mot est celui qui repartit le plus uniformement les feedbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "top-words-intro",
   "metadata": {},
   "source": [
    "### Les 10 meilleurs premiers mots\n",
    "\n",
    "Calculons l'entropie de tous les mots pour trouver le meilleur premier mot (celui qui apporte le plus d'information en moyenne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer l'entropie de tous les mots (peut prendre quelques secondes)\n",
    "start = time.time()\n",
    "top_words = rank_by_entropy(WORD_LIST, top_n=10)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Top 10 des meilleurs premiers mots (calcule en {elapsed:.1f}s)\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"{'Rang':<6} {'Mot':<10} {'Entropie (bits)':<18}\")\n",
    "print(\"-\" * 45)\n",
    "for i, (word, h) in enumerate(top_words, 1):\n",
    "    bar = '#' * int(h * 3)\n",
    "    print(f\"{i:<6} {word:<10} {h:<18.4f} {bar}\")\n",
    "\n",
    "best_word = top_words[0][0]\n",
    "best_h = top_words[0][1]\n",
    "print(f\"\\nMeilleur premier mot : {best_word} (H = {best_h:.4f} bits)\")\n",
    "print(f\"Reduction attendue : {len(WORD_LIST)} -> ~{len(WORD_LIST) / 2**best_h:.0f} candidats en moyenne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "top-words-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : meilleurs premiers mots\n",
    "\n",
    "**Sortie obtenue** : Le classement des mots par entropie revele les mots les plus informatifs.\n",
    "\n",
    "| Observation | Detail |\n",
    "|-------------|--------|\n",
    "| Meilleur mot | Contient des lettres frequentes (E, A, R, S, I, T...) |\n",
    "| Ecart entre top et bas | Significatif : les meilleurs mots apportent 1-2 bits de plus |\n",
    "| Reduction moyenne | Le meilleur mot divise l'espace par $2^H$ en moyenne |\n",
    "\n",
    "**Analogie** : choisir le meilleur mot, c'est comme poser la question la plus discriminante dans un jeu de 20 questions. On veut que chaque reponse possible soit equiprobable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entropy-solver-intro",
   "metadata": {},
   "source": [
    "### Solveur par entropie\n",
    "\n",
    "Combinons le filtrage avec la selection par entropie pour construire un solveur optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entropy-solver",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropySolver:\n",
    "    \"\"\"Solveur Wordle par maximisation de l'entropie.\n",
    "\n",
    "    A chaque etape, choisit le mot qui maximise l'information\n",
    "    attendue (entropie du feedback).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_list: List[str], precomputed_first: str = None):\n",
    "        self.word_list = word_list\n",
    "        self.precomputed_first = precomputed_first\n",
    "\n",
    "    def solve(self, answer: str, verbose: bool = False) -> List[str]:\n",
    "        \"\"\"Resout une partie Wordle avec selection par entropie.\"\"\"\n",
    "        candidates = list(self.word_list)\n",
    "        guesses = []\n",
    "\n",
    "        for attempt in range(6):\n",
    "            if attempt == 0 and self.precomputed_first:\n",
    "                guess = self.precomputed_first\n",
    "            elif len(candidates) <= 2:\n",
    "                # Peu de candidats : choisir le premier\n",
    "                guess = candidates[0]\n",
    "            else:\n",
    "                # Choisir le mot avec la meilleure entropie\n",
    "                ranked = rank_by_entropy(candidates, candidates, top_n=1)\n",
    "                guess = ranked[0][0]\n",
    "\n",
    "            guesses.append(guess)\n",
    "            feedback = compute_feedback(guess, answer)\n",
    "\n",
    "            if verbose:\n",
    "                fb_str = \"\".join([\"_YG\"[v] for v in feedback])\n",
    "                h = compute_entropy(guess, candidates) if len(candidates) > 1 else 0\n",
    "                print(f\"  Tentative {attempt + 1}: {guess} -> {fb_str}\"\n",
    "                      f\"  (H={h:.2f}, {len(candidates)} candidats)\")\n",
    "\n",
    "            if guess == answer:\n",
    "                return guesses\n",
    "\n",
    "            candidates = filter_words(candidates, guess, feedback)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"    -> {len(candidates)} candidats restants\")\n",
    "\n",
    "            if not candidates:\n",
    "                break\n",
    "\n",
    "        return guesses\n",
    "\n",
    "\n",
    "# Precomputer le meilleur premier mot\n",
    "best_first = top_words[0][0]\n",
    "solver_entropy = EntropySolver(WORD_LIST, precomputed_first=best_first)\n",
    "\n",
    "print(f\"EntropySolver defini (premier mot : {best_first})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entropy-test-intro",
   "metadata": {},
   "source": [
    "Testons le solveur par entropie sur les memes mots secrets que precedemment pour comparer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entropy-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du solveur par entropie\n",
    "print(\"Test du solveur par entropie\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_entropy = []\n",
    "for word in test_words:\n",
    "    guesses = solver_entropy.solve(word, verbose=True)\n",
    "    won = guesses[-1] == word\n",
    "    results_entropy.append({\n",
    "        'word': word,\n",
    "        'guesses': len(guesses),\n",
    "        'won': won\n",
    "    })\n",
    "    status = \"Gagne\" if won else \"Perdu\"\n",
    "    print(f\"  => {status} en {len(guesses)} tentative(s)\\n\")\n",
    "\n",
    "avg_entropy = np.mean([r['guesses'] for r in results_entropy if r['won']])\n",
    "print(f\"Moyenne (parties gagnees) : {avg_entropy:.1f} tentatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entropy-test-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : solveur par entropie\n",
    "\n",
    "**Sortie obtenue** : Le solveur par entropie trouve systematiquement le mot en moins de tentatives.\n",
    "\n",
    "| Observation | Detail |\n",
    "|-------------|--------|\n",
    "| Premiere tentative | Toujours le mot optimal precalcule |\n",
    "| Reduction | Plus forte car le mot est choisi pour maximiser l'information |\n",
    "| Convergence | Typiquement 2-4 tentatives au lieu de 3-5 |\n",
    "\n",
    "**Points cles** :\n",
    "1. La selection par entropie est **deterministe** : pas de variabilite aleatoire\n",
    "2. Le cout de calcul est plus eleve ($O(n^2)$ par tentative vs $O(n)$ pour le filtrage simple)\n",
    "3. Le gain en nombre de tentatives compense largement le cout de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entropy-viz-intro",
   "metadata": {},
   "source": [
    "### Visualisation : distribution des feedbacks\n",
    "\n",
    "Comparons la distribution des feedbacks pour le meilleur et le pire premier mot. Le meilleur mot repartit les candidats uniformement entre les patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entropy-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_distribution(guess: str, candidates: List[str]) -> Dict:\n",
    "    \"\"\"Calcule la distribution des feedbacks pour un mot.\"\"\"\n",
    "    pattern_counts = Counter()\n",
    "    for word in candidates:\n",
    "        fb = compute_feedback(guess, word)\n",
    "        pattern_counts[fb] += 1\n",
    "    return pattern_counts\n",
    "\n",
    "\n",
    "# Comparer le meilleur et un mauvais premier mot\n",
    "best = top_words[0][0]\n",
    "# Trouver un mot avec faible entropie\n",
    "all_entropies = [(w, compute_entropy(w, WORD_LIST)) for w in WORD_LIST[:50]]\n",
    "all_entropies.sort(key=lambda x: x[1])\n",
    "worst = all_entropies[0][0]\n",
    "\n",
    "dist_best = feedback_distribution(best, WORD_LIST)\n",
    "dist_worst = feedback_distribution(worst, WORD_LIST)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution du meilleur mot\n",
    "counts_best = sorted(dist_best.values(), reverse=True)\n",
    "ax1.bar(range(len(counts_best)), counts_best, color='#4CAF50', edgecolor='#2E7D32')\n",
    "ax1.set_title(f'Distribution des feedbacks : {best}\\n'\n",
    "              f'H = {compute_entropy(best, WORD_LIST):.3f} bits, '\n",
    "              f'{len(dist_best)} patterns distincts',\n",
    "              fontsize=11, fontweight='bold')\n",
    "ax1.set_xlabel('Pattern de feedback (trie par frequence)', fontsize=10)\n",
    "ax1.set_ylabel('Nombre de mots', fontsize=10)\n",
    "\n",
    "# Distribution du pire mot\n",
    "counts_worst = sorted(dist_worst.values(), reverse=True)\n",
    "ax2.bar(range(len(counts_worst)), counts_worst, color='#F44336', edgecolor='#C62828')\n",
    "ax2.set_title(f'Distribution des feedbacks : {worst}\\n'\n",
    "              f'H = {compute_entropy(worst, WORD_LIST):.3f} bits, '\n",
    "              f'{len(dist_worst)} patterns distincts',\n",
    "              fontsize=11, fontweight='bold')\n",
    "ax2.set_xlabel('Pattern de feedback (trie par frequence)', fontsize=10)\n",
    "ax2.set_ylabel('Nombre de mots', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Meilleur ({best}) : {len(dist_best)} patterns, \"\n",
    "      f\"plus grand groupe = {max(dist_best.values())} mots\")\n",
    "print(f\"Pire ({worst}) : {len(dist_worst)} patterns, \"\n",
    "      f\"plus grand groupe = {max(dist_worst.values())} mots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entropy-viz-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : distribution des feedbacks\n",
    "\n",
    "**Sortie obtenue** : Les deux distributions montrent un contraste marque.\n",
    "\n",
    "| Mot | Patterns distincts | Plus grand groupe | Entropie |\n",
    "|-----|-------------------|-------------------|----------|\n",
    "| Meilleur | Eleve | Petit | Haute |\n",
    "| Pire | Faible | Grand | Basse |\n",
    "\n",
    "**Points cles** :\n",
    "1. Le meilleur mot repartit les candidats **uniformement** : chaque pattern contient peu de mots\n",
    "2. Le pire mot concentre la majorite des candidats dans **un seul pattern** (typiquement tout gris)\n",
    "3. En termes d'information : le meilleur mot permet de distinguer plus finement les candidats\n",
    "\n",
    "> **Analogie** : c'est comme poser une question binaire (oui/non). La meilleure question divise les possibilites en deux groupes egaux. La pire question a une reponse previsible (99% de \"non\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Comparaison et analyse (~5 min)\n",
    "\n",
    "### Benchmark sur 100 parties\n",
    "\n",
    "Comparons les trois approches sur un echantillon significatif de parties. Pour chaque mot secret de la liste (ou un echantillon), nous mesurons le nombre de tentatives necessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_solver(solver, word_list, sample_size=100, seed=42):\n",
    "    \"\"\"Benchmark un solveur sur un echantillon de mots.\"\"\"\n",
    "    random.seed(seed)\n",
    "    sample = random.sample(word_list, min(sample_size, len(word_list)))\n",
    "\n",
    "    results = []\n",
    "    start = time.time()\n",
    "\n",
    "    for answer in sample:\n",
    "        # Reinitialiser la graine pour le solveur simple (reproductibilite)\n",
    "        random.seed(hash(answer) % 2**31)\n",
    "        guesses = solver.solve(answer)\n",
    "        won = guesses[-1] == answer\n",
    "        results.append({\n",
    "            'answer': answer,\n",
    "            'n_guesses': len(guesses),\n",
    "            'won': won\n",
    "        })\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    won_results = [r for r in results if r['won']]\n",
    "\n",
    "    return {\n",
    "        'results': results,\n",
    "        'n_games': len(results),\n",
    "        'n_won': len(won_results),\n",
    "        'avg_guesses': np.mean([r['n_guesses'] for r in won_results]) if won_results else float('inf'),\n",
    "        'worst_case': max([r['n_guesses'] for r in won_results]) if won_results else float('inf'),\n",
    "        'win_rate': len(won_results) / len(results) * 100,\n",
    "        'time_s': elapsed,\n",
    "        'guess_distribution': Counter(r['n_guesses'] for r in won_results)\n",
    "    }\n",
    "\n",
    "\n",
    "# Benchmark des trois solveurs\n",
    "sample_size = min(100, len(WORD_LIST))\n",
    "\n",
    "print(f\"Benchmark sur {sample_size} parties\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Solveur simple\n",
    "print(\"\\n[1/3] Filtrage simple...\")\n",
    "solver_s = SimpleFilterSolver(WORD_LIST)\n",
    "bench_simple = benchmark_solver(solver_s, WORD_LIST, sample_size)\n",
    "print(f\"  Termine en {bench_simple['time_s']:.1f}s\")\n",
    "\n",
    "# Solveur CSP\n",
    "print(\"\\n[2/3] CSP...\")\n",
    "solver_c = CSPWordleSolver(WORD_LIST)\n",
    "bench_csp = benchmark_solver(solver_c, WORD_LIST, sample_size)\n",
    "print(f\"  Termine en {bench_csp['time_s']:.1f}s\")\n",
    "\n",
    "# Solveur entropie\n",
    "print(\"\\n[3/3] Entropie...\")\n",
    "solver_e = EntropySolver(WORD_LIST, precomputed_first=best_first)\n",
    "bench_entropy = benchmark_solver(solver_e, WORD_LIST, sample_size)\n",
    "print(f\"  Termine en {bench_entropy['time_s']:.1f}s\")\n",
    "\n",
    "# Tableau recapitulatif\n",
    "print(f\"\\n{'Approche':<20} {'Moy. tent.':<12} {'Pire cas':<10} {'Win %':<8} {'Temps (s)':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for name, bench in [(\"Filtrage simple\", bench_simple),\n",
    "                     (\"CSP\", bench_csp),\n",
    "                     (\"Entropie\", bench_entropy)]:\n",
    "    print(f\"{name:<20} {bench['avg_guesses']:<12.2f} {bench['worst_case']:<10}\"\n",
    "          f\" {bench['win_rate']:<8.0f} {bench['time_s']:<10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : comparaison des trois approches\n",
    "\n",
    "**Sortie obtenue** : Le tableau ci-dessous resume les performances des trois solveurs.\n",
    "\n",
    "| Approche | Moy. tentatives | Pire cas | Taux de victoire | Temps |\n",
    "|----------|-----------------|----------|-------------------|-------|\n",
    "| Filtrage simple | ~4-5 | 6+ | Variable | Rapide |\n",
    "| CSP | ~4-5 | 6+ | Variable | Rapide |\n",
    "| Entropie | ~3-4 | <= 5 | ~100% | Plus lent |\n",
    "\n",
    "**Points cles** :\n",
    "1. **L'entropie gagne** : en moyenne ~1 tentative de moins que les approches aleatoires\n",
    "2. **Compromis temps/qualite** : l'entropie est plus lente ($O(n^2)$ par tentative) mais plus efficace\n",
    "3. **Filtrage vs CSP** : performances similaires car le goulot d'etranglement est le choix du mot, pas le filtrage\n",
    "4. Le pire cas de l'entropie est generalement meilleur grace au choix optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "histogram",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution du nombre de tentatives\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n",
    "\n",
    "for ax, (name, bench, color) in zip(axes, [\n",
    "    (\"Filtrage simple\", bench_simple, '#42A5F5'),\n",
    "    (\"CSP\", bench_csp, '#66BB6A'),\n",
    "    (\"Entropie\", bench_entropy, '#FFA726')\n",
    "]):\n",
    "    dist = bench['guess_distribution']\n",
    "    x = sorted(dist.keys())\n",
    "    y = [dist[k] for k in x]\n",
    "\n",
    "    ax.bar(x, y, color=color, edgecolor='black', alpha=0.85)\n",
    "    ax.set_xlabel('Nombre de tentatives', fontsize=10)\n",
    "    ax.set_title(f'{name}\\n(moy = {bench[\"avg_guesses\"]:.2f})',\n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_xticks(range(1, 7))\n",
    "\n",
    "    for xi, yi in zip(x, y):\n",
    "        ax.text(xi, yi + 0.3, str(yi), ha='center', fontsize=9)\n",
    "\n",
    "axes[0].set_ylabel('Nombre de parties', fontsize=10)\n",
    "\n",
    "plt.suptitle(f'Distribution des tentatives ({sample_size} parties)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "histogram-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation : histogrammes de performance\n",
    "\n",
    "**Sortie obtenue** : Les histogrammes montrent la distribution du nombre de tentatives pour chaque approche.\n",
    "\n",
    "| Observation | Filtrage simple | CSP | Entropie |\n",
    "|-------------|-----------------|-----|----------|\n",
    "| Pic | 4-5 tentatives | 4-5 tentatives | 3-4 tentatives |\n",
    "| Queue droite | Quelques parties en 6 | Similaire | Rare ou absente |\n",
    "| En 2 tentatives | Rare (chanceux) | Rare | Possible si 2e mot est unique |\n",
    "\n",
    "**Points cles** :\n",
    "1. L'histogramme de l'entropie est **deplace vers la gauche** : le solveur converge plus vite\n",
    "2. Les solveurs aleatoires ont une **plus grande variance** : parfois excellents, parfois mediocres\n",
    "3. Le solveur entropie est plus **regulier** : la distribution est plus concentree\n",
    "\n",
    "> **En resume** : l'entropie transforme la chance en strategie. Au lieu d'esperer tomber sur le bon mot, on choisit systematiquement le plus informatif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Exercices\n",
    "\n",
    "### Exercice 1 : Mode \"hard\" de Wordle\n",
    "\n",
    "En mode \"hard\", chaque tentative doit **utiliser toutes les informations connues** :\n",
    "- Les lettres vertes doivent rester a leur position\n",
    "- Les lettres jaunes doivent etre presentes dans le mot\n",
    "\n",
    "**Tache** : Modifiez le `SimpleFilterSolver` pour qu'il ne propose que des mots compatibles avec les contraintes accumulees (ce qui est deja le cas via le filtrage). Ensuite, verifiez que le solveur entropie fonctionne aussi en mode hard : les mots proposes doivent etre dans les candidats restants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1 : verifier le mode hard\n",
    "\n",
    "# Le filtrage simple est deja en mode hard : on ne choisit que parmi les candidats.\n",
    "# Mais l'approche entropie pourrait choisir un mot HORS des candidats\n",
    "# (pour maximiser l'information). Verifiez que notre implementation\n",
    "# ne fait pas cela.\n",
    "\n",
    "# A COMPLETER\n",
    "# def is_hard_mode_compliant(solver, word_list, answer):\n",
    "#     \"\"\"Verifie que chaque tentative du solveur est dans les candidats restants.\"\"\"\n",
    "#     candidates = list(word_list)\n",
    "#     guesses = solver.solve(answer)\n",
    "#     ...\n",
    "#     return True  # si toutes les tentatives sont dans les candidats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution exercice 1</b></summary>\n",
    "\n",
    "```python\n",
    "def is_hard_mode_compliant(solver, word_list, answer):\n",
    "    \"\"\"Verifie que chaque tentative est dans les candidats restants.\"\"\"\n",
    "    candidates = list(word_list)\n",
    "    guesses = []\n",
    "\n",
    "    for attempt in range(6):\n",
    "        # Simuler le choix du solveur\n",
    "        # (on doit reimplementer le choix car solve() ne renvoie pas les etapes)\n",
    "        if not candidates:\n",
    "            return False\n",
    "\n",
    "        guess = solver.solve(answer)[attempt] if attempt < len(solver.solve(answer)) else None\n",
    "        if guess is None:\n",
    "            break\n",
    "\n",
    "        if guess not in candidates:\n",
    "            print(f\"  VIOLATION : {guess} n'est pas dans les {len(candidates)} candidats\")\n",
    "            return False\n",
    "\n",
    "        feedback = compute_feedback(guess, answer)\n",
    "        if guess == answer:\n",
    "            return True\n",
    "        candidates = filter_words(candidates, guess, feedback)\n",
    "\n",
    "    return True\n",
    "\n",
    "# Notre EntropySolver choisit parmi les candidats, donc il est hard-mode compliant.\n",
    "# Un solveur non-hard pourrait choisir un mot hors candidats pour maximiser\n",
    "# l'information globale (par ex. un mot avec des lettres frequentes jamais testees).\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2-intro",
   "metadata": {},
   "source": [
    "### Exercice 2 : Trouver le mot d'ouverture optimal\n",
    "\n",
    "**Tache** : Calculez l'entropie de **tous** les mots de la liste et identifiez le mot d'ouverture optimal. Verifiez qu'il contient des lettres frequentes dans la langue francaise (E, A, S, R, I, N, T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2 : Analyse du mot d'ouverture optimal\n",
    "\n",
    "# A COMPLETER\n",
    "# 1. Calculer l'entropie de tous les mots\n",
    "# all_scored = rank_by_entropy(WORD_LIST, top_n=len(WORD_LIST))\n",
    "#\n",
    "# 2. Analyser le top 5 : quelles lettres contiennent-ils ?\n",
    "# for word, h in all_scored[:5]:\n",
    "#     letters = set(word)\n",
    "#     print(f\"{word} (H={h:.3f}) : lettres = {letters}\")\n",
    "#\n",
    "# 3. Comparer avec les lettres les plus frequentes en francais\n",
    "# frequences_fr = \"EASIRNTUOL\"  # approximatif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution exercice 2</b></summary>\n",
    "\n",
    "```python\n",
    "# Calculer l'entropie de tous les mots\n",
    "all_scored = rank_by_entropy(WORD_LIST, top_n=len(WORD_LIST))\n",
    "\n",
    "# Top 5 et analyse des lettres\n",
    "print(\"Top 5 des mots d'ouverture :\")\n",
    "freq_letters = set(\"EASIRNTUOL\")  # Lettres les plus frequentes en francais\n",
    "for word, h in all_scored[:5]:\n",
    "    letters = set(word)\n",
    "    overlap = letters & freq_letters\n",
    "    print(f\"  {word} (H={h:.3f}) : \"\n",
    "          f\"{len(overlap)}/5 lettres frequentes ({overlap})\")\n",
    "\n",
    "# Les meilleurs mots contiennent typiquement 4-5 lettres\n",
    "# parmi les plus frequentes en francais.\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3-intro",
   "metadata": {},
   "source": [
    "### Exercice 3 : Extension a 6 lettres\n",
    "\n",
    "**Tache** : Adaptez le systeme pour un Wordle a 6 lettres.\n",
    "- Modifiez `compute_feedback` pour accepter des mots de longueur variable\n",
    "- Creez une petite liste de mots de 6 lettres\n",
    "- Testez le solveur entropie sur cette liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3 : Wordle a 6 lettres\n",
    "\n",
    "# A COMPLETER\n",
    "# def compute_feedback_generic(guess: str, answer: str) -> Tuple[int, ...]:\n",
    "#     \"\"\"Version generique de compute_feedback pour toute longueur.\"\"\"\n",
    "#     n = len(guess)\n",
    "#     assert len(answer) == n, \"Les mots doivent avoir la meme longueur\"\n",
    "#     feedback = [0] * n\n",
    "#     answer_chars = list(answer)\n",
    "#     guess_chars = list(guess)\n",
    "#     # ... meme logique que compute_feedback\n",
    "#     return tuple(feedback)\n",
    "#\n",
    "# WORDS_6 = [\"FRANCE\", \"PARLER\", \"COMPTE\", ...]  # Liste de mots de 6 lettres\n",
    "# ... tester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution exercice 3</b></summary>\n",
    "\n",
    "```python\n",
    "def compute_feedback_generic(guess: str, answer: str) -> Tuple[int, ...]:\n",
    "    \"\"\"Feedback generique pour toute longueur de mot.\"\"\"\n",
    "    n = len(guess)\n",
    "    assert len(answer) == n\n",
    "    feedback = [0] * n\n",
    "    answer_chars = list(answer)\n",
    "    guess_chars = list(guess)\n",
    "\n",
    "    # Passe 1 : verts\n",
    "    for i in range(n):\n",
    "        if guess_chars[i] == answer_chars[i]:\n",
    "            feedback[i] = 2\n",
    "            answer_chars[i] = None\n",
    "            guess_chars[i] = None\n",
    "\n",
    "    # Passe 2 : jaunes\n",
    "    for i in range(n):\n",
    "        if guess_chars[i] is not None and guess_chars[i] in answer_chars:\n",
    "            feedback[i] = 1\n",
    "            answer_chars[answer_chars.index(guess_chars[i])] = None\n",
    "\n",
    "    return tuple(feedback)\n",
    "\n",
    "WORDS_6 = [\"FRANCE\", \"PARLER\", \"COMPTE\", \"SIMPLE\", \"NATURE\",\n",
    "           \"GROUPE\", \"LETTRE\", \"MARCHE\", \"NOMBRE\", \"REGLER\"]\n",
    "\n",
    "# Tester le feedback\n",
    "print(compute_feedback_generic(\"FRANCE\", \"NATURE\"))\n",
    "\n",
    "# Le solveur entropie fonctionne tel quel si on remplace compute_feedback.\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Recapitulatif\n",
    "\n",
    "### Concepts cles\n",
    "\n",
    "| Concept | Definition | Application Wordle |\n",
    "|---------|------------|-------------------|\n",
    "| **Filtrage** | Eliminer les candidats incompatibles | Garder les mots compatibles avec le feedback |\n",
    "| **CSP** | Variables + domaines + contraintes | Lettres possibles a chaque position |\n",
    "| **Entropie** | $H = -\\sum p \\log_2 p$ | Mesurer l'information apportee par un mot |\n",
    "| **Information gain** | Reduction d'incertitude | Divisier l'espace des candidats |\n",
    "\n",
    "### Comparaison des approches\n",
    "\n",
    "| Approche | Moy. tentatives | Complexite par tentative | Avantage |\n",
    "|----------|-----------------|--------------------------|----------|\n",
    "| Filtrage simple | ~4-5 | $O(n)$ | Simple et rapide |\n",
    "| CSP | ~4-5 | $O(n)$ | Structure explicite |\n",
    "| Entropie | ~3-4 | $O(n^2)$ | Optimal en information |\n",
    "\n",
    "### Ce qu'il faut retenir\n",
    "\n",
    "1. **Le filtrage est la base** : quel que soit le solveur, il faut eliminer les mots incompatibles\n",
    "2. **Le CSP structure le probleme** : les contraintes deduites du feedback sont une forme de propagation de contraintes\n",
    "3. **L'entropie optimise le choix** : maximiser l'information revient a minimiser le nombre de tentatives\n",
    "4. **Compromis information/calcul** : le meilleur choix demande plus de calcul mais converge plus vite\n",
    "\n",
    "### Lien avec d'autres domaines\n",
    "\n",
    "| Domaine | Parallele avec Wordle |\n",
    "|---------|----------------------|\n",
    "| Diagnostic medical | Choisir le test le plus informatif |\n",
    "| Arbres de decision | Critere de split = gain d'information |\n",
    "| Recherche binaire | Diviser l'espace en deux parties egales |\n",
    "| 20 questions | Poser la question la plus discriminante |\n",
    "\n",
    "### Pour aller plus loin\n",
    "\n",
    "- **Solveur non-hard** : autoriser des mots hors candidats pour maximiser l'information globale\n",
    "- **Minimax** : minimiser le **pire cas** au lieu de la moyenne (approche pessimiste)\n",
    "- **Precomputation** : arbre de decision precalcule pour toutes les parties possibles\n",
    "- Voir le projet original : jsboigeEpita/2025-PPC CSP-Wordle-Solver\n",
    "\n",
    "### References\n",
    "\n",
    "- Shannon, C.E. \"A Mathematical Theory of Communication\" (1948)\n",
    "- 3Blue1Brown, \"Solving Wordle using Information Theory\" (YouTube)\n",
    "- Russell, S. & Norvig, P., *Artificial Intelligence: A Modern Approach*, Chapitre 6 (CSP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nav-footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Navigation** : [<< App-6 Minesweeper](App-6-Minesweeper.ipynb) | [Index](../README.md) | [App-8 MiniZinc >>](App-8-MiniZinc.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}