#!/usr/bin/env python3
"""
Script de correction des doublons dans argumentum_fallacies_taxonomy.csv

PROBLÃˆME IDENTIFIÃ‰ :
- ValueError: Index has duplicate keys: Index([520, 1000], dtype='int64', name='PK')
- Le fichier CSV contient des clÃ©s primaires dupliquÃ©es

SOLUTION :
- Lecture du CSV
- DÃ©tection et suppression des doublons en gardant la premiÃ¨re occurrence
- Sauvegarde du fichier corrigÃ©

Auteur : Mission SDDD - Validation SymbolicAI Argument Analysis
Date : 2025-10-03
"""

import pandas as pd
import os
from pathlib import Path
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def fix_csv_duplicates():
    """
    Corrige les doublons dans le fichier taxonomie des sophismes
    """
    
    # Chemin vers le fichier CSV
    csv_path = Path("data/argumentum_fallacies_taxonomy.csv")
    backup_path = csv_path.with_suffix('.csv.backup')
    
    try:
        # VÃ©rification de l'existence du fichier
        if not csv_path.exists():
            logger.error(f"âŒ Fichier non trouvÃ© : {csv_path}")
            return False
            
        logger.info(f"ğŸ“‚ Lecture du fichier : {csv_path}")
        
        # Lecture du CSV sans dÃ©finir PK comme index pour Ã©viter l'erreur
        df = pd.read_csv(csv_path)
        
        logger.info(f"ğŸ“Š Fichier chargÃ© : {len(df)} lignes")
        logger.info(f"ğŸ“‹ Colonnes : {list(df.columns)}")
        
        # VÃ©rification si la colonne PK existe
        if 'PK' not in df.columns:
            logger.error("âŒ Colonne 'PK' non trouvÃ©e dans le CSV")
            return False
            
        # Analyse des doublons
        duplicated_pks = df[df['PK'].duplicated(keep=False)]['PK'].unique()
        logger.info(f"ğŸ” PK dupliquÃ©es trouvÃ©es : {duplicated_pks}")
        
        # Affichage dÃ©taillÃ© des doublons
        for pk in duplicated_pks:
            dup_rows = df[df['PK'] == pk]
            logger.info(f"  PK {pk} : {len(dup_rows)} occurrences")
            for idx, row in dup_rows.iterrows():
                logger.info(f"    Ligne {idx}: {row.to_dict()}")
        
        # Sauvegarde de sÃ©curitÃ©
        df.to_csv(backup_path, index=False)
        logger.info(f"ğŸ’¾ Sauvegarde crÃ©Ã©e : {backup_path}")
        
        # Suppression des doublons (garde la premiÃ¨re occurrence)
        df_clean = df.drop_duplicates(subset=['PK'], keep='first')
        
        removed_count = len(df) - len(df_clean)
        logger.info(f"ğŸ§¹ {removed_count} doublons supprimÃ©s")
        logger.info(f"ğŸ“Š Fichier nettoyÃ© : {len(df_clean)} lignes")
        
        # VÃ©rification finale - aucun doublon ne doit rester
        remaining_duplicates = df_clean['PK'].duplicated().sum()
        if remaining_duplicates > 0:
            logger.error(f"âŒ {remaining_duplicates} doublons restants aprÃ¨s nettoyage")
            return False
        
        # Sauvegarde du fichier corrigÃ©
        df_clean.to_csv(csv_path, index=False)
        logger.info(f"âœ… Fichier corrigÃ© sauvegardÃ© : {csv_path}")
        
        # Validation finale - test de lecture avec PK comme index
        try:
            df_test = pd.read_csv(csv_path, index_col='PK')
            logger.info("âœ… Validation : Le fichier peut maintenant Ãªtre lu avec PK comme index")
            return True
        except ValueError as e:
            logger.error(f"âŒ Validation Ã©chouÃ©e : {e}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Erreur lors du traitement : {e}")
        return False

def main():
    """Point d'entrÃ©e principal"""
    logger.info("ğŸš€ DÃ©marrage du script de correction des doublons CSV")
    logger.info("=" * 60)
    
    success = fix_csv_duplicates()
    
    logger.info("=" * 60)
    if success:
        logger.info("ğŸ‰ SUCCÃˆS : Correction des doublons terminÃ©e")
    else:
        logger.info("ğŸ’¥ Ã‰CHEC : Correction des doublons Ã©chouÃ©e")
    
    return success

if __name__ == "__main__":
    main()