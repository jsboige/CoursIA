{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-nav",
   "metadata": {},
   "source": [
    "# SW-13-GraphRAG\n\n**Navigation** : [<< 12-KnowledgeGraphs](SW-12-KnowledgeGraphs.ipynb) | [Index](README.md)\n\n## GraphRAG : Graphes de Connaissances et LLMs\n\n### Duree estimee : 50 minutes\n\n---\n\n## Objectifs d'apprentissage\n\nA la fin de ce notebook, vous saurez :\n1. Comprendre le concept de GraphRAG et pourquoi les graphes ameliorent le RAG classique\n2. Concevoir l'architecture d'un pipeline GraphRAG (extraction, construction, interrogation)\n3. Extraire des entites et relations d'un texte avec un LLM (OpenAI ou Anthropic)\n4. Construire un graphe de connaissances a partir d'entites extraites\n5. Utiliser un KG pour enrichir les prompts LLM (retrieval augmented generation)\n6. Connaitre l'approche Microsoft GraphRAG et la detection de communautes Leiden\n\n### Concepts cles\n\n| Concept | Description |\n|---------|-------------|\n| RAG | Retrieval-Augmented Generation : enrichir un LLM avec des documents recuperes |\n| GraphRAG | RAG augmente par un graphe de connaissances plutot que de simples chunks |\n| Extraction d'entites | Identification des entites et relations dans un texte non structure |\n| Community detection | Regroupement automatique de noeuds en communautes semantiques |\n| Grounding | Ancrage des reponses LLM dans des faits verifies du KG |\n\n### Prerequis\n- Python 3.10+\n- Notebook [SW-12-KnowledgeGraphs](SW-12-KnowledgeGraphs.ipynb) (construction de KG avec rdflib)\n- (Optionnel) Cle API OpenAI ou Anthropic (le notebook fonctionne sans grace aux donnees de demonstration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-title",
   "metadata": {},
   "source": [
    "## Installation des dependances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-pip",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q rdflib networkx matplotlib openai anthropic python-dotenv pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-title",
   "metadata": {},
   "source": [
    "---\n\n## 1. Introduction au GraphRAG\n\n### Le probleme du RAG classique\n\nLe **RAG** (Retrieval-Augmented Generation) est une technique qui enrichit les reponses d'un LLM en recuperant des documents pertinents depuis une base de connaissances. Le flux classique est :\n\n```\nQuestion -> Recherche vectorielle -> Documents pertinents -> LLM -> Reponse\n```\n\nCependant, le RAG classique a des limites importantes :\n\n| Limite du RAG classique | Consequence | Solution GraphRAG |\n|------------------------|-------------|-------------------|\n| Recherche par similarite uniquement | Rate les connexions indirectes | Parcours de graphe (chemins, voisinage) |\n| Pas de raisonnement multi-hop | \"Qui est le realisateur du film ou joue X ?\" echoue | Requetes SPARQL traversant les relations |\n| Fragments decontextualises | Les chunks perdent le contexte global | Le KG preserve les relations entre entites |\n| Pas de structure semantique | Tous les documents sont traites uniformement | Les entites ont des types, attributs, relations |\n| Hallucinations difficiles a detecter | Pas de source verifiable | Le KG fournit des faits verifiables (grounding) |\n\n### GraphRAG : le meilleur des deux mondes\n\nLe **GraphRAG** combine la puissance des LLMs avec la structure des graphes de connaissances :\n\n```\nQuestion -> Extraction entites -> Recherche dans le KG -> Contexte structure -> LLM -> Reponse fondee\n```\n\nL'idee centrale : au lieu de recuperer des chunks de texte brut, on recupere des **sous-graphes** autour des entites mentionnees dans la question. Cela fournit au LLM un contexte riche et structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-comparison",
   "metadata": {},
   "source": [
    "### RAG classique vs GraphRAG : comparaison visuelle\n\n```\nRAG CLASSIQUE                          GRAPHRAG\n=============                          ========\n\nQuestion: \"Quels films de Nolan       Question: \"Quels films de Nolan\navec DiCaprio ?\"                       avec DiCaprio ?\"\n\n1. Embed la question                   1. Extraire entites: Nolan, DiCaprio\n2. Chercher chunks similaires          2. Trouver dans le KG:\n3. Retourner les 5 meilleurs chunks       Nolan --directed--> Inception\n4. Le LLM synthetise                      DiCaprio --acted_in--> Inception\n                                          Inception --genre--> Sci-Fi\nProbleme: les chunks parlent de           ...\nNolan OU de DiCaprio, rarement         3. Sous-graphe pertinent envoye au LLM\ndes deux ensemble.                     4. Reponse fondee avec contexte complet\n```\n\n> **Point cle** : GraphRAG excelle sur les questions necessitant de croiser des informations provenant de sources differentes (raisonnement multi-hop)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-title",
   "metadata": {},
   "source": [
    "---\n\n## 2. Architecture d'un pipeline GraphRAG\n\n### Les quatre etapes\n\nUn pipeline GraphRAG complet se decompose en quatre etapes :\n\n| Etape | Description | Outils |\n|-------|-------------|--------|\n| 1. **Extraction** | Identifier les entites et relations dans les textes | LLM (GPT-4, Claude), spaCy, NER |\n| 2. **Construction** | Transformer les entites extraites en graphe RDF | rdflib, NetworkX |\n| 3. **Indexation** | Detecter des communautes, creer des resumes | Algorithme Leiden, LLM |\n| 4. **Interrogation** | Recuperer le sous-graphe pertinent et generer la reponse | SPARQL + LLM |\n\n```\n  Textes bruts                   Graphe de connaissances\n  +-----------+                  +---------+\n  | Document1 |--+               | Entite1 |---relation---| Entite2 |\n  +-----------+  |  Extraction   +---------+              +---------+\n  +-----------+  +----------->       |                        |\n  | Document2 |--+               relation                  relation\n  +-----------+  |               +---------+              +---------+\n  +-----------+  |               | Entite3 |              | Entite4 |\n  | Document3 |--+               +---------+              +---------+\n  +-----------+\n                                         |\n                                    Interrogation\n                                         |\n                                         v\n                                   +----------+\n                                   |   LLM    |<-- Sous-graphe comme contexte\n                                   +----------+\n                                         |\n                                         v\n                                   Reponse fondee\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-title",
   "metadata": {},
   "source": [
    "---\n\n## 3. Extraction d'entites et relations avec un LLM\n\n### Configuration des cles API\n\nLe notebook utilise les variables d'environnement pour les cles API. Si aucune cle n'est configuree, des **donnees de demonstration** sont utilisees automatiquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nfrom pathlib import Path\n\n# Charger les variables d'environnement depuis .env\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(\".env\")\n    if env_path.exists():\n        load_dotenv(env_path)\n        print(f\"Fichier .env charge depuis : {env_path.resolve()}\")\n    else:\n        print(\"Fichier .env non trouve. Utilisation des variables d'environnement systeme.\")\nexcept ImportError:\n    print(\"python-dotenv non installe. Utilisation des variables d'environnement systeme.\")\n\n# Verifier les cles API disponibles\nopenai_key = os.getenv(\"OPENAI_API_KEY\", \"\")\nanthropic_key = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n\nHAS_OPENAI = bool(openai_key and not openai_key.startswith(\"sk-...\"))\nHAS_ANTHROPIC = bool(anthropic_key and not anthropic_key.startswith(\"sk-ant-...\"))\nHAS_LLM = HAS_OPENAI or HAS_ANTHROPIC\n\nprint()\nprint(\"=== Configuration API ===\")\nprint(f\"  OpenAI    : {'Disponible' if HAS_OPENAI else 'Non configure'}\")\nprint(f\"  Anthropic : {'Disponible' if HAS_ANTHROPIC else 'Non configure'}\")\nprint()\nif HAS_LLM:\n    provider = \"OpenAI\" if HAS_OPENAI else \"Anthropic\"\n    print(f\"Mode : API {provider} (extraction reelle)\")\nelse:\n    print(\"Mode : Demonstration (donnees pre-calculees)\")\n    print(\"  Pour activer l'extraction reelle, copiez .env.example vers .env\")\n    print(\"  et renseignez votre cle API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation-api",
   "metadata": {},
   "source": [
    "### Interpretation\n\nLe notebook detecte automatiquement les cles API disponibles et adapte son comportement :\n\n| Mode | Condition | Comportement |\n|------|-----------|-------------|\n| **API OpenAI** | `OPENAI_API_KEY` configure | Extraction reelle avec GPT |\n| **API Anthropic** | `ANTHROPIC_API_KEY` configure | Extraction reelle avec Claude |\n| **Demonstration** | Aucune cle | Donnees pre-calculees, meme flux pedagogique |\n\n> **Point cle** : L'ensemble du notebook fonctionne sans cle API. Les resultats de demonstration sont representatifs de ce que produit un vrai LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-text",
   "metadata": {},
   "source": [
    "### Texte source pour l'extraction\n\nNous allons extraire des entites et relations a partir d'un texte sur le cinema. Ce texte contient des informations sur des films, des realisateurs et des acteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texte source pour l'extraction d'entites\nsample_text = \"\"\"\nChristopher Nolan est un realisateur britannique ne en 1970 a Londres. Il est connu pour\nses films a la narration complexe. Inception, sorti en 2010, met en vedette Leonardo DiCaprio\ndans le role de Dom Cobb, un voleur specialise dans l'extraction d'informations par le reve.\nLe film a ete produit par Warner Bros et a remporte quatre Oscars.\n\nNolan a egalement realise Interstellar en 2014, avec Matthew McConaughey et Anne Hathaway.\nCe film de science-fiction explore les themes du voyage interstellaire et de la relativite.\nHans Zimmer a compose la bande originale d'Interstellar, qui a ete saluee par la critique.\n\nThe Dark Knight, sorti en 2008, est considere comme l'un des meilleurs films de super-heros.\nHeath Ledger y incarne le Joker dans une performance legendaire qui lui a valu un Oscar\nposthume. Christian Bale joue le role de Batman dans cette trilogie realisee par Nolan.\n\"\"\"\n\nprint(\"Texte source :\")\nprint(f\"  Longueur : {len(sample_text)} caracteres\")\nprint(f\"  Mots     : {len(sample_text.split())} mots\")\nprint()\nprint(sample_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-extraction",
   "metadata": {},
   "source": [
    "### Extraction d'entites avec un LLM\n\nL'extraction d'entites consiste a demander au LLM d'identifier les entites nommees (personnes, films, organisations) et les relations entre elles dans un texte non structure.\n\nLe prompt est structure pour obtenir une sortie JSON exploitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraction-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n\n# Prompt d'extraction d'entites et relations\nEXTRACTION_PROMPT = \"\"\"Analyse le texte suivant et extrais les entites et relations sous forme JSON.\n\nPour chaque entite, identifie :\n- name : le nom de l'entite\n- type : Person, Movie, Organization, Award, Genre, ou Concept\n- attributes : dictionnaire d'attributs (annee, nationalite, role, etc.)\n\nPour chaque relation, identifie :\n- source : nom de l'entite source\n- relation : type de relation (directed, acted_in, produced_by, won, composed_for, genre_of)\n- target : nom de l'entite cible\n\nReponds UNIQUEMENT avec du JSON valide au format :\n{\n  \"entities\": [...],\n  \"relations\": [...]\n}\n\nTexte :\n\"\"\"\n\n\ndef extract_entities_llm(text):\n    \"\"\"Extraire les entites et relations via API LLM.\"\"\"\n    if HAS_OPENAI:\n        try:\n            from openai import OpenAI\n            client = OpenAI()\n            response = client.chat.completions.create(\n                model=\"gpt-4o-mini\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"Tu es un expert en extraction d'entites. Reponds uniquement en JSON.\"},\n                    {\"role\": \"user\", \"content\": EXTRACTION_PROMPT + text}\n                ],\n                temperature=0.0\n            )\n            return json.loads(response.choices[0].message.content)\n        except Exception as e:\n            print(f\"Erreur OpenAI : {e}\")\n            return None\n\n    elif HAS_ANTHROPIC:\n        try:\n            import anthropic\n            client = anthropic.Anthropic()\n            response = client.messages.create(\n                model=\"claude-sonnet-4-20250514\",\n                max_tokens=2000,\n                messages=[\n                    {\"role\": \"user\", \"content\": EXTRACTION_PROMPT + text}\n                ]\n            )\n            content = response.content[0].text\n            start = content.find(\"{\")\n            end = content.rfind(\"}\") + 1\n            return json.loads(content[start:end])\n        except Exception as e:\n            print(f\"Erreur Anthropic : {e}\")\n            return None\n\n    return None\n\n\n# Donnees de demonstration (resultat pre-calcule representatif)\nDEMO_EXTRACTION = {\n    \"entities\": [\n        {\"name\": \"Christopher Nolan\", \"type\": \"Person\", \"attributes\": {\"nationality\": \"britannique\", \"birth_year\": 1970, \"birth_place\": \"Londres\", \"role\": \"realisateur\"}},\n        {\"name\": \"Leonardo DiCaprio\", \"type\": \"Person\", \"attributes\": {\"role\": \"acteur\"}},\n        {\"name\": \"Matthew McConaughey\", \"type\": \"Person\", \"attributes\": {\"role\": \"acteur\"}},\n        {\"name\": \"Anne Hathaway\", \"type\": \"Person\", \"attributes\": {\"role\": \"actrice\"}},\n        {\"name\": \"Heath Ledger\", \"type\": \"Person\", \"attributes\": {\"role\": \"acteur\"}},\n        {\"name\": \"Christian Bale\", \"type\": \"Person\", \"attributes\": {\"role\": \"acteur\"}},\n        {\"name\": \"Hans Zimmer\", \"type\": \"Person\", \"attributes\": {\"role\": \"compositeur\"}},\n        {\"name\": \"Inception\", \"type\": \"Movie\", \"attributes\": {\"year\": 2010}},\n        {\"name\": \"Interstellar\", \"type\": \"Movie\", \"attributes\": {\"year\": 2014, \"genre\": \"science-fiction\"}},\n        {\"name\": \"The Dark Knight\", \"type\": \"Movie\", \"attributes\": {\"year\": 2008, \"genre\": \"super-heros\"}},\n        {\"name\": \"Warner Bros\", \"type\": \"Organization\", \"attributes\": {\"type\": \"studio\"}},\n        {\"name\": \"Oscar\", \"type\": \"Award\", \"attributes\": {}},\n        {\"name\": \"Dom Cobb\", \"type\": \"Person\", \"attributes\": {\"role\": \"personnage\"}},\n        {\"name\": \"Joker\", \"type\": \"Person\", \"attributes\": {\"role\": \"personnage\"}},\n        {\"name\": \"Batman\", \"type\": \"Person\", \"attributes\": {\"role\": \"personnage\"}}\n    ],\n    \"relations\": [\n        {\"source\": \"Christopher Nolan\", \"relation\": \"directed\", \"target\": \"Inception\"},\n        {\"source\": \"Christopher Nolan\", \"relation\": \"directed\", \"target\": \"Interstellar\"},\n        {\"source\": \"Christopher Nolan\", \"relation\": \"directed\", \"target\": \"The Dark Knight\"},\n        {\"source\": \"Leonardo DiCaprio\", \"relation\": \"acted_in\", \"target\": \"Inception\"},\n        {\"source\": \"Matthew McConaughey\", \"relation\": \"acted_in\", \"target\": \"Interstellar\"},\n        {\"source\": \"Anne Hathaway\", \"relation\": \"acted_in\", \"target\": \"Interstellar\"},\n        {\"source\": \"Heath Ledger\", \"relation\": \"acted_in\", \"target\": \"The Dark Knight\"},\n        {\"source\": \"Christian Bale\", \"relation\": \"acted_in\", \"target\": \"The Dark Knight\"},\n        {\"source\": \"Hans Zimmer\", \"relation\": \"composed_for\", \"target\": \"Interstellar\"},\n        {\"source\": \"Inception\", \"relation\": \"produced_by\", \"target\": \"Warner Bros\"},\n        {\"source\": \"Inception\", \"relation\": \"won\", \"target\": \"Oscar\"},\n        {\"source\": \"Heath Ledger\", \"relation\": \"won\", \"target\": \"Oscar\"}\n    ]\n}\n\n\n# Tenter l'extraction reelle, sinon utiliser les donnees de demonstration\nextraction_result = None\nif HAS_LLM:\n    print(\"Extraction via API LLM en cours...\")\n    extraction_result = extract_entities_llm(sample_text)\n\nif extraction_result is None:\n    print(\"Utilisation des donnees de demonstration pre-calculees.\")\n    extraction_result = DEMO_EXTRACTION\n\n# Afficher les resultats\nprint(f\"\"\"\n=== Resultats de l'extraction ===\n  Entites  : {len(extraction_result['entities'])}\n  Relations : {len(extraction_result['relations'])}\n\"\"\")\n\nprint(\"--- Entites ---\")\nfor e in extraction_result[\"entities\"]:\n    attrs = \", \".join(f\"{k}={v}\" for k, v in e.get(\"attributes\", {}).items())\n    print(f\"  [{e['type']:<15}] {e['name']:<25} {attrs}\")\n\nprint()\nprint(\"--- Relations ---\")\nfor r in extraction_result[\"relations\"]:\n    print(f\"  {r['source']:<25} --{r['relation']:<15}--> {r['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation-extraction",
   "metadata": {},
   "source": [
    "### Interpretation\n\nL'extraction a identifie environ 15 entites et 12 relations a partir du texte source.\n\n| Type d'entite | Nombre | Exemples |\n|---------------|--------|----------|\n| Person | ~8 | Nolan, DiCaprio, Ledger, Zimmer |\n| Movie | 3 | Inception, Interstellar, The Dark Knight |\n| Organization | 1 | Warner Bros |\n| Award | 1 | Oscar |\n| Personnage | 3 | Dom Cobb, Joker, Batman |\n\n**Qualite de l'extraction** :\n- Les entites principales sont correctement identifiees\n- Les relations refletent bien le contenu du texte\n- Certaines relations implicites (comme le genre des films) sont inferees\n\n> **Point cle** : La qualite de l'extraction depend fortement du prompt et du modele utilise. GPT-4 et Claude produisent generalement des resultats superieurs a GPT-3.5 pour cette tache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-title",
   "metadata": {},
   "source": [
    "---\n\n## 4. Construction du graphe de connaissances\n\n### Transformation des entites extraites en triplets RDF\n\nNous allons transformer les entites et relations extraites en un graphe RDF interrogeable avec SPARQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-kg",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Namespace, Literal, URIRef, BNode\nfrom rdflib.namespace import RDF, RDFS, XSD\nimport re\n\n# Namespaces\nEX = Namespace(\"http://example.org/graphrag/\")\nSCHEMA = Namespace(\"http://schema.org/\")\nREL = Namespace(\"http://example.org/relation/\")\n\ndef slugify(name):\n    \"\"\"Convertir un nom en identifiant URI valide.\"\"\"\n    slug = re.sub(r'[^a-zA-Z0-9]+', '_', name.strip())\n    return slug.strip('_')\n\n# Creer le graphe RDF\ng = Graph()\ng.bind(\"ex\", EX)\ng.bind(\"schema\", SCHEMA)\ng.bind(\"rel\", REL)\n\n# Mapping des types vers des classes\nTYPE_MAP = {\n    \"Person\": SCHEMA.Person,\n    \"Movie\": SCHEMA.Movie,\n    \"Organization\": SCHEMA.Organization,\n    \"Award\": EX.Award,\n    \"Genre\": EX.Genre,\n    \"Concept\": EX.Concept,\n}\n\n# Mapping des relations vers des proprietes\nREL_MAP = {\n    \"directed\": REL.directed,\n    \"acted_in\": REL.actedIn,\n    \"produced_by\": REL.producedBy,\n    \"won\": REL.won,\n    \"composed_for\": REL.composedFor,\n    \"genre_of\": REL.genreOf,\n}\n\n# Ajouter les entites\nentity_uris = {}\nfor entity in extraction_result[\"entities\"]:\n    uri = EX[slugify(entity[\"name\"])]\n    entity_uris[entity[\"name\"]] = uri\n\n    # Type de l'entite\n    entity_type = TYPE_MAP.get(entity[\"type\"], EX.Entity)\n    g.add((uri, RDF.type, entity_type))\n    g.add((uri, RDFS.label, Literal(entity[\"name\"])))\n\n    # Attributs\n    for attr_name, attr_value in entity.get(\"attributes\", {}).items():\n        attr_prop = EX[attr_name]\n        if isinstance(attr_value, int):\n            g.add((uri, attr_prop, Literal(attr_value, datatype=XSD.integer)))\n        else:\n            g.add((uri, attr_prop, Literal(str(attr_value))))\n\n# Ajouter les relations\nfor rel in extraction_result[\"relations\"]:\n    source_uri = entity_uris.get(rel[\"source\"])\n    target_uri = entity_uris.get(rel[\"target\"])\n    rel_prop = REL_MAP.get(rel[\"relation\"], EX[rel[\"relation\"]])\n\n    if source_uri and target_uri:\n        g.add((source_uri, rel_prop, target_uri))\n\nprint(f\"Graphe de connaissances construit :\")\nprint(f\"  Triplets  : {len(g)}\")\nprint(f\"  Entites   : {len(entity_uris)}\")\nprint(f\"  Relations : {len(extraction_result['relations'])}\")\nprint()\n\n# Serialiser un extrait en Turtle\nturtle_output = g.serialize(format=\"turtle\")\nlines = turtle_output.split(\"\\n\")\nprint(f\"--- Extrait Turtle ({min(40, len(lines))} premieres lignes) ---\")\nfor line in lines[:40]:\n    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation-build-kg",
   "metadata": {},
   "source": [
    "### Interpretation\n\nLe graphe RDF contient toutes les entites et relations extraites du texte. La structure est riche :\n\n| Metrique | Valeur |\n|----------|--------|\n| Triplets totaux | ~60-70 |\n| Entites (noeuds) | ~15 |\n| Relations (aretes) | ~12 |\n| Types d'entites | 5 (Person, Movie, Organization, Award, Genre) |\n| Types de relations | 6 (directed, acted_in, produced_by, won, composed_for, genre_of) |\n\n> **Point cle** : Le passage du texte brut au graphe RDF structure l'information de maniere interrogeable. Chaque fait est maintenant un triplet verifiable et navigable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-viz-title",
   "metadata": {},
   "source": [
    "### Visualisation du graphe extrait\n\nVisualisons le graphe de connaissances construit a partir des entites extraites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-kg",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Construire le graphe NetworkX\nG_nx = nx.DiGraph()\n\n# Couleurs par type d'entite\ncolor_map = {\n    \"Person\": \"#FF6B6B\",\n    \"Movie\": \"#4ECDC4\",\n    \"Organization\": \"#45B7D1\",\n    \"Award\": \"#FFE66D\",\n    \"Genre\": \"#96E6A1\",\n    \"Concept\": \"#DDA0DD\",\n}\n\nnode_colors = {}\nnode_sizes = {}\n\n# Ajouter les entites comme noeuds\nfor entity in extraction_result[\"entities\"]:\n    name = entity[\"name\"]\n    etype = entity[\"type\"]\n    G_nx.add_node(name, entity_type=etype)\n    node_colors[name] = color_map.get(etype, \"#CCCCCC\")\n    node_sizes[name] = 900 if etype == \"Movie\" else 600\n\n# Ajouter les relations comme aretes\nfor rel in extraction_result[\"relations\"]:\n    if rel[\"source\"] in G_nx.nodes() and rel[\"target\"] in G_nx.nodes():\n        G_nx.add_edge(rel[\"source\"], rel[\"target\"], relation=rel[\"relation\"])\n\n# Visualiser\nfig, ax = plt.subplots(1, 1, figsize=(16, 11))\n\npos = nx.spring_layout(G_nx, k=2.5, iterations=80, seed=42)\n\n# Dessiner les noeuds par type\nfor etype, color in color_map.items():\n    nodes = [n for n in G_nx.nodes() if G_nx.nodes[n].get(\"entity_type\") == etype]\n    if nodes:\n        sizes = [node_sizes.get(n, 600) for n in nodes]\n        nx.draw_networkx_nodes(G_nx, pos, nodelist=nodes, node_color=color,\n                              node_size=sizes, alpha=0.9, ax=ax)\n\n# Dessiner les aretes\nnx.draw_networkx_edges(G_nx, pos, alpha=0.4, arrows=True, arrowsize=15,\n                       connectionstyle=\"arc3,rad=0.1\", ax=ax)\n\n# Labels des noeuds\nnx.draw_networkx_labels(G_nx, pos, font_size=7, font_weight=\"bold\", ax=ax)\n\n# Labels des aretes (relations)\nedge_labels = {(u, v): d[\"relation\"] for u, v, d in G_nx.edges(data=True)}\nnx.draw_networkx_edge_labels(G_nx, pos, edge_labels=edge_labels,\n                             font_size=6, font_color=\"#666666\", ax=ax)\n\n# Legende\nlegend_items = [mpatches.Patch(color=c, label=t) for t, c in color_map.items()\n                if any(G_nx.nodes[n].get(\"entity_type\") == t for n in G_nx.nodes())]\nax.legend(handles=legend_items, loc=\"upper left\", fontsize=9)\n\nax.set_title(\"Graphe de connaissances extrait par LLM - Filmographie Nolan\",\n             fontsize=13, fontweight=\"bold\")\nax.axis(\"off\")\nplt.tight_layout()\nplt.show()\nplt.close()\n\nprint(f\"Noeuds : {G_nx.number_of_nodes()} | Aretes : {G_nx.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation-viz-kg",
   "metadata": {},
   "source": [
    "### Interpretation\n\nLa visualisation montre clairement la structure du graphe extrait :\n\n- **Christopher Nolan** est le noeud central, connecte aux trois films\n- Les **films** (turquoise) servent de hubs connectant realisateur, acteurs et prix\n- **Hans Zimmer** est connecte uniquement a Interstellar (bande originale)\n- **Heath Ledger** a une double connexion : vers The Dark Knight (acted_in) et vers Oscar (won)\n\n**Observations structurelles** :\n\n| Observation | Signification pour le GraphRAG |\n|-------------|-------------------------------|\n| Nolan est un hub central | Question sur Nolan -> riche contexte disponible |\n| Deux chemins vers Oscar | DiCaprio via Inception, Ledger via The Dark Knight |\n| Warner Bros connecte a Inception | Information de production recuperable |\n\n> **Avantage GraphRAG** : En RAG classique, ces connexions seraient perdues. Le graphe les preserve et les rend interrogeables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-title",
   "metadata": {},
   "source": [
    "---\n\n## 5. Interrogation augmentee par le graphe\n\n### Principe du GraphRAG querying\n\nL'interrogation GraphRAG suit un processus en trois etapes :\n1. **Analyse de la question** : identifier les entites mentionnees\n2. **Recuperation du sous-graphe** : extraire le voisinage de ces entites dans le KG\n3. **Generation augmentee** : envoyer le sous-graphe comme contexte au LLM\n\nCe processus permet de fournir au LLM un contexte factuel et structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subgraph-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subgraph(graph, entity_names, max_hops=2):\n    \"\"\"\n    Extraire un sous-graphe autour des entites mentionnees.\n\n    Args:\n        graph: graphe rdflib\n        entity_names: liste de noms d'entites\n        max_hops: profondeur maximale de traversee\n\n    Returns:\n        str: sous-graphe serialise en texte lisible\n    \"\"\"\n    relevant_triples = []\n    visited_entities = set()\n\n    # Trouver les URIs des entites mentionnees\n    entity_uris_found = []\n    for name in entity_names:\n        for s, p, o in graph.triples((None, RDFS.label, Literal(name))):\n            entity_uris_found.append(s)\n\n    # BFS pour explorer le voisinage\n    current_level = set(entity_uris_found)\n    for hop in range(max_hops):\n        next_level = set()\n        for entity_uri in current_level:\n            if entity_uri in visited_entities:\n                continue\n            visited_entities.add(entity_uri)\n\n            # Triplets ou l'entite est sujet\n            for s, p, o in graph.triples((entity_uri, None, None)):\n                if p != RDF.type or hop == 0:\n                    relevant_triples.append((s, p, o))\n                    if isinstance(o, URIRef) and o not in visited_entities:\n                        next_level.add(o)\n\n            # Triplets ou l'entite est objet\n            for s, p, o in graph.triples((None, None, entity_uri)):\n                relevant_triples.append((s, p, o))\n                if isinstance(s, URIRef) and s not in visited_entities:\n                    next_level.add(s)\n\n        current_level = next_level\n\n    # Formater en texte lisible\n    lines = []\n    seen = set()\n    for s, p, o in relevant_triples:\n        s_label = str(graph.value(s, RDFS.label) or s).split(\"/\")[-1]\n        p_label = str(p).split(\"/\")[-1]\n        if isinstance(o, Literal):\n            o_label = str(o)\n        else:\n            o_label = str(graph.value(o, RDFS.label) or o).split(\"/\")[-1]\n\n        triple_str = f\"{s_label} -- {p_label} --> {o_label}\"\n        if triple_str not in seen:\n            seen.add(triple_str)\n            lines.append(triple_str)\n\n    return \"\\n\".join(sorted(lines))\n\n\n# Exemple : question sur Nolan et DiCaprio\nquestion_entities = [\"Christopher Nolan\", \"Leonardo DiCaprio\"]\nsubgraph_text = extract_subgraph(g, question_entities, max_hops=1)\n\nprint(f\"=== Sous-graphe pour la question sur {', '.join(question_entities)} ===\")\nprint(f\"  (profondeur : 1 hop)\")\nprint()\nprint(subgraph_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation-subgraph",
   "metadata": {},
   "source": [
    "### Interpretation\n\nLe sous-graphe extrait contient toutes les informations pertinentes autour de Nolan et DiCaprio :\n\n- Le lien direct entre eux : Nolan a realise Inception, DiCaprio y a joue\n- Les attributs de chaque entite : nationalite, annee de naissance, etc.\n- Les relations avec d'autres entites : autres films de Nolan, production Warner Bros\n\nCe sous-graphe sera fourni comme contexte au LLM dans l'etape suivante.\n\n> **Avantage** : Le LLM recoit des faits structures et verifiables, pas des fragments de texte decontextualises. Cela reduit les hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-generation",
   "metadata": {},
   "source": [
    "### Generation de reponse augmentee par le graphe\n\nNous allons maintenant combiner le sous-graphe avec le LLM pour generer une reponse fondee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "augmented-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphrag_query(question, graph, max_hops=1):\n    \"\"\"\n    Pipeline GraphRAG complet : question -> extraction entites -> sous-graphe -> LLM -> reponse.\n    \"\"\"\n    # Etape 1 : Identifier les entites de la question (heuristique simple)\n    all_entity_names = [str(graph.value(s, RDFS.label))\n                        for s in set(graph.subjects(RDF.type, None))\n                        if graph.value(s, RDFS.label)]\n\n    mentioned_entities = [name for name in all_entity_names\n                         if name.lower() in question.lower()]\n\n    if not mentioned_entities:\n        # Fallback : chercher des correspondances partielles\n        question_words = set(question.lower().split())\n        for name in all_entity_names:\n            name_words = set(name.lower().split())\n            if name_words & question_words:\n                mentioned_entities.append(name)\n\n    # Etape 2 : Extraire le sous-graphe\n    context = extract_subgraph(graph, mentioned_entities, max_hops=max_hops)\n\n    # Etape 3 : Construire le prompt augmente\n    augmented_prompt = f\"\"\"En te basant UNIQUEMENT sur les faits suivants extraits d'un graphe de connaissances,\nreponds a la question. Si l'information n'est pas dans le graphe, dis-le explicitement.\n\n=== Faits du graphe de connaissances ===\n{context}\n\n=== Question ===\n{question}\n\n=== Reponse ===\n\"\"\"\n\n    # Etape 4 : Appeler le LLM (ou simuler)\n    answer = None\n    if HAS_LLM:\n        try:\n            if HAS_OPENAI:\n                from openai import OpenAI\n                client = OpenAI()\n                response = client.chat.completions.create(\n                    model=\"gpt-4o-mini\",\n                    messages=[{\"role\": \"user\", \"content\": augmented_prompt}],\n                    temperature=0.0\n                )\n                answer = response.choices[0].message.content\n            else:\n                import anthropic\n                client = anthropic.Anthropic()\n                response = client.messages.create(\n                    model=\"claude-sonnet-4-20250514\",\n                    max_tokens=500,\n                    messages=[{\"role\": \"user\", \"content\": augmented_prompt}]\n                )\n                answer = response.content[0].text\n        except Exception as e:\n            print(f\"Erreur API : {e}\")\n\n    return {\n        \"question\": question,\n        \"entities_found\": mentioned_entities,\n        \"context_lines\": len(context.split(\"\\n\")),\n        \"context\": context,\n        \"answer\": answer,\n        \"prompt\": augmented_prompt,\n    }\n\n\n# Poser une question\nquestion = \"Quels films Christopher Nolan a-t-il realises, et quels acteurs y jouent ?\"\nresult = graphrag_query(question, g)\n\nprint(f\"Question : {result['question']}\")\nprint(f\"Entites detectees : {result['entities_found']}\")\nprint(f\"Faits recuperes : {result['context_lines']} lignes\")\nprint()\n\nif result[\"answer\"]:\n    print(\"=== Reponse du LLM (augmentee par le graphe) ===\")\n    print(result[\"answer\"])\nelse:\n    print(\"=== Reponse simulee (mode demonstration) ===\")\n    print(\"D'apres le graphe de connaissances, Christopher Nolan a realise trois films :\")\n    print()\n    print(\"1. **Inception** (2010) - avec Leonardo DiCaprio\")\n    print(\"   - Produit par Warner Bros, a remporte un Oscar\")\n    print()\n    print(\"2. **Interstellar** (2014) - avec Matthew McConaughey et Anne Hathaway\")\n    print(\"   - Film de science-fiction, bande originale de Hans Zimmer\")\n    print()\n    print(\"3. **The Dark Knight** (2008) - avec Heath Ledger et Christian Bale\")\n    print(\"   - Film de super-heros, Heath Ledger a remporte un Oscar posthume\")\n    print()\n    print(\"(Reponse generee a partir des faits du graphe de connaissances)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation-generation",
   "metadata": {},
   "source": [
    "### Interpretation\n\nLe pipeline GraphRAG a produit une reponse fondee sur les faits du graphe :\n\n| Etape | Resultat |\n|-------|---------|\n| Detection d'entites | Christopher Nolan identifie dans la question |\n| Extraction sous-graphe | ~15-20 faits pertinents recuperes |\n| Generation augmentee | Reponse structuree avec les 3 films et leurs acteurs |\n\n**Comparaison avec une reponse sans GraphRAG** :\n\n| Aspect | LLM seul | LLM + GraphRAG |\n|--------|----------|----------------|\n| Source des faits | Memoire parametrique (peut halluciner) | Graphe de connaissances (verifiable) |\n| Reponse a jour | Limitee au training cutoff | Aussi a jour que le KG |\n| Justification | Aucune | Chaque fait tracable dans le graphe |\n| Questions multi-hop | Approximatif | Precis (traversee du graphe) |\n\n> **Point cle** : GraphRAG ne remplace pas le LLM, il le **renforce** avec des faits verifies. Le LLM conserve sa capacite de synthese et de formulation naturelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-multi",
   "metadata": {},
   "source": [
    "### Exemples de questions supplementaires\n\nTestons le pipeline avec differents types de questions pour evaluer sa polyvalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble de questions de test\ntest_questions = [\n    \"Qui a compose la musique d'Interstellar ?\",\n    \"Quels acteurs ont joue dans The Dark Knight ?\",\n    \"Quel studio a produit Inception ?\",\n    \"Heath Ledger a-t-il remporte un prix ?\",\n]\n\n# Reponses de demonstration (utilisees si pas d'API)\ndemo_answers = {\n    \"Qui a compose la musique d'Interstellar ?\":\n        \"Hans Zimmer a compose la bande originale d'Interstellar (2014), film realise par Christopher Nolan.\",\n    \"Quels acteurs ont joue dans The Dark Knight ?\":\n        \"D'apres le graphe, Heath Ledger et Christian Bale ont joue dans The Dark Knight (2008).\",\n    \"Quel studio a produit Inception ?\":\n        \"Inception a ete produit par Warner Bros.\",\n    \"Heath Ledger a-t-il remporte un prix ?\":\n        \"Oui, Heath Ledger a remporte un Oscar pour son role dans The Dark Knight.\",\n}\n\nprint(\"=== Test du pipeline GraphRAG sur plusieurs questions ===\")\nprint()\n\nfor i, question in enumerate(test_questions, 1):\n    result = graphrag_query(question, g)\n\n    print(f\"--- Question {i} ---\")\n    print(f\"Q: {question}\")\n    print(f\"Entites : {result['entities_found']}\")\n    print(f\"Contexte : {result['context_lines']} faits\")\n\n    if result[\"answer\"]:\n        print(f\"R: {result['answer']}\")\n    else:\n        print(f\"R: {demo_answers.get(question, '(pas de reponse de demonstration)')}\")\n    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation-multi",
   "metadata": {},
   "source": [
    "### Interpretation\n\nLes resultats montrent que le pipeline GraphRAG repond correctement aux differents types de questions :\n\n| Question | Type | Entites detectees | Qualite |\n|----------|------|-------------------|---------|\n| Compositeur d'Interstellar | 1-hop | Interstellar | Precise (Hans Zimmer) |\n| Acteurs de The Dark Knight | 1-hop | The Dark Knight | Complete (Ledger + Bale) |\n| Producteur d'Inception | 1-hop | Inception | Precise (Warner Bros) |\n| Prix de Ledger | 1-hop | Heath Ledger | Precise (Oscar) |\n\n**Limites observees** :\n- La detection d'entites est basee sur une heuristique simple (correspondance de noms)\n- Les questions indirectes (\"le realisateur du film avec DiCaprio\") necessitent un raisonnement multi-hop\n- En production, on utiliserait un NER avance ou un LLM pour l'etape d'extraction des entites de la question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-title",
   "metadata": {},
   "source": [
    "---\n\n## 6. Approche Microsoft GraphRAG\n\n### Presentation\n\n**Microsoft GraphRAG** est un framework publie en 2024 qui pousse le concept plus loin. Au lieu de simplement recuperer des sous-graphes locaux, il pre-traite l'ensemble du corpus pour creer une **hierarchie de communautes** avec des resumes a chaque niveau.\n\n### Architecture Microsoft GraphRAG\n\n```\nCorpus de textes\n       |\n       v\n1. Extraction d'entites et relations (LLM)\n       |\n       v\n2. Construction du graphe global\n       |\n       v\n3. Detection de communautes (algorithme Leiden)\n       |\n       v\n4. Generation de resumes par communaute (LLM)\n       |\n       v\n5. Indexation hierarchique\n       |\n       v\nPret pour l'interrogation\n```\n\n### Deux modes d'interrogation\n\n| Mode | Description | Quand l'utiliser |\n|------|-------------|-----------------|\n| **Local search** | Recupere le voisinage d'entites specifiques | Questions factuelles precises |\n| **Global search** | Parcourt les resumes de communautes haut niveau | Questions thematiques ou de synthese |\n\n### L'algorithme Leiden\n\nL'algorithme **Leiden** (developpe a l'universite de Leiden, Pays-Bas) est un algorithme de **detection de communautes** dans un graphe. Il identifie des groupes de noeuds fortement interconnectes.\n\n| Aspect | Detail |\n|--------|--------|\n| Principe | Optimisation de la modularite (densification intra-communaute) |\n| Avantage vs Louvain | Garantit la connexite des communautes |\n| Complexite | O(n log n) en pratique |\n| Usage dans GraphRAG | Regrouper les entites en themes pour la summarization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leiden-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom networkx.algorithms.community import greedy_modularity_communities\n\n# Convertir en graphe non-dirige pour la detection de communautes\nG_undirected = G_nx.to_undirected()\n\n# Detection de communautes par modularite\ncommunities = list(greedy_modularity_communities(G_undirected))\n\nprint(f\"=== Detection de communautes (modularite) ===\")\nprint(f\"Nombre de communautes detectees : {len(communities)}\")\nprint()\n\n# Attribuer des couleurs aux communautes\ncommunity_colors = [\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\", \"#FFE66D\", \"#96E6A1\", \"#DDA0DD\"]\nnode_community_color = {}\n\nfor i, community in enumerate(communities):\n    color = community_colors[i % len(community_colors)]\n    members = sorted(community)\n    print(f\"Communaute {i+1} ({len(members)} membres) :\")\n    for member in members:\n        etype = G_nx.nodes[member].get('entity_type', '?') if member in G_nx.nodes else '?'\n        print(f\"  - {member} ({etype})\")\n        node_community_color[member] = color\n    print()\n\n# Visualiser les communautes\nfig, ax = plt.subplots(1, 1, figsize=(14, 10))\n\npos = nx.spring_layout(G_undirected, k=2.5, iterations=80, seed=42)\n\ncolors = [node_community_color.get(n, \"#CCCCCC\") for n in G_undirected.nodes()]\nnx.draw_networkx_nodes(G_undirected, pos, node_color=colors, node_size=700, alpha=0.9, ax=ax)\nnx.draw_networkx_edges(G_undirected, pos, alpha=0.3, ax=ax)\nnx.draw_networkx_labels(G_undirected, pos, font_size=7, font_weight=\"bold\", ax=ax)\n\n# Legende des communautes\nlegend_items = [mpatches.Patch(color=community_colors[i], label=f\"Communaute {i+1}\")\n                for i in range(len(communities))]\nax.legend(handles=legend_items, loc=\"upper left\", fontsize=9)\n\nax.set_title(\"Detection de communautes dans le graphe de connaissances\",\n             fontsize=13, fontweight=\"bold\")\nax.axis(\"off\")\nplt.tight_layout()\nplt.show()\nplt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation-leiden",
   "metadata": {},
   "source": [
    "### Interpretation\n\nL'algorithme de detection de communautes a regroupe les noeuds en clusters semantiques :\n\n- Les films et leurs acteurs directs forment des communautes coherentes\n- Le realisateur (Nolan) peut apparaitre dans une communaute \"hub\" ou relier les clusters\n- Les entites peripheriques (Oscar, Warner Bros) sont regroupees selon leur voisinage\n\n**Application dans Microsoft GraphRAG** :\n\n| Niveau | Contenu | Usage |\n|--------|---------|-------|\n| Communaute locale | Entites + relations directes | Questions factuelles |\n| Communaute intermediaire | Themes (filmographie Nolan, acteurs) | Questions thematiques |\n| Communaute globale | Resume de l'ensemble du corpus | Questions de synthese |\n\n> **Point cle** : La detection de communautes permet de generer des resumes a differents niveaux de granularite. Cela rend possible les questions du type \"Quels sont les grands themes de ce corpus ?\" sans parcourir tous les documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-comparison",
   "metadata": {},
   "source": [
    "### Comparaison des approches RAG\n\n| Critere | RAG classique | GraphRAG local | GraphRAG global (Microsoft) |\n|---------|--------------|----------------|----------------------------|\n| Pre-traitement | Embedding des chunks | Construction du KG | KG + communautes + resumes |\n| Cout initial | Faible | Moyen | Eleve (appels LLM multiples) |\n| Requete | Recherche vectorielle | Traversee de graphe | Hierarchie de resumes |\n| Questions factuelles | Bon | Excellent | Bon |\n| Questions multi-hop | Faible | Excellent | Bon |\n| Questions de synthese | Faible | Moyen | Excellent |\n| Explicabilite | Faible (chunks) | Elevee (triplets) | Elevee (communautes) |\n| Mise a jour | Re-embedding | Ajout de triplets | Reconstruction partielle |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7-title",
   "metadata": {},
   "source": [
    "---\n\n## 7. Gestion robuste des cles API\n\n### Bonnes pratiques pour les notebooks avec API externes\n\nUn notebook pedagogique doit fonctionner **avec ou sans** cles API. Voici les patterns recommandes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pattern 1 : Detection et fallback ===\n\nimport os\n\ndef get_api_client():\n    \"\"\"Retourne un client API ou None si aucune cle n'est disponible.\"\"\"\n    openai_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    if openai_key and not openai_key.startswith(\"sk-...\"):\n        try:\n            from openai import OpenAI\n            return (\"openai\", OpenAI())\n        except ImportError:\n            pass\n\n    anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n    if anthropic_key and not anthropic_key.startswith(\"sk-ant-...\"):\n        try:\n            import anthropic\n            return (\"anthropic\", anthropic.Anthropic())\n        except ImportError:\n            pass\n\n    return (None, None)\n\n\n# === Pattern 2 : Execution avec donnees de demonstration ===\n\ndef safe_llm_call(prompt, demo_response=\"(reponse de demonstration)\"):\n    \"\"\"Appeler un LLM avec fallback vers une reponse de demonstration.\"\"\"\n    provider, client = get_api_client()\n\n    if provider == \"openai\":\n        try:\n            response = client.chat.completions.create(\n                model=\"gpt-4o-mini\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.0\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            print(f\"Erreur API OpenAI : {e}\")\n\n    elif provider == \"anthropic\":\n        try:\n            response = client.messages.create(\n                model=\"claude-sonnet-4-20250514\",\n                max_tokens=1000,\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            return response.content[0].text\n        except Exception as e:\n            print(f\"Erreur API Anthropic : {e}\")\n\n    return demo_response\n\n\n# === Pattern 3 : Verification de la configuration ===\n\nprint(\"=== Verification de la configuration API ===\")\nprovider, client = get_api_client()\nif provider:\n    print(f\"API disponible : {provider}\")\n    test_response = safe_llm_call(\"Reponds en un mot : OK\", demo_response=\"OK\")\n    print(f\"Test de connexion : {test_response}\")\nelse:\n    print(\"Aucune API configuree.\")\n    print(\"Le notebook utilise les donnees de demonstration.\")\n    print()\n    print(\"Pour configurer une API :\")\n    print(\"  1. Copier .env.example vers .env\")\n    print(\"  2. Remplir OPENAI_API_KEY ou ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation-api-patterns",
   "metadata": {},
   "source": [
    "### Interpretation\n\nLes trois patterns presentes garantissent un fonctionnement robuste :\n\n| Pattern | Usage | Avantage |\n|---------|-------|----------|\n| Detection et fallback | Au demarrage | Detecte automatiquement le provider disponible |\n| Execution avec demo | Chaque appel LLM | Jamais de cellule en erreur, meme sans cle |\n| Verification config | Diagnostique | L'utilisateur sait immediatement quel mode est actif |\n\n**Bonnes pratiques** :\n1. Ne jamais coder une cle API en dur dans le notebook\n2. Utiliser `.env` + `python-dotenv` pour le chargement local\n3. Toujours fournir un `demo_response` representatif du vrai resultat\n4. Tester la connexion au demarrage pour un feedback rapide\n\n> **Note** : Le fichier `.env.example` documente les variables attendues sans exposer de secrets. Il est commit dans git, contrairement a `.env` qui est dans `.gitignore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises-title",
   "metadata": {},
   "source": [
    "---\n\n## 8. Exercices\n\n### Exercice 1 : Extraire des entites d'un texte personnalise\n\nChoisissez un paragraphe de texte (Wikipedia, article de presse, etc.) et utilisez le pipeline d'extraction pour en construire un graphe de connaissances. Visualisez le resultat.\n\n**Indices** :\n- Preparez un texte de 100-200 mots contenant des entites nommees et des relations\n- Utilisez `extract_entities_llm()` ou creez manuellement le dictionnaire d'entites\n- Transformez en graphe RDF avec le code de la section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1 : Extraction d'entites depuis un texte personnalise\n# TODO : Remplacez le texte ci-dessous par votre propre texte\n\ncustom_text = \"\"\"\n# Collez votre texte ici (100-200 mots)\n# Exemple : un article Wikipedia sur un sujet qui vous interesse\n\"\"\"\n\n# TODO : Creer le dictionnaire d'entites manuellement ou via LLM\n# custom_extraction = {\n#     \"entities\": [\n#         {\"name\": \"...\", \"type\": \"Person\", \"attributes\": {}},\n#         {\"name\": \"...\", \"type\": \"Organization\", \"attributes\": {}},\n#     ],\n#     \"relations\": [\n#         {\"source\": \"...\", \"relation\": \"...\", \"target\": \"...\"},\n#     ]\n# }\n\n# TODO : Construire le graphe RDF\n# g_custom = Graph()\n# ... (adapter le code de la section 4)\n\n# TODO : Visualiser\n# ... (adapter le code de la section 4)\n\nprint(\"Decommentez et completez le code ci-dessus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-title",
   "metadata": {},
   "source": [
    "### Exercice 2 : Construire un mini-KG thematique\n\nA partir du fichier `data/movies.csv` (utilise dans SW-12), construisez un KG et utilisez-le pour repondre a la question : \"Quels realisateurs ont dirige des acteurs en commun ?\"\n\n**Indices** :\n- Reutilisez le code de construction de KG de SW-12\n- Ecrivez une requete SPARQL pour trouver les chemins realisateur -> film -> acteur -> film -> realisateur\n- Formatez le resultat comme contexte pour un prompt LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2 : KG thematique a partir de movies.csv\nimport pandas as pd\n\n# Charger les donnees\n# df_movies = pd.read_csv(\"data/movies.csv\")\n\n# TODO : Construire le graphe RDF (adapter le code de SW-12)\n# g_movies = Graph()\n# ...\n\n# TODO : Requete SPARQL pour trouver les realisateurs partageant des acteurs\n# query = \"\"\"\n# PREFIX ...\n# SELECT ?dir1 ?dir2 ?actor\n# WHERE {\n#     ?film1 ... ?dir1 .\n#     ?film1 ... ?actor .\n#     ?film2 ... ?dir2 .\n#     ?film2 ... ?actor .\n#     FILTER(?dir1 != ?dir2)\n# }\n# \"\"\"\n\n# TODO : Formater comme contexte GraphRAG et generer une reponse\n# context = \"...\"\n# response = safe_llm_call(f\"Question: ... Contexte: {context}\", demo_response=\"...\")\n\nprint(\"Decommentez et completez le code ci-dessus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise3-title",
   "metadata": {},
   "source": [
    "### Exercice 3 : Question multi-hop avec GraphRAG\n\nImplementez une version amelioree de `graphrag_query` qui supporte les questions multi-hop (profondeur > 1). Testez avec la question : \"Quel est le genre des films ou jouent les acteurs diriges par Nolan ?\"\n\n**Indices** :\n- Augmentez le parametre `max_hops` dans `extract_subgraph()`\n- La question necessite le chemin : Nolan -> films -> acteurs -> (autres films de ces acteurs) -> genres\n- Comparez la reponse avec max_hops=1 et max_hops=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3 : Question multi-hop\n# TODO : Tester avec differentes profondeurs\n\n# question_multihop = \"Quel est le genre des films ou jouent les acteurs diriges par Nolan ?\"\n\n# Profondeur 1\n# result_1hop = graphrag_query(question_multihop, g, max_hops=1)\n# print(\"=== Profondeur 1 ===\")\n# print(f\"Faits recuperes : {result_1hop['context_lines']}\")\n# print()\n\n# Profondeur 2\n# result_2hop = graphrag_query(question_multihop, g, max_hops=2)\n# print(\"=== Profondeur 2 ===\")\n# print(f\"Faits recuperes : {result_2hop['context_lines']}\")\n# print()\n\n# TODO : Comparer les reponses\n# print(\"Difference de contexte :\")\n# print(f\"  1-hop : {result_1hop['context_lines']} faits\")\n# print(f\"  2-hop : {result_2hop['context_lines']} faits\")\n\nprint(\"Decommentez et completez le code ci-dessus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-title",
   "metadata": {},
   "source": [
    "---\n\n## Resume\n\nCe notebook a couvert les concepts fondamentaux du GraphRAG, de l'extraction d'entites a l'interrogation augmentee.\n\n### Concepts et outils\n\n| Etape | Outil | Concept |\n|-------|-------|---------|\n| Configuration API | python-dotenv | Gestion securisee des cles, fallback gracieux |\n| Extraction d'entites | LLM (GPT/Claude) | NER + extraction de relations depuis texte brut |\n| Construction du KG | rdflib | Transformation entites/relations en triplets RDF |\n| Visualisation | NetworkX + matplotlib | Graphe avec couleurs par type d'entite |\n| Extraction sous-graphe | BFS sur rdflib | Recuperation du voisinage d'entites |\n| Generation augmentee | LLM + contexte KG | Reponse fondee sur des faits verifiables |\n| Detection communautes | NetworkX (modularite) | Regroupement semantique pour la summarization |\n\n### Competences acquises\n\n1. Comprendre pourquoi GraphRAG ameliore le RAG classique\n2. Extraire des entites et relations d'un texte avec un LLM\n3. Construire un graphe de connaissances a partir des extractions\n4. Implementer un pipeline d'interrogation augmentee par le graphe\n5. Connaitre l'approche Microsoft GraphRAG et la detection de communautes\n6. Gerer les cles API de maniere robuste (fallback, demonstration)\n\n### Pour aller plus loin\n\n- [Microsoft GraphRAG (GitHub)](https://github.com/microsoft/graphrag) - Implementation officielle\n- [From Local to Global: A Graph RAG Approach (paper)](https://arxiv.org/abs/2404.16130) - Article de recherche Microsoft\n- [LangChain GraphRAG](https://python.langchain.com/docs/use_cases/graph/) - Integration LangChain\n- [Neo4j GraphRAG](https://neo4j.com/developer-blog/graphrag/) - Approche Neo4j\n- [Leiden Algorithm (paper)](https://arxiv.org/abs/1810.08473) - Detection de communautes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer-nav",
   "metadata": {},
   "source": [
    "---\n\nCe notebook conclut la serie Semantic Web. Vous avez parcouru l'ensemble de la pile technologique du Web semantique, depuis les bases RDF (SW-1) jusqu'a l'integration avec les grands modeles de langage (SW-13).\n\n---\n\n**Navigation** : [<< 12-KnowledgeGraphs](SW-12-KnowledgeGraphs.ipynb) | [Index](README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}