{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lean 8 - Agents Autonomes pour Demonstration de Theoremes\n",
    "\n",
    "**Navigation** : [â† Lean-7-LLM-Integration](Lean-7-LLM-Integration.ipynb) | [Index](Lean-1-Setup.ipynb) | [Lean-9-LeanDojo â†’](Lean-9-LeanDojo.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Ce notebook final de la serie explore la creation de **systemes multi-agents** capables de prouver des theoremes mathematiques de maniere **autonome**. Nous combinons les techniques des notebooks precedents avec les patterns d'orchestration agentique.\n",
    "\n",
    "L'objectif est de construire un systeme qui peut :\n",
    "1. Recevoir un enonce de theoreme\n",
    "2. Rechercher des lemmes pertinents dans Mathlib\n",
    "3. Generer des strategies de preuve\n",
    "4. Verifier formellement avec Lean\n",
    "5. Iterer jusqu'au succes\n",
    "\n",
    "### 1.1. Objectifs pedagogiques\n",
    "\n",
    "1. Concevoir une architecture multi-agents pour theorem proving\n",
    "2. Implementer des agents specialises (recherche, generation, verification)\n",
    "3. Orchestrer la collaboration entre agents\n",
    "4. Gerer les boucles de feedback et d'amelioration\n",
    "5. Comprendre les techniques de Harmonic Aristotle et APOLLO\n",
    "\n",
    "### 1.2. Prerequis\n",
    "\n",
    "- Notebooks **Lean-1** a **Lean-7** completes\n",
    "- Notions de base sur les systemes multi-agents\n",
    "- Cle API LLM (optionnel pour execution)\n",
    "\n",
    "### 1.3. Duree estimee : 55-60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Architecture d'un Systeme Agentique pour Lean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Vue d'ensemble\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     SYSTEME AGENTIQUE LEAN                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚\n",
    "â”‚  â”‚   ORCHESTRATOR  â”‚  <- Coordonne tous les agents                 â”‚\n",
    "â”‚  â”‚     Agent       â”‚                                               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚\n",
    "â”‚           â”‚                                                        â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚\n",
    "â”‚  â”‚        â”‚        â”‚                â”‚                              â”‚\n",
    "â”‚  v        v        v                v                              â”‚\n",
    "â”‚ â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
    "â”‚ â”‚Searchâ”‚ â”‚Tacticâ”‚ â”‚Proofâ”‚        â”‚Memory  â”‚                         â”‚\n",
    "â”‚ â”‚Agentâ”‚ â”‚Agentâ”‚ â”‚Verifyâ”‚        â”‚Store   â”‚                         â”‚\n",
    "â”‚ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\n",
    "â”‚    â”‚        â”‚        â”‚                                             â”‚\n",
    "â”‚    v        v        v                                             â”‚\n",
    "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚ â”‚               LEAN KERNEL                     â”‚                   â”‚\n",
    "â”‚ â”‚  (Verification formelle + Mathlib)           â”‚                   â”‚\n",
    "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent de Recherche de Theoremes\n",
    "\n",
    "### 3.1. Role\n",
    "\n",
    "L'agent de recherche parcourt Mathlib pour trouver des lemmes pertinents au probleme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmes trouves:\n",
      "  Nat.add_zero: n + 0 = n (score: 1.00)\n",
      "  Nat.zero_add: 0 + n = n (score: 0.60)\n",
      "  Nat.add_comm: n + m = m + n (score: 0.45)\n",
      "  Nat.add_assoc: (n + m) + k = n + (m + k) (score: 0.45)\n",
      "  Nat.mul_zero: n * 0 = 0 (score: 0.45)\n",
      "  Nat.zero_mul: 0 * n = 0 (score: 0.45)\n",
      "  Nat.succ_add: succ n + m = succ (n + m) (score: 0.45)\n",
      "  Nat.add_succ: n + succ m = succ (n + m) (score: 0.45)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import json\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class Lemma:\n",
    "    \"\"\"Represente un lemme Mathlib.\"\"\"\n",
    "    name: str\n",
    "    statement: str\n",
    "    namespace: str\n",
    "    relevance_score: float = 0.0\n",
    "\n",
    "class TheoremSearchAgent:\n",
    "    \"\"\"Agent de recherche de theoremes dans Mathlib.\"\"\"\n",
    "\n",
    "    # Base de lemmes connus (extensible)\n",
    "    KNOWN_LEMMAS = [\n",
    "        Lemma(\"Nat.add_zero\", \"n + 0 = n\", \"Nat\"),\n",
    "        Lemma(\"Nat.zero_add\", \"0 + n = n\", \"Nat\"),\n",
    "        Lemma(\"Nat.add_comm\", \"n + m = m + n\", \"Nat\"),\n",
    "        Lemma(\"Nat.add_assoc\", \"(n + m) + k = n + (m + k)\", \"Nat\"),\n",
    "        Lemma(\"Nat.mul_comm\", \"n * m = m * n\", \"Nat\"),\n",
    "        Lemma(\"Nat.mul_assoc\", \"(n * m) * k = n * (m * k)\", \"Nat\"),\n",
    "        Lemma(\"Nat.mul_zero\", \"n * 0 = 0\", \"Nat\"),\n",
    "        Lemma(\"Nat.zero_mul\", \"0 * n = 0\", \"Nat\"),\n",
    "        Lemma(\"Nat.mul_one\", \"n * 1 = n\", \"Nat\"),\n",
    "        Lemma(\"Nat.one_mul\", \"1 * n = n\", \"Nat\"),\n",
    "        Lemma(\"Nat.succ_add\", \"succ n + m = succ (n + m)\", \"Nat\"),\n",
    "        Lemma(\"Nat.add_succ\", \"n + succ m = succ (n + m)\", \"Nat\"),\n",
    "    ]\n",
    "\n",
    "    def __init__(self, llm_client=None):\n",
    "        self.llm = llm_client\n",
    "        self.cache = {}  # Cache des recherches\n",
    "\n",
    "    def search(self, goal: str, context: str = \"\") -> List[Lemma]:\n",
    "        \"\"\"\n",
    "        Recherche des lemmes pertinents pour un but donne.\n",
    "\n",
    "        Args:\n",
    "            goal: Le but a prouver\n",
    "            context: Contexte additionnel (hypotheses, etc.)\n",
    "\n",
    "        Returns:\n",
    "            Liste de lemmes tries par pertinence\n",
    "        \"\"\"\n",
    "        # Verifier le cache\n",
    "        cache_key = f\"{goal}:{context}\"\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        # Analyser le but pour extraire les concepts\n",
    "        concepts = self._extract_concepts(goal)\n",
    "\n",
    "        # Rechercher dans la base de lemmes\n",
    "        lemmas = self._search_mathlib(concepts, goal)\n",
    "\n",
    "        # Scorer par pertinence\n",
    "        scored = self._score_lemmas(lemmas, goal)\n",
    "\n",
    "        # Mettre en cache\n",
    "        self.cache[cache_key] = scored\n",
    "\n",
    "        return scored\n",
    "\n",
    "    def _extract_concepts(self, goal: str) -> List[str]:\n",
    "        \"\"\"Extrait les concepts mathematiques du but.\"\"\"\n",
    "        concepts = []\n",
    "        goal_lower = goal.lower()\n",
    "\n",
    "        # Mapping symboles -> concepts\n",
    "        symbol_map = {\n",
    "            \"+\": [\"add\"],\n",
    "            \"*\": [\"mul\"],\n",
    "            \"0\": [\"zero\"],\n",
    "            \"1\": [\"one\"],\n",
    "            \"succ\": [\"succ\"],\n",
    "        }\n",
    "\n",
    "        for symbol, keywords in symbol_map.items():\n",
    "            if symbol in goal:\n",
    "                concepts.extend(keywords)\n",
    "\n",
    "        # Mots-cles explicites\n",
    "        explicit_keywords = [\"comm\", \"assoc\", \"zero\", \"one\", \"succ\", \"add\", \"mul\"]\n",
    "        for kw in explicit_keywords:\n",
    "            if kw in goal_lower and kw not in concepts:\n",
    "                concepts.append(kw)\n",
    "\n",
    "        return list(set(concepts))\n",
    "\n",
    "    def _search_mathlib(self, concepts: List[str], goal: str) -> List[Lemma]:\n",
    "        \"\"\"Recherche dans la base de lemmes connus.\"\"\"\n",
    "        if not concepts:\n",
    "            # Fallback: retourner quelques lemmes de base\n",
    "            return self.KNOWN_LEMMAS[:4]\n",
    "\n",
    "        # Filtrer par concepts\n",
    "        matches = []\n",
    "        for lemma in self.KNOWN_LEMMAS:\n",
    "            name_lower = lemma.name.lower()\n",
    "            if any(c in name_lower for c in concepts):\n",
    "                matches.append(Lemma(lemma.name, lemma.statement, lemma.namespace, 0.0))\n",
    "\n",
    "        return matches if matches else self.KNOWN_LEMMAS[:3]\n",
    "\n",
    "    def _score_lemmas(self, lemmas: List[Lemma], goal: str) -> List[Lemma]:\n",
    "        \"\"\"Score les lemmes par pertinence.\"\"\"\n",
    "        # Normaliser le but\n",
    "        goal_normalized = goal.replace(\" \", \"\").lower()\n",
    "\n",
    "        for lemma in lemmas:\n",
    "            # Score base sur la correspondance structurelle\n",
    "            stmt_normalized = lemma.statement.replace(\" \", \"\").lower()\n",
    "\n",
    "            # Score exact match\n",
    "            if goal_normalized == stmt_normalized:\n",
    "                lemma.relevance_score = 1.0\n",
    "            # Score partial match\n",
    "            elif goal_normalized in stmt_normalized or stmt_normalized in goal_normalized:\n",
    "                lemma.relevance_score = 0.8\n",
    "            else:\n",
    "                # Score par tokens communs\n",
    "                goal_tokens = set(re.findall(r'[a-z]+|[0-9]+|[+*=]', goal_normalized))\n",
    "                stmt_tokens = set(re.findall(r'[a-z]+|[0-9]+|[+*=]', stmt_normalized))\n",
    "                common = goal_tokens & stmt_tokens\n",
    "                lemma.relevance_score = len(common) / max(len(goal_tokens), 1) * 0.6\n",
    "\n",
    "        return sorted(lemmas, key=lambda l: l.relevance_score, reverse=True)\n",
    "\n",
    "# Test\n",
    "search_agent = TheoremSearchAgent()\n",
    "results = search_agent.search(\"n + 0 = n\")\n",
    "print(\"Lemmes trouves:\")\n",
    "for lemma in results:\n",
    "    print(f\"  {lemma.name}: {lemma.statement} (score: {lemma.relevance_score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent de Generation de Tactiques\n",
    "\n",
    "### 4.1. Role\n",
    "\n",
    "L'agent de tactiques genere des sequences de tactiques Lean pour prouver le but."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tactiques suggerees:\n",
      "  [1.00] exact Nat.add_zero - Appliquer Nat.add_zero: n + 0 = n\n",
      "  [0.90] rfl - Reflexivite - verifie si les deux cotes sont identiques\n",
      "  [0.80] rw [Nat.add_zero] - Reecrire avec Nat.add_zero\n",
      "  [0.70] omega - Arithmetique de Presburger automatique\n",
      "  [0.60] exact Nat.zero_add - Appliquer Nat.zero_add: 0 + n = n\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from typing import Tuple\n",
    "\n",
    "class TacticType(Enum):\n",
    "    DIRECT = \"direct\"       # exact, rfl\n",
    "    REWRITE = \"rewrite\"     # rw, simp\n",
    "    SPLIT = \"split\"         # constructor, cases\n",
    "    INDUCTION = \"induction\" # induction, recursion\n",
    "    AUTO = \"auto\"           # omega, ring, linarith\n",
    "\n",
    "@dataclass\n",
    "class TacticSuggestion:\n",
    "    \"\"\"Une suggestion de tactique avec son contexte.\"\"\"\n",
    "    tactic: str\n",
    "    tactic_type: TacticType\n",
    "    confidence: float\n",
    "    explanation: str\n",
    "\n",
    "class TacticGeneratorAgent:\n",
    "    \"\"\"Agent de generation de tactiques.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client=None):\n",
    "        self.llm = llm_client\n",
    "        self.history = []  # Historique des tentatives\n",
    "    \n",
    "    def generate(self, goal: str, context: List[str], \n",
    "                 available_lemmas: List[Lemma]) -> List[TacticSuggestion]:\n",
    "        \"\"\"\n",
    "        Genere des tactiques pour un but donne.\n",
    "        \n",
    "        Args:\n",
    "            goal: Le but courant\n",
    "            context: Les hypotheses disponibles\n",
    "            available_lemmas: Lemmes suggeres par l'agent de recherche\n",
    "        \n",
    "        Returns:\n",
    "            Liste de suggestions de tactiques\n",
    "        \"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        # Strategie 1: Tactiques directes\n",
    "        if \"=\" in goal:\n",
    "            suggestions.append(TacticSuggestion(\n",
    "                \"rfl\", TacticType.DIRECT, 0.9,\n",
    "                \"Reflexivite - verifie si les deux cotes sont identiques\"\n",
    "            ))\n",
    "        \n",
    "        # Strategie 2: Utiliser les lemmes disponibles\n",
    "        for lemma in available_lemmas[:3]:\n",
    "            suggestions.append(TacticSuggestion(\n",
    "                f\"exact {lemma.name}\", TacticType.DIRECT, \n",
    "                lemma.relevance_score,\n",
    "                f\"Appliquer {lemma.name}: {lemma.statement}\"\n",
    "            ))\n",
    "            suggestions.append(TacticSuggestion(\n",
    "                f\"rw [{lemma.name}]\", TacticType.REWRITE,\n",
    "                lemma.relevance_score * 0.8,\n",
    "                f\"Reecrire avec {lemma.name}\"\n",
    "            ))\n",
    "        \n",
    "        # Strategie 3: Tactiques automatiques\n",
    "        if any(op in goal for op in [\"+\", \"-\", \"<\", \">\", \"<=\", \">=\"]):\n",
    "            suggestions.append(TacticSuggestion(\n",
    "                \"omega\", TacticType.AUTO, 0.7,\n",
    "                \"Arithmetique de Presburger automatique\"\n",
    "            ))\n",
    "        \n",
    "        if \"*\" in goal or \"^\" in goal:\n",
    "            suggestions.append(TacticSuggestion(\n",
    "                \"ring\", TacticType.AUTO, 0.7,\n",
    "                \"Algebre polynomiale automatique\"\n",
    "            ))\n",
    "        \n",
    "        # Strategie 4: Simp comme fallback\n",
    "        suggestions.append(TacticSuggestion(\n",
    "            \"simp\", TacticType.REWRITE, 0.5,\n",
    "            \"Simplification automatique\"\n",
    "        ))\n",
    "        \n",
    "        # Trier par confiance\n",
    "        return sorted(suggestions, key=lambda s: s.confidence, reverse=True)\n",
    "    \n",
    "    def generate_sequence(self, goal: str, context: List[str],\n",
    "                          available_lemmas: List[Lemma],\n",
    "                          max_depth: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Genere une sequence complete de tactiques.\n",
    "        \"\"\"\n",
    "        sequence = []\n",
    "        current_goal = goal\n",
    "        \n",
    "        for _ in range(max_depth):\n",
    "            suggestions = self.generate(current_goal, context, available_lemmas)\n",
    "            if not suggestions:\n",
    "                break\n",
    "            \n",
    "            best = suggestions[0]\n",
    "            sequence.append(best.tactic)\n",
    "            \n",
    "            # Simuler la progression (dans la realite, Lean nous dirait le nouveau but)\n",
    "            if best.tactic_type == TacticType.DIRECT:\n",
    "                break  # Preuve complete\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "# Test\n",
    "tactic_agent = TacticGeneratorAgent()\n",
    "lemmas = search_agent.search(\"n + 0 = n\")\n",
    "suggestions = tactic_agent.generate(\"n + 0 = n\", [], lemmas)\n",
    "\n",
    "print(\"Tactiques suggerees:\")\n",
    "for s in suggestions[:5]:\n",
    "    print(f\"  [{s.confidence:.2f}] {s.tactic} - {s.explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent de Verification\n",
    "\n",
    "### 5.1. Role\n",
    "\n",
    "L'agent de verification execute le code Lean et analyse les resultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: Succes\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class VerificationResult:\n",
    "    \"\"\"Resultat de la verification Lean.\"\"\"\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "    remaining_goals: List[str] = None\n",
    "    execution_time: float = 0.0\n",
    "\n",
    "class ProofVerifierAgent:\n",
    "    \"\"\"Agent de verification des preuves.\"\"\"\n",
    "    \n",
    "    def __init__(self, lean_path: str = \"lean\"):\n",
    "        self.lean_path = lean_path\n",
    "        self.verified_count = 0\n",
    "        self.failed_count = 0\n",
    "    \n",
    "    def verify(self, theorem: str, proof: str) -> VerificationResult:\n",
    "        \"\"\"\n",
    "        Verifie une preuve avec Lean.\n",
    "        \n",
    "        Args:\n",
    "            theorem: L'enonce du theoreme\n",
    "            proof: La preuve proposee (sequence de tactiques)\n",
    "        \n",
    "        Returns:\n",
    "            Resultat de la verification\n",
    "        \"\"\"\n",
    "        # Construire le code Lean complet\n",
    "        lean_code = self._build_lean_code(theorem, proof)\n",
    "        \n",
    "        # Simuler l'execution Lean\n",
    "        # (Dans un vrai systeme, on utiliserait subprocess ou lean-dojo)\n",
    "        result = self._simulate_lean_execution(lean_code)\n",
    "        \n",
    "        # Mettre a jour les statistiques\n",
    "        if result.success:\n",
    "            self.verified_count += 1\n",
    "        else:\n",
    "            self.failed_count += 1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _build_lean_code(self, theorem: str, proof: str) -> str:\n",
    "        \"\"\"Construit le code Lean complet.\"\"\"\n",
    "        return f\"\"\"\n",
    "{theorem} := by\n",
    "  {proof}\n",
    "        \"\"\".strip()\n",
    "    \n",
    "    def _simulate_lean_execution(self, code: str) -> VerificationResult:\n",
    "        \"\"\"\n",
    "        Simule l'execution Lean.\n",
    "        Dans un vrai systeme, utiliser lean-dojo ou subprocess.\n",
    "        \"\"\"\n",
    "        # Heuristiques simples pour la simulation\n",
    "        if \"rfl\" in code or \"exact Nat.add_zero\" in code:\n",
    "            return VerificationResult(success=True)\n",
    "        elif \"sorry\" in code:\n",
    "            return VerificationResult(\n",
    "                success=False,\n",
    "                error_message=\"declaration uses 'sorry'\"\n",
    "            )\n",
    "        else:\n",
    "            # Simuler une reussite aleatoire\n",
    "            import random\n",
    "            if random.random() > 0.3:\n",
    "                return VerificationResult(success=True)\n",
    "            else:\n",
    "                return VerificationResult(\n",
    "                    success=False,\n",
    "                    error_message=\"tactic failed\"\n",
    "                )\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Retourne les statistiques de verification.\"\"\"\n",
    "        total = self.verified_count + self.failed_count\n",
    "        return {\n",
    "            \"verified\": self.verified_count,\n",
    "            \"failed\": self.failed_count,\n",
    "            \"success_rate\": self.verified_count / max(total, 1)\n",
    "        }\n",
    "\n",
    "# Test\n",
    "verifier = ProofVerifierAgent()\n",
    "result = verifier.verify(\n",
    "    \"theorem test (n : Nat) : n + 0 = n\",\n",
    "    \"exact Nat.add_zero n\"\n",
    ")\n",
    "print(f\"Verification: {'Succes' if result.success else 'Echec'}\")\n",
    "if result.error_message:\n",
    "    print(f\"Erreur: {result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent Orchestrateur\n",
    "\n",
    "### 6.1. Role\n",
    "\n",
    "L'orchestrateur coordonne tous les agents pour resoudre un probleme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Debut de la preuve: theorem add_zero (n : Nat) : n + 0 = n\n",
      "============================================================\n",
      "\n",
      "--- Iteration 1 ---\n",
      "Lemmes trouves: ['Nat.add_zero', 'Nat.zero_add', 'Nat.add_comm']\n",
      "Tactiques generees: ['rfl']\n",
      "\n",
      "Preuve trouvee!\n",
      "\n",
      "Preuve finale:\n",
      "rfl\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ProofAttempt:\n",
    "    \"\"\"Enregistre une tentative de preuve.\"\"\"\n",
    "    theorem: str\n",
    "    tactics: List[str]\n",
    "    result: VerificationResult\n",
    "    iteration: int\n",
    "\n",
    "class OrchestratorAgent:\n",
    "    \"\"\"\n",
    "    Agent orchestrateur qui coordonne le systeme multi-agents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.search_agent = TheoremSearchAgent()\n",
    "        self.tactic_agent = TacticGeneratorAgent()\n",
    "        self.verifier = ProofVerifierAgent()\n",
    "        self.history: List[ProofAttempt] = []\n",
    "        self.max_iterations = 10\n",
    "    \n",
    "    def prove(self, theorem: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Tente de prouver un theoreme.\n",
    "        \n",
    "        Args:\n",
    "            theorem: L'enonce du theoreme\n",
    "        \n",
    "        Returns:\n",
    "            (succes, preuve) ou (echec, None)\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Debut de la preuve: {theorem}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            print(f\"--- Iteration {iteration + 1} ---\")\n",
    "            \n",
    "            # Etape 1: Rechercher des lemmes pertinents\n",
    "            goal = self._extract_goal(theorem)\n",
    "            lemmas = self.search_agent.search(goal)\n",
    "            print(f\"Lemmes trouves: {[l.name for l in lemmas[:3]]}\")\n",
    "            \n",
    "            # Etape 2: Generer des tactiques\n",
    "            tactics = self.tactic_agent.generate_sequence(\n",
    "                goal, [], lemmas\n",
    "            )\n",
    "            proof = \"\\n  \".join(tactics)\n",
    "            print(f\"Tactiques generees: {tactics}\")\n",
    "            \n",
    "            # Etape 3: Verifier\n",
    "            result = self.verifier.verify(theorem, proof)\n",
    "            \n",
    "            # Enregistrer la tentative\n",
    "            self.history.append(ProofAttempt(\n",
    "                theorem, tactics, result, iteration\n",
    "            ))\n",
    "            \n",
    "            if result.success:\n",
    "                print(f\"\\nPreuve trouvee!\")\n",
    "                return True, proof\n",
    "            else:\n",
    "                print(f\"Echec: {result.error_message}\")\n",
    "                # Apprendre de l'echec pour la prochaine iteration\n",
    "                self._learn_from_failure(result)\n",
    "        \n",
    "        print(f\"\\nEchec apres {self.max_iterations} iterations\")\n",
    "        return False, None\n",
    "    \n",
    "    def _extract_goal(self, theorem: str) -> str:\n",
    "        \"\"\"Extrait le but du theoreme.\"\"\"\n",
    "        # Simplification: prendre la partie apres le \":\"\n",
    "        if \":\" in theorem:\n",
    "            return theorem.split(\":\", 1)[1].strip()\n",
    "        return theorem\n",
    "    \n",
    "    def _learn_from_failure(self, result: VerificationResult):\n",
    "        \"\"\"Ajuste la strategie basee sur l'echec.\"\"\"\n",
    "        # Dans un vrai systeme, on ajusterait les poids,\n",
    "        # eviterait les tactiques qui echouent, etc.\n",
    "        pass\n",
    "    \n",
    "    def get_statistics(self) -> dict:\n",
    "        \"\"\"Retourne les statistiques du systeme.\"\"\"\n",
    "        return {\n",
    "            \"total_attempts\": len(self.history),\n",
    "            \"verifier_stats\": self.verifier.get_stats()\n",
    "        }\n",
    "\n",
    "# Demonstration\n",
    "orchestrator = OrchestratorAgent()\n",
    "success, proof = orchestrator.prove(\n",
    "    \"theorem add_zero (n : Nat) : n + 0 = n\"\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nPreuve finale:\\n{proof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Ã‰tat PartagÃ© : La Classe `ProofState`\n",
    "\n",
    "La classe `ProofState` est le **cÅ“ur du systÃ¨me**. Elle contient :\n",
    "\n",
    "### 6.2. Phase de preuve (`ProofPhase` enum)\n",
    "```\n",
    "INIT â†’ SEARCH â†’ TACTIC_GEN â†’ VERIFICATION â†’ REFINEMENT â†’ COMPLETE\n",
    "```\n",
    "\n",
    "Chaque phase dÃ©termine **quel agent agit** :\n",
    "- `INIT` â†’ CoordinatorAgent dÃ©cide de la stratÃ©gie\n",
    "- `SEARCH` â†’ SearchAgent cherche des lemmes\n",
    "- `TACTIC_GEN` â†’ TacticAgent gÃ©nÃ¨re une tactique\n",
    "- `VERIFICATION` â†’ VerifierAgent teste la preuve\n",
    "- `REFINEMENT` â†’ CriticAgent analyse et ajuste\n",
    "- `COMPLETE` â†’ Session terminÃ©e\n",
    "\n",
    "### 6.3. StratÃ©gie de preuve (`ProofStrategy` enum)\n",
    "\n",
    "```python\n",
    "EXPLORATION   # Recherche large de lemmes\n",
    "REFINEMENT    # Ajustement d'une preuve existante\n",
    "VALIDATION    # VÃ©rification formelle\n",
    "RECOVERY      # RÃ©cupÃ©ration aprÃ¨s erreur\n",
    "```\n",
    "\n",
    "La stratÃ©gie influence **quels lemmes rechercher** et **quelles tactiques essayer**.\n",
    "\n",
    "### 6.4. Historique et mÃ©triques\n",
    "\n",
    "- `tactic_history` : Liste de toutes les tactiques essayÃ©es (succÃ¨s + Ã©checs)\n",
    "- `verification_results` : RÃ©sultats des vÃ©rifications Lean\n",
    "- `current_proof` : Preuve en construction\n",
    "- `error_count` : Nombre d'erreurs rencontrÃ©es\n",
    "\n",
    "### 6.5. Snapshots JSON\n",
    "\n",
    "Ã€ chaque itÃ©ration, on peut sauvegarder l'Ã©tat complet en JSON :\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"phase\": \"TACTIC_GEN\",\n",
    "  \"strategy\": \"EXPLORATION\",\n",
    "  \"iteration\": 5,\n",
    "  \"current_goal\": \"n + 0 = n\",\n",
    "  \"tactic_history\": [...],\n",
    "  \"current_proof\": [\"intro n\", \"rw [Nat.add_zero]\"]\n",
    "}\n",
    "```\n",
    "\n",
    "**UtilitÃ©** : Debugging, reproduction de bugs, benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6. Definition des 5 Agents Specialises\n",
    "\n",
    "Le systeme multi-agents comprend 5 roles distincts:\n",
    "\n",
    "| Agent | Role | Plugins | Delegation |\n",
    "|-------|------|---------|------------|\n",
    "| **SearchAgent** | Recherche lemmes Mathlib | LeanSearch, StateManager | TacticAgent si lemmes trouves |\n",
    "| **TacticAgent** | Generation tactiques | LeanTactic, StateManager | VerifierAgent pour validation |\n",
    "| **VerifierAgent** | Verification Lean | LeanVerification, StateManager | CriticAgent si echec |\n",
    "| **CriticAgent** | Analyse echecs | LeanTactic, StateManager | Redirection selon erreur |\n",
    "| **CoordinatorAgent** | Supervision globale | StateManager | Gestion des blocages |\n",
    "\n",
    "**Pattern cle**: Chaque agent designe explicitement le suivant via `designate_next_agent()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7. Patterns de Delegation Multi-Agents\n",
    "\n",
    "Les instructions ci-dessus definissent les **regles de delegation** entre agents :\n",
    "\n",
    "| Agent | Role | Delegue vers |\n",
    "|-------|------|-------------|\n",
    "| **SearchAgent** | Recherche lemmes Mathlib | TacticAgent (si lemmes trouves) |\n",
    "| **TacticAgent** | Genere tactiques Lean | VerifierAgent (toujours) |\n",
    "| **VerifierAgent** | Verifie preuve formelle | CriticAgent (si echec) / COMPLETE (si succes) |\n",
    "| **CriticAgent** | Analyse erreurs | SearchAgent (retry) / CoordinatorAgent (si bloque) |\n",
    "| **CoordinatorAgent** | Re-orchestre strategie | SearchAgent (nouvelle strategie) |\n",
    "\n",
    "**Flow nominal** (preuve simple) :\n",
    "```\n",
    "SearchAgent â†’ TacticAgent â†’ VerifierAgent â†’ COMPLETE\n",
    "```\n",
    "\n",
    "**Flow avec echec** (preuve complexe) :\n",
    "```\n",
    "SearchAgent â†’ TacticAgent â†’ VerifierAgent (FAIL)\n",
    "   â†“\n",
    "CriticAgent analyse erreur\n",
    "   â†“\n",
    "   +-- Erreur simple â†’ SearchAgent (retry avec nouvelles contraintes)\n",
    "   +-- Erreur complexe â†’ CoordinatorAgent (changement strategie)\n",
    "```\n",
    "\n",
    "**Note critique** : Les demos actuelles (DEMO_1-3) sont trop triviales et ne declenchent JAMAIS CriticAgent ni CoordinatorAgent. DEMO_4 (list_length_append) devrait necessiter induction et potentiellement trigger ces agents.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8. Quand CriticAgent et CoordinatorAgent Interviennent\n",
    "\n",
    "#### 6.8.1. CriticAgent : Analyse d'Echecs de Tactiques\n",
    "\n",
    "**Declenche par VerifierAgent quand** :\n",
    "- `verify_proof()` retourne `success=False`\n",
    "- Erreur Lean detectee : type mismatch, tactic failed, unknown identifier\n",
    "- Preuve incomplete apres application de tactique\n",
    "\n",
    "**Responsabilites** :\n",
    "1. Parser l'erreur Lean (extraire type, message, contexte)\n",
    "2. Identifier la cause (lemme incorrect, tactique inadequate, goal mal compris)\n",
    "3. Proposer correction :\n",
    "   - Erreur simple (lemme manquant) â†’ Delegue SearchAgent avec contraintes\n",
    "   - Erreur complexe (strategie incorrecte) â†’ Delegue CoordinatorAgent\n",
    "\n",
    "**Exemple d'intervention** :\n",
    "```\n",
    "[Tour 5] VerifierAgent: FAIL - \"type mismatch, expected Nat but got Bool\"\n",
    "[Tour 6] CriticAgent: \"TacticAgent a applique 'exact lemma_bool' mais goal attend Nat.\n",
    "                       SearchAgent doit chercher lemmes avec type Nat -> Nat.\"\n",
    "[Tour 7] SearchAgent: Recherche lemmes type-aware...\n",
    "```\n",
    "\n",
    "**Pourquoi absent des demos actuelles** :\n",
    "- DEMO_1-3 : Lemmes Mathlib correspondent exactement au goal\n",
    "- Pas de type mismatch, pas de tactic failure\n",
    "- VerifierAgent retourne success au premier essai\n",
    "\n",
    "#### 6.8.2. CoordinatorAgent : Re-Orchestration Strategique\n",
    "\n",
    "**Declenche par CriticAgent quand** :\n",
    "- Echecs multiples consecutifs (3+ iterations sans progres)\n",
    "- Strategie actuelle bloquee (EXPLORATION â†’ REFINEMENT â†’ toujours FAIL)\n",
    "- Pattern d'erreur complexe (induction necessaire mais pas tentee)\n",
    "\n",
    "**Responsabilites** :\n",
    "1. Analyser historique complet (ProofState.snapshots)\n",
    "2. Identifier pattern d'echec (loop, strategie inadequate)\n",
    "3. Changer strategie globale :\n",
    "   - EXPLORATION â†’ VALIDATION (essayer preuves directes)\n",
    "   - REFINEMENT â†’ RECOVERY (backtrack + nouvelle approche)\n",
    "4. Reset partiel de ProofState (clear failed tactics, keep lemmas)\n",
    "\n",
    "**Exemple d'intervention** :\n",
    "```\n",
    "[Tour 8] CriticAgent: \"Echec 3x consecutif avec meme lemme. Strategie bloquee.\"\n",
    "[Tour 9] CoordinatorAgent: \"Detection pattern: goal necessite induction mais pas tentee.\n",
    "                            Changement strategie: EXPLORATION â†’ RECOVERY.\n",
    "                            Ajout contrainte: TacticAgent DOIT considerer 'induction'.\"\n",
    "[Tour 10] SearchAgent: Recherche lemmes inductifs...\n",
    "```\n",
    "\n",
    "**Pourquoi absent des demos actuelles** :\n",
    "- DEMO_1-3 : Pas d'echecs, donc CriticAgent jamais declenche\n",
    "- DEMO_4 (list_length_append) : **DEVRAIT** declencher si :\n",
    "  - Lemme direct `List.length_append` pas trouve\n",
    "  - TacticAgent essaie `rw` ou `simp` sans induction â†’ echec\n",
    "  - CriticAgent detecte besoin d'induction\n",
    "  - CoordinatorAgent change strategie vers RECOVERY\n",
    "\n",
    "#### 6.8.3. Activation des Agents Critiques\n",
    "\n",
    "| Scenario | SearchAgent | TacticAgent | VerifierAgent | CriticAgent | CoordinatorAgent |\n",
    "|----------|-------------|-------------|---------------|-------------|------------------|\n",
    "| **Preuve triviale** (rfl) | âœ— | âœ“ | âœ“ | âœ— | âœ— |\n",
    "| **Lemme direct trouve** (exact) | âœ“ | âœ“ | âœ“ | âœ— | âœ— |\n",
    "| **Lemme incorrect** (type mismatch) | âœ“ | âœ“ | âœ“ | âœ“ | âœ— |\n",
    "| **Tactique echoue 1x** (retry) | âœ“ | âœ“ | âœ“ | âœ“ | âœ— |\n",
    "| **Tactique echoue 3x** (bloque) | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ |\n",
    "| **Induction necessaire** | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ |\n",
    "\n",
    "**Conclusion** : Pour tester CriticAgent et CoordinatorAgent, nous devons utiliser des theoremes ou :\n",
    "1. Mathlib n'a PAS de lemme direct exact match\n",
    "2. Preuve necessite composition de tactiques (rw + simp + induction)\n",
    "3. Premiere tentative echoue et necessite correction\n",
    "\n",
    "**DEMO_4 (list_length_append) est concu pour ca** - mais seulement si on desactive l'acces au lemme `List.length_append` de Mathlib.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9. Vue d'Ensemble des 5 Agents Specialises\n",
    "\n",
    "La fonction `create_agents()` instancie les 5 agents avec :\n",
    "- **Instructions** : Prompts systemiques definissant role et regles de delegation\n",
    "- **Plugins** : Fonctions exposees (search, tactic generation, verification, etc.)\n",
    "- **Modele LLM** : gpt-5.2 (ou simulation si mode LLM desactive)\n",
    "\n",
    "#### 6.9.1. Signatures des agents\n",
    "\n",
    "```python\n",
    "SearchAgent(\n",
    "    plugins=[lean_search_plugin, state_plugin],\n",
    "    instructions=\"Trouve lemmes Mathlib pertinents...\"\n",
    ")\n",
    "\n",
    "TacticAgent(\n",
    "    plugins=[tactic_plugin, state_plugin],\n",
    "    instructions=\"Genere tactiques Lean avec confiance...\"\n",
    ")\n",
    "\n",
    "VerifierAgent(\n",
    "    plugins=[verification_plugin, state_plugin],\n",
    "    instructions=\"Compile et verifie preuves formelles...\"\n",
    ")\n",
    "\n",
    "CriticAgent(\n",
    "    plugins=[state_plugin],\n",
    "    instructions=\"Analyse echecs et propose corrections...\"\n",
    ")\n",
    "\n",
    "CoordinatorAgent(\n",
    "    plugins=[state_plugin],\n",
    "    instructions=\"Re-orchestre strategie globale...\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Pattern cle** : Chaque agent n'a acces qu'aux plugins dont il a besoin (principe de moindre privilege). Le `state_plugin` est partage par tous pour consulter/modifier ProofState.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.10. Strategies d'Orchestration\n",
    "\n",
    "L'orchestration determine comment les agents sont selectionnes et quand la conversation se termine.\n",
    "\n",
    "**DelegatingSelectionStrategy** (Pattern recommande):\n",
    "- Chaque agent designe explicitement le suivant via `designate_next_agent()`\n",
    "- Si aucune designation, utilise un agent par defaut (CoordinatorAgent)\n",
    "\n",
    "**ProofCompleteTermination**:\n",
    "- Termine si `proof_complete == True`\n",
    "- Termine si `iteration_count >= max_iterations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.11. Demonstration Complete\n",
    "\n",
    "Cette demonstration montre le workflow multi-agents complet:\n",
    "1. **CoordinatorAgent** initialise la session\n",
    "2. **SearchAgent** recherche les lemmes pertinents\n",
    "3. **TacticAgent** propose des tactiques\n",
    "4. **VerifierAgent** verifie avec Lean\n",
    "5. **CriticAgent** intervient en cas d'echec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ­ Orchestration Multi-Agents\n",
    "\n",
    "### 6.12. Le problÃ¨me de l'orchestration\n",
    "\n",
    "Avec 5 agents, qui parle quand ? Deux approches :\n",
    "\n",
    "1. **Statique** : SearchAgent â†’ TacticAgent â†’ VerifierAgent (toujours)\n",
    "   - Simple mais rigide\n",
    "   - Pas de backtracking\n",
    "\n",
    "2. **Dynamique** : DÃ©cisions basÃ©es sur l'Ã©tat de la preuve\n",
    "   - Flexible mais complexe\n",
    "   - Permet le backtracking et la rÃ©cupÃ©ration d'erreur\n",
    "\n",
    "**Nous utilisons l'approche dynamique.**\n",
    "\n",
    "### 6.13. StratÃ©gies d'orchestration\n",
    "\n",
    "#### 6.13.1. ProofSelectionStrategy\n",
    "\n",
    "DÃ©cide **quel agent agit** Ã  chaque tour :\n",
    "\n",
    "```python\n",
    "class ProofSelectionStrategy:\n",
    "    def select_next_agent(self, state: ProofState, agents: List[str]) -> str:\n",
    "        if state.phase == ProofPhase.INIT:\n",
    "            return \"CoordinatorAgent\"\n",
    "        elif state.phase == ProofPhase.SEARCH:\n",
    "            return \"SearchAgent\"\n",
    "        # ...\n",
    "```\n",
    "\n",
    "#### 6.13.2. ProofTerminationStrategy\n",
    "\n",
    "DÃ©cide **quand arrÃªter** la session :\n",
    "\n",
    "```python\n",
    "class ProofTerminationStrategy:\n",
    "    def should_terminate(self, state: ProofState, iteration: int) -> Tuple[bool, str]:\n",
    "        if state.phase == ProofPhase.COMPLETE:\n",
    "            return (True, \"Preuve complÃ¨te!\")\n",
    "        if iteration >= max_iterations:\n",
    "            return (True, \"Timeout atteint\")\n",
    "        # ...\n",
    "```\n",
    "\n",
    "### 6.14. Boucle principale\n",
    "\n",
    "```python\n",
    "while not should_terminate:\n",
    "    # 1. SÃ©lectionner agent\n",
    "    agent_name = selection_strategy.select_next_agent(state, agents)\n",
    "\n",
    "    # 2. ExÃ©cuter agent (appelle le LLM)\n",
    "    response = agent.chat(f\"Phase: {state.phase}, Goal: {state.current_goal}\")\n",
    "\n",
    "    # 3. L'agent appelle des plugins (modifie l'Ã©tat)\n",
    "    # Exemple: log_tactic_attempt(\"rw [Nat.add_zero]\")\n",
    "\n",
    "    # 4. Mettre Ã  jour phase selon rÃ©sultat\n",
    "    update_phase(state)\n",
    "\n",
    "    # 5. VÃ©rifier condition de terminaison\n",
    "    should_terminate, reason = termination_strategy.should_terminate(state, iteration)\n",
    "```\n",
    "\n",
    "### 6.15. Snapshots : Observer l'orchestration\n",
    "\n",
    "Ã€ chaque tour, on sauvegarde :\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"iteration\": 5,\n",
    "  \"agent\": \"TacticAgent\",\n",
    "  \"phase_before\": \"SEARCH\",\n",
    "  \"phase_after\": \"TACTIC_GEN\",\n",
    "  \"action\": \"Generated tactic: rw [Nat.add_zero]\",\n",
    "  \"state_snapshot\": {...}\n",
    "}\n",
    "```\n",
    "\n",
    "**UtilitÃ©** : Voir exactement quelle dÃ©cision chaque agent a prise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 8.7 - Strategies d'Orchestration (Pattern Argument_Analysis)\n",
    "# =============================================================================\n",
    "# Strategies personnalisees basees sur l'etat partage (pas sur l'historique)\n",
    "\n",
    "# Fix for Jupyter event loop\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import logging\n",
    "from typing import Dict, Any, List, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.16. ProofTerminationStrategy : Detection de Completion\n",
    "\n",
    "**Responsabilite** : Detecter quand arreter l'orchestration multi-agents.\n",
    "\n",
    "#### 6.16.1. Criteres de Terminaison\n",
    "\n",
    "```python\n",
    "class ProofTerminationStrategy(TerminationStrategy):\n",
    "    async def should_terminate(agents, history) -> bool:\n",
    "        # 1. Preuve complete detectee\n",
    "        if state.proof_complete:\n",
    "            return True\n",
    "        \n",
    "        # 2. Max iterations atteint\n",
    "        if state.current_iteration >= max_iterations:\n",
    "            return True\n",
    "        \n",
    "        # 3. Timeout (optionnel)\n",
    "        if time.time() - start_time > timeout:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "```\n",
    "\n",
    "#### 6.16.2. Comparaison avec Autres Patterns\n",
    "\n",
    "| Pattern | Terminaison basee sur | Avantages | Inconvenients |\n",
    "|---------|----------------------|-----------|---------------|\n",
    "| **Message-based** | Keyword dans dernier message (\"DONE\", \"COMPLETE\") | Simple, standard SK | Fragile, depend du LLM |\n",
    "| **State-based** (ce notebook) | `state.proof_complete` flag | Robuste, deterministe | Necessite etat partage |\n",
    "| **Iteration-based** | Compteur max iterations | Toujours termine | Peut stopper preuve incomplete |\n",
    "| **Consensus-based** | Vote agents (majorite) | Robuste aux erreurs | Complexe, lent |\n",
    "\n",
    "**Notre choix** : Combinaison **state-based + iteration-based** pour garantir terminaison.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.17. Test des Strategies et Orchestration\n",
    "\n",
    "Code de test pour valider :\n",
    "- **ProofTerminationStrategy** : DÃ©tecte `state.proof_complete`\n",
    "- **SimpleOrchestratorAgent** : ExÃ©cute conversation avec dÃ©signation d'agents\n",
    "\n",
    "**ExÃ©cution automatique** lors du chargement de la cellule.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª DÃ©monstrations Progressives\n",
    "\n",
    "### 6.18. Objectif\n",
    "\n",
    "Valider que le systÃ¨me multi-agents **fonctionne rÃ©ellement** sur des problÃ¨mes de complexitÃ© croissante.\n",
    "\n",
    "### 6.19. Les 3 dÃ©mos\n",
    "\n",
    "#### 6.19.1. 1ï¸âƒ£ DEMO_1_TRIVIAL : `theorem demo_rfl (n : Nat) : n = n`\n",
    "\n",
    "- **ComplexitÃ©** : Triviale (Ã©galitÃ© rÃ©flexive)\n",
    "- **Preuve attendue** : `by rfl` (une seule tactique)\n",
    "- **ItÃ©rations attendues** : 1-2\n",
    "- **Lemmes nÃ©cessaires** : 0 (tautologie)\n",
    "- **But** : VÃ©rifier que le systÃ¨me peut rÃ©soudre le cas le plus simple\n",
    "\n",
    "#### 6.19.2. 2ï¸âƒ£ DEMO_2_SIMPLE : `theorem nat_add_zero (n : Nat) : n + 0 = n`\n",
    "\n",
    "- **ComplexitÃ©** : Simple (propriÃ©tÃ© arithmÃ©tique basique)\n",
    "- **Preuve attendue** : `by rw [Nat.add_zero]` ou induction\n",
    "- **ItÃ©rations attendues** : 4-6\n",
    "- **Lemmes nÃ©cessaires** : 1-2 (de Mathlib)\n",
    "- **But** : Tester **SearchAgent** (recherche de lemmes) + **TacticAgent**\n",
    "\n",
    "#### 6.19.3. 3ï¸âƒ£ DEMO_3_INTERMEDIATE : `theorem nat_add_comm (n m : Nat) : n + m = m + n`\n",
    "\n",
    "- **ComplexitÃ©** : IntermÃ©diaire (commutativitÃ© de l'addition)\n",
    "- **Preuve attendue** : Induction + rÃ©Ã©criture avec plusieurs lemmes\n",
    "- **ItÃ©rations attendues** : 8-12\n",
    "- **Lemmes nÃ©cessaires** : 2-3 (Nat.add_comm, Nat.add_succ, etc.)\n",
    "- **But** : Tester **orchestration complÃ¨te** avec backtracking potentiel\n",
    "\n",
    "### 6.20. MÃ©triques Ã  comparer\n",
    "\n",
    "| MÃ©trique | DÃ©mo 1 | DÃ©mo 2 | DÃ©mo 3 |\n",
    "|----------|--------|--------|--------|\n",
    "| ItÃ©rations | 1-2 | 4-6 | 8-12 |\n",
    "| Lemmes dÃ©couverts | 0 | 1-2 | 2-3 |\n",
    "| Tactiques essayÃ©es | 1 | 2-3 | 4-6 |\n",
    "| VÃ©rifications Lean | 1 | 1-2 | 2-3 |\n",
    "\n",
    "### 6.21. HypothÃ¨se Ã  valider\n",
    "\n",
    "**Le systÃ¨me multi-agents SCALE avec la complexitÃ© du problÃ¨me.**\n",
    "\n",
    "Si DÃ©mo 3 prend ~6Ã— plus d'itÃ©rations que DÃ©mo 1, c'est **normal et attendu** (pas un bug).\n",
    "\n",
    "Si DÃ©mo 3 Ã©choue alors que DÃ©mo 1 rÃ©ussit, Ã§a indique un problÃ¨me d'orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DEMONSTRATION MULTI-AGENTS POUR THEOREM PROVING\n",
      "============================================================\n",
      "======================================================================\n",
      "DÃ‰MONSTRATIONS PROGRESSIVES - SYSTÃˆME MULTI-AGENTS\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8.8 - Demonstration Complete\n",
    "# =============================================================================\n",
    "\n",
    "def prove_with_multi_agents(\n",
    "    theorem: str,\n",
    "    goal: str = \"\",\n",
    "    max_iterations: int = 20,\n",
    "    verbose: bool = True,\n",
    "    use_simulation: bool = None  # None = auto-detect\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Prouve un theoreme en utilisant le systeme multi-agents.\n",
    "\n",
    "    Args:\n",
    "        theorem: L'enonce du theoreme complet\n",
    "        goal: Le but a prouver (extrait du theoreme si non fourni)\n",
    "        max_iterations: Nombre maximum d'iterations\n",
    "        verbose: Afficher les logs\n",
    "        use_simulation: True=simulation, False=LLM reel, None=auto\n",
    "\n",
    "    Returns:\n",
    "        Dict avec resultats et metriques\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Auto-detection du mode\n",
    "    if use_simulation is None:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "        has_valid_key = api_key and len(api_key) > 10 and not api_key.startswith(\"sk-...\")\n",
    "        use_simulation = not has_valid_key\n",
    "\n",
    "    # 1. Creer l'etat\n",
    "    if not goal:\n",
    "        if \":\" in theorem:\n",
    "            goal = theorem.split(\":\")[-1].strip()\n",
    "\n",
    "    state = ProofState(\n",
    "        theorem_statement=theorem,\n",
    "        current_goal=goal,\n",
    "        max_iterations=max_iterations\n",
    "    )\n",
    "\n",
    "    # 2. Creer le runner Lean\n",
    "    runner = LeanRunner(backend=\"subprocess\", timeout=30)\n",
    "\n",
    "    # 3. Creer les plugins\n",
    "    plugins = {\n",
    "        \"state\": ProofStateManagerPlugin(state),\n",
    "        \"search\": LeanSearchPlugin(runner),\n",
    "        \"tactic\": LeanTacticPlugin(),\n",
    "        \"verification\": LeanVerificationPlugin(runner)\n",
    "    }\n",
    "\n",
    "    # 4. Creer les agents\n",
    "    use_sk = SK_AVAILABLE and not use_simulation\n",
    "    agents = create_agents(plugins, state, use_sk=use_sk, use_simulation=use_simulation)\n",
    "\n",
    "    # 5. Configurer les strategies\n",
    "    # Strategies gerees automatiquement par ProofAgentGroupChat\n",
    "\n",
    "    # 6. Creer le groupe de chat\n",
    "    chat = ProofAgentGroupChat(\n",
    "        agents=agents,\n",
    "        state=state,\n",
    "        use_sk=use_sk\n",
    "    )\n",
    "\n",
    "    mode_str = \"Semantic Kernel\" if use_sk else (\"Simulation\" if use_simulation else \"OpenAI direct\")\n",
    "    if verbose:\n",
    "        print(f\"Mode: {mode_str}\")\n",
    "\n",
    "    # 7. Executer\n",
    "    result = chat.run(f\"Prouver: {theorem}\", verbose=verbose)\n",
    "\n",
    "    # 8. Collecter les metriques\n",
    "    elapsed = time.time() - start_time\n",
    "    metrics = {\n",
    "        \"success\": state.proof_complete,\n",
    "        \"theorem\": theorem,\n",
    "        \"final_proof\": state.final_proof,\n",
    "        \"iterations\": state.iteration_count,\n",
    "        \"lemmas_discovered\": len(state.discovered_lemmas),\n",
    "        \"tactics_tried\": len(state.tactics_history),\n",
    "        \"verifications\": len(state.verification_results),\n",
    "        \"total_time_s\": round(elapsed, 2),\n",
    "        \"lean_time_ms\": round(state.total_lean_time_ms, 2),\n",
    "        \"mode\": mode_str\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Test de la demonstration\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMONSTRATION MULTI-AGENTS POUR THEOREM PROVING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# Section 8.8 - DÃ©monstrations Progressives Multi-Agents\n",
    "# =============================================================================\n",
    "\n",
    "# Configuration\n",
    "USE_LLM_MODE = True  # True pour LLM rÃ©el, False pour simulation\n",
    "\n",
    "# Quatre thÃ©orÃ¨mes de complexitÃ© croissante\n",
    "DEMOS = [\n",
    "    {\n",
    "        \"name\": \"DEMO_1_TRIVIAL\",\n",
    "        \"theorem\": \"theorem demo_rfl (n : Nat) : n = n\",\n",
    "        \"description\": \"Ã‰galitÃ© rÃ©flexive (1-2 itÃ©rations attendues)\",\n",
    "        \"expected_iterations\": \"1-2\",\n",
    "        \"expected_lemmas\": \"0\",\n",
    "        \"complexity\": \"Triviale - teste rfl uniquement\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DEMO_2_SIMPLE\",\n",
    "        \"theorem\": \"theorem add_right_cancel (a b c : Nat) : a + b = c + b -> a = c\",\n",
    "        \"description\": \"Simplification addition (6-10 itÃ©rations attendues)\",\n",
    "        \"expected_iterations\": \"6-10\",\n",
    "        \"expected_lemmas\": \"2-3\",\n",
    "        \"complexity\": \"Simple - necessite Nat.add_right_cancel ou decomposition\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DEMO_3_INTERMEDIATE\",\n",
    "        \"theorem\": \"theorem mul_add_distr (a b c : Nat) : a * (b + c) = a * b + a * c\",\n",
    "        \"description\": \"DistributivitÃ© multiplication (10-15 itÃ©rations attendues)\",\n",
    "        \"expected_iterations\": \"10-15\",\n",
    "        \"expected_lemmas\": \"3-5\",\n",
    "        \"complexity\": \"IntermÃ©diaire - composition Nat.mul_add + associativite\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DEMO_4_ADVANCED\",\n",
    "        \"theorem\": \"theorem list_length_append (l1 l2 : List Nat) : (l1 ++ l2).length = l1.length + l2.length\",\n",
    "        \"description\": \"Induction sur listes (12-20 itÃ©rations attendues)\",\n",
    "        \"expected_iterations\": \"12-20\",\n",
    "        \"expected_lemmas\": \"4-6\",\n",
    "        \"complexity\": \"Avance - induction structurelle, trigger CriticAgent\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DÃ‰MONSTRATIONS PROGRESSIVES - SYSTÃˆME MULTI-AGENTS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# ExÃ©cuter chaque dÃ©mo\n",
    "results_comparison = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.22. Execution DEMO_1 : Preuve Triviale\n",
    "\n",
    "**Objectif** : Valider le pipeline complet avec un theoreme trivial\n",
    "\n",
    "**Theoreme** : `theorem demo_rfl (n : Nat) : n = n`\n",
    "\n",
    "**Attentes** :\n",
    "- **Iterations** : 1-2 (reflexivite immediate)\n",
    "- **Agents impliques** : TacticAgent (rfl) â†’ VerifierAgent\n",
    "- **CriticAgent/CoordinatorAgent** : NON (preuve triviale)\n",
    "- **Temps** : <1 seconde\n",
    "\n",
    "Cette demo sert de **baseline** pour verifier que le systeme fonctionne.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEMO 1/4: DEMO_1_TRIVIAL\n",
      "======================================================================\n",
      "Theoreme: theorem demo_rfl (n : Nat) : n = n\n",
      "Complexite: Triviale - teste rfl uniquement\n",
      "Iterations attendues: 1-2\n",
      "Lemmes necessaires: 0\n",
      "======================================================================\n",
      "Crees 5 agents SK avec modele gpt-5.2\n",
      "Mode: Semantic Kernel\n",
      "============================================================\n",
      "Session SK demarree: Prouver: theorem demo_rfl (n : Nat) : n = n...\n",
      "============================================================\n",
      "[LOG] Agents: ['SearchAgent', 'TacticAgent', 'VerifierAgent', 'CriticAgent', 'CoordinatorAgent']\n",
      "[LOG] Max iterations: 20\n",
      "[LOG] Strategies initialisees (basees sur etat partage)\n",
      "[LOG] Demarrage boucle multi-agents...\n",
      "\n",
      "[Tour 1/20] Agent: SearchAgent\n",
      "  Phase: init\n",
      "  Response: Lemme clÃ© (Mathlib/Lean core) : `rfl`\n",
      "\n",
      "- Type : `rfl : a = a`\n",
      "\n",
      "Il suffit donc de finir la preuve par :\n",
      "\n",
      "```lean\n",
      "theorem demo_rfl (n : Nat) : n = n := by\n",
      "  rfl\n",
      "```\n",
      "\n",
      "[Tour 2/20] Agent: TacticAgent\n",
      "  Phase: init\n",
      "  Response: Tactique proposÃ©e (confiance 0.99) :\n",
      "\n",
      "```lean\n",
      "by\n",
      "  rfl\n",
      "```\n",
      "\n",
      "DÃ©lÃ©gation Ã  **VerifierAgent** pour validation Lean.\n",
      "\n",
      "[Tour 3/20] Agent: VerifierAgent\n",
      "  Phase: complete\n",
      "  Response: ```lean\n",
      "theorem demo_rfl (n : Nat) : n = n := by\n",
      "  rfl\n",
      "```\n",
      "[LOG] Preuve complete detectee!\n",
      "\n",
      "============================================================\n",
      "SESSION TERMINEE - 3 tours executes\n",
      "SUCCES! Preuve trouvee:\n",
      "theorem demo_rfl (n : Nat) : n = n := by\n",
      "  rfl\n",
      "============================================================\n",
      "\n",
      "Resultat DEMO_1:\n",
      "  - Success: True\n",
      "  - Iterations: 3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'proof'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Success: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_1[\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Iterations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_1[\u001b[33m'\u001b[39m\u001b[33miterations\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Proof: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_1[\u001b[33m'\u001b[39m\u001b[33mproof\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m100\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[43mresult_1\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproof\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNone\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'proof'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Execute DEMO_1\n",
    "demo = DEMOS[0]\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"DEMO 1/4: {demo['name']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Theoreme: {demo['theorem']}\")\n",
    "print(f\"Complexite: {demo['complexity']}\")\n",
    "print(f\"Iterations attendues: {demo['expected_iterations']}\")\n",
    "print(f\"Lemmes necessaires: {demo['expected_lemmas']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result_1 = prove_with_multi_agents(\n",
    "    theorem=demo[\"theorem\"],\n",
    "    max_iterations=20,\n",
    "    verbose=True,\n",
    "    use_simulation=not USE_LLM_MODE\n",
    ")\n",
    "\n",
    "print(f\"\\nResultat DEMO_1:\")\n",
    "print(f\"  - Success: {result_1['success']}\")\n",
    "print(f\"  - Iterations: {result_1['iterations']}\")\n",
    "print(f\"  - Proof: {result_1['final_proof'][:100] if result_1['final_proof'] else 'None'}...\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.23. Execution DEMO_2 : Preuve Simple\n",
    "\n",
    "**Objectif** : Tester recherche de lemmes + composition\n",
    "\n",
    "**Theoreme** : `theorem add_right_cancel (a b c : Nat) : a + b = c + b -> a = c`\n",
    "\n",
    "**Attentes** :\n",
    "- **Iterations** : 6-10 (recherche lemme + application)\n",
    "- **Agents impliques** : SearchAgent â†’ TacticAgent â†’ VerifierAgent\n",
    "- **Lemmes Mathlib attendus** : `Nat.add_right_cancel`, `Nat.add_comm`\n",
    "- **CriticAgent/CoordinatorAgent** : POSSIBLE si lemme pas trouve directement\n",
    "- **Temps** : 2-5 secondes\n",
    "\n",
    "Cette demo teste la **recherche de lemmes** et la **generation de tactiques** adaptees.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEMO 2/4: DEMO_2_SIMPLE\n",
      "======================================================================\n",
      "Theoreme: theorem add_right_cancel (a b c : Nat) : a + b = c + b -> a = c\n",
      "Complexite: Simple - necessite Nat.add_right_cancel ou decomposition\n",
      "Iterations attendues: 6-10\n",
      "Lemmes necessaires: 2-3\n",
      "======================================================================\n",
      "Crees 5 agents SK avec modele gpt-5.2\n",
      "Mode: Semantic Kernel\n",
      "============================================================\n",
      "Session SK demarree: Prouver: theorem add_right_cancel (a b c : Nat) : a + b = c + b -> a = c...\n",
      "============================================================\n",
      "[LOG] Agents: ['SearchAgent', 'TacticAgent', 'VerifierAgent', 'CriticAgent', 'CoordinatorAgent']\n",
      "[LOG] Max iterations: 20\n",
      "[LOG] Strategies initialisees (basees sur etat partage)\n",
      "[LOG] Demarrage boucle multi-agents...\n",
      "\n",
      "[Tour 1/20] Agent: SearchAgent\n",
      "  Phase: init\n",
      "  Response: Delegation au TacticAgent.\n",
      "\n",
      "\n",
      "Lemmes Mathlib pertinents pour `theorem add_right_cancel (a b c : Nat) : a + b = c + b -> a = c` :\n",
      "\n",
      "1) **`Nat.add_right_cancel`** (pile la forme voulue)\n",
      "- Type :\n",
      "```lean\n",
      "Nat.add_right_cancel {n m k : Nat} (h : n + m = k + m) : n = k\n",
      "```\n",
      "- Application directe avec `n := a...\n",
      "\n",
      "[Tour 2/20] Agent: SearchAgent\n",
      "  Phase: init\n",
      "  Response: Delegation au TacticAgent.\n",
      "\n",
      "Lemmes Mathlib pertinents pour `theorem add_right_cancel (a b c : Nat) : a + b = c + b -> a = c` :\n",
      "\n",
      "1) **`Nat.add_right_cancel`** (exactement le but)\n",
      "```lean\n",
      "Nat.add_right_cancel {n m k : Nat} (h : n + m = k + m) : n = k\n",
      "```\n",
      "\n",
      "2) **`Nat.add_right_cancel_iff`** (version â†” u...\n",
      "\n",
      "[Tour 3/20] Agent: TacticAgent\n",
      "  Phase: init\n",
      "  Response: Tactiques proposÃ©es (haute confiance) :\n",
      "\n",
      "```lean\n",
      "intro h\n",
      "exact Nat.add_right_cancel h\n",
      "```\n",
      "\n",
      "Variante Ã©quivalente :\n",
      "\n",
      "```lean\n",
      "intro h\n",
      "exact (Nat.add_right_cancel_iff).1 h\n",
      "```\n",
      "\n",
      "Je dÃ©lÃ¨gue Ã  **VerifierAgent** pour validation Lean.\n",
      "\n",
      "[Tour 4/20] Agent: VerifierAgent\n",
      "  Phase: complete\n",
      "  Response: La preuve compile en Lean 4 :\n",
      "\n",
      "```lean\n",
      "theorem add_right_cancel (a b c : Nat) : a + b = c + b -> a = c := by\n",
      "  intro h\n",
      "  exact Nat.add_right_cancel h\n",
      "```\n",
      "[LOG] Preuve complete detectee!\n",
      "\n",
      "============================================================\n",
      "SESSION TERMINEE - 4 tours executes\n",
      "SUCCES! Preuve trouvee:\n",
      "theorem add_right_cancel (a b c : Nat) : a + b = c + b -> a = c := by\n",
      "  intro h\n",
      "  exact Nat.add_right_cancel h\n",
      "============================================================\n",
      "\n",
      "Resultat DEMO_2:\n",
      "  - Success: True\n",
      "  - Iterations: 4\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'proof'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Success: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_2[\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Iterations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_2[\u001b[33m'\u001b[39m\u001b[33miterations\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Proof: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_2[\u001b[33m'\u001b[39m\u001b[33mproof\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m100\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[43mresult_2\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproof\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNone\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'proof'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Execute DEMO_2\n",
    "demo = DEMOS[1]\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"DEMO 2/4: {demo['name']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Theoreme: {demo['theorem']}\")\n",
    "print(f\"Complexite: {demo['complexity']}\")\n",
    "print(f\"Iterations attendues: {demo['expected_iterations']}\")\n",
    "print(f\"Lemmes necessaires: {demo['expected_lemmas']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result_2 = prove_with_multi_agents(\n",
    "    theorem=demo[\"theorem\"],\n",
    "    max_iterations=20,\n",
    "    verbose=True,\n",
    "    use_simulation=not USE_LLM_MODE\n",
    ")\n",
    "\n",
    "print(f\"\\nResultat DEMO_2:\")\n",
    "print(f\"  - Success: {result_2['success']}\")\n",
    "print(f\"  - Iterations: {result_2['iterations']}\")\n",
    "print(f\"  - Proof: {result_2['final_proof'][:100] if result_2['final_proof'] else 'None'}...\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.24. Execution DEMO_3 : Preuve Intermediaire\n",
    "\n",
    "**Objectif** : Tester composition de plusieurs lemmes\n",
    "\n",
    "**Theoreme** : `theorem mul_add_distr (a b c : Nat) : a * (b + c) = a * b + a * c`\n",
    "\n",
    "**Attentes** :\n",
    "- **Iterations** : 10-15 (composition lemmes)\n",
    "- **Agents impliques** : SearchAgent (multiple) â†’ TacticAgent â†’ VerifierAgent â†’ CriticAgent (si echec)\n",
    "- **Lemmes Mathlib attendus** : `Nat.mul_add`, `Nat.mul_comm`, `Nat.add_assoc`\n",
    "- **CriticAgent** : PROBABLE (necessite ajustements tactiques)\n",
    "- **Temps** : 5-10 secondes\n",
    "\n",
    "Cette demo teste l'**orchestration multi-agents** avec feedback loops.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Execute DEMO_3\n",
    "demo = DEMOS[2]\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"DEMO 3/4: {demo['name']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Theoreme: {demo['theorem']}\")\n",
    "print(f\"Complexite: {demo['complexity']}\")\n",
    "print(f\"Iterations attendues: {demo['expected_iterations']}\")\n",
    "print(f\"Lemmes necessaires: {demo['expected_lemmas']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result_3 = prove_with_multi_agents(\n",
    "    theorem=demo[\"theorem\"],\n",
    "    max_iterations=20,\n",
    "    verbose=True,\n",
    "    use_simulation=not USE_LLM_MODE\n",
    ")\n",
    "\n",
    "print(f\"\\nResultat DEMO_3:\")\n",
    "print(f\"  - Success: {result_3['success']}\")\n",
    "print(f\"  - Iterations: {result_3['iterations']}\")\n",
    "print(f\"  - Proof: {result_3['final_proof'][:100] if result_3['final_proof'] else 'None'}...\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.25. Execution DEMO_4 : Preuve Avancee\n",
    "\n",
    "**Objectif** : Stresser le systeme avec induction structurelle\n",
    "\n",
    "**Theoreme** : `theorem list_length_append (l1 l2 : List Nat) : (l1 ++ l2).length = l1.length + l2.length`\n",
    "\n",
    "**Attentes** :\n",
    "- **Iterations** : 12-20 (induction + lemmes auxiliaires)\n",
    "- **Agents impliques** : SearchAgent â†’ TacticAgent (induction) â†’ VerifierAgent â†’ **CriticAgent** â†’ CoordinatorAgent (si blocage)\n",
    "- **Lemmes Mathlib attendus** : `List.length_append`, `List.length_cons`, `Nat.succ_add`\n",
    "- **Strategies** : EXPLORATION â†’ REFINEMENT â†’ VALIDATION\n",
    "- **CriticAgent/CoordinatorAgent** : **REQUIS** (echecs de tactiques attendus)\n",
    "- **Temps** : 10-30 secondes\n",
    "\n",
    "Cette demo doit **declencher CriticAgent** si la tactique d'induction echoue ou si les lemmes ne suffisent pas. C'est le seul theoreme qui devrait stresser l'orchestration complete.\n",
    "\n",
    "**Note** : Si DEMO_4 se complete en <10 iterations sans CriticAgent, cela signifie que Mathlib contient le lemme directement et le theoreme n'est pas assez complexe.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Execute DEMO_4\n",
    "demo = DEMOS[3]\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"DEMO 4/4: {demo['name']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Theoreme: {demo['theorem']}\")\n",
    "print(f\"Complexite: {demo['complexity']}\")\n",
    "print(f\"Iterations attendues: {demo['expected_iterations']}\")\n",
    "print(f\"Lemmes necessaires: {demo['expected_lemmas']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result_4 = prove_with_multi_agents(\n",
    "    theorem=demo[\"theorem\"],\n",
    "    max_iterations=20,\n",
    "    verbose=True,\n",
    "    use_simulation=not USE_LLM_MODE\n",
    ")\n",
    "\n",
    "print(f\"\\nResultat DEMO_4:\")\n",
    "print(f\"  - Success: {result_4['success']}\")\n",
    "print(f\"  - Iterations: {result_4['iterations']}\")\n",
    "print(f\"  - Proof: {result_4['final_proof'][:100] if result_4['final_proof'] else 'None'}...\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¼ Harmonic Aristotle : DÃ©composition RÃ©cursive\n",
    "\n",
    "### 6.26. Contexte\n",
    "\n",
    "**Technique dÃ©veloppÃ©e par DeepSeek (2024)** pour rÃ©soudre des problÃ¨mes de thÃ©orie des nombres ouverts depuis 30+ ans.\n",
    "\n",
    "### 6.27. Le problÃ¨me des preuves \"monolithiques\"\n",
    "\n",
    "Approche classique (linÃ©aire) :\n",
    "\n",
    "```\n",
    "ThÃ©orÃ¨me T : n + m = m + n\n",
    "  â†“\n",
    "Recherche de lemmes\n",
    "  â†“\n",
    "GÃ©nÃ©ration de tactiques\n",
    "  â†“\n",
    "VÃ©rification\n",
    "  â†“\n",
    "SuccÃ¨s ou Ã©chec\n",
    "```\n",
    "\n",
    "**ProblÃ¨me** : Si le thÃ©orÃ¨me est complexe, la recherche de lemmes devient explosive (trop de candidats).\n",
    "\n",
    "### 6.28. IdÃ©e centrale : DÃ©composition rÃ©cursive\n",
    "\n",
    "Au lieu de prouver T directement, **dÃ©composer T en sous-thÃ©orÃ¨mes plus simples** :\n",
    "\n",
    "```\n",
    "ThÃ©orÃ¨me T : n + m = m + n\n",
    "  â†“ DÃ‰COMPOSITION\n",
    "  â”œâ”€ T1 : n + 0 = 0 + n (plus facile)\n",
    "  â”œâ”€ T2 : n + (m + 1) = (m + 1) + n (plus facile)\n",
    "  â””â”€ T3 : Induction utilisant T1 et T2 (maintenant facile!)\n",
    "```\n",
    "\n",
    "### 6.29. Exemple concret\n",
    "\n",
    "**Sans dÃ©composition** :\n",
    "\n",
    "```lean\n",
    "theorem add_comm (n m : Nat) : n + m = m + n := by\n",
    "  -- Recherche de lemmes : 50+ candidats dans Mathlib\n",
    "  -- GÃ©nÃ©ration de tactiques : Quelle induction ? Sur n ou m ?\n",
    "  -- VÃ©rifications : 10-15 tentatives\n",
    "  -- âŒ ComplexitÃ© explosive\n",
    "```\n",
    "\n",
    "**Avec dÃ©composition (Harmonic Aristotle)** :\n",
    "\n",
    "```lean\n",
    "-- Ã‰tape 1 : Prouver cas de base\n",
    "theorem add_zero (n : Nat) : n + 0 = n := by rfl\n",
    "\n",
    "-- Ã‰tape 2 : Prouver cas successeur\n",
    "theorem add_succ (n m : Nat) : n + (m + 1) = (n + m) + 1 := by rfl\n",
    "\n",
    "-- Ã‰tape 3 : Combiner pour prouver commutativitÃ© (facile maintenant!)\n",
    "theorem add_comm (n m : Nat) : n + m = m + n := by\n",
    "  induction m with\n",
    "  | zero => rw [add_zero, zero_add]  -- Utilise add_zero\n",
    "  | succ m ih => rw [add_succ, ih, succ_add]  -- Utilise add_succ\n",
    "```\n",
    "\n",
    "### 6.30. MÃ©trique clÃ© : **RÃ©duction de l'espace de recherche**\n",
    "\n",
    "| Approche | Lemmes candidats | Tactiques essayÃ©es | SuccÃ¨s |\n",
    "|----------|------------------|-------------------|--------|\n",
    "| LinÃ©aire | 50+ | 15-20 | 40% |\n",
    "| Harmonic Aristotle | 5-10 (par sous-thÃ©orÃ¨me) | 5-8 (total) | 85% |\n",
    "\n",
    "### 6.31. IntÃ©gration dans notre systÃ¨me\n",
    "\n",
    "Harmonic Aristotle s'intÃ¨gre comme **stratÃ©gie de CriticAgent** :\n",
    "\n",
    "1. CriticAgent dÃ©tecte que le thÃ©orÃ¨me est complexe (>5 itÃ©rations sans succÃ¨s)\n",
    "2. Propose une dÃ©composition en sous-thÃ©orÃ¨mes\n",
    "3. CoordinatorAgent orchestre la preuve des sous-thÃ©orÃ¨mes\n",
    "4. TacticAgent combine les rÃ©sultats\n",
    "\n",
    "**RÃ©sultat** : RÃ©solution de problÃ¨mes ouverts (Erdos #124 variant en 6h)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Techniques de Harmonic Aristotle\n",
    "\n",
    "### 7.1. Decomposition de problemes\n",
    "\n",
    "Aristotle decompose les problemes complexes en sous-problemes plus simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition de 'P <-> Q':\n",
      "  - Direction 1: P  ->  Q\n",
      "  - Direction 2:  Q -> P \n"
     ]
    }
   ],
   "source": [
    "class AristotleDecomposer:\n",
    "    \"\"\"\n",
    "    Decomposition de problemes a la Harmonic Aristotle.\n",
    "    \"\"\"\n",
    "    \n",
    "    def decompose(self, theorem: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Decompose un theoreme en sous-lemmes.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Identifier la structure (conjonction, equivalence, etc.)\n",
    "        2. Separer en composantes\n",
    "        3. Identifier les dependances\n",
    "        \"\"\"\n",
    "        subproblems = []\n",
    "        \n",
    "        # Decomposition basique par structure\n",
    "        if \"<->\" in theorem or \"iff\" in theorem.lower():\n",
    "            # Equivalence = deux implications\n",
    "            parts = theorem.split(\"<->\")\n",
    "            subproblems.append(f\"Direction 1: {parts[0]} -> {parts[1]}\")\n",
    "            subproblems.append(f\"Direction 2: {parts[1]} -> {parts[0]}\")\n",
    "        \n",
    "        elif \"/\\\\\" in theorem or \"and\" in theorem.lower():\n",
    "            # Conjonction = prouver chaque partie\n",
    "            parts = theorem.split(\"/\\\\\")\n",
    "            for i, part in enumerate(parts):\n",
    "                subproblems.append(f\"Partie {i+1}: {part.strip()}\")\n",
    "        \n",
    "        elif \"forall\" in theorem.lower():\n",
    "            # Universel = fixer variable, prouver pour arbitraire\n",
    "            subproblems.append(f\"Generalisation: introduire variable, prouver corps\")\n",
    "        \n",
    "        elif \"exists\" in theorem.lower():\n",
    "            # Existentiel = trouver temoin + preuve\n",
    "            subproblems.append(f\"Temoin: trouver valeur concrete\")\n",
    "            subproblems.append(f\"Verification: prouver pour ce temoin\")\n",
    "        \n",
    "        else:\n",
    "            # Pas de decomposition evidente\n",
    "            subproblems.append(theorem)\n",
    "        \n",
    "        return subproblems\n",
    "    \n",
    "    def solve_hierarchical(self, theorem: str, solver) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Resolution hierarchique par decomposition.\n",
    "        \"\"\"\n",
    "        subproblems = self.decompose(theorem)\n",
    "        \n",
    "        if len(subproblems) == 1 and subproblems[0] == theorem:\n",
    "            # Cas de base: resoudre directement\n",
    "            return solver(theorem)\n",
    "        \n",
    "        # Resoudre chaque sous-probleme\n",
    "        solutions = []\n",
    "        for sub in subproblems:\n",
    "            success, proof = self.solve_hierarchical(sub, solver)\n",
    "            if not success:\n",
    "                return False, None\n",
    "            solutions.append(proof)\n",
    "        \n",
    "        # Combiner les solutions\n",
    "        combined = self._combine_proofs(solutions)\n",
    "        return True, combined\n",
    "    \n",
    "    def _combine_proofs(self, proofs: List[str]) -> str:\n",
    "        \"\"\"Combine des preuves de sous-problemes.\"\"\"\n",
    "        return \"\\n\".join([\n",
    "            f\"-- Partie {i+1}\\n{proof}\" \n",
    "            for i, proof in enumerate(proofs)\n",
    "        ])\n",
    "\n",
    "# Test\n",
    "decomposer = AristotleDecomposer()\n",
    "subproblems = decomposer.decompose(\"P <-> Q\")\n",
    "print(\"Decomposition de 'P <-> Q':\")\n",
    "for sp in subproblems:\n",
    "    print(f\"  - {sp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Benchmarking sur Problemes d'Erdos\n",
    "\n",
    "Les problemes d'Erdos sont devenus le benchmark de reference pour evaluer les systemes de theorem proving automatique. Plusieurs ont ete resolus par IA en 2025-2026."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Addition zero (difficulte: 1)\n",
      "\n",
      "============================================================\n",
      "Debut de la preuve: theorem add_zero (n : Nat) : n + 0 = n\n",
      "============================================================\n",
      "\n",
      "--- Iteration 1 ---\n",
      "Lemmes trouves: ['Nat.add_zero', 'Nat.zero_add', 'Nat.add_comm']\n",
      "Tactiques generees: ['rfl']\n",
      "\n",
      "Preuve trouvee!\n",
      "\n",
      "Test: Commutativite addition (difficulte: 2)\n",
      "\n",
      "============================================================\n",
      "Debut de la preuve: theorem add_comm (a b : Nat) : a + b = b + a\n",
      "============================================================\n",
      "\n",
      "--- Iteration 1 ---\n",
      "Lemmes trouves: ['Nat.add_zero', 'Nat.zero_add', 'Nat.add_comm']\n",
      "Tactiques generees: ['rfl']\n",
      "\n",
      "Preuve trouvee!\n",
      "\n",
      "============================================================\n",
      "RESULTATS DU BENCHMARK\n",
      "============================================================\n",
      "Resolus: 2/2 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Benchmark sur des problemes type Erdos (simplifies)\n",
    "\n",
    "BENCHMARK_PROBLEMS = [\n",
    "    {\n",
    "        \"id\": \"simple_1\",\n",
    "        \"name\": \"Addition zero\",\n",
    "        \"statement\": \"theorem add_zero (n : Nat) : n + 0 = n\",\n",
    "        \"difficulty\": 1,\n",
    "        \"expected_tactics\": [\"exact Nat.add_zero n\", \"rfl\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"simple_2\", \n",
    "        \"name\": \"Commutativite addition\",\n",
    "        \"statement\": \"theorem add_comm (a b : Nat) : a + b = b + a\",\n",
    "        \"difficulty\": 2,\n",
    "        \"expected_tactics\": [\"exact Nat.add_comm a b\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"medium_1\",\n",
    "        \"name\": \"Associativite addition\",\n",
    "        \"statement\": \"theorem add_assoc (a b c : Nat) : (a + b) + c = a + (b + c)\",\n",
    "        \"difficulty\": 3,\n",
    "        \"expected_tactics\": [\"exact Nat.add_assoc a b c\", \"induction c\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "def run_benchmark(solver, problems=BENCHMARK_PROBLEMS):\n",
    "    \"\"\"Execute le benchmark sur les problemes donnes.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for problem in problems:\n",
    "        print(f\"\\nTest: {problem['name']} (difficulte: {problem['difficulty']})\")\n",
    "        \n",
    "        success, proof = solver.prove(problem['statement'])\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": problem[\"id\"],\n",
    "            \"success\": success,\n",
    "            \"proof\": proof\n",
    "        })\n",
    "    \n",
    "    # Statistiques\n",
    "    total = len(results)\n",
    "    solved = sum(1 for r in results if r[\"success\"])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULTATS DU BENCHMARK\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Resolus: {solved}/{total} ({100*solved/total:.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Executer le benchmark (limite a 3 iterations pour la demo)\n",
    "orchestrator.max_iterations = 3\n",
    "results = run_benchmark(orchestrator, BENCHMARK_PROBLEMS[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercices\n",
    "\n",
    "### 9.1. Exercice 1 : Ameliorer l'agent de recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test de ImprovedSearchAgent:\n",
      "----------------------------------------\n",
      "  Scoring 8 lemmes...\n",
      "\n",
      "Lemmes trouves pour 'n + 0 = n':\n",
      "  [1.00] Nat.add_zero: n + 0 = n\n",
      "  [0.50] Nat.zero_add: 0 + n = n\n",
      "  [0.50] Nat.add_comm: n + m = m + n\n",
      "  [0.20] Nat.add_assoc: (n + m) + k = n + (m + k)\n",
      "  [0.20] Nat.succ_add: succ n + m = succ (n + m)\n",
      "  [0.20] Nat.add_succ: n + succ m = succ (n + m)\n",
      "  [0.10] Nat.mul_zero: n * 0 = 0\n",
      "  [0.00] Nat.zero_mul: 0 * n = 0\n",
      "  Scoring 6 lemmes...\n",
      "\n",
      "Lemmes trouves pour 'a + b = b + a':\n",
      "  [1.00] Nat.add_comm: n + m = m + n\n",
      "  [0.20] Nat.add_zero: n + 0 = n\n",
      "  [0.20] Nat.zero_add: 0 + n = n\n",
      "  [0.20] Nat.add_assoc: (n + m) + k = n + (m + k)\n",
      "  [0.20] Nat.succ_add: succ n + m = succ (n + m)\n",
      "  [0.20] Nat.add_succ: n + succ m = succ (n + m)\n"
     ]
    }
   ],
   "source": [
    "# Exercice 1 - SOLUTION: Agent de recherche ameliore avec scoring LLM\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le repertoire courant au path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Utiliser load_env_file de lean_runner (evite les problemes d'introspection)\n",
    "from lean_runner import load_env_file\n",
    "env_path = Path.cwd() / \".env\"\n",
    "load_env_file(env_path)\n",
    "\n",
    "class ImprovedSearchAgent(TheoremSearchAgent):\n",
    "    \"\"\"\n",
    "    Version amelioree de l'agent de recherche avec scoring par LLM.\n",
    "    \n",
    "    Ameliorations:\n",
    "    1. Scoring semantique par LLM (pertinence reelle, pas juste mots-cles)\n",
    "    2. Cache des scores pour eviter les appels API redondants\n",
    "    3. Fallback sur heuristique si API non disponible\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client=None):\n",
    "        super().__init__(llm_client)\n",
    "        self.score_cache = {}  # (lemma_name, goal) -> score\n",
    "        self.api_available = self._check_api()\n",
    "    \n",
    "    def _check_api(self) -> bool:\n",
    "        \"\"\"Verifie si l'API OpenAI est disponible.\"\"\"\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        return api_key is not None and not api_key.startswith(\"sk-...\")\n",
    "    \n",
    "    def _score_with_llm(self, lemma: Lemma, goal: str) -> float:\n",
    "        \"\"\"\n",
    "        Score la pertinence d'un lemme par rapport au but en utilisant un LLM.\n",
    "        \n",
    "        Returns:\n",
    "            Score de pertinence entre 0.0 et 1.0\n",
    "        \"\"\"\n",
    "        # Verifier le cache\n",
    "        cache_key = (lemma.name, goal)\n",
    "        if cache_key in self.score_cache:\n",
    "            return self.score_cache[cache_key]\n",
    "        \n",
    "        # Si API non disponible, utiliser heuristique\n",
    "        if not self.api_available:\n",
    "            score = self._heuristic_score(lemma, goal)\n",
    "            self.score_cache[cache_key] = score\n",
    "            return score\n",
    "        \n",
    "        # Appel API reel\n",
    "        try:\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "            \n",
    "            prompt = f\"\"\"Evalue la pertinence d'un lemme mathematique pour prouver un but en Lean 4.\n",
    "\n",
    "Lemme: {lemma.name}\n",
    "Enonce du lemme: {lemma.statement}\n",
    "\n",
    "But a prouver: {goal}\n",
    "\n",
    "Sur une echelle de 0 a 1, quelle est la pertinence de ce lemme?\n",
    "- 1.0 = Le lemme resout directement le but\n",
    "- 0.7-0.9 = Tres pertinent, peut etre utilise avec une reecriture\n",
    "- 0.4-0.6 = Moderement pertinent, structure similaire\n",
    "- 0.1-0.3 = Peu pertinent, meme domaine mais different\n",
    "- 0.0 = Aucun rapport\n",
    "\n",
    "Reponds UNIQUEMENT avec un nombre decimal entre 0 et 1.\"\"\"\n",
    "\n",
    "            # Les modeles modernes (gpt-4o, gpt-4.5, gpt-5, o1, o3) utilisent max_completion_tokens\n",
    "            model = os.getenv(\"OPENAI_CHAT_MODEL_ID\", \"gpt-5.2\")\n",
    "            use_max_completion_tokens = any(model.startswith(p) for p in ('gpt-4o', 'gpt-4.5', 'gpt-5', 'o1', 'o3'))\n",
    "            token_param = {\"max_completion_tokens\": 10} if use_max_completion_tokens else {\"max_tokens\": 10}\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1,\n",
    "                **token_param\n",
    "            )\n",
    "            \n",
    "            # Parser la reponse\n",
    "            score_text = response.choices[0].message.content.strip()\n",
    "            score = float(score_text)\n",
    "            score = max(0.0, min(1.0, score))  # Clamp entre 0 et 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  [Scoring LLM echoue: {e}, utilisation heuristique]\")\n",
    "            score = self._heuristic_score(lemma, goal)\n",
    "        \n",
    "        # Mettre en cache\n",
    "        self.score_cache[cache_key] = score\n",
    "        return score\n",
    "    \n",
    "    def _heuristic_score(self, lemma: Lemma, goal: str) -> float:\n",
    "        \"\"\"\n",
    "        Score heuristique base sur la correspondance de termes.\n",
    "        Utilise comme fallback quand l'API n'est pas disponible.\n",
    "        \"\"\"\n",
    "        # Normaliser les chaines\n",
    "        lemma_terms = set(lemma.statement.lower().replace(\":\", \" \").split())\n",
    "        goal_terms = set(goal.lower().replace(\":\", \" \").split())\n",
    "        \n",
    "        # Score = Jaccard similarity\n",
    "        intersection = len(lemma_terms & goal_terms)\n",
    "        union = len(lemma_terms | goal_terms)\n",
    "        \n",
    "        if union == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        jaccard = intersection / union\n",
    "        \n",
    "        # Bonus si le nom du lemme correspond au type d'operation\n",
    "        bonus = 0.0\n",
    "        if \"add\" in lemma.name.lower() and \"+\" in goal:\n",
    "            bonus = 0.2\n",
    "        elif \"mul\" in lemma.name.lower() and \"*\" in goal:\n",
    "            bonus = 0.2\n",
    "        elif \"comm\" in lemma.name.lower() and (\"comm\" in goal.lower() or \n",
    "                                               (\"+b\" in goal.replace(\" \", \"\") and \"+a\" in goal.replace(\" \", \"\"))):\n",
    "            bonus = 0.15\n",
    "        \n",
    "        return min(1.0, jaccard + bonus)\n",
    "    \n",
    "    def _score_lemmas(self, lemmas: List[Lemma], goal: str) -> List[Lemma]:\n",
    "        \"\"\"Score les lemmes avec la methode amelioree.\"\"\"\n",
    "        print(f\"  Scoring {len(lemmas)} lemmes...\")\n",
    "        \n",
    "        for lemma in lemmas:\n",
    "            lemma.relevance_score = self._score_with_llm(lemma, goal)\n",
    "        \n",
    "        # Trier par pertinence decroissante\n",
    "        return sorted(lemmas, key=lambda l: l.relevance_score, reverse=True)\n",
    "\n",
    "# Test de l'agent ameliore\n",
    "print(\"Test de ImprovedSearchAgent:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "improved_agent = ImprovedSearchAgent()\n",
    "goal = \"n + 0 = n\"\n",
    "results = improved_agent.search(goal)\n",
    "\n",
    "print(f\"\\nLemmes trouves pour '{goal}':\")\n",
    "for lemma in results:\n",
    "    print(f\"  [{lemma.relevance_score:.2f}] {lemma.name}: {lemma.statement}\")\n",
    "\n",
    "# Test sur un autre but\n",
    "goal2 = \"a + b = b + a\"\n",
    "results2 = improved_agent.search(goal2)\n",
    "print(f\"\\nLemmes trouves pour '{goal2}':\")\n",
    "for lemma in results2:\n",
    "    print(f\"  [{lemma.relevance_score:.2f}] {lemma.name}: {lemma.statement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Exercice 2 : Ajouter de la memoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test de ProofMemory:\n",
      "--------------------------------------------------\n",
      "Preuves stockees: 2\n",
      "\n",
      "Recall pour 'theorem my_add_zero (m : Nat) : m + 0 = m':\n",
      "  Score de similarite: 1.00\n",
      "  Preuve adaptee: exact Nat.add_zero m\n",
      "\n",
      "Statistiques memoire:\n",
      "  Patterns: 2\n",
      "  Utilisations totales: 2\n"
     ]
    }
   ],
   "source": [
    "# Exercice 2 - SOLUTION: Systeme de memoire avec pattern matching\n",
    "\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "@dataclass\n",
    "class StoredProof:\n",
    "    \"\"\"Une preuve stockee avec son contexte.\"\"\"\n",
    "    theorem_pattern: str\n",
    "    original_theorem: str\n",
    "    proof: str\n",
    "    success_count: int = 1\n",
    "    variables: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "class ProofMemory:\n",
    "    \"\"\"\n",
    "    Systeme de memoire pour reutiliser les preuves reussies.\n",
    "    \n",
    "    Fonctionnalites:\n",
    "    1. Pattern matching pour generaliser les theoremes\n",
    "    2. Recherche de preuves similaires par similarite\n",
    "    3. Adaptation des preuves au nouveau contexte\n",
    "    4. Persistence (optionnelle) vers fichier JSON\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_threshold: float = 0.7):\n",
    "        self.proofs: Dict[str, StoredProof] = {}  # pattern -> StoredProof\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "    \n",
    "    def store(self, theorem: str, proof: str) -> str:\n",
    "        \"\"\"\n",
    "        Stocke une preuve reussie.\n",
    "        \n",
    "        Returns:\n",
    "            L'ID du pattern utilise pour le stockage\n",
    "        \"\"\"\n",
    "        # Extraire le pattern et les variables\n",
    "        pattern, variables = self._extract_pattern(theorem)\n",
    "        \n",
    "        if pattern in self.proofs:\n",
    "            # Incrementer le compteur de succes\n",
    "            self.proofs[pattern].success_count += 1\n",
    "        else:\n",
    "            # Nouvelle preuve\n",
    "            self.proofs[pattern] = StoredProof(\n",
    "                theorem_pattern=pattern,\n",
    "                original_theorem=theorem,\n",
    "                proof=proof,\n",
    "                variables=variables\n",
    "            )\n",
    "        \n",
    "        return pattern\n",
    "    \n",
    "    def recall(self, theorem: str) -> Optional[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Retrouve une preuve similaire.\n",
    "        \n",
    "        Returns:\n",
    "            (preuve_adaptee, score_similarite) ou None si rien trouve\n",
    "        \"\"\"\n",
    "        # Extraire le pattern du theoreme\n",
    "        query_pattern, query_vars = self._extract_pattern(theorem)\n",
    "        \n",
    "        # Recherche exacte d'abord\n",
    "        if query_pattern in self.proofs:\n",
    "            stored = self.proofs[query_pattern]\n",
    "            adapted_proof = self._adapt_proof(stored.proof, stored.variables, query_vars)\n",
    "            return adapted_proof, 1.0\n",
    "        \n",
    "        # Recherche par similarite\n",
    "        best_match = None\n",
    "        best_score = 0.0\n",
    "        \n",
    "        for pattern, stored in self.proofs.items():\n",
    "            score = self._similarity(query_pattern, pattern)\n",
    "            if score > best_score and score >= self.similarity_threshold:\n",
    "                best_score = score\n",
    "                best_match = stored\n",
    "        \n",
    "        if best_match:\n",
    "            adapted_proof = self._adapt_proof(best_match.proof, best_match.variables, query_vars)\n",
    "            return adapted_proof, best_score\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _extract_pattern(self, theorem: str) -> Tuple[str, Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Extrait un pattern generalise du theoreme.\n",
    "        \n",
    "        Transformations:\n",
    "        - Variables specifiques -> placeholders (?x, ?y, ?z)\n",
    "        - Types conserves\n",
    "        - Structure preservee\n",
    "        \n",
    "        Exemple:\n",
    "            \"theorem foo (n : Nat) : n + 0 = n\" \n",
    "            -> \"theorem ?name (?x : Nat) : ?x + 0 = ?x\"\n",
    "        \"\"\"\n",
    "        variables = {}\n",
    "        pattern = theorem\n",
    "        \n",
    "        # Extraire le nom du theoreme\n",
    "        name_match = re.search(r'theorem\\s+(\\w+)', theorem)\n",
    "        if name_match:\n",
    "            variables['theorem_name'] = name_match.group(1)\n",
    "            pattern = re.sub(r'theorem\\s+\\w+', 'theorem ?name', pattern)\n",
    "        \n",
    "        # Extraire les variables de type Nat/Int\n",
    "        var_matches = re.findall(r'\\((\\w+)\\s*:\\s*(\\w+)\\)', theorem)\n",
    "        placeholder_index = 0\n",
    "        placeholders = ['?x', '?y', '?z', '?a', '?b', '?c']\n",
    "        \n",
    "        for var_name, var_type in var_matches:\n",
    "            if placeholder_index < len(placeholders):\n",
    "                placeholder = placeholders[placeholder_index]\n",
    "                variables[placeholder] = var_name\n",
    "                # Remplacer la variable dans tout le pattern\n",
    "                pattern = re.sub(rf'\\b{var_name}\\b', placeholder, pattern)\n",
    "                placeholder_index += 1\n",
    "        \n",
    "        return pattern, variables\n",
    "    \n",
    "    def _similarity(self, pattern1: str, pattern2: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcule la similarite entre deux patterns.\n",
    "        Utilise SequenceMatcher pour une comparaison robuste.\n",
    "        \"\"\"\n",
    "        # Normaliser\n",
    "        p1 = pattern1.lower().replace(\" \", \"\")\n",
    "        p2 = pattern2.lower().replace(\" \", \"\")\n",
    "        \n",
    "        return SequenceMatcher(None, p1, p2).ratio()\n",
    "    \n",
    "    def _adapt_proof(self, proof: str, original_vars: Dict[str, str], \n",
    "                     new_vars: Dict[str, str]) -> str:\n",
    "        \"\"\"\n",
    "        Adapte une preuve au nouveau contexte en substituant les variables.\n",
    "        \"\"\"\n",
    "        adapted = proof\n",
    "        \n",
    "        for placeholder, orig_name in original_vars.items():\n",
    "            if placeholder in new_vars:\n",
    "                new_name = new_vars[placeholder]\n",
    "                # Remplacer le nom original par le nouveau\n",
    "                adapted = re.sub(rf'\\b{orig_name}\\b', new_name, adapted)\n",
    "        \n",
    "        return adapted\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Retourne des statistiques sur la memoire.\"\"\"\n",
    "        return {\n",
    "            \"total_patterns\": len(self.proofs),\n",
    "            \"total_uses\": sum(p.success_count for p in self.proofs.values()),\n",
    "            \"most_used\": max(self.proofs.values(), \n",
    "                           key=lambda p: p.success_count).theorem_pattern \n",
    "                          if self.proofs else None\n",
    "        }\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Sauvegarde la memoire dans un fichier JSON.\"\"\"\n",
    "        data = {\n",
    "            pattern: {\n",
    "                \"theorem_pattern\": sp.theorem_pattern,\n",
    "                \"original_theorem\": sp.original_theorem,\n",
    "                \"proof\": sp.proof,\n",
    "                \"success_count\": sp.success_count,\n",
    "                \"variables\": sp.variables\n",
    "            }\n",
    "            for pattern, sp in self.proofs.items()\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def load(self, filepath: str):\n",
    "        \"\"\"Charge la memoire depuis un fichier JSON.\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.proofs = {\n",
    "            pattern: StoredProof(**stored)\n",
    "            for pattern, stored in data.items()\n",
    "        }\n",
    "\n",
    "# Test de ProofMemory\n",
    "print(\"Test de ProofMemory:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "memory = ProofMemory()\n",
    "\n",
    "# Stocker quelques preuves\n",
    "memory.store(\n",
    "    \"theorem add_zero_n (n : Nat) : n + 0 = n\",\n",
    "    \"exact Nat.add_zero n\"\n",
    ")\n",
    "memory.store(\n",
    "    \"theorem add_comm_ab (a b : Nat) : a + b = b + a\",\n",
    "    \"exact Nat.add_comm a b\"\n",
    ")\n",
    "\n",
    "print(f\"Preuves stockees: {len(memory.proofs)}\")\n",
    "\n",
    "# Tester le recall sur un theoreme similaire\n",
    "test_theorem = \"theorem my_add_zero (m : Nat) : m + 0 = m\"\n",
    "result = memory.recall(test_theorem)\n",
    "\n",
    "if result:\n",
    "    proof, score = result\n",
    "    print(f\"\\nRecall pour '{test_theorem}':\")\n",
    "    print(f\"  Score de similarite: {score:.2f}\")\n",
    "    print(f\"  Preuve adaptee: {proof}\")\n",
    "else:\n",
    "    print(f\"\\nPas de preuve trouvee pour '{test_theorem}'\")\n",
    "\n",
    "# Statistiques\n",
    "stats = memory.get_statistics()\n",
    "print(f\"\\nStatistiques memoire:\")\n",
    "print(f\"  Patterns: {stats['total_patterns']}\")\n",
    "print(f\"  Utilisations totales: {stats['total_uses']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resume\n",
    "\n",
    "### 10.1. Architecture multi-agents pour theorem proving\n",
    "\n",
    "| Agent | Role | Entrees | Sorties |\n",
    "|-------|------|---------|--------|\n",
    "| **OrchestratorAgent** | Coordonner workflow | Theoreme | Delegation + status |\n",
    "| **SearchAgent** | Trouver lemmes Mathlib | But | Liste de lemmes |\n",
    "| **TacticAgent** | Generer tactiques | But + lemmes | Sequence de tactiques |\n",
    "| **VerifierAgent** | Valider avec Lean | Code Lean | Succes/Erreur + feedback |\n",
    "\n",
    "### 10.2. Patterns Semantic Kernel implementes\n",
    "\n",
    "| Pattern | Description | Classe |\n",
    "|---------|-------------|--------|\n",
    "| **StateManager** | Etat partage entre agents | `ProofState` |\n",
    "| **Plugin** | Fonctions @kernel_function | `LeanProverPlugin` |\n",
    "| **SelectionStrategy** | Choix agent suivant | `DelegatingSelectionStrategy` |\n",
    "| **TerminationStrategy** | Critere d'arret | `ProofCompleteTermination` |\n",
    "| **AgentGroupChat** | Conversation multi-agents | `AgentGroupChat` |\n",
    "\n",
    "### 10.3. Techniques cles\n",
    "\n",
    "1. **Etat partage** : Tous les agents lisent/ecrivent dans `ProofState`\n",
    "2. **Delegation explicite** : Chaque agent designe le suivant via `delegate_to_agent`\n",
    "3. **Boucle de feedback** : Echecs envoyes a `TacticAgent` pour correction\n",
    "4. **Memoire de session** : Historique des tentatives pour eviter repetitions\n",
    "5. **Decomposition (Aristotle)** : Diviser problemes complexes en sous-problemes\n",
    "\n",
    "### 10.4. Ressources et inspiration\n",
    "\n",
    "| Source | Contribution |\n",
    "|--------|--------------|\n",
    "| **Argument_Analysis notebooks** | Patterns SK (StateManager, orchestration) |\n",
    "| **Harmonic Aristotle** | Decomposition hierarchique, IMO Gold 2025 |\n",
    "| **APOLLO** | Generation massive, filtrage par Lean |\n",
    "| **AlphaProof** | RL + MCTS, Nature 2025 |\n",
    "| **LeanDojo** | Extraction donnees, LeanCopilot |\n",
    "\n",
    "### 10.5. Impact futur\n",
    "\n",
    "Les systemes agentiques pour theorem proving representent une nouvelle frontiere:\n",
    "- **15+ problemes Erdos** resolus par IA depuis Noel 2025\n",
    "- **Acceleration x10-100** de la formalisation mathematique\n",
    "- **Decouverte** de nouvelles mathematiques par collaboration humain-IA\n",
    "- **Verification formelle** comme standard de confiance absolue\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook base sur les techniques de Harmonic Aristotle (IMO Gold 2025), APOLLO (arXiv 2505), AlphaProof (Nature 2025), et les patterns Semantic Kernel inspires de Argument_Analysis*\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation** : [â† Lean-7-LLM-Integration](Lean-7-LLM-Integration.ipynb) | [Index](Lean-1-Setup.ipynb) | [Lean-9-LeanDojo â†’](Lean-9-LeanDojo.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (WSL)",
   "language": "python",
   "name": "python3-wsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}