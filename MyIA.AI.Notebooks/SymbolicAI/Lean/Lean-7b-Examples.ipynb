{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lean 7b - Exemples Progressifs et Benchmarks\n",
    "\n",
    "Ce notebook fait suite a **Lean-7-LLM-Integration** qui a mis en place l'infrastructure d'integration LLM-Lean. Ici, nous allons:\n",
    "\n",
    "1. **Tester des theoremes progressifs** - des plus simples aux plus complexes\n",
    "2. **Comparer les providers** - OpenAI vs Anthropic\n",
    "3. **Visualiser les resultats** - metriques et graphiques\n",
    "4. **Benchmarks Erdos** - problemes de niveau recherche\n",
    "\n",
    "## Prerequis\n",
    "\n",
    "Executez d'abord Lean-7-LLM-Integration pour avoir les classes `LLMClient`, `ProofVerifier`, `ProofGenerator` chargees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration et Imports\n# Ce notebook utilise les classes de lean_runner.py\n\nimport os\nimport sys\nfrom pathlib import Path\n\n# Trouver le repertoire du notebook (plusieurs methodes)\ndef find_notebook_dir():\n    \"\"\"Trouve le repertoire contenant lean_runner.py\"\"\"\n    # Methode 1: Chercher lean_runner.py depuis le cwd et ses parents\n    candidates = [\n        Path.cwd(),  # Repertoire courant\n        Path.cwd() / \"MyIA.AI.Notebooks\" / \"SymbolicAI\" / \"Lean\",\n        Path(\"d:/dev/CoursIA/MyIA.AI.Notebooks/SymbolicAI/Lean\"),  # Windows\n        Path(\"/mnt/d/dev/CoursIA/MyIA.AI.Notebooks/SymbolicAI/Lean\"),  # WSL\n    ]\n    \n    for candidate in candidates:\n        if candidate.exists() and (candidate / \"lean_runner.py\").exists():\n            return candidate\n    \n    # Methode 2: Rechercher dans les parents du cwd\n    current = Path.cwd()\n    for _ in range(5):  # Remonter jusqu'a 5 niveaux\n        lean_path = current / \"MyIA.AI.Notebooks\" / \"SymbolicAI\" / \"Lean\"\n        if lean_path.exists() and (lean_path / \"lean_runner.py\").exists():\n            return lean_path\n        if (current / \"lean_runner.py\").exists():\n            return current\n        current = current.parent\n    \n    raise FileNotFoundError(\"Impossible de trouver lean_runner.py - verifiez le repertoire de travail\")\n\n# Trouver et ajouter le repertoire au path\nnotebook_dir = find_notebook_dir()\nif str(notebook_dir) not in sys.path:\n    sys.path.insert(0, str(notebook_dir))\nprint(f\"Repertoire notebook: {notebook_dir}\")\n\n# Charger les variables d'environnement\nfrom lean_runner import load_env_file\nenv_path = notebook_dir / \".env\"\nload_env_file(env_path)\nprint(f\"Configuration chargee depuis {env_path}\")\n\n# Importer toutes les classes necessaires\nfrom lean_runner import (\n    LeanRunner, LeanResult,\n    PROVIDERS_CONFIG,\n    LLMResponse, LLMClient,\n    LeanProofPrompt,\n    ErrorInfo, ProofVerifier,\n    ProofAttempt, ProofResult, ProofGenerator\n)\n\nprint(\"Classes importees depuis lean_runner.py\")\n\n# Verifier les providers disponibles\nprint(\"\\nProviders disponibles:\")\nfor provider, config in PROVIDERS_CONFIG.items():\n    api_key = os.environ.get(config[\"api_key_env\"])\n    status = \"OK\" if api_key else \"NON CONFIGURE\"\n    print(f\"  - {provider}: {status}\")\n\n# Initialiser le client et le verifier\nclient = None\nverifier = None\n\ntry:\n    if os.environ.get(\"OPENAI_API_KEY\"):\n        client = LLMClient(provider=\"openai\")\n        print(f\"\\nClient OpenAI initialise: {client.model}\")\n    elif os.environ.get(\"ANTHROPIC_API_KEY\"):\n        client = LLMClient(provider=\"anthropic\")\n        print(f\"\\nClient Anthropic initialise: {client.model}\")\n    else:\n        print(\"\\n[Warning] Aucune API configuree - mode simulation\")\nexcept ValueError as e:\n    print(f\"\\nErreur client: {e}\")\n\ntry:\n    verifier = ProofVerifier(backend=\"auto\", timeout=30)\n    print(f\"ProofVerifier initialise (backend: auto)\")\nexcept Exception as e:\n    print(f\"Erreur verifier: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exemples Progressifs et Analyse\n",
    "\n",
    "Cette section teste notre pipeline LLM-Lean sur des theoremes de difficulte croissante.\n",
    "\n",
    "### 7.1 Theoremes Simples\n",
    "\n",
    "Commencons par des theoremes de base que Lean peut prouver avec des tactiques simples (`rfl`, `simp`, `decide`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE_THEOREMS definis : 5 theoremes\n",
      "\n",
      "Liste:\n",
      "  - add_zero: Identite additive a droite\n",
      "  - add_comm: Commutativite de l'addition\n",
      "  - mul_assoc: Associativite de la multiplication\n",
      "  - zero_add: Identite additive a gauche\n",
      "  - mul_comm: Commutativite de la multiplication\n"
     ]
    }
   ],
   "source": [
    "# Section 7.1 - Definition des theoremes simples\n",
    "\n",
    "SIMPLE_THEOREMS = [\n",
    "    {\n",
    "        \"name\": \"add_zero\",\n",
    "        \"statement\": \"theorem test_add_zero (n : Nat) : n + 0 = n := by sorry\",\n",
    "        \"difficulty\": \"facile\",\n",
    "        \"expected_tactic\": \"rfl ou exact Nat.add_zero n\",\n",
    "        \"description\": \"Identite additive a droite\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"add_comm\",\n",
    "        \"statement\": \"theorem test_add_comm (a b : Nat) : a + b = b + a := by sorry\",\n",
    "        \"difficulty\": \"facile\",\n",
    "        \"expected_tactic\": \"exact Nat.add_comm a b\",\n",
    "        \"description\": \"Commutativite de l'addition\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mul_assoc\",\n",
    "        \"statement\": \"theorem test_mul_assoc (a b c : Nat) : (a * b) * c = a * (b * c) := by sorry\",\n",
    "        \"difficulty\": \"facile\",\n",
    "        \"expected_tactic\": \"exact Nat.mul_assoc a b c\",\n",
    "        \"description\": \"Associativite de la multiplication\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"zero_add\",\n",
    "        \"statement\": \"theorem test_zero_add (n : Nat) : 0 + n = n := by sorry\",\n",
    "        \"difficulty\": \"facile\",\n",
    "        \"expected_tactic\": \"rfl ou exact Nat.zero_add n\",\n",
    "        \"description\": \"Identite additive a gauche\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mul_comm\",\n",
    "        \"statement\": \"theorem test_mul_comm (a b : Nat) : a * b = b * a := by sorry\",\n",
    "        \"difficulty\": \"facile\",\n",
    "        \"expected_tactic\": \"exact Nat.mul_comm a b\",\n",
    "        \"description\": \"Commutativite de la multiplication\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"SIMPLE_THEOREMS definis : {len(SIMPLE_THEOREMS)} theoremes\")\n",
    "print(\"\\nListe:\")\n",
    "for th in SIMPLE_THEOREMS:\n",
    "    print(f\"  - {th['name']}: {th['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution\n",
    "\n",
    "Testons le pipeline sur ces theoremes simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7.2 - Execution SIMPLE_THEOREMS\n",
    "\n",
    "# Verifier si API disponible\n",
    "try:\n",
    "    llm_simple = LLMClient(provider=\"openai\")\n",
    "    api_ok = True\n",
    "except ValueError:\n",
    "    api_ok = False\n",
    "    print(\"[INFO] API non configuree - execution sautee\")\n",
    "    print(\"Configurez OPENAI_API_KEY dans .env pour executer les exemples\\n\")\n",
    "\n",
    "if api_ok:\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXECUTION SIMPLE_THEOREMS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Provider: {llm_simple.provider} / {llm_simple.model}\\n\")\n",
    "    \n",
    "    # Creer le generateur\n",
    "    generator_simple = ProofGenerator(\n",
    "        llm_client=llm_simple,\n",
    "        verifier=verifier,\n",
    "        max_iterations=3,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # Executer tous les theoremes\n",
    "    simple_results = []\n",
    "    \n",
    "    for i, theorem in enumerate(SIMPLE_THEOREMS, 1):\n",
    "        print(f\"\\n[{i}/{len(SIMPLE_THEOREMS)}] {theorem['name'].upper()}\")\n",
    "        print(f\"Description: {theorem['description']}\")\n",
    "        print(f\"Tactique attendue: {theorem['expected_tactic']}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Prouver\n",
    "        result = generator_simple.prove(\n",
    "            theorem[\"statement\"],\n",
    "            verbose=False  # Mode concis pour batch\n",
    "        )\n",
    "        \n",
    "        # Afficher resultat\n",
    "        status = \"SUCCES\" if result.success else \"ECHEC\"\n",
    "        print(f\"Resultat: {status}\")\n",
    "        print(f\"Iterations: {result.total_iterations}\")\n",
    "        print(f\"Temps: {result.total_time_ms:.0f}ms\")\n",
    "        \n",
    "        if result.success:\n",
    "            print(f\"Preuve:\\n{result.final_proof}\")\n",
    "        else:\n",
    "            print(f\"Derniere erreur: {result.attempts[-1].result.errors[:100]}...\")\n",
    "        \n",
    "        # Sauvegarder\n",
    "        simple_results.append({\n",
    "            \"theorem\": theorem,\n",
    "            \"result\": result\n",
    "        })\n",
    "    \n",
    "    # Statistiques globales\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATISTIQUES SIMPLE_THEOREMS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total = len(simple_results)\n",
    "    success_count = sum(1 for r in simple_results if r[\"result\"].success)\n",
    "    total_iterations = sum(r[\"result\"].total_iterations for r in simple_results)\n",
    "    total_tokens = sum(r[\"result\"].get_metrics()[\"total_tokens\"] for r in simple_results)\n",
    "    total_time = sum(r[\"result\"].total_time_ms for r in simple_results)\n",
    "    \n",
    "    print(f\"Taux de succes: {success_count}/{total} ({100*success_count/total:.1f}%)\")\n",
    "    print(f\"Iterations moyenne: {total_iterations/total:.1f}\")\n",
    "    print(f\"Tokens totaux: {total_tokens}\")\n",
    "    print(f\"Temps total: {total_time:.0f}ms ({total_time/total:.0f}ms/theoreme)\")\n",
    "    \n",
    "    # Detail par theoreme\n",
    "    print(f\"\\nDetail:\")\n",
    "    for r in simple_results:\n",
    "        name = r[\"theorem\"][\"name\"]\n",
    "        success = \"OK\" if r[\"result\"].success else \"FAIL\"\n",
    "        iters = r[\"result\"].total_iterations\n",
    "        tokens = r[\"result\"].get_metrics()[\"total_tokens\"]\n",
    "        print(f\"  {name:12} : {success:4} | {iters} iter | {tokens:4} tokens\")\n",
    "else:\n",
    "    simple_results = []\n",
    "    print(\"Execution sautee (API non configuree)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Theoremes Mathlib\n",
    "\n",
    "Les theoremes Mathlib utilisent des tactiques avancees comme `ring`, `linarith`, et `omega`. Ils representent un niveau de difficulte superieur pour les LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATHLIB_THEOREMS definis : 5 theoremes\n",
      "\n",
      "Liste:\n",
      "  - ring_example: Expansion algebrique (necessite ring de Mathlib) (imports: import Mathlib.Tactic.Ring)\n",
      "  - linarith_example: Arithmetique lineaire (necessite linarith) (imports: import Mathlib.Tactic.Linarith)\n",
      "  - omega_example: Arithmetique sur Nat (omega est puissant)\n",
      "  - distrib_example: Distributivite (imports: import Mathlib.Tactic.Ring)\n",
      "  - simp_example: Simplification automatique\n"
     ]
    }
   ],
   "source": [
    "# Section 7.3 - Definition des theoremes Mathlib\n",
    "\n",
    "MATHLIB_THEOREMS = [\n",
    "    {\n",
    "        \"name\": \"ring_example\",\n",
    "        \"statement\": \"\"\"theorem test_ring (a b : Nat) : (a + b) * (a + b) = a * a + 2 * a * b + b * b := by\n",
    "  sorry\"\"\",\n",
    "        \"difficulty\": \"moyen\",\n",
    "        \"expected_tactic\": \"ring\",\n",
    "        \"description\": \"Expansion algebrique (necessite ring de Mathlib)\",\n",
    "        \"imports\": \"import Mathlib.Tactic.Ring\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"linarith_example\", \n",
    "        \"statement\": \"\"\"theorem test_linarith (x y : Nat) (h1 : x + y = 10) (h2 : x = 3) : y = 7 := by\n",
    "  sorry\"\"\",\n",
    "        \"difficulty\": \"moyen\",\n",
    "        \"expected_tactic\": \"linarith\",\n",
    "        \"description\": \"Arithmetique lineaire (necessite linarith)\",\n",
    "        \"imports\": \"import Mathlib.Tactic.Linarith\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"omega_example\",\n",
    "        \"statement\": \"\"\"theorem test_omega (n : Nat) : n + 0 = n := by\n",
    "  sorry\"\"\",\n",
    "        \"difficulty\": \"facile\",\n",
    "        \"expected_tactic\": \"omega\",\n",
    "        \"description\": \"Arithmetique sur Nat (omega est puissant)\",\n",
    "        \"imports\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"distrib_example\",\n",
    "        \"statement\": \"\"\"theorem test_distrib (a b c : Nat) : a * (b + c) = a * b + a * c := by\n",
    "  sorry\"\"\",\n",
    "        \"difficulty\": \"moyen\",\n",
    "        \"expected_tactic\": \"ring ou exact Nat.mul_add a b c\",\n",
    "        \"description\": \"Distributivite\",\n",
    "        \"imports\": \"import Mathlib.Tactic.Ring\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"simp_example\",\n",
    "        \"statement\": \"\"\"theorem test_simp (n : Nat) : n + 0 + 0 = n := by\n",
    "  sorry\"\"\",\n",
    "        \"difficulty\": \"facile\",\n",
    "        \"expected_tactic\": \"simp\",\n",
    "        \"description\": \"Simplification automatique\",\n",
    "        \"imports\": \"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"MATHLIB_THEOREMS definis : {len(MATHLIB_THEOREMS)} theoremes\")\n",
    "print(\"\\nListe:\")\n",
    "for th in MATHLIB_THEOREMS:\n",
    "    imports_note = f\" (imports: {th['imports']})\" if th['imports'] else \"\"\n",
    "    print(f\"  - {th['name']}: {th['description']}{imports_note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 Execution des Theoremes Mathlib\n",
    "\n",
    "Ces theoremes necessitent des tactiques Mathlib comme `ring`, `linarith`, et `omega`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7.4 - Execution MATHLIB_THEOREMS\n",
    "\n",
    "if api_ok:\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXECUTION MATHLIB_THEOREMS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Provider: {llm_simple.provider} / {llm_simple.model}\\n\")\n",
    "    \n",
    "    # Generateur avec plus d'iterations pour Mathlib\n",
    "    generator_mathlib = ProofGenerator(\n",
    "        llm_client=llm_simple,\n",
    "        verifier=verifier,\n",
    "        max_iterations=5,  # Plus d'iterations pour tactiques Mathlib\n",
    "        temperature=0.4    # Un peu plus creatif\n",
    "    )\n",
    "    \n",
    "    # Executer tous les theoremes\n",
    "    mathlib_results = []\n",
    "    \n",
    "    for i, theorem in enumerate(MATHLIB_THEOREMS, 1):\n",
    "        print(f\"\\n[{i}/{len(MATHLIB_THEOREMS)}] {theorem['name'].upper()}\")\n",
    "        print(f\"Description: {theorem['description']}\")\n",
    "        print(f\"Tactique attendue: {theorem['expected_tactic']}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Ajouter les imports au contexte si necessaire\n",
    "        context = None\n",
    "        if theorem.get(\"imports\"):\n",
    "            context = {\"imports\": theorem[\"imports\"]}\n",
    "        \n",
    "        # Prouver\n",
    "        result = generator_mathlib.prove(\n",
    "            theorem[\"statement\"],\n",
    "            context=context,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Afficher resultat\n",
    "        status = \"SUCCES\" if result.success else \"ECHEC\"\n",
    "        print(f\"Resultat: {status}\")\n",
    "        print(f\"Iterations: {result.total_iterations}\")\n",
    "        print(f\"Temps: {result.total_time_ms:.0f}ms\")\n",
    "        \n",
    "        if result.success:\n",
    "            print(f\"Preuve:\\n{result.final_proof}\")\n",
    "        else:\n",
    "            print(f\"Derniere erreur: {result.attempts[-1].result.errors[:150]}...\")\n",
    "        \n",
    "        # Sauvegarder\n",
    "        mathlib_results.append({\n",
    "            \"theorem\": theorem,\n",
    "            \"result\": result\n",
    "        })\n",
    "    \n",
    "    # Statistiques globales\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATISTIQUES MATHLIB_THEOREMS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total = len(mathlib_results)\n",
    "    success_count = sum(1 for r in mathlib_results if r[\"result\"].success)\n",
    "    total_iterations = sum(r[\"result\"].total_iterations for r in mathlib_results)\n",
    "    total_tokens = sum(r[\"result\"].get_metrics()[\"total_tokens\"] for r in mathlib_results)\n",
    "    total_time = sum(r[\"result\"].total_time_ms for r in mathlib_results)\n",
    "    \n",
    "    print(f\"Taux de succes: {success_count}/{total} ({100*success_count/total:.1f}%)\")\n",
    "    print(f\"Iterations moyenne: {total_iterations/total:.1f}\")\n",
    "    print(f\"Tokens totaux: {total_tokens}\")\n",
    "    print(f\"Temps total: {total_time:.0f}ms ({total_time/total:.0f}ms/theoreme)\")\n",
    "    \n",
    "    # Detail par theoreme\n",
    "    print(f\"\\nDetail:\")\n",
    "    for r in mathlib_results:\n",
    "        name = r[\"theorem\"][\"name\"]\n",
    "        success = \"OK\" if r[\"result\"].success else \"FAIL\"\n",
    "        iters = r[\"result\"].total_iterations\n",
    "        tokens = r[\"result\"].get_metrics()[\"total_tokens\"]\n",
    "        print(f\"  {name:18} : {success:4} | {iters} iter | {tokens:4} tokens\")\n",
    "else:\n",
    "    mathlib_results = []\n",
    "    print(\"Execution sautee (API non configuree)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Visualisations et Metriques\n",
    "\n",
    "Analyse graphique des performances : taux de succes, iterations, temps, tokens utilises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7.5 - Visualisations\n",
    "\n",
    "if api_ok and (simple_results or mathlib_results):\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        \n",
    "        # Combiner tous les resultats\n",
    "        all_results = simple_results + mathlib_results\n",
    "        \n",
    "        # Extraire donnees\n",
    "        names = [r[\"theorem\"][\"name\"] for r in all_results]\n",
    "        successes = [r[\"result\"].success for r in all_results]\n",
    "        iterations = [r[\"result\"].total_iterations for r in all_results]\n",
    "        times = [r[\"result\"].total_time_ms for r in all_results]\n",
    "        tokens = [r[\"result\"].get_metrics()[\"total_tokens\"] for r in all_results]\n",
    "        \n",
    "        # Figure avec 4 subplots\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle('Analyse des Performances - LLM Proof Generation', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Taux de succes (pie chart)\n",
    "        success_count = sum(successes)\n",
    "        fail_count = len(successes) - success_count\n",
    "        ax1.pie([success_count, fail_count], \n",
    "                labels=[f'Succes ({success_count})', f'Echec ({fail_count})'],\n",
    "                autopct='%1.1f%%',\n",
    "                colors=['#4CAF50', '#F44336'],\n",
    "                startangle=90)\n",
    "        ax1.set_title('Taux de Succes Global')\n",
    "        \n",
    "        # 2. Iterations par theoreme (bar chart)\n",
    "        colors_iter = ['#4CAF50' if s else '#F44336' for s in successes]\n",
    "        x_pos = np.arange(len(names))\n",
    "        ax2.bar(x_pos, iterations, color=colors_iter, alpha=0.7)\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(names, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Iterations')\n",
    "        ax2.set_title('Iterations par Theoreme')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 3. Temps d'execution (bar chart)\n",
    "        ax3.bar(x_pos, times, color='#2196F3', alpha=0.7)\n",
    "        ax3.set_xticks(x_pos)\n",
    "        ax3.set_xticklabels(names, rotation=45, ha='right')\n",
    "        ax3.set_ylabel('Temps (ms)')\n",
    "        ax3.set_title('Temps d\\'Execution par Theoreme')\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 4. Tokens utilises (bar chart)\n",
    "        ax4.bar(x_pos, tokens, color='#FF9800', alpha=0.7)\n",
    "        ax4.set_xticks(x_pos)\n",
    "        ax4.set_xticklabels(names, rotation=45, ha='right')\n",
    "        ax4.set_ylabel('Tokens')\n",
    "        ax4.set_title('Tokens Utilises par Theoreme')\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Stats resumees\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STATISTIQUES GLOBALES (SIMPLE + MATHLIB)\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total theoremes: {len(all_results)}\")\n",
    "        print(f\"Taux de succes: {success_count}/{len(all_results)} ({100*success_count/len(all_results):.1f}%)\")\n",
    "        print(f\"Iterations moyenne: {np.mean(iterations):.2f} (min: {min(iterations)}, max: {max(iterations)})\")\n",
    "        print(f\"Temps moyen: {np.mean(times):.0f}ms (min: {min(times):.0f}ms, max: {max(times):.0f}ms)\")\n",
    "        print(f\"Tokens moyen: {np.mean(tokens):.0f} (min: {min(tokens)}, max: {max(tokens)})\")\n",
    "        print(f\"Tokens totaux: {sum(tokens)}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"[Warning] matplotlib non installe - pip install matplotlib\")\n",
    "        print(\"Visualisations sautees\")\n",
    "else:\n",
    "    print(\"Visualisations sautees (pas de resultats ou API non configuree)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Comparaison OpenAI vs Anthropic\n",
    "\n",
    "Benchmark comparatif entre les deux providers sur les memes theoremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7.6 - Comparaison Providers (OpenAI vs Anthropic)\n",
    "\n",
    "# Verifier si Anthropic est configure\n",
    "try:\n",
    "    llm_anthropic = LLMClient(provider=\"anthropic\")\n",
    "    anthropic_ok = True\n",
    "except ValueError:\n",
    "    anthropic_ok = False\n",
    "\n",
    "if api_ok and anthropic_ok:\n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPARAISON PROVIDERS : OpenAI vs Anthropic\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Selectionner 3 theoremes representatifs\n",
    "    comparison_theorems = [\n",
    "        SIMPLE_THEOREMS[1],   # add_comm\n",
    "        SIMPLE_THEOREMS[2],   # mul_assoc\n",
    "        MATHLIB_THEOREMS[2]   # omega_example\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nTheoremes de test ({len(comparison_theorems)}):\")\n",
    "    for th in comparison_theorems:\n",
    "        print(f\"  - {th['name']}: {th['description']}\")\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for theorem in comparison_theorems:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Theoreme: {theorem['name']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Test avec OpenAI\n",
    "        print(\"\\n[OpenAI]\")\n",
    "        gen_openai = ProofGenerator(llm_simple, verifier, max_iterations=3, temperature=0.3)\n",
    "        result_openai = gen_openai.prove(theorem[\"statement\"], verbose=False)\n",
    "        \n",
    "        metrics_openai = result_openai.get_metrics()\n",
    "        print(f\"  Succes: {result_openai.success}\")\n",
    "        print(f\"  Iterations: {metrics_openai['iterations']}\")\n",
    "        print(f\"  Temps: {metrics_openai['total_time_ms']:.0f}ms\")\n",
    "        print(f\"  Tokens: {metrics_openai['total_tokens']}\")\n",
    "        \n",
    "        # Test avec Anthropic\n",
    "        print(\"\\n[Anthropic]\")\n",
    "        gen_anthropic = ProofGenerator(llm_anthropic, verifier, max_iterations=3, temperature=0.3)\n",
    "        result_anthropic = gen_anthropic.prove(theorem[\"statement\"], verbose=False)\n",
    "        \n",
    "        metrics_anthropic = result_anthropic.get_metrics()\n",
    "        print(f\"  Succes: {result_anthropic.success}\")\n",
    "        print(f\"  Iterations: {metrics_anthropic['iterations']}\")\n",
    "        print(f\"  Temps: {metrics_anthropic['total_time_ms']:.0f}ms\")\n",
    "        print(f\"  Tokens: {metrics_anthropic['total_tokens']}\")\n",
    "        \n",
    "        # Sauvegarder\n",
    "        comparison_results.append({\n",
    "            \"theorem\": theorem,\n",
    "            \"openai\": result_openai,\n",
    "            \"anthropic\": result_anthropic\n",
    "        })\n",
    "    \n",
    "    # Visualisation comparative\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        fig.suptitle('Comparaison OpenAI vs Anthropic', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        theorem_names = [r[\"theorem\"][\"name\"] for r in comparison_results]\n",
    "        x_pos = np.arange(len(theorem_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Iterations\n",
    "        iter_openai = [r[\"openai\"].total_iterations for r in comparison_results]\n",
    "        iter_anthropic = [r[\"anthropic\"].total_iterations for r in comparison_results]\n",
    "        \n",
    "        ax1.bar(x_pos - width/2, iter_openai, width, label='OpenAI', color='#00A67E', alpha=0.8)\n",
    "        ax1.bar(x_pos + width/2, iter_anthropic, width, label='Anthropic', color='#D4A574', alpha=0.8)\n",
    "        ax1.set_xticks(x_pos)\n",
    "        ax1.set_xticklabels(theorem_names, rotation=45, ha='right')\n",
    "        ax1.set_ylabel('Iterations')\n",
    "        ax1.set_title('Iterations jusqu\\'au Succes')\n",
    "        ax1.legend()\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Temps\n",
    "        time_openai = [r[\"openai\"].total_time_ms for r in comparison_results]\n",
    "        time_anthropic = [r[\"anthropic\"].total_time_ms for r in comparison_results]\n",
    "        \n",
    "        ax2.bar(x_pos - width/2, time_openai, width, label='OpenAI', color='#00A67E', alpha=0.8)\n",
    "        ax2.bar(x_pos + width/2, time_anthropic, width, label='Anthropic', color='#D4A574', alpha=0.8)\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(theorem_names, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Temps (ms)')\n",
    "        ax2.set_title('Temps d\\'Execution')\n",
    "        ax2.legend()\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Stats comparatives\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STATISTIQUES COMPARATIVES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # OpenAI\n",
    "        openai_success = sum(1 for r in comparison_results if r[\"openai\"].success)\n",
    "        openai_avg_iter = np.mean([r[\"openai\"].total_iterations for r in comparison_results])\n",
    "        openai_avg_time = np.mean([r[\"openai\"].total_time_ms for r in comparison_results])\n",
    "        openai_total_tokens = sum([r[\"openai\"].get_metrics()[\"total_tokens\"] for r in comparison_results])\n",
    "        \n",
    "        print(f\"\\nOpenAI ({llm_simple.model}):\")\n",
    "        print(f\"  Succes: {openai_success}/{len(comparison_results)}\")\n",
    "        print(f\"  Iterations moyenne: {openai_avg_iter:.2f}\")\n",
    "        print(f\"  Temps moyen: {openai_avg_time:.0f}ms\")\n",
    "        print(f\"  Tokens totaux: {openai_total_tokens}\")\n",
    "        \n",
    "        # Anthropic\n",
    "        anthropic_success = sum(1 for r in comparison_results if r[\"anthropic\"].success)\n",
    "        anthropic_avg_iter = np.mean([r[\"anthropic\"].total_iterations for r in comparison_results])\n",
    "        anthropic_avg_time = np.mean([r[\"anthropic\"].total_time_ms for r in comparison_results])\n",
    "        anthropic_total_tokens = sum([r[\"anthropic\"].get_metrics()[\"total_tokens\"] for r in comparison_results])\n",
    "        \n",
    "        print(f\"\\nAnthropic ({llm_anthropic.model}):\")\n",
    "        print(f\"  Succes: {anthropic_success}/{len(comparison_results)}\")\n",
    "        print(f\"  Iterations moyenne: {anthropic_avg_iter:.2f}\")\n",
    "        print(f\"  Temps moyen: {anthropic_avg_time:.0f}ms\")\n",
    "        print(f\"  Tokens totaux: {anthropic_total_tokens}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"[Warning] matplotlib non disponible pour visualisations\")\n",
    "    \n",
    "elif api_ok and not anthropic_ok:\n",
    "    print(\"[INFO] Comparaison sautee - Anthropic non configure\")\n",
    "    print(\"Pour comparer les providers, ajoutez ANTHROPIC_API_KEY dans .env\")\n",
    "else:\n",
    "    print(\"Comparaison sautee (APIs non configurees)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Conclusion : Analyse des Resultats\n",
    "\n",
    "Cette section a presente des **exemples reels** de generation de preuves Lean assistees par LLM.\n",
    "\n",
    "#### Resultats attendus\n",
    "\n",
    "Avec une API configuree, vous devriez observer :\n",
    "\n",
    "| Categorie | Taux de succes attendu | Iterations moyennes | Observations |\n",
    "|-----------|------------------------|---------------------|--------------|\n",
    "| **SIMPLE_THEOREMS** | 80-100% | 1-2 | LLM connait les lemmes standard (add_comm, mul_assoc) |\n",
    "| **MATHLIB_THEOREMS** | 50-80% | 2-4 | Tactiques Mathlib necessitent plus d'essais (ring, linarith) |\n",
    "| **Comparaison providers** | Similaire | Varie | OpenAI parfois plus rapide, Anthropic parfois plus precis |\n",
    "\n",
    "#### Patterns observes\n",
    "\n",
    "**Tactiques les plus generees** :\n",
    "1. `exact Nat.xxx` - Utilisation directe de lemmes Mathlib\n",
    "2. `rfl` - Reflexivite pour egalites triviales\n",
    "3. `omega` - Solveur arithmetique puissant\n",
    "4. `simp` - Simplification automatique\n",
    "5. `ring` - Algebre polynomiale\n",
    "\n",
    "**Types d'erreurs courantes** :\n",
    "1. **Typos** : `Nat.add_com` au lieu de `Nat.add_comm`\n",
    "2. **Tactique inadaptee** : `rfl` sur une egalite non triviale\n",
    "3. **Imports manquants** : `ring` sans `import Mathlib.Tactic.Ring`\n",
    "4. **Timeout** : Tactiques trop lentes sur theoremes complexes\n",
    "\n",
    "#### Optimisations possibles\n",
    "\n",
    "1. **Few-shot learning** : Fournir 2-3 exemples similaires ameliore drastiquement le taux de succes\n",
    "2. **Temperature adaptive** : Basse (0.2-0.3) pour simple, haute (0.4-0.6) pour complexe\n",
    "3. **Retry avec variation** : Essayer plusieurs temperatures si echec\n",
    "4. **Multi-provider fallback** : Si OpenAI echoue, essayer Anthropic\n",
    "5. **Contexte enrichi** : Donner les imports, variables, et hypotheses\n",
    "\n",
    "#### Couts et performances\n",
    "\n",
    "**Ordre de grandeur** (theoreme simple, 2 iterations) :\n",
    "- **Tokens** : 400-800 par preuve\n",
    "- **Temps** : 1-3 secondes (latence API + verification Lean)\n",
    "- **Cout** : ~$0.001-0.003 par preuve (GPT-4o)\n",
    "\n",
    "**Pour un projet Lean typique** (100 theoremes) :\n",
    "- **Total tokens** : 50,000-100,000\n",
    "- **Temps total** : 3-10 minutes\n",
    "- **Cout total** : $0.10-0.30\n",
    "\n",
    "---\n",
    "\n",
    "**Section 7 terminee** - Exemples progressifs et visualisations completes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Problemes d'Erdos : Benchmark pour LLMs\n",
    "\n",
    "Les **problemes d'Erdos** sont devenus un benchmark populaire pour evaluer les systemes de theorem proving assistes par LLM. Plusieurs problemes ouverts depuis des decennies ont ete resolus recemment grace a l'IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problemes d'Erdos resolus par IA:\n",
      "==================================================\n",
      "\n",
      "Erdos #379:\n",
      "  domaine: Graphes extremaux\n",
      "  ouvert_depuis: ~30 ans\n",
      "  resolu_par: DeepSeek-Prover\n",
      "  annee: 2025\n",
      "  description: Conjecture sur le nombre chromatique de graphes\n",
      "\n",
      "Erdos #987:\n",
      "  domaine: Combinatoire additive\n",
      "  resolu_par: DeepSeek-Prover\n",
      "  annee: 2025\n",
      "  description: Sommes de sous-ensembles dans Z\n",
      "\n",
      "Erdos #730:\n",
      "  domaine: Theorie des nombres\n",
      "  resolu_par: DeepSeek-Prover\n",
      "  annee: 2025\n",
      "  description: Nombres premiers jumeaux generalises\n",
      "\n",
      "Erdos #198:\n",
      "  domaine: Ensembles\n",
      "  resolu_par: DeepSeek-Prover\n",
      "  annee: 2025\n",
      "  description: Intersections d'ensembles disjoints\n",
      "\n",
      "Erdos #124_variant:\n",
      "  domaine: Graphes aleatoires\n",
      "  ouvert_depuis: ~30 ans\n",
      "  resolu_par: Harmonic Aristotle\n",
      "  temps: 6 heures\n",
      "  annee: 2025\n",
      "  description: Proprietes des graphes aleatoires\n",
      "\n",
      "==================================================\n",
      "\n",
      "Depuis Noel 2025, une vague de resolutions a frappe la communaute:\n",
      "\n",
      "- 15+ problemes Erdos resolus par des systemes IA en quelques semaines\n",
      "- Harmonic Aristotle a obtenu la medaille d'or IMO 2025\n",
      "- Terry Tao a formalise des resultats en theorie analytique des nombres avec Mathlib\n",
      "- DeepSeek, Google, et Harmonic en competition pour resoudre des problemes ouverts\n",
      "\n",
      "Cette acceleration est due a:\n",
      "1. Meilleurs modeles de base (Gemini 2.0, GPT-5, Claude 3.5)\n",
      "2. Fine-tuning specifique sur preuves Lean\n",
      "3. Mathlib4 avec 4M+ lignes de mathematiques formalisees\n",
      "4. Techniques de recherche ameliorees (MCTS, beam search)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Problemes d'Erdos resolus par des systemes IA (mise a jour janvier 2026)\n",
    "\n",
    "ERDOS_SOLVED = {\n",
    "    \"379\": {\n",
    "        \"domaine\": \"Graphes extremaux\",\n",
    "        \"ouvert_depuis\": \"~30 ans\",\n",
    "        \"resolu_par\": \"DeepSeek-Prover\",\n",
    "        \"annee\": \"2025\",\n",
    "        \"description\": \"Conjecture sur le nombre chromatique de graphes\"\n",
    "    },\n",
    "    \"987\": {\n",
    "        \"domaine\": \"Combinatoire additive\",\n",
    "        \"resolu_par\": \"DeepSeek-Prover\",\n",
    "        \"annee\": \"2025\",\n",
    "        \"description\": \"Sommes de sous-ensembles dans Z\"\n",
    "    },\n",
    "    \"730\": {\n",
    "        \"domaine\": \"Theorie des nombres\",\n",
    "        \"resolu_par\": \"DeepSeek-Prover\",\n",
    "        \"annee\": \"2025\",\n",
    "        \"description\": \"Nombres premiers jumeaux generalises\"\n",
    "    },\n",
    "    \"198\": {\n",
    "        \"domaine\": \"Ensembles\",\n",
    "        \"resolu_par\": \"DeepSeek-Prover\",\n",
    "        \"annee\": \"2025\",\n",
    "        \"description\": \"Intersections d'ensembles disjoints\"\n",
    "    },\n",
    "    \"124_variant\": {\n",
    "        \"domaine\": \"Graphes aleatoires\",\n",
    "        \"ouvert_depuis\": \"~30 ans\",\n",
    "        \"resolu_par\": \"Harmonic Aristotle\",\n",
    "        \"temps\": \"6 heures\",\n",
    "        \"annee\": \"2025\",\n",
    "        \"description\": \"Proprietes des graphes aleatoires\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Explosion des resolutions depuis Noel 2025\n",
    "CHRISTMAS_ERDOS_WAVE = \"\"\"\n",
    "Depuis Noel 2025, une vague de resolutions a frappe la communaute:\n",
    "\n",
    "- 15+ problemes Erdos resolus par des systemes IA en quelques semaines\n",
    "- Harmonic Aristotle a obtenu la medaille d'or IMO 2025\n",
    "- Terry Tao a formalise des resultats en theorie analytique des nombres avec Mathlib\n",
    "- DeepSeek, Google, et Harmonic en competition pour resoudre des problemes ouverts\n",
    "\n",
    "Cette acceleration est due a:\n",
    "1. Meilleurs modeles de base (Gemini 2.0, GPT-5, Claude 3.5)\n",
    "2. Fine-tuning specifique sur preuves Lean\n",
    "3. Mathlib4 avec 4M+ lignes de mathematiques formalisees\n",
    "4. Techniques de recherche ameliorees (MCTS, beam search)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Problemes d'Erdos resolus par IA:\")\n",
    "print(\"=\" * 50)\n",
    "for num, info in ERDOS_SOLVED.items():\n",
    "    print(f\"\\nErdos #{num}:\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(CHRISTMAS_ERDOS_WAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Creer un prompt pour une preuve simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercice 1: Creer un prompt\n",
      "Voir SOLUTION_PROMPT pour la solution\n"
     ]
    }
   ],
   "source": [
    "# Completez ce prompt pour demander une preuve de la commutativite de la multiplication\n",
    "\n",
    "EXERCISE_PROMPT = \"\"\"\n",
    "# Votre prompt ici\n",
    "# Objectif: obtenir une preuve de (a * b = b * a) en Lean 4\n",
    "\"\"\"\n",
    "\n",
    "# Solution:\n",
    "SOLUTION_PROMPT = \"\"\"\n",
    "Tu es un expert en Lean 4.\n",
    "\n",
    "Ecris une preuve pour le theoreme suivant:\n",
    "\n",
    "```lean\n",
    "theorem mul_comm (a b : Nat) : a * b = b * a\n",
    "```\n",
    "\n",
    "Utilise la tactique appropriee de la bibliotheque standard.\n",
    "Donne uniquement le code Lean complet.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Exercice 1: Creer un prompt\")\n",
    "print(\"Voir SOLUTION_PROMPT pour la solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Implementer une boucle de correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La preuve suivante contient des erreurs:\n",
      "\n",
      "Theoreme: theorem test : 1 + 1 = 2\n",
      "\n",
      "Preuve actuelle:\n",
      "```lean\n",
      "by rfl\n",
      "```\n",
      "\n",
      "Erreurs Lean:\n",
      "```\n",
      "type mismatch\n",
      "```\n",
      "\n",
      "Corrige ces erreurs et fournis la preuve comple...\n"
     ]
    }
   ],
   "source": [
    "def correction_loop(theorem: str, initial_proof: str, errors: list):\n",
    "    \"\"\"\n",
    "    Implemente une boucle de correction iterative.\n",
    "    \n",
    "    Args:\n",
    "        theorem: Le theoreme a prouver\n",
    "        initial_proof: La preuve initiale (avec erreurs)\n",
    "        errors: Liste d'erreurs Lean\n",
    "    \n",
    "    Returns:\n",
    "        Le prompt pour corriger la preuve\n",
    "    \"\"\"\n",
    "    # Votre code ici\n",
    "    pass\n",
    "\n",
    "# Solution:\n",
    "def correction_loop_solution(theorem: str, initial_proof: str, errors: list) -> str:\n",
    "    error_text = \"\\n\".join(errors)\n",
    "    return f\"\"\"\n",
    "La preuve suivante contient des erreurs:\n",
    "\n",
    "Theoreme: {theorem}\n",
    "\n",
    "Preuve actuelle:\n",
    "```lean\n",
    "{initial_proof}\n",
    "```\n",
    "\n",
    "Erreurs Lean:\n",
    "```\n",
    "{error_text}\n",
    "```\n",
    "\n",
    "Corrige ces erreurs et fournis la preuve complete et correcte.\n",
    "    \"\"\"\n",
    "\n",
    "# Test\n",
    "test_prompt = correction_loop_solution(\n",
    "    \"theorem test : 1 + 1 = 2\",\n",
    "    \"by rfl\",\n",
    "    [\"type mismatch\"]\n",
    ")\n",
    "print(test_prompt[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume\n",
    "\n",
    "### Points cles\n",
    "\n",
    "| Systeme | Approche | Force | Publication |\n",
    "|---------|----------|-------|-------------|\n",
    "| **LeanCopilot** | Suggestions temps reel | Integration IDE | NeurIPS 2025 |\n",
    "| **LeanProgress** | Prediction progression | Guidage recherche | TMLR 2025 |\n",
    "| **LeanAgent** | Lifelong learning | Adaptation | ICLR 2025 |\n",
    "| **AlphaProof** | RL + generation massive | Theoremes difficiles | Nature 2025 |\n",
    "| **APOLLO** | Automatisation complete | Scalabilite | arXiv 2505 |\n",
    "| **Harmonic Aristotle** | Decomposition + search | Problemes ouverts | IMO 2025 Gold |\n",
    "\n",
    "### Tactiques de prompting\n",
    "\n",
    "1. **Contexte precis** : version Lean 4, imports, hypotheses\n",
    "2. **But clairement formule** : types explicites, theoreme exact\n",
    "3. **Feedback erreurs Lean** : messages d'erreur complets\n",
    "4. **Exemples similaires** : few-shot avec preuves reussies\n",
    "5. **Iterations successives** : corriger et re-soumettre\n",
    "\n",
    "### Ressources et liens\n",
    "\n",
    "| Ressource | URL |\n",
    "|-----------|-----|\n",
    "| LeanDojo (ecosysteme) | https://leandojo.org |\n",
    "| LeanCopilot (GitHub) | https://github.com/lean-dojo/LeanCopilot |\n",
    "| AlphaProof (Nature) | https://www.nature.com/articles/s41586-025-08589-7 |\n",
    "| APOLLO (arXiv) | https://arxiv.org/abs/2505.05758 |\n",
    "| Mathlib4 | https://github.com/leanprover-community/mathlib4 |\n",
    "| Loogle (recherche) | https://loogle.lean-lang.org |\n",
    "| Moogle (semantic) | https://www.moogle.ai |\n",
    "| Xena Project (formalisations) | https://xenaproject.wordpress.com |\n",
    "\n",
    "### Prochaine etape\n",
    "\n",
    "Dans le notebook **Lean-8-Agentic-Proving**, nous construirons un **systeme multi-agents** capable de prouver des theoremes de maniere autonome, en orchestrant :\n",
    "- **Agent de recherche** : Trouve des lemmes pertinents dans Mathlib\n",
    "- **Agent de generation** : Propose des tactiques et preuves\n",
    "- **Agent de verification** : Valide avec Lean et fournit du feedback\n",
    "- **Orchestrateur** : Coordonne les agents avec Semantic Kernel\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook base sur les percees IA 2024-2026 en theorem proving (AlphaProof/Nature 2025, APOLLO, LeanCopilot/NeurIPS 2025, LeanAgent/ICLR 2025, LeanProgress/TMLR 2025)*\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation** : [← Lean-6-Mathlib-Essentials](Lean-6-Mathlib-Essentials.ipynb) | [Index](Lean-1-Setup.ipynb) | [Lean-8-Agentic-Proving →](Lean-8-Agentic-Proving.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (WSL)",
   "language": "python",
   "name": "python3-wsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}