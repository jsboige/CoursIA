{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navigation** : [‚Üê Lean-8-Agentic-Proving](Lean-8-Agentic-Proving.ipynb) | [Index](Lean-1-Setup.ipynb) | [Lean-10-LeanDojo ‚Üí](Lean-10-LeanDojo.ipynb)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lean 9 : Multi-Agents avec Semantic Kernel\n",
    "\n",
    "## üéØ Architecture du Syst√®me Multi-Agents\n",
    "\n",
    "### Vue d'ensemble\n",
    "\n",
    "Notre syst√®me utilise **5 agents sp√©cialis√©s** qui collaborent pour prouver des th√©or√®mes Lean :\n",
    "\n",
    "1. **SearchAgent** : Recherche de lemmes pertinents dans Mathlib\n",
    "2. **TacticAgent** : G√©n√©ration de tactiques Lean appropri√©es\n",
    "3. **VerifierAgent** : V√©rification formelle des preuves\n",
    "4. **CriticAgent** : Analyse et suggestions d'am√©lioration\n",
    "5. **CoordinatorAgent** : Orchestration et d√©cisions strat√©giques\n",
    "\n",
    "### Pourquoi 5 agents ?\n",
    "\n",
    "Chaque agent a une **responsabilit√© unique** (principe de s√©paration des pr√©occupations) :\n",
    "\n",
    "- **S√©paration des comp√©tences** : Recherche ‚â† G√©n√©ration ‚â† V√©rification\n",
    "- **Sp√©cialisation** : Chaque LLM est prompt√© pour une t√¢che pr√©cise\n",
    "- **Robustesse** : Si un agent √©choue, les autres continuent\n",
    "- **Tra√ßabilit√©** : On sait quel agent a pris quelle d√©cision\n",
    "\n",
    "### Communication : √âtat partag√© vs Message passing\n",
    "\n",
    "Deux approches classiques en multi-agents :\n",
    "\n",
    "| **Message Passing** | **√âtat Partag√©** (notre choix) |\n",
    "|---------------------|--------------------------------|\n",
    "| Agents s'envoient des messages | Tous les agents lisent/√©crivent un √©tat central |\n",
    "| D√©centralis√© | Centralis√© |\n",
    "| Complexe √† orchestrer | Facile √† suivre |\n",
    "| Pas de snapshot global | Snapshot complet √† chaque it√©ration |\n",
    "\n",
    "**Pourquoi √©tat partag√© ?**\n",
    "\n",
    "- Besoin de **coh√©rence globale** (historique des tactiques, m√©triques)\n",
    "- **Debugging facilit√©** : On peut inspecter l'√©tat apr√®s chaque tour\n",
    "- **Snapshots JSON** : Permet de reproduire exactement une session\n",
    "- Semantic Kernel supporte ce pattern avec les **plugins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction : Semantic Kernel pour Preuves (Python)\n",
    "\n",
    "### 5.1 Vue d'ensemble\n",
    "\n",
    "Microsoft **Semantic Kernel** est un SDK qui permet d'orchestrer des LLMs avec des plugins, de la memoire et des agents intelligents. Nous allons implementer un systeme multi-agents pour theorem proving inspire des patterns utilises dans l'analyse argumentative (voir `Argument_Analysis` notebooks).\n",
    "\n",
    "**Composants cles** :\n",
    "- **Kernel** : Point d'entree principal, configure les services LLM\n",
    "- **Plugins** : Fonctions appelables par les agents (decorated avec `@kernel_function`)\n",
    "- **Agents** : Entites autonomes avec instructions et capacites\n",
    "- **Orchestration** : Strategies de selection et terminaison des agents\n",
    "\n",
    "### 5.2 Dependances\n",
    "\n",
    "```python\n",
    "# Installation\n",
    "pip install semantic-kernel openai python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä √âtat Partag√© : La Classe `ProofState`\n",
    "\n",
    "La classe `ProofState` est le **c≈ìur du syst√®me**. Elle contient :\n",
    "\n",
    "### 1. Phase de preuve (`ProofPhase` enum)\n",
    "```\n",
    "INIT ‚Üí SEARCH ‚Üí TACTIC_GEN ‚Üí VERIFICATION ‚Üí REFINEMENT ‚Üí COMPLETE\n",
    "```\n",
    "\n",
    "Chaque phase d√©termine **quel agent agit** :\n",
    "- `INIT` ‚Üí CoordinatorAgent d√©cide de la strat√©gie\n",
    "- `SEARCH` ‚Üí SearchAgent cherche des lemmes\n",
    "- `TACTIC_GEN` ‚Üí TacticAgent g√©n√®re une tactique\n",
    "- `VERIFICATION` ‚Üí VerifierAgent teste la preuve\n",
    "- `REFINEMENT` ‚Üí CriticAgent analyse et ajuste\n",
    "- `COMPLETE` ‚Üí Session termin√©e\n",
    "\n",
    "### 2. Strat√©gie de preuve (`ProofStrategy` enum)\n",
    "\n",
    "```python\n",
    "EXPLORATION   # Recherche large de lemmes\n",
    "REFINEMENT    # Ajustement d'une preuve existante\n",
    "VALIDATION    # V√©rification formelle\n",
    "RECOVERY      # R√©cup√©ration apr√®s erreur\n",
    "```\n",
    "\n",
    "La strat√©gie influence **quels lemmes rechercher** et **quelles tactiques essayer**.\n",
    "\n",
    "### 3. Historique et m√©triques\n",
    "\n",
    "- `tactic_history` : Liste de toutes les tactiques essay√©es (succ√®s + √©checs)\n",
    "- `verification_results` : R√©sultats des v√©rifications Lean\n",
    "- `current_proof` : Preuve en construction\n",
    "- `error_count` : Nombre d'erreurs rencontr√©es\n",
    "\n",
    "### 4. Snapshots JSON\n",
    "\n",
    "√Ä chaque it√©ration, on peut sauvegarder l'√©tat complet en JSON :\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"phase\": \"TACTIC_GEN\",\n",
    "  \"strategy\": \"EXPLORATION\",\n",
    "  \"iteration\": 5,\n",
    "  \"current_goal\": \"n + 0 = n\",\n",
    "  \"tactic_history\": [...],\n",
    "  \"current_proof\": [\"intro n\", \"rw [Nat.add_zero]\"]\n",
    "}\n",
    "```\n",
    "\n",
    "**Utilit√©** : Debugging, reproduction de bugs, benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration chargee depuis: /mnt/d/dev/CoursIA/MyIA.AI.Notebooks/SymbolicAI/Lean/.env\n",
      "lean_runner importe avec succes depuis /mnt/d/dev/CoursIA/MyIA.AI.Notebooks/SymbolicAI/Lean\n",
      "\n",
      "============================================================\n",
      "ProofState initialise avec succes\n",
      "LeanRunner disponible: True\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8.1 - ProofState: Etat Partage pour Multi-Agents\n",
    "# =============================================================================\n",
    "# Pattern inspire de RhetoricalAnalysisState dans Argument_Analysis\n",
    "# Permet la synchronisation entre agents avec designation explicite\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Detection robuste du repertoire du notebook ---\n",
    "# Fonctionne sous Windows, Linux, et WSL\n",
    "notebook_dir = None\n",
    "\n",
    "# Chemins connus (Windows et WSL)\n",
    "KNOWN_PATHS = [\n",
    "    Path(\"/mnt/d/dev/CoursIA/MyIA.AI.Notebooks/SymbolicAI/Lean\"),  # WSL\n",
    "    Path(\"/mnt/c/dev/CoursIA/MyIA.AI.Notebooks/SymbolicAI/Lean\"),  # WSL (C:)\n",
    "    Path(\"d:/dev/CoursIA/MyIA.AI.Notebooks/SymbolicAI/Lean\"),      # Windows\n",
    "    Path(\"D:/dev/CoursIA/MyIA.AI.Notebooks/SymbolicAI/Lean\"),      # Windows\n",
    "]\n",
    "\n",
    "# Strategie 1: Variable d'environnement LEAN_NOTEBOOK_DIR\n",
    "if os.getenv(\"LEAN_NOTEBOOK_DIR\"):\n",
    "    notebook_dir = Path(os.getenv(\"LEAN_NOTEBOOK_DIR\"))\n",
    "    if not (notebook_dir / \"lean_runner.py\").exists():\n",
    "        notebook_dir = None\n",
    "\n",
    "# Strategie 2: Chemins connus\n",
    "if not notebook_dir:\n",
    "    for known_path in KNOWN_PATHS:\n",
    "        if known_path.exists() and (known_path / \"lean_runner.py\").exists():\n",
    "            notebook_dir = known_path\n",
    "            break\n",
    "\n",
    "# Strategie 3: Chercher dans cwd et parents\n",
    "if not notebook_dir:\n",
    "    cwd = Path.cwd()\n",
    "    candidates = [cwd, cwd / \"MyIA.AI.Notebooks\" / \"SymbolicAI\" / \"Lean\"]\n",
    "\n",
    "    # Remonter jusqu'a 5 niveaux\n",
    "    current = cwd\n",
    "    for _ in range(5):\n",
    "        candidates.append(current)\n",
    "        lean_path = current / \"MyIA.AI.Notebooks\" / \"SymbolicAI\" / \"Lean\"\n",
    "        if lean_path.exists():\n",
    "            candidates.append(lean_path)\n",
    "        if current.parent == current:\n",
    "            break\n",
    "        current = current.parent\n",
    "\n",
    "    for candidate in candidates:\n",
    "        if candidate.exists() and (candidate / \"lean_runner.py\").exists():\n",
    "            notebook_dir = candidate\n",
    "            break\n",
    "\n",
    "# Strategie 4: Fallback sur cwd\n",
    "if not notebook_dir:\n",
    "    notebook_dir = Path.cwd()\n",
    "    print(f\"[WARN] lean_runner.py non trouve, fallback sur: {notebook_dir}\")\n",
    "\n",
    "# --- Charger .env ---\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    env_paths = [\n",
    "        notebook_dir / \".env\",\n",
    "        notebook_dir.parent / \".env\",\n",
    "        Path.home() / \".env\"\n",
    "    ]\n",
    "    for p in env_paths:\n",
    "        if p.exists():\n",
    "            load_dotenv(p, override=True)\n",
    "            print(f\"Configuration chargee depuis: {p}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"Aucun fichier .env trouve\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv non installe\")\n",
    "\n",
    "# --- Importer lean_runner.py ---\n",
    "if notebook_dir and str(notebook_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_dir))\n",
    "\n",
    "try:\n",
    "    from lean_runner import LeanRunner, LeanResult\n",
    "    print(f\"lean_runner importe avec succes depuis {notebook_dir}\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERREUR: Impossible d'importer lean_runner: {e}\")\n",
    "    print(f\"Repertoire de travail: {Path.cwd()}\")\n",
    "    print(f\"notebook_dir detecte: {notebook_dir}\")\n",
    "    print(f\"sys.path: {sys.path[:5]}\")\n",
    "    raise\n",
    "\n",
    "# --- Enumerations ---\n",
    "\n",
    "class ProofStrategy(Enum):\n",
    "    \"\"\"Strategie de preuve en cours.\"\"\"\n",
    "    EXPLORATION = \"exploration\"      # Recherche initiale de lemmes\n",
    "    REFINEMENT = \"refinement\"        # Affinage des tactiques\n",
    "    VALIDATION = \"validation\"        # Verification finale\n",
    "    RECOVERY = \"recovery\"            # Recuperation apres echecs\n",
    "\n",
    "class TacticDifficulty(Enum):\n",
    "    \"\"\"Niveau de difficulte des tactiques.\"\"\"\n",
    "    SIMPLE = \"simple\"      # rfl, exact, omega\n",
    "    INTERMEDIATE = \"intermediate\"  # simp, ring, linarith\n",
    "    ADVANCED = \"advanced\"  # induction, cases\n",
    "\n",
    "class ProofPhase(Enum):\n",
    "    \"\"\"Phase de la boucle de preuve.\"\"\"\n",
    "    INIT = \"init\"\n",
    "    SEARCH = \"search\"\n",
    "    GENERATE = \"generate\"\n",
    "    VERIFY = \"verify\"\n",
    "    ANALYZE = \"analyze\"\n",
    "    COMPLETE = \"complete\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "# --- ProofState: Etat partage entre agents ---\n",
    "\n",
    "@dataclass\n",
    "class TacticAttempt:\n",
    "    \"\"\"Une tentative de tactique.\"\"\"\n",
    "    tactic: str\n",
    "    success: bool\n",
    "    error: Optional[str] = None\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "    state_before: Optional[str] = None\n",
    "    confidence: Optional[float] = None\n",
    "    explanation: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class ProofState:\n",
    "    \"\"\"\n",
    "    Etat partage entre les agents pour la preuve d'un theoreme.\n",
    "    Permet la coordination sans couplage fort.\n",
    "    \"\"\"\n",
    "    # Identifiants\n",
    "    session_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])\n",
    "    theorem_name: str = \"\"\n",
    "    theorem_statement: str = \"\"\n",
    "\n",
    "    # Etat de la preuve\n",
    "    current_goal: str = \"\"\n",
    "    current_proof: List[str] = field(default_factory=list)\n",
    "    phase: ProofPhase = ProofPhase.INIT\n",
    "    strategy: ProofStrategy = ProofStrategy.EXPLORATION\n",
    "\n",
    "    # Resultats des agents\n",
    "    discovered_lemmas: List[str] = field(default_factory=list)\n",
    "    generated_tactics: List[str] = field(default_factory=list)\n",
    "    tactic_history: List[TacticAttempt] = field(default_factory=list)\n",
    "\n",
    "    # Metriques\n",
    "    iteration: int = 0\n",
    "    max_iterations: int = 10\n",
    "    start_time: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "    # Erreurs et diagnostics\n",
    "    last_error: Optional[str] = None\n",
    "    final_proof: Optional[str] = None\n",
    "    error_count: int = 0\n",
    "\n",
    "    # Verification tracking\n",
    "    verification_results: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    total_lean_time_ms: float = 0.0\n",
    "\n",
    "    # Agent designation for orchestration\n",
    "    _next_agent: Optional[str] = field(default=None, repr=False)\n",
    "\n",
    "    def add_tactic_attempt(self, tactic: str, state_before: Optional[str] = None,\n",
    "                           confidence: Optional[float] = None, explanation: Optional[str] = None,\n",
    "                           success: bool = False, error: Optional[str] = None) -> str:\n",
    "        \"\"\"Enregistre une tentative de tactique.\"\"\"\n",
    "        attempt_id = f\"attempt_{len(self.tactic_history) + 1}\"\n",
    "        self.tactic_history.append(TacticAttempt(\n",
    "            tactic=tactic,\n",
    "            success=success,\n",
    "            error=error,\n",
    "            state_before=state_before,\n",
    "            confidence=confidence,\n",
    "            explanation=explanation\n",
    "        ))\n",
    "        if success:\n",
    "            self.current_proof.append(tactic)\n",
    "        else:\n",
    "            self.error_count += 1\n",
    "            self.last_error = error\n",
    "        return attempt_id\n",
    "\n",
    "    def add_lemma(self, name: str, statement: str, namespace: str = \"\", relevance: float = 0.5) -> str:\n",
    "        \"\"\"Ajoute un lemme decouvert a la liste.\"\"\"\n",
    "        lemma_id = f\"{namespace}.{name}\" if namespace else name\n",
    "        lemma_info = f\"{lemma_id}: {statement} (relevance: {relevance})\"\n",
    "        if lemma_info not in self.discovered_lemmas:\n",
    "            self.discovered_lemmas.append(lemma_info)\n",
    "        return lemma_id\n",
    "\n",
    "    def get_context_summary(self) -> str:\n",
    "        \"\"\"Resume le contexte pour les agents.\"\"\"\n",
    "        return f\"\"\"\n",
    "Theoreme: {self.theorem_name}\n",
    "Enonce: {self.theorem_statement}\n",
    "But actuel: {self.current_goal}\n",
    "Phase: {self.phase.value}\n",
    "Strategie: {self.strategy.value}\n",
    "Iteration: {self.iteration}/{self.max_iterations}\n",
    "Tactiques reussies: {len(self.current_proof)}\n",
    "Erreurs: {self.error_count}\n",
    "Derniere erreur: {self.last_error or 'Aucune'}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # --- Properties for compatibility ---\n",
    "    @property\n",
    "    def tactics_history(self) -> List[TacticAttempt]:\n",
    "        \"\"\"Alias pour tactic_history (compatibilite).\"\"\"\n",
    "        return self.tactic_history\n",
    "\n",
    "    @property\n",
    "    def proof_complete(self) -> bool:\n",
    "        \"\"\"True si la preuve est complete.\"\"\"\n",
    "        return self.phase == ProofPhase.COMPLETE\n",
    "    \n",
    "    @proof_complete.setter\n",
    "    def proof_complete(self, value: bool):\n",
    "        \"\"\"Definit la completion de la preuve.\"\"\"\n",
    "        if value:\n",
    "            self.phase = ProofPhase.COMPLETE\n",
    "        elif self.phase == ProofPhase.COMPLETE:\n",
    "            self.phase = ProofPhase.VERIFY\n",
    "    \n",
    "    @property\n",
    "    def iteration_count(self) -> int:\n",
    "        \"\"\"Alias pour iteration (compatibilite).\"\"\"\n",
    "        return self.iteration\n",
    "    \n",
    "    @iteration_count.setter\n",
    "    def iteration_count(self, value: int):\n",
    "        \"\"\"Definit le compteur d'iterations.\"\"\"\n",
    "        self.iteration = value\n",
    "\n",
    "    def increment_iteration(self):\n",
    "        \"\"\"Incremente le compteur d'iterations.\"\"\"\n",
    "        self.iteration += 1\n",
    "    \n",
    "    def designate_next_agent(self, agent_name: str):\n",
    "        \"\"\"Designe l'agent qui doit intervenir ensuite.\"\"\"\n",
    "        self._next_agent = agent_name\n",
    "    \n",
    "    def consume_next_agent_designation(self) -> Optional[str]:\n",
    "        \"\"\"Retourne et efface la designation d'agent.\"\"\"\n",
    "        agent = self._next_agent\n",
    "        self._next_agent = None\n",
    "        return agent\n",
    "    \n",
    "    def get_state_snapshot(self, summarize: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Retourne un snapshot de l'etat pour les plugins.\"\"\"\n",
    "        if summarize:\n",
    "            return {\n",
    "                \"session_id\": self.session_id,\n",
    "                \"theorem\": self.theorem_statement,\n",
    "                \"goal\": self.current_goal,\n",
    "                \"phase\": self.phase.value,\n",
    "                \"strategy\": self.strategy.value,\n",
    "                \"iteration\": f\"{self.iteration}/{self.max_iterations}\",\n",
    "                \"proof_steps\": len(self.current_proof),\n",
    "                \"discovered_lemmas\": len(self.discovered_lemmas),\n",
    "                \"errors\": self.error_count,\n",
    "                \"last_error\": self.last_error\n",
    "            }\n",
    "        else:\n",
    "            return self.to_dict()\n",
    "\n",
    "\n",
    "    def add_verification(self, attempt_id: str, success: bool, output: str, errors: str,\n",
    "                         remaining_goals: Optional[str] = None, exec_time_ms: float = 0.0,\n",
    "                         mode: str = \"subprocess\") -> str:\n",
    "        \"\"\"Enregistre un r√©sultat de v√©rification Lean.\"\"\"\n",
    "        verif_id = f\"verif_{len(self.verification_results) + 1}\"\n",
    "        self.verification_results.append({\n",
    "            \"id\": verif_id,\n",
    "            \"attempt_id\": attempt_id,\n",
    "            \"success\": success,\n",
    "            \"output\": output,\n",
    "            \"errors\": errors,\n",
    "            \"remaining_goals\": remaining_goals,\n",
    "            \"exec_time_ms\": exec_time_ms,\n",
    "            \"mode\": mode,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        return verif_id\n",
    "\n",
    "\n",
    "    def set_proof_complete(self, proof: str):\n",
    "        \"\"\"Marque la preuve comme termin√©e et change la phase.\"\"\"\n",
    "        self.final_proof = proof\n",
    "        self.phase = ProofPhase.COMPLETE\n",
    "\n",
    "\n",
    "    def set_strategy(self, strategy: 'ProofStrategy'):\n",
    "        \"\"\"Change la strat√©gie de preuve.\"\"\"\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Serialise l'etat.\"\"\"\n",
    "        return {\n",
    "            \"session_id\": self.session_id,\n",
    "            \"theorem_name\": self.theorem_name,\n",
    "            \"theorem_statement\": self.theorem_statement,\n",
    "            \"current_goal\": self.current_goal,\n",
    "            \"current_proof\": self.current_proof,\n",
    "            \"phase\": self.phase.value,\n",
    "            \"strategy\": self.strategy.value,\n",
    "            \"discovered_lemmas\": self.discovered_lemmas,\n",
    "            \"generated_tactics\": self.generated_tactics,\n",
    "            \"iteration\": self.iteration,\n",
    "            \"max_iterations\": self.max_iterations,\n",
    "            \"error_count\": self.error_count,\n",
    "            \"last_error\": self.last_error\n",
    "        }\n",
    "\n",
    "# --- Test de l'initialisation ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ProofState initialise avec succes\")\n",
    "print(f\"LeanRunner disponible: {LeanRunner is not None}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Vue d'ensemble des Plugins\n",
    "\n",
    "L'architecture utilise 4 plugins specialises, chacun exposant des fonctions via `@kernel_function`:\n",
    "\n",
    "| Plugin | Role | Fonctions cles |\n",
    "|--------|------|----------------|\n",
    "| **ProofStateManagerPlugin** | Gestion de l'etat | get_proof_state, add_lemma, designate_next_agent |\n",
    "| **LeanSearchPlugin** | Recherche Mathlib | search_mathlib_lemmas, check_lemma_type |\n",
    "| **LeanTacticPlugin** | Generation tactiques | generate_tactics, analyze_tactic_failure |\n",
    "| **LeanVerificationPlugin** | Verification Lean | verify_proof, verify_tactic_step |\n",
    "\n",
    "Ce pattern permet aux agents d'appeler ces fonctions automatiquement grace au `FunctionChoiceBehavior.Auto()` de Semantic Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîå Plugins Semantic Kernel : Exposer l'√âtat aux Agents\n",
    "\n",
    "### Probl√®me\n",
    "\n",
    "Les agents LLM ne peuvent pas acc√©der directement √† `ProofState` (objet Python).\n",
    "\n",
    "### Solution : Plugins\n",
    "\n",
    "Un **plugin Semantic Kernel** expose des m√©thodes Python comme **fonctions appelables par le LLM**.\n",
    "\n",
    "```python\n",
    "@kernel_function(\n",
    "    description=\"Enregistre une tentative de tactique\",\n",
    "    name=\"log_tactic_attempt\"\n",
    ")\n",
    "def log_tactic_attempt(self, tactic: str, confidence: float) -> str:\n",
    "    attempt_id = self._state.add_tactic_attempt(tactic, confidence=confidence)\n",
    "    return f\"Tactique {tactic} enregistr√©e avec ID {attempt_id}\"\n",
    "```\n",
    "\n",
    "### D√©corateur `@kernel_function`\n",
    "\n",
    "- `description` : Ce que le LLM voit (\"√Ä quoi sert cette fonction ?\")\n",
    "- `name` : Nom de la fonction pour le LLM\n",
    "- Param√®tres : Doivent correspondre **EXACTEMENT** √† ce que le plugin appelle\n",
    "\n",
    "### Les 4 plugins\n",
    "\n",
    "1. **log_tactic_attempt** : Enregistrer une tactique essay√©e\n",
    "2. **add_verification_result** : Enregistrer le r√©sultat Lean\n",
    "3. **set_proof_strategy** : Changer la strat√©gie de recherche\n",
    "4. **mark_proof_complete** : D√©clarer la preuve termin√©e\n",
    "\n",
    "### Pourquoi c'est critique ?\n",
    "\n",
    "Sans plugins, le LLM ne peut que **parler** de preuves. Avec plugins, il peut **agir** :\n",
    "\n",
    "- Essayer des tactiques\n",
    "- V√©rifier formellement\n",
    "- Ajuster sa strat√©gie en temps r√©el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel disponible - utilisation des vrais decorateurs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8.2-8.5 - Plugins Semantic Kernel\n",
    "# =============================================================================\n",
    "# Architecture en 4 plugins specialises:\n",
    "# - ProofStateManagerPlugin: Gestion de l'etat partage\n",
    "# - LeanSearchPlugin: Recherche de lemmes Mathlib\n",
    "# - LeanTacticPlugin: Generation de tactiques\n",
    "# - LeanVerificationPlugin: Verification avec lean_runner.py\n",
    "\n",
    "# Import du decorateur kernel_function\n",
    "try:\n",
    "    from semantic_kernel.functions import kernel_function\n",
    "    SK_AVAILABLE = True\n",
    "    print(\"Semantic Kernel disponible - utilisation des vrais decorateurs\")\n",
    "except ImportError:\n",
    "    SK_AVAILABLE = False\n",
    "    print(\"Semantic Kernel non disponible - mode simulation\")\n",
    "    # Decorateur de simulation\n",
    "    def kernel_function(description=\"\", name=None):\n",
    "        def decorator(func):\n",
    "            func._sk_function = True\n",
    "            func._sk_description = description\n",
    "            func._sk_name = name or func.__name__\n",
    "            return func\n",
    "        return decorator\n",
    "\n",
    "# =============================================================================\n",
    "# 8.2 ProofStateManagerPlugin\n",
    "# =============================================================================\n",
    "\n",
    "class ProofStateManagerPlugin:\n",
    "    \"\"\"\n",
    "    Plugin pour gerer l'etat partage de la preuve.\n",
    "    Expose les methodes de ProofState via @kernel_function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state: ProofState):\n",
    "        self._state = state\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Obtient un apercu de l'etat actuel de la preuve (theoreme, lemmes, tactiques, etc.)\",\n",
    "        name=\"get_proof_state\"\n",
    "    )\n",
    "    def get_proof_state(self, summarize: bool = True) -> str:\n",
    "        \"\"\"Retourne l'etat actuel sous forme JSON.\"\"\"\n",
    "        snapshot = self._state.get_state_snapshot(summarize=summarize)\n",
    "        return json.dumps(snapshot, indent=2, ensure_ascii=False)\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Ajoute un lemme decouvert a l'etat partage\",\n",
    "        name=\"add_discovered_lemma\"\n",
    "    )\n",
    "    def add_discovered_lemma(\n",
    "        self, name: str, statement: str, namespace: str = \"\", relevance: float = 0.5\n",
    "    ) -> str:\n",
    "        \"\"\"Enregistre un lemme trouve par SearchAgent.\"\"\"\n",
    "        lemma_id = self._state.add_lemma(name, statement, namespace, relevance)\n",
    "        return f\"Lemme ajoute: {lemma_id} ({name})\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Enregistre une tentative de tactique avec son niveau de confiance\",\n",
    "        name=\"log_tactic_attempt\"\n",
    "    )\n",
    "    def log_tactic_attempt(\n",
    "        self, tactic: str, state_before: str, confidence: float = 0.5, explanation: str = \"\"\n",
    "    ) -> str:\n",
    "        \"\"\"Enregistre une tactique tentee par TacticAgent.\"\"\"\n",
    "        attempt_id = self._state.add_tactic_attempt(tactic, state_before, confidence, explanation)\n",
    "        return f\"Tactique enregistree: {attempt_id}\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Enregistre le resultat d'une verification Lean\",\n",
    "        name=\"add_verification_result\"\n",
    "    )\n",
    "    def add_verification_result(\n",
    "        self, attempt_id: str, success: bool, output: str, errors: str,\n",
    "        remaining_goals: str = \"\", exec_time_ms: float = 0.0\n",
    "    ) -> str:\n",
    "        \"\"\"Enregistre un resultat de verification.\"\"\"\n",
    "        verif_id = self._state.add_verification(\n",
    "            attempt_id, success, output, errors,\n",
    "            remaining_goals if remaining_goals else None, exec_time_ms, \"subprocess\"\n",
    "        )\n",
    "        status = \"OK\" if success else \"ECHEC\"\n",
    "        return f\"Verification {verif_id}: {status}\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Designe l'agent qui doit parler au prochain tour. IMPORTANT: utiliser le nom exact.\",\n",
    "        name=\"designate_next_agent\"\n",
    "    )\n",
    "    def designate_next_agent(self, agent_name: str) -> str:\n",
    "        \"\"\"Delegue au prochain agent.\"\"\"\n",
    "        valid_agents = [\"SearchAgent\", \"TacticAgent\", \"VerifierAgent\", \"CriticAgent\", \"CoordinatorAgent\"]\n",
    "        if agent_name not in valid_agents:\n",
    "            return f\"ERREUR: Agent invalide '{agent_name}'. Valides: {valid_agents}\"\n",
    "        self._state.designate_next_agent(agent_name)\n",
    "        return f\"Prochain agent: {agent_name}\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Marque la preuve comme terminee avec le code final\",\n",
    "        name=\"set_proof_complete\"\n",
    "    )\n",
    "    def set_proof_complete(self, proof_code: str) -> str:\n",
    "        \"\"\"Marque la preuve comme reussie.\"\"\"\n",
    "        self._state.set_proof_complete(proof_code)\n",
    "        return f\"PREUVE COMPLETE! Code: {proof_code[:100]}...\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Change la strategie de preuve (exploration, refinement, validation, recovery)\",\n",
    "        name=\"set_proof_strategy\"\n",
    "    )\n",
    "    def set_proof_strategy(self, strategy: str) -> str:\n",
    "        \"\"\"Change la strategie de preuve.\"\"\"\n",
    "        try:\n",
    "            self._state.set_strategy(ProofStrategy(strategy))\n",
    "            return f\"Strategie changee: {strategy}\"\n",
    "        except ValueError:\n",
    "            return f\"ERREUR: Strategie invalide '{strategy}'. Valides: exploration, refinement, validation, recovery\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. LeanSearchPlugin : Recherche de Lemmes Mathlib\n",
    "\n",
    "**Plugin exposant les m√©thodes de recherche** pour SearchAgent.\n",
    "\n",
    "#### M√©thodes Expos√©es\n",
    "\n",
    "```python\n",
    "@kernel_function\n",
    "def search_lemmas(goal: str, keywords: List[str]) -> List[Lemma]:\n",
    "    # Recherche dans Mathlib par keywords\n",
    "    # Retourne lemmes tri√©s par pertinence\n",
    "```\n",
    "\n",
    "**Pattern** : SearchAgent appelle ce plugin pour trouver lemmes Mathlib pertinents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8.3 LeanSearchPlugin\n",
    "# =============================================================================\n",
    "\n",
    "class LeanSearchPlugin:\n",
    "    \"\"\"\n",
    "    Plugin pour la recherche de lemmes dans Mathlib.\n",
    "    Utilise des patterns connus + verification #check via lean_runner.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, runner: LeanRunner):\n",
    "        self._runner = runner\n",
    "        # Base de lemmes connus (extensible)\n",
    "        self._known_lemmas = {\n",
    "            # Arithmetique de base\n",
    "            \"Nat.add_zero\": (\"n + 0 = n\", \"Nat\"),\n",
    "            \"Nat.zero_add\": (\"0 + n = n\", \"Nat\"),\n",
    "            \"Nat.add_comm\": (\"n + m = m + n\", \"Nat\"),\n",
    "            \"Nat.add_assoc\": (\"(n + m) + k = n + (m + k)\", \"Nat\"),\n",
    "            \"Nat.mul_one\": (\"n * 1 = n\", \"Nat\"),\n",
    "            \"Nat.one_mul\": (\"1 * n = n\", \"Nat\"),\n",
    "            \"Nat.mul_comm\": (\"n * m = m * n\", \"Nat\"),\n",
    "            \"Nat.mul_assoc\": (\"(n * m) * k = n * (m * k)\", \"Nat\"),\n",
    "            \"Nat.left_distrib\": (\"n * (m + k) = n * m + n * k\", \"Nat\"),\n",
    "            \"Nat.right_distrib\": (\"(n + m) * k = n * k + m * k\", \"Nat\"),\n",
    "            # Logique\n",
    "            \"And.intro\": (\"a -> b -> a /\\\\ b\", \"Logic\"),\n",
    "            \"And.left\": (\"a /\\\\ b -> a\", \"Logic\"),\n",
    "            \"And.right\": (\"a /\\\\ b -> b\", \"Logic\"),\n",
    "            \"Or.inl\": (\"a -> a \\\\/ b\", \"Logic\"),\n",
    "            \"Or.inr\": (\"b -> a \\\\/ b\", \"Logic\"),\n",
    "            \"Eq.refl\": (\"a = a\", \"Logic\"),\n",
    "            \"Eq.symm\": (\"a = b -> b = a\", \"Logic\"),\n",
    "            \"Eq.trans\": (\"a = b -> b = c -> a = c\", \"Logic\"),\n",
    "        }\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Recherche des lemmes Mathlib pertinents pour un but donne\",\n",
    "        name=\"search_mathlib_lemmas\"\n",
    "    )\n",
    "    def search_mathlib_lemmas(self, goal: str, max_results: int = 10) -> str:\n",
    "        \"\"\"\n",
    "        Recherche des lemmes par mots-cles.\n",
    "\n",
    "        Args:\n",
    "            goal: Description du but ou mots-cles (ex: \"addition commutative\")\n",
    "            max_results: Nombre maximum de resultats\n",
    "\n",
    "        Returns:\n",
    "            JSON avec les lemmes trouves\n",
    "        \"\"\"\n",
    "        goal_lower = goal.lower()\n",
    "        results = []\n",
    "\n",
    "        # Recherche par mots-cles\n",
    "        keywords = goal_lower.replace(\"+\", \"add\").replace(\"*\", \"mul\").replace(\"=\", \"eq\").split()\n",
    "\n",
    "        for name, (statement, namespace) in self._known_lemmas.items():\n",
    "            score = 0.0\n",
    "            name_lower = name.lower()\n",
    "\n",
    "            # Scoring par mots-cles\n",
    "            for kw in keywords:\n",
    "                if kw in name_lower:\n",
    "                    score += 0.3\n",
    "                if kw in statement.lower():\n",
    "                    score += 0.2\n",
    "\n",
    "            # Patterns specifiques\n",
    "            if \"comm\" in goal_lower and \"comm\" in name_lower:\n",
    "                score += 0.4\n",
    "            if \"assoc\" in goal_lower and \"assoc\" in name_lower:\n",
    "                score += 0.4\n",
    "            if \"zero\" in goal_lower and \"zero\" in name_lower:\n",
    "                score += 0.3\n",
    "            if \"distrib\" in goal_lower and \"distrib\" in name_lower:\n",
    "                score += 0.4\n",
    "\n",
    "            if score > 0:\n",
    "                results.append({\n",
    "                    \"name\": name,\n",
    "                    \"statement\": statement,\n",
    "                    \"namespace\": namespace,\n",
    "                    \"relevance\": min(score, 1.0)\n",
    "                })\n",
    "\n",
    "        # Trier par pertinence\n",
    "        results.sort(key=lambda x: x[\"relevance\"], reverse=True)\n",
    "        return json.dumps(results[:max_results], indent=2, ensure_ascii=False)\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Verifie qu'un lemme existe et retourne son type via #check\",\n",
    "        name=\"check_lemma_type\"\n",
    "    )\n",
    "    def check_lemma_type(self, lemma_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Verifie l'existence d'un lemme via #check.\n",
    "\n",
    "        Args:\n",
    "            lemma_name: Nom du lemme (ex: \"Nat.add_comm\")\n",
    "\n",
    "        Returns:\n",
    "            JSON {exists, type, error}\n",
    "        \"\"\"\n",
    "        code = f\"#check {lemma_name}\"\n",
    "        result = self._runner.run(code)\n",
    "\n",
    "        if result.success and not result.errors:\n",
    "            # Extraire le type de la sortie\n",
    "            return json.dumps({\n",
    "                \"exists\": True,\n",
    "                \"type\": result.output.strip(),\n",
    "                \"error\": None\n",
    "            })\n",
    "        else:\n",
    "            return json.dumps({\n",
    "                \"exists\": False,\n",
    "                \"type\": None,\n",
    "                \"error\": result.errors or \"Lemme non trouve\"\n",
    "            })\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8.4 LeanTacticPlugin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Plugins de Tactiques et Verification\n",
    "\n",
    "Les deux plugins restants gerent la **generation de tactiques** et la **verification formelle** :\n",
    "\n",
    "#### LeanTacticPlugin\n",
    "\n",
    "- **Responsabilite** : Generer des tactiques Lean adaptees au contexte\n",
    "- **Methodes exposees** :\n",
    "  - `generate_tactic()` : Genere une tactique basee sur goal + lemmes + historique\n",
    "  - `estimate_confidence()` : Estime la probabilite de succes (0.0-1.0)\n",
    "- **LLM-aware** : Utilise un prompt structure pour le LLM avec exemples de tactiques Lean\n",
    "- **Strategies** : `exact`, `rw`, `apply`, `simp`, `induction`, `cases`, etc.\n",
    "\n",
    "#### LeanVerificationPlugin\n",
    "\n",
    "- **Responsabilite** : Verifier les preuves via compilation Lean\n",
    "- **Methodes exposees** :\n",
    "  - `verify_proof()` : Compile le theoreme avec tactiques et retourne succes/echec\n",
    "  - `parse_lean_errors()` : Parse les messages d'erreur Lean pour feedback agents\n",
    "- **Detection de completion** : Reconnait \"no goals\" = preuve complete\n",
    "- **Gestion d'erreurs** : Extrait type d'erreur (type mismatch, tactic failed, etc.) pour CriticAgent\n",
    "\n",
    "**Flow typique** :\n",
    "```\n",
    "SearchAgent trouve lemmes\n",
    "   |\n",
    "   v\n",
    "TacticAgent genere tactique (via LeanTacticPlugin)\n",
    "   |\n",
    "   v\n",
    "VerifierAgent compile (via LeanVerificationPlugin)\n",
    "   |\n",
    "   +-- Success ‚Üí COMPLETE\n",
    "   +-- Failure ‚Üí CriticAgent analyse ‚Üí retry\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "\n",
    "class LeanTacticPlugin:\n",
    "    \"\"\"\n",
    "    Plugin pour la generation de tactiques.\n",
    "    Fournit des heuristiques et analyse les echecs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Tactiques par difficulte\n",
    "        self._tactics = {\n",
    "            \"simple\": [\"rfl\", \"trivial\", \"exact ?_\", \"assumption\"],\n",
    "            \"medium\": [\"simp\", \"omega\", \"decide\", \"constructor\", \"intro\", \"apply\"],\n",
    "            \"complex\": [\"ring\", \"linarith\", \"aesop\", \"induction\", \"cases\", \"rcases\"]\n",
    "        }\n",
    "\n",
    "        # Heuristiques par pattern de but\n",
    "        self._heuristics = {\n",
    "            \"equality\": [\"rfl\", \"exact\", \"simp\", \"ring\", \"omega\"],\n",
    "            \"forall\": [\"intro\", \"intros\", \"apply\"],\n",
    "            \"exists\": [\"use\", \"exists\", \"exact\"],\n",
    "            \"and\": [\"constructor\", \"exact And.intro\"],\n",
    "            \"or\": [\"left\", \"right\"],\n",
    "            \"implication\": [\"intro\", \"apply\", \"exact\"],\n",
    "            \"nat_arithmetic\": [\"omega\", \"simp\", \"decide\"],\n",
    "            \"ring_expression\": [\"ring\", \"ring_nf\"]\n",
    "        }\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Genere des tactiques appropriees pour un but donne\",\n",
    "        name=\"generate_tactics\"\n",
    "    )\n",
    "    def generate_tactics(self, goal: str, context: str = \"\", difficulty: str = \"simple\") -> str:\n",
    "        \"\"\"\n",
    "        Genere des tactiques pour le but courant.\n",
    "\n",
    "        Args:\n",
    "            goal: Le but Lean a prouver\n",
    "            context: Contexte additionnel (lemmes disponibles, etc.)\n",
    "            difficulty: simple, medium, ou complex\n",
    "\n",
    "        Returns:\n",
    "            JSON [{tactic, confidence, explanation}]\n",
    "        \"\"\"\n",
    "        suggestions = []\n",
    "        goal_lower = goal.lower()\n",
    "\n",
    "        # Detecter le type de but\n",
    "        detected_patterns = []\n",
    "        if \"=\" in goal:\n",
    "            detected_patterns.append(\"equality\")\n",
    "        if \"forall\" in goal_lower or \"‚àÄ\" in goal:\n",
    "            detected_patterns.append(\"forall\")\n",
    "        if \"exists\" in goal_lower or \"‚àÉ\" in goal:\n",
    "            detected_patterns.append(\"exists\")\n",
    "        if \"/\\\\\" in goal or \"‚àß\" in goal or \"And\" in goal:\n",
    "            detected_patterns.append(\"and\")\n",
    "        if \"\\\\/\" in goal or \"‚à®\" in goal or \"Or\" in goal:\n",
    "            detected_patterns.append(\"or\")\n",
    "        if \"->\" in goal or \"‚Üí\" in goal:\n",
    "            detected_patterns.append(\"implication\")\n",
    "        if any(x in goal_lower for x in [\"nat\", \"n +\", \"m +\", \"+ 0\", \"0 +\"]):\n",
    "            detected_patterns.append(\"nat_arithmetic\")\n",
    "        if any(x in goal for x in [\"*\", \"+\"]) and \"=\" in goal:\n",
    "            detected_patterns.append(\"ring_expression\")\n",
    "\n",
    "        # Collecter les tactiques suggeres\n",
    "        seen = set()\n",
    "        for pattern in detected_patterns:\n",
    "            for tactic in self._heuristics.get(pattern, []):\n",
    "                if tactic not in seen:\n",
    "                    seen.add(tactic)\n",
    "                    confidence = 0.7 if difficulty == \"simple\" else 0.5\n",
    "                    suggestions.append({\n",
    "                        \"tactic\": tactic,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"explanation\": f\"Pattern detecte: {pattern}\"\n",
    "                    })\n",
    "\n",
    "        # Ajouter des tactiques de base\n",
    "        base_tactics = self._tactics.get(difficulty, self._tactics[\"simple\"])\n",
    "        for tactic in base_tactics[:3]:\n",
    "            if tactic not in seen:\n",
    "                suggestions.append({\n",
    "                    \"tactic\": tactic,\n",
    "                    \"confidence\": 0.3,\n",
    "                    \"explanation\": f\"Tactique {difficulty} generique\"\n",
    "                })\n",
    "\n",
    "        # Trier par confiance\n",
    "        suggestions.sort(key=lambda x: x[\"confidence\"], reverse=True)\n",
    "        return json.dumps(suggestions[:8], indent=2, ensure_ascii=False)\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Analyse un echec de tactique et suggere des alternatives\",\n",
    "        name=\"analyze_tactic_failure\"\n",
    "    )\n",
    "    def analyze_tactic_failure(self, failed_tactic: str, error_msg: str) -> str:\n",
    "        \"\"\"\n",
    "        Analyse pourquoi une tactique a echoue.\n",
    "\n",
    "        Args:\n",
    "            failed_tactic: La tactique qui a echoue\n",
    "            error_msg: Message d'erreur Lean\n",
    "\n",
    "        Returns:\n",
    "            JSON {diagnosis, alternatives, error_type}\n",
    "        \"\"\"\n",
    "        error_lower = error_msg.lower()\n",
    "        diagnosis = \"\"\n",
    "        alternatives = []\n",
    "        error_type = \"unknown\"\n",
    "\n",
    "        # Classifier l'erreur\n",
    "        if \"unknown identifier\" in error_lower or \"unknown constant\" in error_lower:\n",
    "            error_type = \"unknown_identifier\"\n",
    "            diagnosis = \"Lemme ou identifiant non reconnu. Verifier l'import ou le nom.\"\n",
    "            alternatives = [\"Chercher le bon nom avec #check\", \"Verifier les imports\"]\n",
    "\n",
    "        elif \"type mismatch\" in error_lower:\n",
    "            error_type = \"type_mismatch\"\n",
    "            diagnosis = \"Les types ne correspondent pas. Verifier les arguments.\"\n",
    "            alternatives = [\"exact\", \"apply\", \"simp\"]\n",
    "\n",
    "        elif \"unsolved goals\" in error_lower or \"goals remain\" in error_lower:\n",
    "            error_type = \"unsolved_goals\"\n",
    "            diagnosis = \"Des sous-buts restent. La tactique n'a pas complete la preuve.\"\n",
    "            alternatives = [\"Ajouter d'autres tactiques\", \"Essayer simp\", \"Decomposer avec have\"]\n",
    "\n",
    "        elif \"tactic failed\" in error_lower:\n",
    "            error_type = \"tactic_failed\"\n",
    "            diagnosis = f\"La tactique '{failed_tactic}' n'a pas pu s'appliquer.\"\n",
    "            # Suggerer des alternatives\n",
    "            if failed_tactic in [\"ring\", \"linarith\"]:\n",
    "                alternatives = [\"omega\", \"simp\", \"decide\"]\n",
    "            elif failed_tactic == \"simp\":\n",
    "                alternatives = [\"simp only\", \"rfl\", \"exact\"]\n",
    "            else:\n",
    "                alternatives = [\"simp\", \"omega\", \"exact ?_\"]\n",
    "\n",
    "        elif \"declaration uses 'sorry'\" in error_lower:\n",
    "            error_type = \"sorry\"\n",
    "            diagnosis = \"La preuve contient 'sorry' - incomplete.\"\n",
    "            alternatives = [\"Completer la preuve\", \"Remplacer sorry par une vraie tactique\"]\n",
    "\n",
    "        else:\n",
    "            error_type = \"other\"\n",
    "            diagnosis = f\"Erreur non classifiee: {error_msg[:100]}\"\n",
    "            alternatives = [\"Verifier la syntaxe\", \"Essayer une approche differente\"]\n",
    "\n",
    "        return json.dumps({\n",
    "            \"diagnosis\": diagnosis,\n",
    "            \"alternatives\": alternatives,\n",
    "            \"error_type\": error_type,\n",
    "            \"original_error\": error_msg[:200]\n",
    "        }, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. LeanVerificationPlugin : Compilation et V√©rification\n",
    "\n",
    "**Plugin exposant les m√©thodes de v√©rification** pour VerifierAgent.\n",
    "\n",
    "#### M√©thodes Expos√©es\n",
    "\n",
    "```python\n",
    "@kernel_function\n",
    "def verify_proof(theorem: str, tactics: str) -> VerificationResult:\n",
    "    # Compile le th√©or√®me avec tactiques\n",
    "    # Parse output Lean (success/errors)\n",
    "    # D√©tecte \"no goals\" = preuve compl√®te\n",
    "```\n",
    "\n",
    "**Pattern** : VerifierAgent appelle ce plugin pour compiler preuves avec LeanRunner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test des Plugins ===\n",
      "\n",
      "1. Recherche de lemmes pour 'addition zero':\n",
      "[\n",
      "  {\n",
      "    \"name\": \"Nat.add_zero\",\n",
      "    \"statement\": \"n + 0 = n\",\n",
      "    \"namespace\": \"Nat\",\n",
      "    \"relevance\": 0.6\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Nat.zero_add\",\n",
      "    \"statement\": \"0 + n = n\",\n",
      "    \"namespace\": \"Nat\",\n",
      "    \"relevance\": 0.6\n",
      "  }\n",
      "]\n",
      "\n",
      "2. Tactiques pour 'n + 0 = n':\n",
      "[\n",
      "  {\n",
      "    \"tactic\": \"rfl\",\n",
      "    \"confidence\": 0.7,\n",
      "    \"explanation\": \"Pattern detecte: equality\"\n",
      "  },\n",
      "  {\n",
      "    \"tactic\": \"exact\",\n",
      "    \"confidence\": 0.7,\n",
      "    \"explanation\": \"Pattern detecte: equality\"\n",
      "  },\n",
      "  {\n",
      "    \"tactic\": \"simp\",\n",
      "    \"confidence\": 0.7,\n",
      "    \"explanation\": \"Pattern detecte: equality\"\n",
      "  },\n",
      "  {\n",
      "    \"tactic\": \"ring\",\n",
      "    \"confidence\": 0.7,\n",
      "    \"explanation\": \"Pattern detecte: equality\"\n",
      "  },\n",
      "  {\n",
      "    \"tactic\": \"omega\",\n",
      "    \"confidence\": 0.7,\n",
      "    \"explanation\": \"Pattern detecte: equality\"\n",
      "  },\n",
      "  {\n",
      "    \"tactic\": \"decide\",\n",
      "    \"confidence\": 0.7,\n",
      "    \"explanation\": \"Pattern detecte: nat_arithmetic\"\n",
      "  },\n",
      "  {\n",
      "    \"tactic\": \"ring_nf\",\n",
      "    \"confidence\": 0.7,\n",
      "    \"explanation\": \"Pattern detecte: ring_expression\"\n",
      "  },\n",
      "  {\n",
      "    \"tactic\": \"trivial\",\n",
      "    \"confidence\": 0.3,\n",
      "    \"explanation\": \"Tactique simple generique\"\n",
      "  }\n",
      "]\n",
      "\n",
      "3. Verification d'une preuve:\n",
      "{\n",
      "  \"success\": true,\n",
      "  \"output\": \"\",\n",
      "  \"errors\": \"\",\n",
      "  \"exit_code\": 0,\n",
      "  \"exec_time_ms\": 579.34,\n",
      "  \"backend\": \"subprocess\",\n",
      "  \"code\": \"theorem test_rfl : 2 + 2 = 4 := by rfl\"\n",
      "}\n",
      "\n",
      "4. Ajout via StateManagerPlugin:\n",
      "Lemme ajoute: Nat.Nat.add_zero (Nat.add_zero)\n",
      "{\n",
      "  \"session_id\": \"05476d61\",\n",
      "  \"theorem\": \"theorem test_add (n : Nat) : n + 0 = n\",\n",
      "  \"goal\": \"\",\n",
      "  \"phase\": \"init\",\n",
      "  \"strategy\": \"exploration\",\n",
      "  \"iteration\": \"0/10\",\n",
      "  \"proof_steps\": 0,\n",
      "  \"discovered_lemmas\": 1,\n",
      "  \"errors\": 0,\n",
      "  \"last_error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8.5 LeanVerificationPlugin\n",
    "# =============================================================================\n",
    "\n",
    "class LeanVerificationPlugin:\n",
    "    \"\"\"\n",
    "    Plugin pour la verification des preuves avec lean_runner.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, runner: LeanRunner):\n",
    "        self._runner = runner\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Verifie une preuve complete (theoreme + tactiques)\",\n",
    "        name=\"verify_proof\"\n",
    "    )\n",
    "    def verify_proof(self, theorem_statement: str, proof_tactics: str) -> str:\n",
    "        \"\"\"\n",
    "        Verifie un theoreme avec sa preuve.\n",
    "\n",
    "        Args:\n",
    "            theorem_statement: L'enonce du theoreme (ex: \"theorem add_zero (n : Nat) : n + 0 = n\")\n",
    "            proof_tactics: La preuve (ex: \"exact Nat.add_zero n\")\n",
    "\n",
    "        Returns:\n",
    "            JSON {success, output, errors, exec_time_ms, backend}\n",
    "        \"\"\"\n",
    "        import time\n",
    "\n",
    "        # Construire le code complet\n",
    "        if \"by\" not in proof_tactics and \":=\" not in proof_tactics:\n",
    "            code = f\"{theorem_statement} := by {proof_tactics}\"\n",
    "        elif \":=\" in proof_tactics:\n",
    "            code = f\"{theorem_statement} {proof_tactics}\"\n",
    "        else:\n",
    "            code = f\"{theorem_statement} := {proof_tactics}\"\n",
    "\n",
    "        start = time.time()\n",
    "        result = self._runner.run(code)\n",
    "        exec_time = (time.time() - start) * 1000\n",
    "\n",
    "        return json.dumps({\n",
    "            \"success\": result.success,\n",
    "            \"output\": result.output,\n",
    "            \"errors\": result.errors,\n",
    "            \"exit_code\": result.exit_code,\n",
    "            \"exec_time_ms\": round(exec_time, 2),\n",
    "            \"backend\": result.backend,\n",
    "            \"code\": code\n",
    "        }, indent=2, ensure_ascii=False)\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Verifie une etape de tactique incrementale\",\n",
    "        name=\"verify_tactic_step\"\n",
    "    )\n",
    "    def verify_tactic_step(\n",
    "        self, partial_proof: str, next_tactic: str, theorem_statement: str\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Verifie une tactique incrementale.\n",
    "\n",
    "        Args:\n",
    "            partial_proof: Les tactiques deja appliquees (separees par ;)\n",
    "            next_tactic: La prochaine tactique a essayer\n",
    "            theorem_statement: L'enonce du theoreme\n",
    "\n",
    "        Returns:\n",
    "            JSON {tactic_valid, remaining_goals, error, exec_time_ms}\n",
    "        \"\"\"\n",
    "        import time\n",
    "\n",
    "        # Combiner les tactiques\n",
    "        if partial_proof:\n",
    "            all_tactics = f\"{partial_proof}; {next_tactic}\"\n",
    "        else:\n",
    "            all_tactics = next_tactic\n",
    "\n",
    "        code = f\"{theorem_statement} := by {all_tactics}\"\n",
    "\n",
    "        start = time.time()\n",
    "        result = self._runner.run(code)\n",
    "        exec_time = (time.time() - start) * 1000\n",
    "\n",
    "        # Analyser les goals restants\n",
    "        remaining_goals = None\n",
    "        if \"unsolved goals\" in result.errors.lower():\n",
    "            # Extraire les goals du message d'erreur\n",
    "            remaining_goals = result.errors\n",
    "\n",
    "        return json.dumps({\n",
    "            \"tactic_valid\": result.success or \"unsolved goals\" not in result.errors.lower(),\n",
    "            \"remaining_goals\": remaining_goals,\n",
    "            \"error\": result.errors if not result.success else None,\n",
    "            \"exec_time_ms\": round(exec_time, 2),\n",
    "            \"applied_tactics\": all_tactics\n",
    "        }, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Test des Plugins\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Test des Plugins ===\")\n",
    "\n",
    "# Creer l'etat et le runner\n",
    "test_state = ProofState(theorem_statement=\"theorem test_add (n : Nat) : n + 0 = n\")\n",
    "runner = LeanRunner(backend=\"subprocess\", timeout=30)\n",
    "\n",
    "# Instancier les plugins\n",
    "state_plugin = ProofStateManagerPlugin(test_state)\n",
    "search_plugin = LeanSearchPlugin(runner)\n",
    "tactic_plugin = LeanTacticPlugin()\n",
    "verif_plugin = LeanVerificationPlugin(runner)\n",
    "\n",
    "# Test 1: Recherche de lemmes\n",
    "print(\"\\n1. Recherche de lemmes pour 'addition zero':\")\n",
    "lemmas = search_plugin.search_mathlib_lemmas(\"addition zero\", max_results=3)\n",
    "print(lemmas)\n",
    "\n",
    "# Test 2: Generation de tactiques\n",
    "print(\"\\n2. Tactiques pour 'n + 0 = n':\")\n",
    "tactics = tactic_plugin.generate_tactics(\"n + 0 = n\", difficulty=\"simple\")\n",
    "print(tactics)\n",
    "\n",
    "# Test 3: Verification avec lean_runner\n",
    "print(\"\\n3. Verification d'une preuve:\")\n",
    "result = verif_plugin.verify_proof(\"theorem test_rfl : 2 + 2 = 4\", \"rfl\")\n",
    "print(result)\n",
    "\n",
    "# Test 4: Plugin StateManager\n",
    "print(\"\\n4. Ajout via StateManagerPlugin:\")\n",
    "print(state_plugin.add_discovered_lemma(\"Nat.add_zero\", \"n + 0 = n\", \"Nat\", 0.9))\n",
    "print(state_plugin.get_proof_state(summarize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture : 5 Agents Specialises\n",
    "\n",
    "Le systeme multi-agents comprend 5 roles distincts:\n",
    "\n",
    "| Agent | Role | Plugins | Delegation |\n",
    "|-------|------|---------|------------|\n",
    "| **SearchAgent** | Recherche lemmes Mathlib | LeanSearch, StateManager | TacticAgent si lemmes trouves |\n",
    "| **TacticAgent** | Generation tactiques | LeanTactic, StateManager | VerifierAgent pour validation |\n",
    "| **VerifierAgent** | Verification Lean | LeanVerification, StateManager | CriticAgent si echec |\n",
    "| **CriticAgent** | Analyse echecs | LeanTactic, StateManager | Redirection selon erreur |\n",
    "| **CoordinatorAgent** | Supervision globale | StateManager | Gestion des blocages |\n",
    "\n",
    "**Pattern cle**: Chaque agent designe explicitement le suivant via `designate_next_agent()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Cr√©ation des Agents Semantic Kernel\n",
    "\n",
    "### Anatomie d'un agent\n",
    "\n",
    "Chaque agent a :\n",
    "\n",
    "1. **Un nom** : \"SearchAgent\", \"TacticAgent\", etc.\n",
    "2. **Des instructions** : Prompt syst√®me qui d√©finit son r√¥le\n",
    "3. **Des plugins** : Fonctions qu'il peut appeler (via StatePlugin)\n",
    "4. **Un mod√®le LLM** : GPT-5.2, Claude, etc.\n",
    "\n",
    "### Exemple : SearchAgent\n",
    "\n",
    "```python\n",
    "search_agent = kernel.add_agent(\n",
    "    name=\"SearchAgent\",\n",
    "    instructions=\"\"\"Tu es un expert en recherche de lemmes Mathlib.\n",
    "    Ton r√¥le : Trouver les lemmes pertinents pour le but actuel.\n",
    "    D√©l√®gue √† TacticAgent une fois les lemmes trouv√©s.\"\"\",\n",
    "    plugins=[state_plugin]\n",
    ")\n",
    "```\n",
    "\n",
    "### Instructions : Le \"m√©tier\" de l'agent\n",
    "\n",
    "Les instructions d√©finissent :\n",
    "\n",
    "- **Responsabilit√©** : \"Recherche de lemmes\" vs \"G√©n√©ration de tactiques\"\n",
    "- **Crit√®res de succ√®s** : \"Trouver au moins 2 lemmes pertinents\"\n",
    "- **D√©l√©gation** : \"Quand d√©l√©guer √† un autre agent ?\"\n",
    "\n",
    "**Principe cl√©** : Instructions pr√©cises ‚Üí Comportement pr√©visible\n",
    "\n",
    "### Pattern : Strat√©gies bas√©es sur l'√©tat\n",
    "\n",
    "Au lieu de coder en dur \"SearchAgent ‚Üí TacticAgent\", on utilise :\n",
    "\n",
    "```python\n",
    "def select_next_agent(state: ProofState) -> str:\n",
    "    if state.phase == ProofPhase.SEARCH:\n",
    "        return \"SearchAgent\"\n",
    "    elif state.phase == ProofPhase.TACTIC_GEN:\n",
    "        return \"TacticAgent\"\n",
    "    # ...\n",
    "```\n",
    "\n",
    "**Avantage** : Orchestration dynamique bas√©e sur l'√©tat r√©el de la preuve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel disponible - utilisation de ChatCompletionAgent\n",
      "Anthropic connector disponible\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8.6 - Definition des 5 Agents Specialises avec Semantic Kernel\n",
    "# =============================================================================\n",
    "# Utilise ChatCompletionAgent de Semantic Kernel avec FunctionChoiceBehavior.Auto()\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "# --- Instructions des Agents ---\n",
    "\n",
    "SEARCH_AGENT_INSTRUCTIONS = \"\"\"\n",
    "Tu es l'agent de RECHERCHE de lemmes pour le theorem proving en Lean 4.\n",
    "\n",
    "TON ROLE UNIQUE:\n",
    "- Chercher des lemmes Mathlib pertinents pour le theoreme courant\n",
    "- Identifier les lemmes qui peuvent aider a la preuve\n",
    "- Enregistrer les lemmes trouves dans l'etat partage\n",
    "\n",
    "WORKFLOW:\n",
    "1. Lis l'etat avec get_proof_state() pour comprendre le theoreme\n",
    "2. Utilise search_mathlib_lemmas() avec des mots-cles pertinents\n",
    "3. Verifie les lemmes prometteurs avec check_lemma_type()\n",
    "4. Enregistre les lemmes utiles avec add_discovered_lemma()\n",
    "5. Delegue a TacticAgent quand tu as trouve des lemmes\n",
    "\n",
    "IMPORTANT:\n",
    "- Cherche des lemmes LIES au but (egalites, arithmetique, logique)\n",
    "- Delegation: Apres avoir trouve au moins 2-3 lemmes, delegue a TacticAgent\n",
    "- Si aucun lemme pertinent, delegue quand meme a TacticAgent\n",
    "\"\"\"\n",
    "\n",
    "TACTIC_AGENT_INSTRUCTIONS = \"\"\"\n",
    "Tu es l'agent de GENERATION DE TACTIQUES pour le theorem proving en Lean 4.\n",
    "\n",
    "TON ROLE UNIQUE:\n",
    "- Generer des sequences de tactiques Lean pour prouver le but\n",
    "- Explorer les tactiques systematiquement\n",
    "\n",
    "STRATEGIE D'EXPLORATION OBLIGATOIRE:\n",
    "Tu DOIS essayer les tactiques dans cet ORDRE EXACT, meme si tu penses qu'elles echoueront:\n",
    "\n",
    "PREMIERE TENTATIVE: Toujours essayer rfl ou trivial\n",
    "DEUXIEME TENTATIVE: simp sans arguments\n",
    "TROISIEME TENTATIVE: Lemmes de SearchAgent (exact Lemma_name)\n",
    "QUATRIEME TENTATIVE: Tactiques avancees (omega, ring, linarith)\n",
    "CINQUIEME+ TENTATIVE: Approches structurelles (induction, cases)\n",
    "\n",
    "POURQUOI CETTE APPROCHE:\n",
    "Cette strategie pedagogique montre le processus de decouverte.\n",
    "Ne pas proposer la solution optimale immediatement.\n",
    "Laisser le systeme iterer vers la solution.\n",
    "\n",
    "WORKFLOW:\n",
    "1. get_proof_state() pour comprendre le contexte\n",
    "2. Choisir UNE tactique selon l'ordre ci-dessus\n",
    "3. log_tactic_attempt() pour enregistrer\n",
    "4. Deleguer a VerifierAgent\n",
    "\n",
    "IMPORTANT:\n",
    "- Proposer UNE SEULE tactique a la fois\n",
    "- Si echec, CriticAgent analysera et guidera\n",
    "- Ne pas \"tricher\" en donnant la reponse finale directement\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "VERIFIER_AGENT_INSTRUCTIONS = \"\"\"\n",
    "Tu es l'agent de VERIFICATION pour le theorem proving en Lean 4.\n",
    "\n",
    "TON ROLE UNIQUE:\n",
    "- Verifier les tactiques proposees avec le compilateur Lean\n",
    "- Enregistrer les resultats de verification\n",
    "- Determiner si la preuve est complete ou s'il faut continuer\n",
    "\n",
    "WORKFLOW:\n",
    "1. Lis l'etat avec get_proof_state() pour voir la derniere tactique\n",
    "2. Utilise verify_proof() pour tester la preuve\n",
    "3. Enregistre le resultat avec add_verification_result()\n",
    "4. Si succes: set_proof_complete() et termine\n",
    "5. Si echec: delegue a CriticAgent pour analyse\n",
    "\n",
    "IMPORTANT:\n",
    "- Teste TOUJOURS la derniere tactique proposee\n",
    "- Si la preuve compile sans erreur, utilise set_proof_complete()\n",
    "- Si echec, enregistre l'erreur et delegue a CriticAgent\n",
    "\"\"\"\n",
    "\n",
    "CRITIC_AGENT_INSTRUCTIONS = \"\"\"\n",
    "Tu es l'agent CRITIQUE pour le theorem proving en Lean 4.\n",
    "\n",
    "TON ROLE UNIQUE:\n",
    "- Analyser les echecs de verification\n",
    "- Diagnostiquer les erreurs Lean\n",
    "- Orienter vers la bonne strategie de correction\n",
    "\n",
    "WORKFLOW:\n",
    "1. Lis l'etat avec get_proof_state() pour voir les echecs recents\n",
    "2. Utilise analyze_tactic_failure() pour comprendre l'erreur\n",
    "3. Decide quelle direction prendre:\n",
    "   - \"unknown identifier\" -> delegue a SearchAgent\n",
    "   - \"type mismatch\" ou \"tactic failed\" -> delegue a TacticAgent\n",
    "   - Echecs repetes (>3) -> delegue a CoordinatorAgent\n",
    "\n",
    "IMPORTANT:\n",
    "- Analyse les 3 derniers echecs pour detecter des patterns\n",
    "- Si >3 echecs similaires, delegue a CoordinatorAgent\n",
    "\"\"\"\n",
    "\n",
    "COORDINATOR_AGENT_INSTRUCTIONS = \"\"\"\n",
    "Tu es l'agent COORDINATEUR (superviseur) pour le theorem proving en Lean 4.\n",
    "\n",
    "TON ROLE UNIQUE:\n",
    "- Superviser l'ensemble de la session de preuve\n",
    "- Debloquer les situations cycliques\n",
    "- Ajuster la strategie globale\n",
    "\n",
    "QUAND TU INTERVIENS:\n",
    "- Appele par CriticAgent apres echecs repetes\n",
    "- Appele si max_iterations approche\n",
    "- Appele pour decisions strategiques majeures\n",
    "\n",
    "IMPORTANT:\n",
    "- Tu es le dernier recours, prends des decisions audacieuses\n",
    "- Si >40 iterations, suggere de simplifier le theoreme\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# Detection de Semantic Kernel\n",
    "# =============================================================================\n",
    "\n",
    "SK_AVAILABLE = False\n",
    "ANTHROPIC_AVAILABLE = False\n",
    "try:\n",
    "    from semantic_kernel import Kernel\n",
    "    from semantic_kernel.agents import ChatCompletionAgent, AgentGroupChat\n",
    "    from semantic_kernel.agents.strategies import (\n",
    "        KernelFunctionSelectionStrategy,\n",
    "        KernelFunctionTerminationStrategy,\n",
    "    )\n",
    "    from semantic_kernel.agents.strategies.selection.sequential_selection_strategy import SequentialSelectionStrategy\n",
    "    from semantic_kernel.agents.strategies.termination.default_termination_strategy import DefaultTerminationStrategy\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "    from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "    from semantic_kernel.functions import KernelFunctionFromPrompt, KernelArguments\n",
    "    from semantic_kernel.contents import ChatHistoryTruncationReducer\n",
    "    from semantic_kernel.agents.strategies.selection.selection_strategy import SelectionStrategy\n",
    "    from semantic_kernel.agents.strategies.termination.termination_strategy import TerminationStrategy\n",
    "    from semantic_kernel.contents.chat_message_content import ChatMessageContent\n",
    "    from pydantic import PrivateAttr\n",
    "    SK_AVAILABLE = True\n",
    "    print(\"Semantic Kernel disponible - utilisation de ChatCompletionAgent\")\n",
    "    \n",
    "    # Import Anthropic connector (optional)\n",
    "    try:\n",
    "        from semantic_kernel.connectors.ai.anthropic import AnthropicChatCompletion\n",
    "        ANTHROPIC_AVAILABLE = True\n",
    "        print(\"Anthropic connector disponible\")\n",
    "    except ImportError:\n",
    "        print(\"Anthropic connector non disponible (pip install semantic-kernel[anthropic])\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"Semantic Kernel non disponible: {e}\")\n",
    "    print(\"Installation: pip install semantic-kernel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. SimpleAgent : Agent Fallback : Agent Fallback (Simulation)\n",
    "\n",
    "**Classe de secours** pour simuler agents quand Semantic Kernel non disponible.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "```python\n",
    "class SimpleAgent:\n",
    "    def __init__(self, name: str, instructions: str):\n",
    "        self.name = name\n",
    "        self.instructions = instructions\n",
    "    \n",
    "    def invoke(self, message: str) -> str:\n",
    "        # Simulation simple bas√©e sur r√®gles\n",
    "        # Utilis√© en mode fallback si OpenAI API indisponible\n",
    "```\n",
    "\n",
    "**Usage** : Mode simulation pour tests sans LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Mode Simulation (fallback si SK non disponible)\n",
    "# =============================================================================\n",
    "\n",
    "# Flag global pour le mode d√©mo (r√©ponses hardcod√©es p√©dagogiques)\n",
    "USE_DEMO_MODE = os.getenv(\"USE_DEMO_MODE\", \"false\").lower() == \"true\"\n",
    "\n",
    "class SimpleAgent:\n",
    "    \"\"\"\n",
    "    Agent simplifie pour simulation ou fallback.\n",
    "\n",
    "    Modes disponibles:\n",
    "    - DEMO mode (USE_DEMO_MODE=True): R√©ponses hardcod√©es pour les 4 d√©mos p√©dagogiques\n",
    "    - Simulation mode (use_simulation=True): Logique g√©n√©rique bas√©e sur les th√©or√®mes\n",
    "    - LLM mode (use_simulation=False): Appels r√©els √† OpenAI avec function calling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        instructions: str,\n",
    "        plugins: Dict[str, Any],\n",
    "        use_simulation: bool = True\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.instructions = instructions\n",
    "        self.plugins = plugins\n",
    "        self.use_simulation = use_simulation\n",
    "        self._openai_client = None\n",
    "\n",
    "        # Initialiser le client OpenAI si mode reel\n",
    "        if not use_simulation:\n",
    "            try:\n",
    "                from openai import OpenAI\n",
    "                api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "                if api_key and len(api_key) > 10 and not api_key.startswith(\"sk-...\"):\n",
    "                    self._openai_client = OpenAI(api_key=api_key)\n",
    "            except ImportError:\n",
    "                pass\n",
    "\n",
    "    def _build_openai_tools(self) -> list:\n",
    "        \"\"\"Construit les outils au format OpenAI function calling.\"\"\"\n",
    "        import inspect\n",
    "        tools = []\n",
    "        for plugin_name, plugin in self.plugins.items():\n",
    "            for attr_name in dir(plugin):\n",
    "                attr = getattr(plugin, attr_name)\n",
    "                if not callable(attr):\n",
    "                    continue\n",
    "                # Supporter les deux d√©corateurs\n",
    "                is_sk_func = hasattr(attr, '_sk_function') or hasattr(attr, '__kernel_function__')\n",
    "                if not is_sk_func:\n",
    "                    continue\n",
    "\n",
    "                sig = inspect.signature(attr)\n",
    "                properties = {}\n",
    "                required = []\n",
    "                for param_name, param in sig.parameters.items():\n",
    "                    if param_name == 'self':\n",
    "                        continue\n",
    "                    param_type = \"string\"\n",
    "                    if param.annotation != inspect.Parameter.empty:\n",
    "                        if param.annotation == bool:\n",
    "                            param_type = \"boolean\"\n",
    "                        elif param.annotation in (int, float):\n",
    "                            param_type = \"number\"\n",
    "                    properties[param_name] = {\n",
    "                        \"type\": param_type,\n",
    "                        \"description\": f\"Parameter {param_name}\"\n",
    "                    }\n",
    "                    if param.default == inspect.Parameter.empty:\n",
    "                        required.append(param_name)\n",
    "\n",
    "                # Obtenir nom et description\n",
    "                if hasattr(attr, '__kernel_function_name__'):\n",
    "                    func_name = attr.__kernel_function_name__\n",
    "                    func_desc = getattr(attr, \"__kernel_function_description__\", \"\")\n",
    "                elif hasattr(attr, '_sk_name'):\n",
    "                    func_name = attr._sk_name\n",
    "                    func_desc = getattr(attr, \"_sk_description\", \"\")\n",
    "                else:\n",
    "                    func_name = attr_name\n",
    "                    func_desc = \"\"\n",
    "\n",
    "                tools.append({\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": f\"{plugin_name}__{func_name}\",\n",
    "                        \"description\": func_desc,\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": properties,\n",
    "                            \"required\": required\n",
    "                        }\n",
    "                    }\n",
    "                })\n",
    "        return tools\n",
    "\n",
    "    def _execute_tool_call(self, tool_name: str, arguments: dict) -> str:\n",
    "        \"\"\"Execute un appel de fonction sur un plugin.\"\"\"\n",
    "        parts = tool_name.split(\"__\", 1)\n",
    "        if len(parts) != 2:\n",
    "            return f\"Erreur: format invalide: {tool_name}\"\n",
    "\n",
    "        plugin_name, func_name = parts\n",
    "        plugin = self.plugins.get(plugin_name)\n",
    "        if not plugin:\n",
    "            return f\"Erreur: plugin {plugin_name} non trouve\"\n",
    "\n",
    "        for attr_name in dir(plugin):\n",
    "            attr = getattr(plugin, attr_name)\n",
    "            if not callable(attr):\n",
    "                continue\n",
    "            is_sk = hasattr(attr, '_sk_function') or hasattr(attr, '__kernel_function__')\n",
    "            if not is_sk:\n",
    "                continue\n",
    "\n",
    "            if hasattr(attr, '__kernel_function_name__'):\n",
    "                name = attr.__kernel_function_name__\n",
    "            elif hasattr(attr, '_sk_name'):\n",
    "                name = attr._sk_name\n",
    "            else:\n",
    "                name = attr_name\n",
    "\n",
    "            if name == func_name:\n",
    "                try:\n",
    "                    result = attr(**arguments)\n",
    "                    return str(result)\n",
    "                except Exception as e:\n",
    "                    return f\"Erreur {func_name}: {e}\"\n",
    "\n",
    "        return f\"Erreur: {func_name} non trouve dans {plugin_name}\"\n",
    "\n",
    "    def invoke(self, message: str, state: ProofState) -> str:\n",
    "        \"\"\"Execute l'agent sur un message.\"\"\"\n",
    "        state.increment_iteration()\n",
    "\n",
    "        if self.use_simulation or not self._openai_client:\n",
    "            return self._simulate_response(message, state)\n",
    "        else:\n",
    "            return self._call_llm(message, state)\n",
    "\n",
    "    def _simulate_response(self, message: str, state: ProofState) -> str:\n",
    "        \"\"\"\n",
    "        Simulation realiste basee sur l'analyse du theoreme.\n",
    "        Si USE_DEMO_MODE=True, utilise les r√©ponses hardcod√©es pour les DEMOs.\n",
    "        Sinon, utilise la logique g√©n√©rique.\n",
    "        \"\"\"\n",
    "        theorem = state.theorem_statement.lower()\n",
    "        goal = state.current_goal or \"\"\n",
    "\n",
    "        if self.name == \"SearchAgent\":\n",
    "            return self._do_search(state, theorem, goal)\n",
    "        elif self.name == \"TacticAgent\":\n",
    "            return self._do_tactic(state, theorem, goal)\n",
    "        elif self.name == \"VerifierAgent\":\n",
    "            return self._do_verify(state, theorem, goal)\n",
    "        elif self.name == \"CriticAgent\":\n",
    "            return self._do_critic(state, theorem)\n",
    "        elif self.name == \"CoordinatorAgent\":\n",
    "            return self._do_coordinate(state, theorem)\n",
    "\n",
    "        return f\"[{self.name}] Action simulee.\"\n",
    "\n",
    "    # =========================================================================\n",
    "    # SEARCH AGENT\n",
    "    # =========================================================================\n",
    "    def _do_search(self, state: ProofState, theorem: str, goal: str) -> str:\n",
    "        \"\"\"Recherche de lemmes - Mode DEMO ou g√©n√©rique.\"\"\"\n",
    "        state_mgr = self.plugins.get(\"state\")\n",
    "        search = self.plugins.get(\"search\")\n",
    "\n",
    "        if not state_mgr:\n",
    "            return \"[SearchAgent] Plugins manquants.\"\n",
    "\n",
    "        # --- MODE DEMO: R√©ponses hardcod√©es pour les 4 d√©mos p√©dagogiques ---\n",
    "        if USE_DEMO_MODE:\n",
    "            # DEMO_1: n = n (reflexivit√©)\n",
    "            if \"n = n\" in theorem and \"demo_rfl\" in theorem:\n",
    "                state_mgr.add_lemma(\"Eq.refl\")\n",
    "                state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "                return \"[SearchAgent] Lemme trouve: Eq.refl (reflexivite). -> TacticAgent\"\n",
    "\n",
    "            # DEMO_2: 0 + n = n (zero_add)\n",
    "            if \"0 + n = n\" in theorem or \"zero_add\" in theorem:\n",
    "                state_mgr.add_lemma(\"Nat.zero_add\")\n",
    "                state_mgr.add_lemma(\"Nat.add_zero\")\n",
    "                state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "                return \"[SearchAgent] Lemmes: Nat.zero_add, Nat.add_zero. -> TacticAgent\"\n",
    "\n",
    "            # DEMO_3: a * c + b * c = (a + b) * c (distributivit√©)\n",
    "            if \"a * c + b * c\" in theorem or \"add_mul_distrib\" in theorem:\n",
    "                state_mgr.add_lemma(\"Nat.add_mul\")\n",
    "                state_mgr.add_lemma(\"Nat.mul_add\")\n",
    "                state_mgr.add_lemma(\"Nat.right_distrib\")\n",
    "                state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "                return \"[SearchAgent] Lemmes distributivite: Nat.add_mul, Nat.mul_add, Nat.right_distrib. -> TacticAgent\"\n",
    "\n",
    "            # DEMO_4: m * n = n * m (commutativit√© multiplication)\n",
    "            if \"m * n = n * m\" in theorem or \"mul_comm_manual\" in theorem:\n",
    "                state_mgr.add_lemma(\"Nat.mul_comm\")\n",
    "                state_mgr.add_lemma(\"Nat.mul_succ\")\n",
    "                state_mgr.add_lemma(\"Nat.succ_mul\")\n",
    "                state_mgr.add_lemma(\"Nat.mul_zero\")\n",
    "                state_mgr.add_lemma(\"Nat.zero_mul\")\n",
    "                state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "                return \"[SearchAgent] Lemmes pour induction: mul_succ, succ_mul, mul_zero, zero_mul. -> TacticAgent\"\n",
    "\n",
    "        # --- MODE GENERIQUE: Logique bas√©e sur l'analyse du th√©or√®me ---\n",
    "        lemmas_found = []\n",
    "\n",
    "        # R√©flexivit√©\n",
    "        if \"n = n\" in theorem or goal.strip() == \"n = n\":\n",
    "            if hasattr(state_mgr, 'add_discovered_lemma'):\n",
    "                state_mgr.add_discovered_lemma(\"Eq.refl\", \"a = a\", \"Logic\", 1.0)\n",
    "            else:\n",
    "                state_mgr.add_lemma(\"Eq.refl\")\n",
    "            lemmas_found.append(\"Eq.refl\")\n",
    "\n",
    "        # Addition avec z√©ro\n",
    "        if \"+ 0\" in theorem or \"0 +\" in theorem:\n",
    "            if hasattr(state_mgr, 'add_discovered_lemma'):\n",
    "                state_mgr.add_discovered_lemma(\"Nat.add_zero\", \"n + 0 = n\", \"Nat\", 0.9)\n",
    "                state_mgr.add_discovered_lemma(\"Nat.zero_add\", \"0 + n = n\", \"Nat\", 0.9)\n",
    "            else:\n",
    "                state_mgr.add_lemma(\"Nat.add_zero\")\n",
    "                state_mgr.add_lemma(\"Nat.zero_add\")\n",
    "            lemmas_found.extend([\"Nat.add_zero\", \"Nat.zero_add\"])\n",
    "\n",
    "        # Commutativit√© addition\n",
    "        if \"+\" in theorem and (\"m + n\" in theorem or \"n + m\" in theorem or \"b + a\" in theorem):\n",
    "            if hasattr(state_mgr, 'add_discovered_lemma'):\n",
    "                state_mgr.add_discovered_lemma(\"Nat.add_comm\", \"n + m = m + n\", \"Nat\", 0.85)\n",
    "            else:\n",
    "                state_mgr.add_lemma(\"Nat.add_comm\")\n",
    "            lemmas_found.append(\"Nat.add_comm\")\n",
    "\n",
    "        # Associativit√©\n",
    "        if theorem.count(\"+\") >= 2:\n",
    "            if hasattr(state_mgr, 'add_discovered_lemma'):\n",
    "                state_mgr.add_discovered_lemma(\"Nat.add_assoc\", \"(n + m) + k = n + (m + k)\", \"Nat\", 0.8)\n",
    "            else:\n",
    "                state_mgr.add_lemma(\"Nat.add_assoc\")\n",
    "            lemmas_found.append(\"Nat.add_assoc\")\n",
    "\n",
    "        # Distributivit√©\n",
    "        if \"*\" in theorem and \"+\" in theorem:\n",
    "            if hasattr(state_mgr, 'add_discovered_lemma'):\n",
    "                state_mgr.add_discovered_lemma(\"Nat.right_distrib\", \"(n + m) * k = n * k + m * k\", \"Nat\", 0.9)\n",
    "                state_mgr.add_discovered_lemma(\"Nat.add_mul\", \"a * c + b * c = (a + b) * c\", \"Nat\", 0.9)\n",
    "            else:\n",
    "                state_mgr.add_lemma(\"Nat.right_distrib\")\n",
    "                state_mgr.add_lemma(\"Nat.add_mul\")\n",
    "            lemmas_found.extend([\"Nat.right_distrib\", \"Nat.add_mul\"])\n",
    "\n",
    "        # Commutativit√© multiplication\n",
    "        if \"*\" in theorem and (\"m * n\" in theorem or \"n * m\" in theorem):\n",
    "            if hasattr(state_mgr, 'add_discovered_lemma'):\n",
    "                state_mgr.add_discovered_lemma(\"Nat.mul_comm\", \"m * n = n * m\", \"Nat\", 0.85)\n",
    "            else:\n",
    "                state_mgr.add_lemma(\"Nat.mul_comm\")\n",
    "            lemmas_found.append(\"Nat.mul_comm\")\n",
    "\n",
    "        state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "        if lemmas_found:\n",
    "            return f\"[SearchAgent] Lemmes: {', '.join(lemmas_found[:3])}. -> TacticAgent\"\n",
    "        return \"[SearchAgent] Recherche generique. -> TacticAgent\"\n",
    "\n",
    "    # =========================================================================\n",
    "    # TACTIC AGENT\n",
    "    # =========================================================================\n",
    "    def _do_tactic(self, state: ProofState, theorem: str, goal: str) -> str:\n",
    "        \"\"\"Generation de tactiques - Mode DEMO ou g√©n√©rique.\"\"\"\n",
    "        state_mgr = self.plugins.get(\"state\")\n",
    "        if not state_mgr:\n",
    "            return \"[TacticAgent] Plugin manquant.\"\n",
    "\n",
    "        n = len(state.tactics_history)\n",
    "\n",
    "        # --- MODE DEMO: S√©quences hardcod√©es pour progression p√©dagogique ---\n",
    "        if USE_DEMO_MODE:\n",
    "            # DEMO_1: n = n (reflexivity) - SUCCESS IMMEDIAT\n",
    "            if \"n = n\" in theorem and \"demo_rfl\" in theorem:\n",
    "                state_mgr.log_tactic_attempt(\"rfl\", goal, 1.0, \"Reflexivite directe\")\n",
    "                state_mgr.designate_next_agent(\"VerifierAgent\")\n",
    "                return \"[TacticAgent] Tactique: rfl (reflexivite). -> VerifierAgent\"\n",
    "\n",
    "            # DEMO_2: 0 + n = n - 2 ECHECS AVANT SUCCES\n",
    "            if \"0 + n = n\" in theorem or \"zero_add\" in theorem:\n",
    "                if n == 0:\n",
    "                    state_mgr.log_tactic_attempt(\"rfl\", goal, 0.3, \"Tentative naive\")\n",
    "                    state_mgr.designate_next_agent(\"VerifierAgent\")\n",
    "                    return \"[TacticAgent] Tentative 1: rfl (devrait echouer). -> VerifierAgent\"\n",
    "                elif n == 1:\n",
    "                    state_mgr.log_tactic_attempt(\"simp\", goal, 0.4, \"Simplification\")\n",
    "                    state_mgr.designate_next_agent(\"VerifierAgent\")\n",
    "                    return \"[TacticAgent] Tentative 2: simp (insuffisant). -> VerifierAgent\"\n",
    "                else:\n",
    "                    state_mgr.log_tactic_attempt(\"exact Nat.zero_add n\", goal, 0.95, \"Lemme exact\")\n",
    "                    state_mgr.designate_next_agent(\"VerifierAgent\")\n",
    "                    return \"[TacticAgent] Tactique: exact Nat.zero_add n. -> VerifierAgent\"\n",
    "\n",
    "            # DEMO_3: a * c + b * c = (a + b) * c - 4 ECHECS AVANT SUCCES\n",
    "            if \"a * c + b * c\" in theorem or \"add_mul_distrib\" in theorem:\n",
    "                tactics_sequence = [\n",
    "                    (\"rfl\", 0.2, \"Tentative naive\"),\n",
    "                    (\"simp\", 0.3, \"Simplification basique\"),\n",
    "                    (\"ring\", 0.4, \"Solveur arithmetique\"),\n",
    "                    (\"rw [Nat.add_mul]\", 0.5, \"R√©√©criture partielle\"),\n",
    "                    (\"rw [<- Nat.add_mul]\", 0.95, \"Forme correcte de distributivit√©\")\n",
    "                ]\n",
    "                if n < len(tactics_sequence):\n",
    "                    tactic, conf, desc = tactics_sequence[n]\n",
    "                    state_mgr.log_tactic_attempt(tactic, goal, conf, desc)\n",
    "                    state_mgr.designate_next_agent(\"VerifierAgent\")\n",
    "                    return f\"[TacticAgent] Tentative {n+1}: {tactic}. -> VerifierAgent\"\n",
    "                else:\n",
    "                    state_mgr.log_tactic_attempt(\"rw [<- Nat.add_mul]\", goal, 0.95, \"Solution\")\n",
    "                    state_mgr.designate_next_agent(\"VerifierAgent\")\n",
    "                    return \"[TacticAgent] Tactique finale: rw [<- Nat.add_mul]. -> VerifierAgent\"\n",
    "\n",
    "            # DEMO_4: m * n = n * m - Exploration par induction (8-10 iterations)\n",
    "            if \"m * n = n * m\" in theorem or \"mul_comm_manual\" in theorem:\n",
    "                tactics_sequence = [\n",
    "                    (\"rfl\", 0.1, \"Tentative naive\"),\n",
    "                    (\"simp\", 0.15, \"Simplification\"),\n",
    "                    (\"ring\", 0.2, \"Ring ne suffit pas pour axiomes\"),\n",
    "                    (\"omega\", 0.2, \"Omega: arithm√©tique lin√©aire seulement\"),\n",
    "                    (\"induction m\", 0.4, \"Induction sur m\"),\n",
    "                    (\"exact Nat.mul_zero n\", 0.5, \"Cas de base\"),\n",
    "                    (\"simp [Nat.succ_mul]\", 0.6, \"Cas inductif √©tape 1\"),\n",
    "                    (\"rw [ih]\", 0.7, \"Utiliser hypoth√®se induction\"),\n",
    "                    (\"simp [Nat.mul_succ]\", 0.8, \"Transformation finale\"),\n",
    "                    (\"exact Nat.mul_comm m n\", 0.95, \"Lemme direct\")\n",
    "                ]\n",
    "                if n < len(tactics_sequence):\n",
    "                    tactic, conf, desc = tactics_sequence[n]\n",
    "                    state_mgr.log_tactic_attempt(tactic, goal, conf, desc)\n",
    "                    state_mgr.designate_next_agent(\"VerifierAgent\")\n",
    "                    return f\"[TacticAgent] Tentative {n+1}: {tactic}. -> VerifierAgent\"\n",
    "                else:\n",
    "                    state_mgr.log_tactic_attempt(\"exact Nat.mul_comm m n\", goal, 0.95, \"Solution\")\n",
    "                    state_mgr.designate_next_agent(\"VerifierAgent\")\n",
    "                    return \"[TacticAgent] Tactique finale: exact Nat.mul_comm. -> VerifierAgent\"\n",
    "\n",
    "        # --- MODE GENERIQUE: Strat√©gie adaptative ---\n",
    "        # Strat√©gie: essayer les tactiques simples d'abord\n",
    "        if n == 0:\n",
    "            tactic = \"rfl\"\n",
    "            desc = \"Reflexivite\"\n",
    "        elif n == 1:\n",
    "            tactic = \"simp\"\n",
    "            desc = \"Simplification\"\n",
    "        elif n == 2:\n",
    "            # Utiliser les lemmes trouv√©s\n",
    "            if state.lemmas_found:\n",
    "                lemma = state.lemmas_found[0].name if hasattr(state.lemmas_found[0], 'name') else str(state.lemmas_found[0])\n",
    "                tactic = f\"exact {lemma}\"\n",
    "                desc = \"Lemme exact\"\n",
    "            else:\n",
    "                tactic = \"ring\"\n",
    "                desc = \"Ring solver\"\n",
    "        elif n == 3:\n",
    "            tactic = \"omega\"\n",
    "            desc = \"Arithm√©tique lin√©aire\"\n",
    "        elif n == 4:\n",
    "            tactic = \"linarith\"\n",
    "            desc = \"Arithm√©tique lin√©aire avanc√©e\"\n",
    "        else:\n",
    "            tactic = \"sorry\"\n",
    "            desc = \"Abandon (preuve incompl√®te)\"\n",
    "\n",
    "        state_mgr.log_tactic_attempt(tactic, goal, 0.3 + n * 0.1, desc)\n",
    "        state_mgr.designate_next_agent(\"VerifierAgent\")\n",
    "        return f\"[TacticAgent] Tactique: {tactic}. -> VerifierAgent\"\n",
    "\n",
    "    # =========================================================================\n",
    "    # VERIFIER AGENT\n",
    "    # =========================================================================\n",
    "    def _do_verify(self, state: ProofState, theorem: str, goal: str) -> str:\n",
    "        \"\"\"Verification de la preuve.\"\"\"\n",
    "        state_mgr = self.plugins.get(\"state\")\n",
    "        if not state_mgr or not state.tactics_history:\n",
    "            return \"[VerifierAgent] Rien a verifier.\"\n",
    "\n",
    "        last = state.tactics_history[-1]\n",
    "        n = len(state.tactics_history)\n",
    "        attempt_id = f\"attempt_{n}\"\n",
    "\n",
    "        # --- MODE DEMO: V√©rification hardcod√©e ---\n",
    "        if USE_DEMO_MODE:\n",
    "            # DEMO_1: n = n - rfl r√©ussit imm√©diatement\n",
    "            if \"rfl\" in last.tactic and \"n = n\" in theorem and \"demo_rfl\" in theorem:\n",
    "                state_mgr.add_verification_result(attempt_id, True, \"OK\", \"\", \"\", 50.0)\n",
    "                state_mgr.set_proof_complete(last.tactic)\n",
    "                return f\"[VerifierAgent] SUCCES! Preuve par reflexivite: {last.tactic}\"\n",
    "\n",
    "            # DEMO_2: 0 + n = n - succ√®s √† la 3√®me tentative\n",
    "            if \"0 + n = n\" in theorem or \"zero_add\" in theorem:\n",
    "                if n < 3:\n",
    "                    state_mgr.add_verification_result(attempt_id, False, f\"Tentative {n}\", \"echec\", \"\", 80.0)\n",
    "                    state_mgr.designate_next_agent(\"CriticAgent\")\n",
    "                    return f\"[VerifierAgent] Echec tentative {n}. -> CriticAgent\"\n",
    "                else:\n",
    "                    state_mgr.add_verification_result(attempt_id, True, \"OK\", \"\", \"\", 100.0)\n",
    "                    state_mgr.set_proof_complete(last.tactic)\n",
    "                    return f\"[VerifierAgent] SUCCES apres {n} tentatives! {last.tactic}\"\n",
    "\n",
    "            # DEMO_3: distributivit√© - succ√®s √† la 5√®me tentative\n",
    "            if \"a * c + b * c\" in theorem or \"add_mul_distrib\" in theorem:\n",
    "                if n < 5:\n",
    "                    state_mgr.add_verification_result(attempt_id, False, f\"{n}/5\", \"continue\", \"\", 100.0)\n",
    "                    state_mgr.designate_next_agent(\"CriticAgent\")\n",
    "                    return f\"[VerifierAgent] Etape {n}/5. -> CriticAgent\"\n",
    "                else:\n",
    "                    state_mgr.add_verification_result(attempt_id, True, \"OK\", \"\", \"\", 150.0)\n",
    "                    state_mgr.set_proof_complete(last.tactic)\n",
    "                    return f\"[VerifierAgent] SUCCES! Distributivite prouvee apres {n} etapes.\"\n",
    "\n",
    "            # DEMO_4: mul_comm - succ√®s √† la 10√®me tentative\n",
    "            if \"m * n = n * m\" in theorem or \"mul_comm_manual\" in theorem:\n",
    "                if n < 10:\n",
    "                    state_mgr.add_verification_result(attempt_id, False, f\"{n}/10\", \"continue\", \"\", 120.0)\n",
    "                    state_mgr.designate_next_agent(\"CriticAgent\")\n",
    "                    return f\"[VerifierAgent] Etape {n}/10. -> CriticAgent\"\n",
    "                else:\n",
    "                    state_mgr.add_verification_result(attempt_id, True, \"OK\", \"\", \"\", 200.0)\n",
    "                    state_mgr.set_proof_complete(last.tactic)\n",
    "                    return f\"[VerifierAgent] SUCCES! Commutativite prouvee apres {n} iterations.\"\n",
    "\n",
    "        # --- MODE GENERIQUE: V√©rification simul√©e ---\n",
    "        # Simuler la v√©rification (en mode r√©el, appeler Lean)\n",
    "        success_tactics = [\"rfl\", \"simp\", \"ring\", \"omega\", \"linarith\", \"exact\"]\n",
    "        is_success = any(t in last.tactic for t in success_tactics) and n >= 2\n",
    "\n",
    "        if is_success or \"sorry\" in last.tactic:\n",
    "            state_mgr.add_verification_result(attempt_id, True, \"OK\", \"\", \"\", 100.0)\n",
    "            state_mgr.set_proof_complete(last.tactic)\n",
    "            return f\"[VerifierAgent] SUCCES! {last.tactic}\"\n",
    "        else:\n",
    "            state_mgr.add_verification_result(attempt_id, False, goal, \"echec\", \"\", 50.0)\n",
    "            state_mgr.designate_next_agent(\"CriticAgent\")\n",
    "            return f\"[VerifierAgent] Echec: {last.tactic}. -> CriticAgent\"\n",
    "\n",
    "    # =========================================================================\n",
    "    # CRITIC AGENT\n",
    "    # =========================================================================\n",
    "    def _do_critic(self, state: ProofState, theorem: str) -> str:\n",
    "        \"\"\"Analyse critique et feedback.\"\"\"\n",
    "        state_mgr = self.plugins.get(\"state\")\n",
    "        if not state_mgr:\n",
    "            return \"[CriticAgent] Plugin manquant.\"\n",
    "\n",
    "        n = len(state.tactics_history)\n",
    "\n",
    "        # --- MODE DEMO: Feedback p√©dagogique ---\n",
    "        if USE_DEMO_MODE:\n",
    "            if \"0 + n = n\" in theorem or \"zero_add\" in theorem:\n",
    "                if n == 1:\n",
    "                    state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "                    return \"[CriticAgent] rfl echoue car 0+n n'est pas syntaxiquement n. Essayer simp. -> TacticAgent\"\n",
    "                else:\n",
    "                    state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "                    return \"[CriticAgent] simp ne suffit pas. Utiliser le lemme Nat.zero_add directement. -> TacticAgent\"\n",
    "\n",
    "            if \"a * c + b * c\" in theorem or \"add_mul_distrib\" in theorem:\n",
    "                hints = [\n",
    "                    \"rfl echoue: pas d'√©galit√© syntaxique.\",\n",
    "                    \"simp ne connait pas cette forme de distributivit√©.\",\n",
    "                    \"ring ne peut pas g√©rer les Nat directement.\",\n",
    "                    \"Essayer rw avec Nat.add_mul dans le bon sens.\"\n",
    "                ]\n",
    "                hint = hints[min(n-1, len(hints)-1)]\n",
    "                state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "                return f\"[CriticAgent] {hint} -> TacticAgent\"\n",
    "\n",
    "            if \"m * n = n * m\" in theorem or \"mul_comm_manual\" in theorem:\n",
    "                if n < 5:\n",
    "                    state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "                    return f\"[CriticAgent] Tentative {n} echouee. Explorer l'induction. -> TacticAgent\"\n",
    "                else:\n",
    "                    state_mgr.designate_next_agent(\"CoordinatorAgent\")\n",
    "                    return \"[CriticAgent] Besoin de coordination pour strat√©gie induction. -> CoordinatorAgent\"\n",
    "\n",
    "        # --- MODE GENERIQUE ---\n",
    "        state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "        return f\"[CriticAgent] Tentative {n} echouee. Essayer une autre approche. -> TacticAgent\"\n",
    "\n",
    "    # =========================================================================\n",
    "    # COORDINATOR AGENT\n",
    "    # =========================================================================\n",
    "    def _do_coordinate(self, state: ProofState, theorem: str) -> str:\n",
    "        \"\"\"Coordination de la strat√©gie globale.\"\"\"\n",
    "        state_mgr = self.plugins.get(\"state\")\n",
    "        if not state_mgr:\n",
    "            return \"[CoordinatorAgent] Plugin manquant.\"\n",
    "\n",
    "        # --- MODE DEMO ---\n",
    "        if USE_DEMO_MODE:\n",
    "            if \"a * c + b * c\" in theorem or \"add_mul_distrib\" in theorem:\n",
    "                state_mgr.set_proof_strategy(\"distributivity_rewrite\")\n",
    "                state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "                return \"[CoordinatorAgent] Strategie: r√©√©criture avec Nat.add_mul. -> TacticAgent\"\n",
    "\n",
    "            if \"m * n = n * m\" in theorem or \"mul_comm_manual\" in theorem:\n",
    "                state_mgr.set_proof_strategy(\"induction_then_lemma\")\n",
    "                state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "                return \"[CoordinatorAgent] Strategie: induction puis lemme direct. -> TacticAgent\"\n",
    "\n",
    "        # --- MODE GENERIQUE ---\n",
    "        if \"*\" in theorem and \"+\" in theorem:\n",
    "            state_mgr.set_proof_strategy(\"ring_solver\")\n",
    "            state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "            return \"[CoordinatorAgent] Strategie: ring solver pour arithm√©tique. -> TacticAgent\"\n",
    "\n",
    "        if theorem.count(\"+\") >= 2 or theorem.count(\"*\") >= 2:\n",
    "            state_mgr.set_proof_strategy(\"ac_normalization\")\n",
    "            state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "            return \"[CoordinatorAgent] Strategie: AC normalization. -> TacticAgent\"\n",
    "\n",
    "        state_mgr.designate_next_agent(\"TacticAgent\")\n",
    "        return \"[CoordinatorAgent] Strategie par defaut. -> TacticAgent\"\n",
    "\n",
    "    # =========================================================================\n",
    "    # LLM MODE (Appels r√©els √† OpenAI)\n",
    "    # =========================================================================\n",
    "    def _call_llm(self, message: str, state: ProofState) -> str:\n",
    "        \"\"\"Appelle le LLM OpenAI avec function calling.\"\"\"\n",
    "        state_summary = json.dumps(state.get_state_snapshot(summarize=True), indent=2)\n",
    "        tools = self._build_openai_tools()\n",
    "\n",
    "        nl = chr(10)\n",
    "        user_content = f\"ETAT ACTUEL:{nl}{state_summary}{nl}{nl}TACHE:{nl}{message}\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.instructions},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "\n",
    "        max_tool_calls = 10\n",
    "        tool_results = []\n",
    "\n",
    "        for iteration in range(max_tool_calls):\n",
    "            try:\n",
    "                model = os.getenv(\"OPENAI_CHAT_MODEL_ID\", \"gpt-4o\")\n",
    "                use_mct = any(model.startswith(p) for p in ('gpt-4.5', 'gpt-5', 'o1', 'o3'))\n",
    "                token_param = {\"max_completion_tokens\": 1000} if use_mct else {\"max_tokens\": 1000}\n",
    "\n",
    "                response = self._openai_client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    tools=tools if tools else None,\n",
    "                    tool_choice=\"auto\" if tools else None,\n",
    "                    temperature=0.3,\n",
    "                    **token_param\n",
    "                )\n",
    "\n",
    "                assistant_message = response.choices[0].message\n",
    "\n",
    "                if assistant_message.tool_calls:\n",
    "                    messages.append(assistant_message.model_dump())\n",
    "\n",
    "                    for tool_call in assistant_message.tool_calls:\n",
    "                        func_name = tool_call.function.name\n",
    "                        try:\n",
    "                            arguments = json.loads(tool_call.function.arguments)\n",
    "                        except json.JSONDecodeError:\n",
    "                            arguments = {}\n",
    "\n",
    "                        result = self._execute_tool_call(func_name, arguments)\n",
    "                        tool_results.append(func_name.split(\"__\")[-1])\n",
    "\n",
    "                        messages.append({\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"content\": result\n",
    "                        })\n",
    "                else:\n",
    "                    final_response = assistant_message.content or \"(pas de reponse)\"\n",
    "                    if tool_results:\n",
    "                        actions = \", \".join(tool_results[:5])\n",
    "                        final_response = f\"Actions: {actions}{nl}{final_response}\"\n",
    "                    return f\"[{self.name}] {final_response}\"\n",
    "\n",
    "            except Exception as e:\n",
    "                return f\"[{self.name}] Erreur LLM: {e}\"\n",
    "\n",
    "        actions = \", \".join(tool_results[:5])\n",
    "        return f\"[{self.name}] Max tool calls. Actions: {actions}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Patterns de Delegation Multi-Agents\n",
    "\n",
    "Les instructions ci-dessus definissent les **regles de delegation** entre agents :\n",
    "\n",
    "| Agent | Role | Delegue vers |\n",
    "|-------|------|-------------|\n",
    "| **SearchAgent** | Recherche lemmes Mathlib | TacticAgent (si lemmes trouves) |\n",
    "| **TacticAgent** | Genere tactiques Lean | VerifierAgent (toujours) |\n",
    "| **VerifierAgent** | Verifie preuve formelle | CriticAgent (si echec) / COMPLETE (si succes) |\n",
    "| **CriticAgent** | Analyse erreurs | SearchAgent (retry) / CoordinatorAgent (si bloque) |\n",
    "| **CoordinatorAgent** | Re-orchestre strategie | SearchAgent (nouvelle strategie) |\n",
    "\n",
    "**Flow nominal** (preuve simple) :\n",
    "```\n",
    "SearchAgent ‚Üí TacticAgent ‚Üí VerifierAgent ‚Üí COMPLETE\n",
    "```\n",
    "\n",
    "**Flow avec echec** (preuve complexe) :\n",
    "```\n",
    "SearchAgent ‚Üí TacticAgent ‚Üí VerifierAgent (FAIL)\n",
    "   ‚Üì\n",
    "CriticAgent analyse erreur\n",
    "   ‚Üì\n",
    "   +-- Erreur simple ‚Üí SearchAgent (retry avec nouvelles contraintes)\n",
    "   +-- Erreur complexe ‚Üí CoordinatorAgent (changement strategie)\n",
    "```\n",
    "\n",
    "**Note critique** : Les demos actuelles (DEMO_1-3) sont trop triviales et ne declenchent JAMAIS CriticAgent ni CoordinatorAgent. DEMO_4 (list_length_append) devrait necessiter induction et potentiellement trigger ces agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Quand CriticAgent et CoordinatorAgent Interviennent\n",
    "\n",
    "#### CriticAgent : Analyse d'Echecs de Tactiques\n",
    "\n",
    "**Declenche par VerifierAgent quand** :\n",
    "- `verify_proof()` retourne `success=False`\n",
    "- Erreur Lean detectee : type mismatch, tactic failed, unknown identifier\n",
    "- Preuve incomplete apres application de tactique\n",
    "\n",
    "**Responsabilites** :\n",
    "1. Parser l'erreur Lean (extraire type, message, contexte)\n",
    "2. Identifier la cause (lemme incorrect, tactique inadequate, goal mal compris)\n",
    "3. Proposer correction :\n",
    "   - Erreur simple (lemme manquant) ‚Üí Delegue SearchAgent avec contraintes\n",
    "   - Erreur complexe (strategie incorrecte) ‚Üí Delegue CoordinatorAgent\n",
    "\n",
    "**Exemple d'intervention** :\n",
    "```\n",
    "[Tour 5] VerifierAgent: FAIL - \"type mismatch, expected Nat but got Bool\"\n",
    "[Tour 6] CriticAgent: \"TacticAgent a applique 'exact lemma_bool' mais goal attend Nat.\n",
    "                       SearchAgent doit chercher lemmes avec type Nat -> Nat.\"\n",
    "[Tour 7] SearchAgent: Recherche lemmes type-aware...\n",
    "```\n",
    "\n",
    "**Pourquoi absent des demos actuelles** :\n",
    "- DEMO_1-3 : Lemmes Mathlib correspondent exactement au goal\n",
    "- Pas de type mismatch, pas de tactic failure\n",
    "- VerifierAgent retourne success au premier essai\n",
    "\n",
    "#### CoordinatorAgent : Re-Orchestration Strategique\n",
    "\n",
    "**Declenche par CriticAgent quand** :\n",
    "- Echecs multiples consecutifs (3+ iterations sans progres)\n",
    "- Strategie actuelle bloquee (EXPLORATION ‚Üí REFINEMENT ‚Üí toujours FAIL)\n",
    "- Pattern d'erreur complexe (induction necessaire mais pas tentee)\n",
    "\n",
    "**Responsabilites** :\n",
    "1. Analyser historique complet (ProofState.snapshots)\n",
    "2. Identifier pattern d'echec (loop, strategie inadequate)\n",
    "3. Changer strategie globale :\n",
    "   - EXPLORATION ‚Üí VALIDATION (essayer preuves directes)\n",
    "   - REFINEMENT ‚Üí RECOVERY (backtrack + nouvelle approche)\n",
    "4. Reset partiel de ProofState (clear failed tactics, keep lemmas)\n",
    "\n",
    "**Exemple d'intervention** :\n",
    "```\n",
    "[Tour 8] CriticAgent: \"Echec 3x consecutif avec meme lemme. Strategie bloquee.\"\n",
    "[Tour 9] CoordinatorAgent: \"Detection pattern: goal necessite induction mais pas tentee.\n",
    "                            Changement strategie: EXPLORATION ‚Üí RECOVERY.\n",
    "                            Ajout contrainte: TacticAgent DOIT considerer 'induction'.\"\n",
    "[Tour 10] SearchAgent: Recherche lemmes inductifs...\n",
    "```\n",
    "\n",
    "**Pourquoi absent des demos actuelles** :\n",
    "- DEMO_1-3 : Pas d'echecs, donc CriticAgent jamais declenche\n",
    "- DEMO_4 (list_length_append) : **DEVRAIT** declencher si :\n",
    "  - Lemme direct `List.length_append` pas trouve\n",
    "  - TacticAgent essaie `rw` ou `simp` sans induction ‚Üí echec\n",
    "  - CriticAgent detecte besoin d'induction\n",
    "  - CoordinatorAgent change strategie vers RECOVERY\n",
    "\n",
    "#### Activation des Agents Critiques\n",
    "\n",
    "| Scenario | SearchAgent | TacticAgent | VerifierAgent | CriticAgent | CoordinatorAgent |\n",
    "|----------|-------------|-------------|---------------|-------------|------------------|\n",
    "| **Preuve triviale** (rfl) | ‚úó | ‚úì | ‚úì | ‚úó | ‚úó |\n",
    "| **Lemme direct trouve** (exact) | ‚úì | ‚úì | ‚úì | ‚úó | ‚úó |\n",
    "| **Lemme incorrect** (type mismatch) | ‚úì | ‚úì | ‚úì | ‚úì | ‚úó |\n",
    "| **Tactique echoue 1x** (retry) | ‚úì | ‚úì | ‚úì | ‚úì | ‚úó |\n",
    "| **Tactique echoue 3x** (bloque) | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì |\n",
    "| **Induction necessaire** | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì |\n",
    "\n",
    "**Conclusion** : Pour tester CriticAgent et CoordinatorAgent, nous devons utiliser des theoremes ou :\n",
    "1. Mathlib n'a PAS de lemme direct exact match\n",
    "2. Preuve necessite composition de tactiques (rw + simp + induction)\n",
    "3. Premiere tentative echoue et necessite correction\n",
    "\n",
    "**DEMO_4 (list_length_append) est concu pour ca** - mais seulement si on desactive l'acces au lemme `List.length_append` de Mathlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test des Agents ===\n",
      "Mode: Semantic Kernel avec provider OpenAI\n",
      "\n",
      "============================================================\n",
      "Configuration LLM Service: OpenAI\n",
      "============================================================\n",
      "[LLM Provider] OpenAI - Model: gpt-5.2\n",
      "\n",
      "Crees 5 agents SK avec provider OpenAI et modele gpt-5.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Factory pour creer les agents (SK ou fallback)\n",
    "# =============================================================================\n",
    "\n",
    "def create_agents(\n",
    "    plugins: Dict[str, Any],\n",
    "    state: ProofState,\n",
    "    use_sk: bool = True,\n",
    "    use_simulation: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Cree les 5 agents specialises.\n",
    "\n",
    "    Args:\n",
    "        plugins: Dictionnaire des plugins SK\n",
    "        state: Etat partage de la preuve\n",
    "        use_sk: Utiliser Semantic Kernel si disponible\n",
    "        use_simulation: Mode simulation (sans appels LLM)\n",
    "\n",
    "    Returns:\n",
    "        Dictionnaire {nom_agent: agent}\n",
    "    \"\"\"\n",
    "    if use_sk and SK_AVAILABLE and not use_simulation:\n",
    "        return _create_sk_agents(plugins, state)\n",
    "    else:\n",
    "        return _create_simple_agents(plugins, use_simulation)\n",
    "\n",
    "\n",
    "def _create_simple_agents(plugins: Dict[str, Any], use_simulation: bool) -> Dict[str, Any]:\n",
    "    \"\"\"Cree les agents en mode fallback/simulation.\"\"\"\n",
    "    return {\n",
    "        \"SearchAgent\": SimpleAgent(\"SearchAgent\", SEARCH_AGENT_INSTRUCTIONS, plugins, use_simulation),\n",
    "        \"TacticAgent\": SimpleAgent(\"TacticAgent\", TACTIC_AGENT_INSTRUCTIONS, plugins, use_simulation),\n",
    "        \"VerifierAgent\": SimpleAgent(\"VerifierAgent\", VERIFIER_AGENT_INSTRUCTIONS, plugins, use_simulation),\n",
    "        \"CriticAgent\": SimpleAgent(\"CriticAgent\", CRITIC_AGENT_INSTRUCTIONS, plugins, use_simulation),\n",
    "        \"CoordinatorAgent\": SimpleAgent(\"CoordinatorAgent\", COORDINATOR_AGENT_INSTRUCTIONS, plugins, use_simulation),\n",
    "    }\n",
    "\n",
    "\n",
    "def _create_llm_service(service_name: str):\n",
    "    \"\"\"\n",
    "    Cree le service LLM en fonction du provider configure.\n",
    "\n",
    "    Args:\n",
    "        service_name: Nom du service (\"OpenAI\", \"Anthropic\", \"OpenRouter\")\n",
    "\n",
    "    Returns:\n",
    "        Tuple (service, service_id, model_name)\n",
    "    \"\"\"\n",
    "    service_name = service_name.lower()\n",
    "\n",
    "    if service_name == \"anthropic\":\n",
    "        # Anthropic Claude\n",
    "        if not ANTHROPIC_AVAILABLE:\n",
    "            raise ImportError(\"Anthropic connector not available. Install: pip install semantic-kernel[anthropic]\")\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"ANTHROPIC_API_KEY not set in environment\")\n",
    "        model = os.getenv(\"ANTHROPIC_CHAT_MODEL_ID\", \"claude-sonnet-4-5\")\n",
    "        print(f\"[LLM Provider] Anthropic - Model: {model}\")\n",
    "        service = AnthropicChatCompletion(\n",
    "            service_id=\"anthropic\",\n",
    "            ai_model_id=model,\n",
    "            api_key=api_key\n",
    "        )\n",
    "        return service, \"anthropic\", model\n",
    "\n",
    "    elif service_name == \"openrouter\":\n",
    "        # OpenRouter (OpenAI-compatible API)\n",
    "        api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENROUTER_API_KEY not set in environment\")\n",
    "        base_url = os.getenv(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")\n",
    "        model = os.getenv(\"OPENROUTER_CHAT_MODEL_ID\", \"anthropic/claude-sonnet-4\")\n",
    "        print(f\"[LLM Provider] OpenRouter - Model: {model} - Base URL: {base_url}\")\n",
    "        service = OpenAIChatCompletion(\n",
    "            service_id=\"openrouter\",\n",
    "            ai_model_id=model,\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        return service, \"openrouter\", model\n",
    "\n",
    "    else:\n",
    "        # OpenAI (default)\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not set in environment\")\n",
    "        model = os.getenv(\"OPENAI_CHAT_MODEL_ID\", \"gpt-5.2\")\n",
    "        print(f\"[LLM Provider] OpenAI - Model: {model}\")\n",
    "        service = OpenAIChatCompletion(\n",
    "            service_id=\"openai\",\n",
    "            ai_model_id=model,\n",
    "            api_key=api_key\n",
    "        )\n",
    "        return service, \"openai\", model\n",
    "\n",
    "\n",
    "def _create_sk_agents(plugins: Dict[str, Any], state: ProofState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Cree les agents avec Semantic Kernel ChatCompletionAgent.\n",
    "\n",
    "    Utilise:\n",
    "    - Le service LLM configure via GLOBAL_LLM_SERVICE (OpenAI, Anthropic, OpenRouter)\n",
    "    - FunctionChoiceBehavior.Auto() pour le function calling automatique\n",
    "    - Les plugins existants sont passes aux agents\n",
    "\n",
    "    Configuration via .env:\n",
    "        GLOBAL_LLM_SERVICE: \"OpenAI\" | \"Anthropic\" | \"OpenRouter\" (defaut: \"Anthropic\")\n",
    "\n",
    "    Pour OpenAI:\n",
    "        OPENAI_API_KEY, OPENAI_CHAT_MODEL_ID\n",
    "\n",
    "    Pour Anthropic:\n",
    "        ANTHROPIC_API_KEY, ANTHROPIC_CHAT_MODEL_ID\n",
    "\n",
    "    Pour OpenRouter:\n",
    "        OPENROUTER_API_KEY, OPENROUTER_BASE_URL, OPENROUTER_CHAT_MODEL_ID\n",
    "    \"\"\"\n",
    "    # Creer le kernel\n",
    "    kernel = Kernel()\n",
    "\n",
    "    # Determiner le provider LLM (defaut: Anthropic car quota OpenAI epuise)\n",
    "    llm_service_name = os.getenv(\"GLOBAL_LLM_SERVICE\", \"Anthropic\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Configuration LLM Service: {llm_service_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Creer le service LLM\n",
    "    service, service_id, model = _create_llm_service(llm_service_name)\n",
    "    kernel.add_service(service)\n",
    "\n",
    "    # Ajouter les plugins au kernel\n",
    "    for plugin_name, plugin in plugins.items():\n",
    "        kernel.add_plugin(plugin, plugin_name=plugin_name)\n",
    "\n",
    "    # Configuration pour auto function calling\n",
    "    settings = kernel.get_prompt_execution_settings_from_service_id(service_id=service_id)\n",
    "    settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "    # Creer les agents\n",
    "    agents = {}\n",
    "    agent_configs = [\n",
    "        (\"SearchAgent\", SEARCH_AGENT_INSTRUCTIONS),\n",
    "        (\"TacticAgent\", TACTIC_AGENT_INSTRUCTIONS),\n",
    "        (\"VerifierAgent\", VERIFIER_AGENT_INSTRUCTIONS),\n",
    "        (\"CriticAgent\", CRITIC_AGENT_INSTRUCTIONS),\n",
    "        (\"CoordinatorAgent\", COORDINATOR_AGENT_INSTRUCTIONS),\n",
    "    ]\n",
    "\n",
    "    for name, instructions in agent_configs:\n",
    "        agents[name] = ChatCompletionAgent(\n",
    "            kernel=kernel,\n",
    "            name=name,\n",
    "            instructions=instructions,\n",
    "            arguments=KernelArguments(settings=settings)\n",
    "        )\n",
    "\n",
    "    print(f\"\\nCrees {len(agents)} agents SK avec provider {llm_service_name} et modele {model}\")\n",
    "    return agents\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Test des Agents\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Test des Agents ===\")\n",
    "\n",
    "# Creer l'environnement\n",
    "test_state = ProofState(\n",
    "    theorem_statement=\"theorem add_zero (n : Nat) : n + 0 = n\",\n",
    "    current_goal=\"n + 0 = n\"\n",
    ")\n",
    "runner = LeanRunner(backend=\"subprocess\", timeout=30)\n",
    "\n",
    "# Creer les plugins\n",
    "plugins = {\n",
    "    \"state\": ProofStateManagerPlugin(test_state),\n",
    "    \"search\": LeanSearchPlugin(runner),\n",
    "    \"tactic\": LeanTacticPlugin(),\n",
    "    \"verification\": LeanVerificationPlugin(runner)\n",
    "}\n",
    "\n",
    "# Determiner le mode de fonctionnement\n",
    "# Verifier si au moins un provider LLM est configure\n",
    "llm_service = os.getenv(\"GLOBAL_LLM_SERVICE\", \"Anthropic\").lower()\n",
    "has_api_key = False\n",
    "\n",
    "if llm_service == \"openai\":\n",
    "    has_api_key = bool(os.getenv(\"OPENAI_API_KEY\"))\n",
    "elif llm_service == \"anthropic\":\n",
    "    has_api_key = bool(os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "elif llm_service == \"openrouter\":\n",
    "    has_api_key = bool(os.getenv(\"OPENROUTER_API_KEY\"))\n",
    "\n",
    "USE_SK = SK_AVAILABLE and has_api_key\n",
    "USE_SIMULATION = not USE_SK  # Simulation si SK non disponible ou pas de cle API\n",
    "\n",
    "if USE_SK:\n",
    "    print(f\"Mode: Semantic Kernel avec provider {os.getenv('GLOBAL_LLM_SERVICE', 'Anthropic')}\")\n",
    "else:\n",
    "    print(f\"Mode: Simulation (SK_AVAILABLE={SK_AVAILABLE}, has_api_key={has_api_key})\")\n",
    "\n",
    "# Creer les agents\n",
    "agents = create_agents(plugins, test_state, use_sk=USE_SK, use_simulation=USE_SIMULATION)\n",
    "\n",
    "# Test rapide en mode simulation\n",
    "if USE_SIMULATION:\n",
    "    print(\"\\nTest SearchAgent (simulation):\")\n",
    "    response = agents[\"SearchAgent\"].invoke(\"Trouve des lemmes pour n + 0 = n\", test_state)\n",
    "    print(response)\n",
    "    print(f\"Etat apres SearchAgent:\\n{test_state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Vue d'Ensemble des 5 Agents Specialises\n",
    "\n",
    "La fonction `create_agents()` instancie les 5 agents avec :\n",
    "- **Instructions** : Prompts systemiques definissant role et regles de delegation\n",
    "- **Plugins** : Fonctions exposees (search, tactic generation, verification, etc.)\n",
    "- **Modele LLM** : gpt-5.2 (ou simulation si mode LLM desactive)\n",
    "\n",
    "#### Signatures des agents\n",
    "\n",
    "```python\n",
    "SearchAgent(\n",
    "    plugins=[lean_search_plugin, state_plugin],\n",
    "    instructions=\"Trouve lemmes Mathlib pertinents...\"\n",
    ")\n",
    "\n",
    "TacticAgent(\n",
    "    plugins=[tactic_plugin, state_plugin],\n",
    "    instructions=\"Genere tactiques Lean avec confiance...\"\n",
    ")\n",
    "\n",
    "VerifierAgent(\n",
    "    plugins=[verification_plugin, state_plugin],\n",
    "    instructions=\"Compile et verifie preuves formelles...\"\n",
    ")\n",
    "\n",
    "CriticAgent(\n",
    "    plugins=[state_plugin],\n",
    "    instructions=\"Analyse echecs et propose corrections...\"\n",
    ")\n",
    "\n",
    "CoordinatorAgent(\n",
    "    plugins=[state_plugin],\n",
    "    instructions=\"Re-orchestre strategie globale...\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Pattern cle** : Chaque agent n'a acces qu'aux plugins dont il a besoin (principe de moindre privilege). Le `state_plugin` est partage par tous pour consulter/modifier ProofState.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Orchestration Multi-Agents\n",
    "\n",
    "L'orchestration determine comment les agents sont selectionnes et quand la conversation se termine.\n",
    "\n",
    "**DelegatingSelectionStrategy** (Pattern recommande):\n",
    "- Chaque agent designe explicitement le suivant via `designate_next_agent()`\n",
    "- Si aucune designation, utilise un agent par defaut (CoordinatorAgent)\n",
    "\n",
    "**ProofCompleteTermination**:\n",
    "- Termine si `proof_complete == True`\n",
    "- Termine si `iteration_count >= max_iterations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Workflow Complet de Preuve\n",
    "\n",
    "Cette demonstration montre le workflow multi-agents complet:\n",
    "1. **CoordinatorAgent** initialise la session\n",
    "2. **SearchAgent** recherche les lemmes pertinents\n",
    "3. **TacticAgent** propose des tactiques\n",
    "4. **VerifierAgent** verifie avec Lean\n",
    "5. **CriticAgent** intervient en cas d'echec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Orchestration Multi-Agents\n",
    "\n",
    "### Le probl√®me de l'orchestration\n",
    "\n",
    "Avec 5 agents, qui parle quand ? Deux approches :\n",
    "\n",
    "1. **Statique** : SearchAgent ‚Üí TacticAgent ‚Üí VerifierAgent (toujours)\n",
    "   - Simple mais rigide\n",
    "   - Pas de backtracking\n",
    "\n",
    "2. **Dynamique** : D√©cisions bas√©es sur l'√©tat de la preuve\n",
    "   - Flexible mais complexe\n",
    "   - Permet le backtracking et la r√©cup√©ration d'erreur\n",
    "\n",
    "**Nous utilisons l'approche dynamique.**\n",
    "\n",
    "### Strat√©gies d'orchestration\n",
    "\n",
    "#### ProofSelectionStrategy\n",
    "\n",
    "D√©cide **quel agent agit** √† chaque tour :\n",
    "\n",
    "```python\n",
    "class ProofSelectionStrategy:\n",
    "    def select_next_agent(self, state: ProofState, agents: List[str]) -> str:\n",
    "        if state.phase == ProofPhase.INIT:\n",
    "            return \"CoordinatorAgent\"\n",
    "        elif state.phase == ProofPhase.SEARCH:\n",
    "            return \"SearchAgent\"\n",
    "        # ...\n",
    "```\n",
    "\n",
    "#### ProofTerminationStrategy\n",
    "\n",
    "D√©cide **quand arr√™ter** la session :\n",
    "\n",
    "```python\n",
    "class ProofTerminationStrategy:\n",
    "    def should_terminate(self, state: ProofState, iteration: int) -> Tuple[bool, str]:\n",
    "        if state.phase == ProofPhase.COMPLETE:\n",
    "            return (True, \"Preuve compl√®te!\")\n",
    "        if iteration >= max_iterations:\n",
    "            return (True, \"Timeout atteint\")\n",
    "        # ...\n",
    "```\n",
    "\n",
    "### Boucle principale\n",
    "\n",
    "```python\n",
    "while not should_terminate:\n",
    "    # 1. S√©lectionner agent\n",
    "    agent_name = selection_strategy.select_next_agent(state, agents)\n",
    "\n",
    "    # 2. Ex√©cuter agent (appelle le LLM)\n",
    "    response = agent.chat(f\"Phase: {state.phase}, Goal: {state.current_goal}\")\n",
    "\n",
    "    # 3. L'agent appelle des plugins (modifie l'√©tat)\n",
    "    # Exemple: log_tactic_attempt(\"rw [Nat.add_zero]\")\n",
    "\n",
    "    # 4. Mettre √† jour phase selon r√©sultat\n",
    "    update_phase(state)\n",
    "\n",
    "    # 5. V√©rifier condition de terminaison\n",
    "    should_terminate, reason = termination_strategy.should_terminate(state, iteration)\n",
    "```\n",
    "\n",
    "### Snapshots : Observer l'orchestration\n",
    "\n",
    "√Ä chaque tour, on sauvegarde :\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"iteration\": 5,\n",
    "  \"agent\": \"TacticAgent\",\n",
    "  \"phase_before\": \"SEARCH\",\n",
    "  \"phase_after\": \"TACTIC_GEN\",\n",
    "  \"action\": \"Generated tactic: rw [Nat.add_zero]\",\n",
    "  \"state_snapshot\": {...}\n",
    "}\n",
    "```\n",
    "\n",
    "**Utilit√©** : Voir exactement quelle d√©cision chaque agent a prise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Section 8.7 - Strategies d'Orchestration (Pattern Argument_Analysis)\n",
    "# =============================================================================\n",
    "# Strategies personnalisees basees sur l'etat partage (pas sur l'historique)\n",
    "\n",
    "# Fix for Jupyter event loop\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import logging\n",
    "from typing import Dict, Any, List, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. ProofSelectionStrategy : Selection d'Agent : Selection d'Agent Basee sur l'Etat\n",
    "\n",
    "**Pattern inspire de Argument_Analysis** : Selection d'agent via **designation explicite** dans ProofState.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "```python\n",
    "class ProofSelectionStrategy(SelectionStrategy):\n",
    "    async def next(agents, history) -> Agent:\n",
    "        # 1. Lire designation explicite\n",
    "        designated = state.consume_next_agent_designation()\n",
    "        \n",
    "        # 2. Si designation presente, utiliser cet agent\n",
    "        if designated:\n",
    "            return agents_map[designated]\n",
    "        \n",
    "        # 3. Sinon, utiliser agent par defaut (SearchAgent)\n",
    "        return agents_map[default_agent_name]\n",
    "```\n",
    "\n",
    "#### Difference avec Semantic Kernel Standard\n",
    "\n",
    "| Semantic Kernel Standard | ProofSelectionStrategy (Custom) |\n",
    "|-------------------------|--------------------------------|\n",
    "| Parse historique messages | Lit `state.next_agent_designation` |\n",
    "| Selection basee sur keywords | Selection basee sur etat partage |\n",
    "| Stateless (pas de memoire) | Stateful (ProofState) |\n",
    "| Complexite O(n) messages | Complexite O(1) |\n",
    "\n",
    "**Avantage** : Chaque agent designe explicitement son successeur via `state.designate_next_agent()`, evitant parsing d'historique fragile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ProofSelectionStrategy - Selection basee sur l'etat partage\n",
    "# =============================================================================\n",
    "# NOTE: Ces classes SK ne sont definies que si SK est disponible.\n",
    "# Le mode simulation n'en a pas besoin.\n",
    "\n",
    "if SK_AVAILABLE:\n",
    "    from semantic_kernel.agents.strategies.selection.selection_strategy import SelectionStrategy\n",
    "\n",
    "    class ProofSelectionStrategy(SelectionStrategy):\n",
    "        \"\"\"Strategie de selection SK (non utilisee en mode simulation).\"\"\"\n",
    "        pass\n",
    "else:\n",
    "    print(\"ProofSelectionStrategy: Skipped (SK non disponible)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. ProofTerminationStrategy : Detection de Completion : Detection de Completion\n",
    "\n",
    "**Responsabilite** : Detecter quand arreter l'orchestration multi-agents.\n",
    "\n",
    "#### Criteres de Terminaison\n",
    "\n",
    "```python\n",
    "class ProofTerminationStrategy(TerminationStrategy):\n",
    "    async def should_terminate(agents, history) -> bool:\n",
    "        # 1. Preuve complete detectee\n",
    "        if state.proof_complete:\n",
    "            return True\n",
    "        \n",
    "        # 2. Max iterations atteint\n",
    "        if state.current_iteration >= max_iterations:\n",
    "            return True\n",
    "        \n",
    "        # 3. Timeout (optionnel)\n",
    "        if time.time() - start_time > timeout:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "```\n",
    "\n",
    "#### Comparaison avec Autres Patterns\n",
    "\n",
    "| Pattern | Terminaison basee sur | Avantages | Inconvenients |\n",
    "|---------|----------------------|-----------|---------------|\n",
    "| **Message-based** | Keyword dans dernier message (\"DONE\", \"COMPLETE\") | Simple, standard SK | Fragile, depend du LLM |\n",
    "| **State-based** (ce notebook) | `state.proof_complete` flag | Robuste, deterministe | Necessite etat partage |\n",
    "| **Iteration-based** | Compteur max iterations | Toujours termine | Peut stopper preuve incomplete |\n",
    "| **Consensus-based** | Vote agents (majorite) | Robuste aux erreurs | Complexe, lent |\n",
    "\n",
    "**Notre choix** : Combinaison **state-based + iteration-based** pour garantir terminaison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ProofTerminationStrategy - Terminaison basee sur l'etat partage\n",
    "# =============================================================================\n",
    "# NOTE: Ces classes SK ne sont definies que si SK est disponible.\n",
    "# Le mode simulation n'en a pas besoin.\n",
    "\n",
    "if SK_AVAILABLE:\n",
    "    from semantic_kernel.agents.strategies.termination.termination_strategy import TerminationStrategy\n",
    "\n",
    "    class ProofTerminationStrategy(TerminationStrategy):\n",
    "        \"\"\"Strategie de terminaison SK (non utilisee en mode simulation).\"\"\"\n",
    "        pass\n",
    "else:\n",
    "    print(\"ProofTerminationStrategy: Skipped (SK non disponible)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. ProofAgentGroupChat : Chat Multi-Agents : Orchestration Multi-Agents\n",
    "\n",
    "**Classe orchestrateur** pour g√©rer la conversation multi-agents avec Semantic Kernel.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "```python\n",
    "class ProofAgentGroupChat:\n",
    "    def __init__(agents, state, use_sk=True):\n",
    "        self.agents = agents  # Dict[str, ChatCompletionAgent]\n",
    "        self.state = state    # ProofState partage\n",
    "    \n",
    "    def run(initial_message, verbose=True) -> str:\n",
    "        # Execute conversation multi-agents\n",
    "        if use_sk:\n",
    "            return await _run_sk(...)  # Semantic Kernel\n",
    "        else:\n",
    "            return _run_fallback(...)   # Simulation\n",
    "```\n",
    "\n",
    "**Pattern cl√©** :\n",
    "- Utilise `ProofSelectionStrategy` pour s√©lectionner agents\n",
    "- Utilise `ProofTerminationStrategy` pour d√©tecter fin\n",
    "- Cr√©e `AgentGroupChat` de Semantic Kernel avec ces strat√©gies\n",
    "- Fallback en mode simulation si SK non disponible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProofAgentGroupChat:\n",
    "    \"\"\"\n",
    "    Orchestre les agents pour la preuve de theoremes.\n",
    "    Supporte mode simulation (SimpleAgent) et mode SK (ChatCompletionAgent).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agents: Dict[str, Any], state: ProofState, use_sk: bool = True):\n",
    "        self.agents = agents\n",
    "        self.state = state\n",
    "        self.use_sk = use_sk and SK_AVAILABLE\n",
    "        self.history = []\n",
    "        self._proof_tactics_found = []  # Track tactics found across iterations\n",
    "\n",
    "    def run(self, initial_message: str, verbose: bool = True) -> str:\n",
    "        \"\"\"Execute la conversation multi-agents.\"\"\"\n",
    "        if self.use_sk:\n",
    "            # Mode Semantic Kernel - utilise async\n",
    "            import asyncio\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            try:\n",
    "                loop = asyncio.get_event_loop()\n",
    "                return loop.run_until_complete(self._run_sk(initial_message, verbose))\n",
    "            except RuntimeError:\n",
    "                return asyncio.run(self._run_sk(initial_message, verbose))\n",
    "        else:\n",
    "            # Mode simulation - sync\n",
    "            return self._run_fallback(initial_message, verbose)\n",
    "\n",
    "    async def _run_sk(self, initial_message: str, verbose: bool = True) -> str:\n",
    "        \"\"\"Execution avec Semantic Kernel ChatCompletionAgent - LOGGING AMELIORE.\"\"\"\n",
    "        from semantic_kernel.contents.chat_history import ChatHistory\n",
    "        from semantic_kernel.contents.chat_message_content import ChatMessageContent\n",
    "        from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "        from datetime import datetime\n",
    "        import re\n",
    "\n",
    "        def clean_response(text: str) -> str:\n",
    "            \"\"\"Nettoie les reponses LLM (supprime newlines excessifs).\"\"\"\n",
    "            # Remplacer sequences de 3+ newlines par 2\n",
    "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "            # Supprimer espaces en debut/fin\n",
    "            text = text.strip()\n",
    "            return text\n",
    "\n",
    "        def format_timestamp() -> str:\n",
    "            \"\"\"Retourne timestamp lisible.\"\"\"\n",
    "            return datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "\n",
    "        if verbose:\n",
    "            session_start = datetime.now()\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"SESSION MULTI-AGENTS (SK)\")\n",
    "            print(f\"Theoreme: {initial_message[:100]}...\")\n",
    "            print(\"=\" * 70)\n",
    "\n",
    "        # Creer l'historique de chat partage entre les agents\n",
    "        chat_history = ChatHistory()\n",
    "        chat_history.add_user_message(initial_message)\n",
    "\n",
    "        current_message = initial_message\n",
    "        agent_order = [\"SearchAgent\", \"TacticAgent\", \"VerifierAgent\", \"CriticAgent\", \"CoordinatorAgent\"]\n",
    "\n",
    "        for i in range(self.state.max_iterations):\n",
    "            self.state.iteration = i + 1\n",
    "            self.state.increment_iteration()\n",
    "            iter_start = datetime.now()\n",
    "\n",
    "            # Determiner l'agent a utiliser\n",
    "            designated = self.state.consume_next_agent_designation()\n",
    "            if designated and designated in self.agents:\n",
    "                agent_name = designated\n",
    "            else:\n",
    "                agent_name = agent_order[i % len(agent_order)]\n",
    "\n",
    "            agent = self.agents.get(agent_name)\n",
    "            if not agent:\n",
    "                continue\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\n{'‚îÄ' * 70}\")\n",
    "                elapsed = (datetime.now() - session_start).total_seconds()\n",
    "                print(f\"[+{elapsed:.2f}s] TOUR {self.state.iteration_count} | Agent: {agent_name}\")\n",
    "                print(f\"{'‚îÄ' * 70}\")\n",
    "\n",
    "            # Invoquer l'agent SK de maniere asynchrone\n",
    "            try:\n",
    "                response_text = \"\"\n",
    "\n",
    "                # ChatCompletionAgent.invoke() prend un ChatHistory et retourne un AsyncIterable\n",
    "                async for response in agent.invoke(chat_history):\n",
    "                    if hasattr(response, 'content') and response.content:\n",
    "                        response_text += str(response.content)\n",
    "                    elif hasattr(response, 'items'):\n",
    "                        # Si c'est un ChatMessageContent avec items\n",
    "                        for item in response.items:\n",
    "                            if hasattr(item, 'text'):\n",
    "                                response_text += item.text\n",
    "\n",
    "                if not response_text:\n",
    "                    response_text = f\"[{agent_name}] Pas de reponse\"\n",
    "\n",
    "                # Nettoyer la reponse\n",
    "                response_text = clean_response(response_text)\n",
    "\n",
    "                # Ajouter la reponse a l'historique\n",
    "                chat_history.add_assistant_message(response_text)\n",
    "\n",
    "                # Ajouter le prochain message utilisateur (contexte pour le prochain agent)\n",
    "                if i < self.state.max_iterations - 1:\n",
    "                    next_context = f\"Continue la preuve. Reponse precedente de {agent_name}: {response_text[:200]}\"\n",
    "                    chat_history.add_user_message(next_context)\n",
    "\n",
    "                # Mettre a jour l'etat selon la reponse\n",
    "                self._update_state_from_response(agent_name, response_text)\n",
    "\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                response_text = f\"Erreur agent {agent_name}: {str(e)}\"\n",
    "                if verbose:\n",
    "                    print(f\"  [ERROR] {e}\")\n",
    "                    traceback.print_exc()\n",
    "\n",
    "            iter_duration = (datetime.now() - iter_start).total_seconds()\n",
    "\n",
    "            self.history.append({\n",
    "                \"iteration\": self.state.iteration_count,\n",
    "                \"agent\": agent_name,\n",
    "                \"response\": response_text,\n",
    "                \"duration_s\": iter_duration\n",
    "            })\n",
    "\n",
    "            if verbose:\n",
    "                # Afficher reponse complete (pas tronquee)\n",
    "                print(f\"  Reponse ({len(response_text)} chars, {iter_duration:.2f}s):\")\n",
    "                # Indenter chaque ligne pour lisibilite\n",
    "                for line in response_text.split('\\n')[:30]:  # Max 30 lignes\n",
    "                    if line.strip():\n",
    "                        print(f\"    {line}\")\n",
    "                if response_text.count('\\n') > 30:\n",
    "                    print(f\"    ... ({response_text.count(chr(10)) - 30} lignes supprimees)\")\n",
    "\n",
    "            if self.state.proof_complete:\n",
    "                if verbose:\n",
    "                    elapsed = (datetime.now() - session_start).total_seconds(); print(f\"\\n[+{elapsed:.2f}s] PREUVE TROUVEE!\")\n",
    "                    print(f\"  Tactique finale: {self.state.final_proof}\")\n",
    "                break\n",
    "\n",
    "            current_message = response_text\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            total_time = (datetime.now() - session_start).total_seconds()\n",
    "            print(f\"SESSION TERMINEE (duree totale: {total_time:.2f}s)\")\n",
    "            print(f\"  Iterations: {self.state.iteration_count}\")\n",
    "            print(f\"  Lemmes decouverts: {len(self.state.discovered_lemmas)}\")\n",
    "            print(f\"  Tactiques essayees: {len(self.state.tactics_history)}\")\n",
    "            print(\"=\" * 70)\n",
    "\n",
    "        return self.state.final_proof or \"Preuve non trouvee\"\n",
    "\n",
    "\n",
    "    def _update_state_from_response(self, agent_name: str, response: str):\n",
    "        \"\"\"Met a jour l'etat partage en fonction de la reponse de l'agent.\"\"\"\n",
    "        import re\n",
    "        response_lower = response.lower()\n",
    "\n",
    "        # Detection des lemmes decouverts\n",
    "        if \"lemma:\" in response_lower or \"found:\" in response_lower or \"nat.\" in response_lower:\n",
    "            lemma_matches = re.findall(r'(Nat\\.\\w+|Eq\\.\\w+|List\\.\\w+)', response)\n",
    "            for lemma in lemma_matches:\n",
    "                if lemma not in self.state.discovered_lemmas:\n",
    "                    self.state.discovered_lemmas.append(lemma)\n",
    "\n",
    "        # Detection des tactiques - track across iterations\n",
    "        proof_patterns = [\n",
    "            (r'simp\\s*\\[[^\\]]*\\]', 'simp'),\n",
    "            (r'\\brfl\\b', 'rfl'),\n",
    "            (r'exact\\s+\\w+', 'exact'),\n",
    "            (r'\\bring\\b', 'ring'),\n",
    "            (r'\\bomega\\b', 'omega'),\n",
    "            (r'\\blinarith\\b', 'linarith'),\n",
    "            (r'\\bdecide\\b', 'decide'),\n",
    "        ]\n",
    "\n",
    "        for pattern, tactic_name in proof_patterns:\n",
    "            if re.search(pattern, response, re.IGNORECASE):\n",
    "                if tactic_name not in self._proof_tactics_found:\n",
    "                    self._proof_tactics_found.append(tactic_name)\n",
    "                    self.state.tactics_history.append(response[:100])\n",
    "\n",
    "        # Detection de preuve complete - multiple signals\n",
    "        proof_complete_signals = [\n",
    "            \"proof complete\",\n",
    "            \"qed\",\n",
    "            \"verified\",\n",
    "            \"goals accomplished\",\n",
    "            \"proof found\",\n",
    "            \"la preuve est terminee\",\n",
    "            \"la preuve est cloturee\",\n",
    "            \"preuve reussie\",\n",
    "        ]\n",
    "\n",
    "        if any(signal in response_lower for signal in proof_complete_signals):\n",
    "            # If we have found tactics earlier, mark as complete\n",
    "            if self._proof_tactics_found:\n",
    "                self.state.phase = ProofPhase.COMPLETE\n",
    "                if not self.state.final_proof:\n",
    "                    self.state.final_proof = self._proof_tactics_found[0]\n",
    "        elif \":= by\" in response and self._proof_tactics_found:\n",
    "            # Lean-style proof block detected with tactics\n",
    "            self.state.phase = ProofPhase.COMPLETE\n",
    "            if not self.state.final_proof:\n",
    "                self.state.final_proof = self._proof_tactics_found[0]\n",
    "\n",
    "        # Alternative: detect complete proof in code block\n",
    "        code_block_match = re.search(r'```lean\\n(.*?)```', response, re.DOTALL)\n",
    "        if code_block_match:\n",
    "            code_content = code_block_match.group(1)\n",
    "            if \":= by\" in code_content or \":= rfl\" in code_content:\n",
    "                # Check for proof tactics in the code block\n",
    "                for pattern, tactic_name in proof_patterns:\n",
    "                    if re.search(pattern, code_content, re.IGNORECASE):\n",
    "                        self.state.phase = ProofPhase.COMPLETE\n",
    "                        self.state.final_proof = code_content.strip()[:200]\n",
    "                        break\n",
    "\n",
    "        # Detection de delegation\n",
    "        delegate_patterns = [\n",
    "            (r'@TacticAgent|delegate.*TacticAgent', 'TacticAgent'),\n",
    "            (r'@VerifierAgent|delegate.*VerifierAgent', 'VerifierAgent'),\n",
    "            (r'@CriticAgent|delegate.*CriticAgent', 'CriticAgent'),\n",
    "            (r'@CoordinatorAgent|delegate.*CoordinatorAgent', 'CoordinatorAgent'),\n",
    "            (r'@SearchAgent|delegate.*SearchAgent', 'SearchAgent'),\n",
    "        ]\n",
    "        for pattern, target in delegate_patterns:\n",
    "            if re.search(pattern, response, re.IGNORECASE):\n",
    "                self.state.designate_next_agent(target)\n",
    "                break\n",
    "\n",
    "    def _run_fallback(self, initial_message: str, verbose: bool = True) -> str:\n",
    "        \"\"\"Execution sans Semantic Kernel (mode simulation) - LOGGING AMELIORE.\"\"\"\n",
    "        from datetime import datetime\n",
    "\n",
    "        def format_timestamp() -> str:\n",
    "            return datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "\n",
    "        session_start = datetime.now()\n",
    "        if verbose:\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"SESSION MULTI-AGENTS (Simulation)\")\n",
    "            print(f\"Theoreme: {initial_message[:100]}...\")\n",
    "            print(\"=\" * 70)\n",
    "\n",
    "        current_message = initial_message\n",
    "        agent_order = [\"SearchAgent\", \"TacticAgent\", \"VerifierAgent\", \"CriticAgent\", \"CoordinatorAgent\"]\n",
    "\n",
    "        for i in range(self.state.max_iterations):\n",
    "            self.state.iteration = i + 1\n",
    "            iter_start = datetime.now()\n",
    "\n",
    "            designated = self.state.consume_next_agent_designation()\n",
    "            if designated and designated in self.agents:\n",
    "                agent_name = designated\n",
    "            else:\n",
    "                agent_name = agent_order[i % len(agent_order)]\n",
    "\n",
    "            agent = self.agents.get(agent_name)\n",
    "            if not agent:\n",
    "                continue\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\n{'‚îÄ' * 70}\")\n",
    "                elapsed = (datetime.now() - session_start).total_seconds()\n",
    "                print(f\"[+{elapsed:.2f}s] TOUR {self.state.iteration_count} | Agent: {agent_name}\")\n",
    "                print(f\"{'‚îÄ' * 70}\")\n",
    "\n",
    "            response = agent.invoke(current_message, self.state)\n",
    "            iter_duration = (datetime.now() - iter_start).total_seconds()\n",
    "\n",
    "            self.history.append({\n",
    "                \"iteration\": self.state.iteration_count,\n",
    "                \"agent\": agent_name,\n",
    "                \"response\": response,\n",
    "                \"duration_s\": iter_duration\n",
    "            })\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"  Reponse ({len(response)} chars, {iter_duration:.3f}s):\")\n",
    "                for line in response.split('\\n')[:20]:\n",
    "                    if line.strip():\n",
    "                        print(f\"    {line}\")\n",
    "                if response.count('\\n') > 20:\n",
    "                    print(f\"    ... ({response.count(chr(10)) - 20} lignes supprimees)\")\n",
    "\n",
    "            if self.state.proof_complete:\n",
    "                if verbose:\n",
    "                    elapsed = (datetime.now() - session_start).total_seconds(); print(f\"\\n[+{elapsed:.2f}s] PREUVE TROUVEE!\")\n",
    "                    print(f\"  Tactique finale: {self.state.final_proof}\")\n",
    "                break\n",
    "\n",
    "            current_message = response\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            total_time = (datetime.now() - session_start).total_seconds()\n",
    "            print(f\"SESSION TERMINEE (duree totale: {total_time:.2f}s)\")\n",
    "            print(f\"  Iterations: {self.state.iteration_count}\")\n",
    "            print(f\"  Lemmes decouverts: {len(self.state.discovered_lemmas)}\")\n",
    "            print(f\"  Tactiques essayees: {len(self.state.tactics_history)}\")\n",
    "            print(\"=\" * 70)\n",
    "\n",
    "        return self.state.final_proof or \"Preuve non trouvee\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Test des Strategies\n",
    "\n",
    "Code de test pour valider :\n",
    "- **ProofTerminationStrategy** : D√©tecte `state.proof_complete`\n",
    "- **SimpleOrchestratorAgent** : Ex√©cute conversation avec d√©signation d'agents\n",
    "\n",
    "**Ex√©cution automatique** lors du chargement de la cellule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test des Strategies ===\n",
      "State cree: dc87d04f\n",
      "Phase initiale: init\n",
      "Designation test: TacticAgent\n",
      "proof_complete initial: False\n",
      "proof_complete apres COMPLETE: True\n",
      "\n",
      "Strategies pretes pour utilisation avec AgentGroupChat\n"
     ]
    }
   ],
   "source": [
    "# Test des Strategies\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== Test des Strategies ===\")\n",
    "\n",
    "test_state = ProofState(\n",
    "    theorem_statement=\"theorem test (n : Nat) : n = n\",\n",
    "    current_goal=\"n = n\",\n",
    "    max_iterations=5\n",
    ")\n",
    "\n",
    "print(f\"State cree: {test_state.session_id}\")\n",
    "print(f\"Phase initiale: {test_state.phase.value}\")\n",
    "\n",
    "# Test designation\n",
    "test_state.designate_next_agent(\"TacticAgent\")\n",
    "designated = test_state.consume_next_agent_designation()\n",
    "print(f\"Designation test: {designated}\")\n",
    "\n",
    "# Test proof_complete\n",
    "print(f\"proof_complete initial: {test_state.proof_complete}\")\n",
    "test_state.phase = ProofPhase.COMPLETE\n",
    "print(f\"proof_complete apres COMPLETE: {test_state.proof_complete}\")\n",
    "\n",
    "print(\"\\nStrategies pretes pour utilisation avec AgentGroupChat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demonstrations Progressives\n",
    "\n",
    "Les 4 demonstrations suivantes illustrent le fonctionnement du systeme multi-agents avec une progression de complexite croissante :\n",
    "\n",
    "| Demo | Theoreme | Complexite | Iterations | Technique |\n",
    "|------|----------|------------|------------|-----------|\n",
    "| DEMO_1 | `n = n` | Triviale | 2 | `rfl` direct |\n",
    "| DEMO_2 | `a*c + b*c = (a+b)*c` | Simple | 5 | Recherche + reecriture inversee |\n",
    "| DEMO_3 | `m * n = n * m` | Intermediaire | 8 | Lemme `Nat.mul_comm` |\n",
    "| DEMO_4 | `a^(m+n) = a^m * a^n` | Avancee | 15 | Induction + lemmes multiples |\n",
    "\n",
    "**Progression pedagogique** :\n",
    "- DEMO_1 : Validation du pipeline (cas trivial)\n",
    "- DEMO_2 : Introduction de la recherche de lemmes\n",
    "- DEMO_3 : Propriete fondamentale avec `exact`\n",
    "- DEMO_4 : Stress-test avec induction complexe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d304fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMONSTRATIONS PROGRESSIVES - SYSTEME MULTI-AGENTS\n",
      "======================================================================\n",
      "Mode: LLM (OpenAI)\n",
      "Les 4 DEMOs testent des theoremes de complexite croissante:\n",
      "  DEMO_1: Reflexivite (rfl)\n",
      "  DEMO_2: Recherche de lemme (Nat.zero_add)\n",
      "  DEMO_3: Reecriture inversee (add_mul)\n",
      "  DEMO_4: Induction double (mul_comm)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Configuration du mode d'execution\n",
    "# =============================================================================\n",
    "\n",
    "# Mode LLM ou Simulation - MODIFIEZ ICI selon vos besoins\n",
    "USE_LLM_MODE = True  # True = appels LLM reels, False = simulation\n",
    "\n",
    "# Les DEMOs suivantes utilisent des definitions inline pour iteration independante.\n",
    "# Chaque DEMO peut etre executee et corrigee independamment.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMONSTRATIONS PROGRESSIVES - SYSTEME MULTI-AGENTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Mode: {'LLM (OpenAI)' if USE_LLM_MODE else 'Simulation'}\")\n",
    "print(\"Les 4 DEMOs testent des theoremes de complexite croissante:\")\n",
    "print(\"  DEMO_1: Reflexivite (rfl)\")\n",
    "print(\"  DEMO_2: Recherche de lemme (Nat.zero_add)\")\n",
    "print(\"  DEMO_3: Reecriture inversee (add_mul)\")\n",
    "print(\"  DEMO_4: Induction double (mul_comm)\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DEMONSTRATION MULTI-AGENTS POUR THEOREM PROVING\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8.8 - Demonstration Complete\n",
    "# =============================================================================\n",
    "\n",
    "def prove_with_multi_agents(\n",
    "    theorem: str,\n",
    "    goal: str = \"\",\n",
    "    max_iterations: int = 20,\n",
    "    verbose: bool = True,\n",
    "    use_simulation: bool = None  # None = auto-detect\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Prouve un theoreme en utilisant le systeme multi-agents.\n",
    "\n",
    "    Args:\n",
    "        theorem: L'enonce du theoreme complet\n",
    "        goal: Le but a prouver (extrait du theoreme si non fourni)\n",
    "        max_iterations: Nombre maximum d'iterations\n",
    "        verbose: Afficher les logs\n",
    "        use_simulation: True=simulation, False=LLM reel, None=auto\n",
    "\n",
    "    Returns:\n",
    "        Dict avec resultats et metriques\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Auto-detection du mode\n",
    "    if use_simulation is None:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "        has_valid_key = api_key and len(api_key) > 10 and not api_key.startswith(\"sk-...\")\n",
    "        use_simulation = not has_valid_key\n",
    "\n",
    "    # 1. Creer l'etat\n",
    "    if not goal:\n",
    "        if \":\" in theorem:\n",
    "            goal = theorem.split(\":\")[-1].strip()\n",
    "\n",
    "    state = ProofState(\n",
    "        theorem_statement=theorem,\n",
    "        current_goal=goal,\n",
    "        max_iterations=max_iterations\n",
    "    )\n",
    "\n",
    "    # 2. Creer le runner Lean\n",
    "    runner = LeanRunner(backend=\"subprocess\", timeout=30)\n",
    "\n",
    "    # 3. Creer les plugins\n",
    "    plugins = {\n",
    "        \"state\": ProofStateManagerPlugin(state),\n",
    "        \"search\": LeanSearchPlugin(runner),\n",
    "        \"tactic\": LeanTacticPlugin(),\n",
    "        \"verification\": LeanVerificationPlugin(runner)\n",
    "    }\n",
    "\n",
    "    # 4. Creer les agents\n",
    "    use_sk = SK_AVAILABLE and not use_simulation\n",
    "    agents = create_agents(plugins, state, use_sk=use_sk, use_simulation=use_simulation)\n",
    "\n",
    "    # 5. Configurer les strategies\n",
    "    # Strategies gerees automatiquement par ProofAgentGroupChat\n",
    "\n",
    "    # 6. Creer le groupe de chat\n",
    "    chat = ProofAgentGroupChat(\n",
    "        agents=agents,\n",
    "        state=state,\n",
    "        use_sk=use_sk\n",
    "    )\n",
    "\n",
    "    mode_str = \"Semantic Kernel\" if use_sk else (\"Simulation\" if use_simulation else \"OpenAI direct\")\n",
    "    if verbose:\n",
    "        print(f\"Mode: {mode_str}\")\n",
    "\n",
    "    # 7. Executer\n",
    "    result = chat.run(f\"Prouver: {theorem}\", verbose=verbose)\n",
    "\n",
    "    # 8. Collecter les metriques\n",
    "    elapsed = time.time() - start_time\n",
    "    metrics = {\n",
    "        \"success\": state.proof_complete,\n",
    "        \"theorem\": theorem,\n",
    "        \"final_proof\": state.final_proof,\n",
    "        \"iterations\": state.iteration_count,\n",
    "        \"lemmas_discovered\": len(state.discovered_lemmas),\n",
    "        \"tactics_tried\": len(state.tactics_history),\n",
    "        \"verifications\": len(state.verification_results),\n",
    "        \"total_time_s\": round(elapsed, 2),\n",
    "        \"lean_time_ms\": round(state.total_lean_time_ms, 2),\n",
    "        \"mode\": mode_str\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Test de la demonstration\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMONSTRATION MULTI-AGENTS POUR THEOREM PROVING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# Section 8.8 - D√©monstrations Progressives Multi-Agents\n",
    "# =============================================================================\n",
    "\n",
    "# Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. DEMO_1 : Reflexivite Pure\n",
    "\n",
    "**Objectif** : Valider le pipeline complet avec un theoreme trivial\n",
    "\n",
    "**Theoreme** : `theorem demo_rfl (n : Nat) : n = n`\n",
    "\n",
    "**Comportement attendu** : 1-2 iterations : `rfl` suffit immediatement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEMO 1/4: DEMO_1_REFLEXIVITY\n",
      "======================================================================\n",
      "Theoreme: theorem demo_rfl (n : Nat) : n = n\n",
      "Complexite: Triviale - rfl suffit\n",
      "Iterations attendues: 2\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Configuration LLM Service: OpenAI\n",
      "============================================================\n",
      "[LLM Provider] OpenAI - Model: gpt-5.2\n",
      "\n",
      "Crees 5 agents SK avec provider OpenAI et modele gpt-5.2\n",
      "Mode: Semantic Kernel\n",
      "======================================================================\n",
      "SESSION MULTI-AGENTS (SK)\n",
      "Theoreme: Prouver: theorem demo_rfl (n : Nat) : n = n...\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+0.00s] TOUR 2 | Agent: SearchAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (364 chars, 18.34s):\n",
      "    Delegation √† TacticAgent.\n",
      "    Lemmes pertinents trouv√©s (Mathlib / Init) pour `theorem demo_rfl (n : Nat) : n = n` :\n",
      "    1) **`rfl`**\n",
      "    - Type : `rfl : a = a`\n",
      "    - Sert directement √† fermer le but `n = n`.\n",
      "    2) **`Eq.refl`**\n",
      "    - Type : `Eq.refl (a : Œ±) : a = a`\n",
      "    - Alternative explicite √† `rfl` (preuve : `exact Eq.refl n`).\n",
      "    Je d√©l√®gue √† **TacticAgent** pour assembler la preuve.\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+18.34s] TOUR 3 | Agent: TacticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (131 chars, 8.12s):\n",
      "    Tactique (1√®re tentative, ordre impos√©) :\n",
      "    ```lean\n",
      "    rfl\n",
      "    ```\n",
      "    Je d√©l√®gue √† **VerifierAgent** pour v√©rifier que cela cl√¥t bien le but.\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+26.46s] TOUR 4 | Agent: VerifierAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (94 chars, 12.96s):\n",
      "    La preuve compile et est compl√®te.\n",
      "    ```lean\n",
      "    theorem demo_rfl (n : Nat) : n = n := by\n",
      "      rfl\n",
      "    ```\n",
      "\n",
      "[+39.42s] PREUVE TROUVEE!\n",
      "  Tactique finale: theorem demo_rfl (n : Nat) : n = n := by\n",
      "  rfl\n",
      "\n",
      "======================================================================\n",
      "SESSION TERMINEE (duree totale: 39.42s)\n",
      "  Iterations: 4\n",
      "  Lemmes decouverts: 2\n",
      "  Tactiques essayees: 3\n",
      "======================================================================\n",
      "\n",
      "Resultat DEMO_1:\n",
      "  - Success: True\n",
      "  - Iterations: 4 (attendu: 2)\n",
      "  - Proof: theorem demo_rfl (n : Nat) : n = n := by\n",
      "  rfl\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEMO_1 : REFLEXIVITY\n",
    "# =============================================================================\n",
    "\n",
    "# Definition inline pour iteration independante\n",
    "demo_1 = {\n",
    "    \"name\": \"DEMO_1_REFLEXIVITY\",\n",
    "    \"theorem\": \"theorem demo_rfl (n : Nat) : n = n\",\n",
    "    \"expected_iterations\": 2,\n",
    "    \"expected_lemmas\": 0,\n",
    "    \"complexity\": \"Triviale - rfl suffit\",\n",
    "    \"strategy\": \"rfl\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"DEMO 1/4: {demo_1['name']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Theoreme: {demo_1['theorem']}\")\n",
    "print(f\"Complexite: {demo_1['complexity']}\")\n",
    "print(f\"Iterations attendues: {demo_1['expected_iterations']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result_1 = prove_with_multi_agents(\n",
    "    theorem=demo_1[\"theorem\"],\n",
    "    max_iterations=20,\n",
    "    verbose=True,\n",
    "    use_simulation=not USE_LLM_MODE\n",
    ")\n",
    "\n",
    "print(f\"\\nResultat DEMO_1:\")\n",
    "print(f\"  - Success: {result_1['success']}\")\n",
    "print(f\"  - Iterations: {result_1['iterations']} (attendu: {demo_1['expected_iterations']})\")\n",
    "print(f\"  - Proof: {result_1['final_proof']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. DEMO_2 : Distributivite Inversee\n",
    "\n",
    "**Objectif** : Montrer la recherche de lemme avec reecriture inversee\n",
    "\n",
    "**Theoreme** : `theorem add_mul_distrib (a b c : Nat) : a * c + b * c = (a + b) * c`\n",
    "\n",
    "**Comportement attendu** : 4-6 iterations : recherche `Nat.add_mul`, reecriture inversee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEMO 2/4: DEMO_2_DISTRIBUTIVITY\n",
      "======================================================================\n",
      "Theoreme: theorem add_mul_distrib (a b c : Nat) : a * c + b * c = (a + b) * c\n",
      "Complexite: Simple - forme inversee du lemme standard\n",
      "Iterations attendues: 5\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Configuration LLM Service: OpenAI\n",
      "============================================================\n",
      "[LLM Provider] OpenAI - Model: gpt-5.2\n",
      "\n",
      "Crees 5 agents SK avec provider OpenAI et modele gpt-5.2\n",
      "Mode: Semantic Kernel\n",
      "======================================================================\n",
      "SESSION MULTI-AGENTS (SK)\n",
      "Theoreme: Prouver: theorem add_mul_distrib (a b c : Nat) : a * c + b * c = (a + b) * c...\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+0.00s] TOUR 2 | Agent: SearchAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (264 chars, 52.23s):\n",
      "    {\"name\":\"Nat.add_mul\",\"statement\":\"Nat.add_mul (n m k : Nat) : (n + m) * k = n * k + m * k\",\"namespace\":\"Nat\",\"relevance\":0.95}Kwamamaza\n",
      "    {\"name\":\"Nat.mul_add\",\"statement\":\"Nat.mul_add (n m k : Nat) : n * (m + k) = n * m + n * k\",\"namespace\":\"Nat\",\"relevance\":0.9}\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+52.23s] TOUR 3 | Agent: TacticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (57 chars, 9.00s):\n",
      "    Tactique (1√®re tentative, obligatoire) :\n",
      "    ```lean\n",
      "    rfl\n",
      "    ```\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+61.23s] TOUR 4 | Agent: VerifierAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (290 chars, 11.54s):\n",
      "    La tactique `rfl` ne marche pas (but non d√©finitionnellement √©gal).\n",
      "    R√©sultat Lean (extrait) :\n",
      "    - `error: The rfl tactic failed`\n",
      "    - But restant : `a * c + b * c = (a + b) * c`\n",
      "    Je d√©l√®gue √† **CriticAgent** pour proposer une tactique correcte (probablement via `Nat.add_mul` puis `simp`/`rw`).\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+72.77s] TOUR 5 | Agent: CriticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (308 chars, 17.00s):\n",
      "    On utilise directement le lemme standard `Nat.add_mul`, mais dans le sens sym√©trique.\n",
      "    ```lean\n",
      "    theorem add_mul_distrib (a b c : Nat) : a * c + b * c = (a + b) * c := by\n",
      "      simpa [Nat.add_mul] using (Nat.add_mul a b c).symm\n",
      "    ```\n",
      "    (`Nat.add_mul a b c` donne `(a + b) * c = a * c + b * c`, donc on prend `.symm`.)\n",
      "\n",
      "[+89.77s] PREUVE TROUVEE!\n",
      "  Tactique finale: rfl\n",
      "\n",
      "======================================================================\n",
      "SESSION TERMINEE (duree totale: 89.77s)\n",
      "  Iterations: 5\n",
      "  Lemmes decouverts: 3\n",
      "  Tactiques essayees: 3\n",
      "======================================================================\n",
      "\n",
      "Resultat DEMO_2:\n",
      "  - Success: True\n",
      "  - Iterations: 5 (attendu: 5)\n",
      "  - Proof: rfl\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEMO_2 : DISTRIBUTIVITY\n",
    "# =============================================================================\n",
    "\n",
    "# Definition inline pour iteration independante\n",
    "demo_2 = {\n",
    "    \"name\": \"DEMO_2_DISTRIBUTIVITY\",\n",
    "    \"theorem\": \"theorem add_mul_distrib (a b c : Nat) : a * c + b * c = (a + b) * c\",\n",
    "    \"expected_iterations\": 5,\n",
    "    \"expected_lemmas\": 1,\n",
    "    \"complexity\": \"Simple - forme inversee du lemme standard\",\n",
    "    \"strategy\": \"rw [<- Nat.add_mul]\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"DEMO 2/4: {demo_2['name']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Theoreme: {demo_2['theorem']}\")\n",
    "print(f\"Complexite: {demo_2['complexity']}\")\n",
    "print(f\"Iterations attendues: {demo_2['expected_iterations']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result_2 = prove_with_multi_agents(\n",
    "    theorem=demo_2[\"theorem\"],\n",
    "    max_iterations=20,\n",
    "    verbose=True,\n",
    "    use_simulation=not USE_LLM_MODE\n",
    ")\n",
    "\n",
    "print(f\"\\nResultat DEMO_2:\")\n",
    "print(f\"  - Success: {result_2['success']}\")\n",
    "print(f\"  - Iterations: {result_2['iterations']} (attendu: {demo_2['expected_iterations']})\")\n",
    "print(f\"  - Proof: {result_2['final_proof']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. DEMO_3 : Commutativite Multiplication\n",
    "\n",
    "**Objectif** : Tester une propriete fondamentale avec recherche\n",
    "\n",
    "**Theoreme** : `theorem mul_comm_manual (m n : Nat) : m * n = n * m`\n",
    "\n",
    "**Comportement attendu** : 6-10 iterations : decouverte de `Nat.mul_comm`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEMO 3/4: DEMO_3_MUL_COMM\n",
      "======================================================================\n",
      "Theoreme: theorem mul_comm_manual (m n : Nat) : m * n = n * m\n",
      "Complexite: Intermediaire - necessite lemme de commutativite\n",
      "Iterations attendues: 8\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Configuration LLM Service: OpenAI\n",
      "============================================================\n",
      "[LLM Provider] OpenAI - Model: gpt-5.2\n",
      "\n",
      "Crees 5 agents SK avec provider OpenAI et modele gpt-5.2\n",
      "Mode: Semantic Kernel\n",
      "======================================================================\n",
      "SESSION MULTI-AGENTS (SK)\n",
      "Theoreme: Prouver: theorem mul_comm_manual (m n : Nat) : m * n = n * m...\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+0.00s] TOUR 2 | Agent: SearchAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (196 chars, 88.18s):\n",
      "    {\"recipient_name\":\"functions.state-add_discovered_lemma\",\"parameters\":{\"name\":\"Nat.succ_mul\",\"statement\":\"Nat.succ_mul (n m : Nat) : n.succ * m = n * m + m\",\"namespace\":\"Nat\",\"relevance\":0.75}}}]}\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+88.18s] TOUR 3 | Agent: TacticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (3 chars, 6.51s):\n",
      "    rfl\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+94.69s] TOUR 4 | Agent: VerifierAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (386 chars, 23.71s):\n",
      "    √âchec de la tactique `rfl`.\n",
      "    - **But courant** : `m * n = n * m`\n",
      "    - **Essai v√©rifi√©** :\n",
      "      ```lean\n",
      "      by\n",
      "        rfl\n",
      "      ```\n",
      "    - **Erreur Lean** : `rfl` ne marche pas car `m * n` et `n * m` ne sont pas d√©finitionnellement √©gaux.\n",
      "    Je d√©l√®gue √† **CriticAgent** pour proposer une preuve correcte (probablement par r√©currence en utilisant `Nat.mul_succ` / `Nat.succ_mul` et des lemmes sur l‚Äôaddition).\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+118.40s] TOUR 5 | Agent: CriticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (854 chars, 15.15s):\n",
      "    Le dernier √©chec (`rfl`) est normal : `m * n` et `n * m` ne sont pas √©gaux par r√©duction.\n",
      "    Direction de preuve recommand√©e : **r√©currence sur `n`** (ou sur `m`). Les lemmes disponibles et pertinents :\n",
      "    - `Nat.mul_zero (m) : m * 0 = 0`\n",
      "    - `Nat.zero_mul (n) : 0 * n = 0`\n",
      "    - `Nat.mul_succ (m n) : m * (n+1) = m * n + m`\n",
      "    - `Nat.succ_mul (n m) : (n+1) * m = n * m + m` (d√©j√† trouv√©)\n",
      "    Sch√©ma :\n",
      "    1. `induction n with`\n",
      "       - **cas z√©ro**: but `m * 0 = 0 * m`, r√©soudre par `simp [Nat.mul_zero, Nat.zero_mul]`.\n",
      "       - **cas succ** `n = n' + 1` avec IH : `m * n' = n' * m`.\n",
      "         Transformer le but via :\n",
      "         - c√¥t√© gauche : `simp [Nat.mul_succ, ih]` donne `n' * m + m`\n",
      "         - c√¥t√© droit : `simp [Nat.succ_mul]` donne aussi `n' * m + m`\n",
      "         puis `rfl`/`simp`.\n",
      "    Je d√©l√®gue √† **TacticAgent** pour produire le script Lean exact (tactiques `induction`, `simp`, r√©√©criture).\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+133.54s] TOUR 6 | Agent: TacticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (49 chars, 15.55s):\n",
      "    Je propose la tactique suivante (√©tape 2): `simp`\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+149.09s] TOUR 7 | Agent: SearchAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (600 chars, 21.28s):\n",
      "    Je d√©l√®gue √† TacticAgent avec les lemmes: Nat.mul_succ, Nat.succ_mul, Nat.mul_zero, Nat.zero_mul, et usage de `induction n <;> simp [...]`.\n",
      "    Lemmes Mathlib utiles pour cette preuve (par r√©currence) :\n",
      "    - `Nat.mul_zero (n : Nat) : n * 0 = 0`\n",
      "    - `Nat.zero_mul (n : Nat) : 0 * n = 0`\n",
      "    - `Nat.mul_succ (n m : Nat) : n * m.succ = n * m + n`\n",
      "    - `Nat.succ_mul (n m : Nat) : n.succ * m = n * m + m`\n",
      "    Ils suffisent pour faire une preuve par `induction n` puis `simp` avec ces r√®gles.\n",
      "    Je d√©l√®gue √† **TacticAgent** pour produire le script Lean complet (du style `induction n with | zero => ... | succ n ih => ...`).\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+170.37s] TOUR 8 | Agent: TacticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (325 chars, 12.54s):\n",
      "    La tentative `trivial` ne peut pas marcher ici (et on a en plus eu un souci de **parse** dans le v√©rifieur d√ª au formatage de la preuve). On reprend proprement.\n",
      "    Pour avancer, il faut passer √† l‚Äô√©tape suivante de la strat√©gie : **`simp` sans arguments**.\n",
      "    Tactique suivante √† essayer (√©tape 2 obligatoire) :\n",
      "    ```lean\n",
      "    simp\n",
      "    ```\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+182.91s] TOUR 9 | Agent: CriticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (603 chars, 11.35s):\n",
      "    On n‚Äôa encore **aucune √©tape de preuve valid√©e** (phase `init`, but inchang√© `m * n = n * m`). Donc `simp` tout seul ne fera rien d‚Äôutile tant qu‚Äôon n‚Äôa pas lanc√© la r√©currence.\n",
      "    Erreur de strat√©gie d√©tect√©e : proposer `simp` comme ‚Äú√©tape 2‚Äù sans avoir fait `induction` ne progresse pas sur ce but.\n",
      "    Direction correcte : **commencer par** une r√©currence, typiquement sur `n`, puis utiliser `simp` avec les lemmes `Nat.mul_zero`, `Nat.zero_mul`, `Nat.mul_succ`, `Nat.succ_mul`.\n",
      "    Je d√©l√®gue √† **TacticAgent** pour fournir le script Lean (par ex. `by induction n <;> simp [Nat.mul_succ, Nat.succ_mul, *]`).\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+194.26s] TOUR 10 | Agent: TacticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (15 chars, 7.24s):\n",
      "    ```lean\n",
      "    rfl\n",
      "    ```\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+201.50s] TOUR 11 | Agent: VerifierAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (446 chars, 15.29s):\n",
      "    R√©sultat de v√©rification (Lean) pour la tactique propos√©e `rfl` :\n",
      "    - **But** : `m * n = n * m`\n",
      "    - **Preuve test√©e** :\n",
      "      ```lean\n",
      "      by\n",
      "        rfl\n",
      "      ```\n",
      "    - **Verdict** : **√©chec**\n",
      "    - **Erreur Lean** : `The rfl tactic failed` (les deux c√¥t√©s ne sont pas d√©finitionnellement √©gaux).\n",
      "    Je d√©l√®gue √† **CriticAgent** pour proposer une tactique/proof qui progresse (typiquement `induction n` puis `simp [Nat.mul_succ, Nat.succ_mul, Nat.mul_zero, Nat.zero_mul]`).\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+216.79s] TOUR 12 | Agent: CriticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (900 chars, 14.65s):\n",
      "    Pattern des 3 derniers √©checs : `rfl` (et variantes ‚Äúsimp seul‚Äù) **n‚Äôavance pas** sur un but non-d√©finitionnel comme la commutativit√© de `(*)` sur `Nat`. C‚Äôest un cas typique de **tactic_failed** ‚Üí il faut changer de strat√©gie : **r√©currence + r√©√©criture/simp**.\n",
      "    Diagnostic Lean :\n",
      "    - `rfl` √©choue car `m * n` et `n * m` ne se r√©duisent pas au m√™me terme.\n",
      "    - `simp` seul avant `induction` ne d√©clenche aucune r√®gle structurante.\n",
      "    Direction correcte :\n",
      "    - `induction n with\n",
      "      | zero => simp [Nat.mul_zero, Nat.zero_mul]\n",
      "      | succ n ih => ...`\n",
      "    - Dans le cas succ, utiliser `Nat.mul_succ` et `Nat.succ_mul`, puis `simp [Nat.mul_succ, Nat.succ_mul, ih, Nat.add_comm, Nat.add_left_comm, Nat.add_assoc]` (ou `simp [Nat.mul_succ, Nat.succ_mul, ih, Nat.add_assoc, Nat.add_comm, Nat.add_left_comm]`) pour faire co√Øncider les deux c√¥t√©s.\n",
      "    Je d√©l√®gue √† **TacticAgent** pour fournir le script Lean complet et robuste.\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+231.44s] TOUR 13 | Agent: TacticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (16 chars, 8.61s):\n",
      "    ```lean\n",
      "    simp\n",
      "    ```\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+240.06s] TOUR 14 | Agent: VerifierAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (400 chars, 18.58s):\n",
      "    √âchec v√©rifi√© pour la tactique propos√©e `simp`.\n",
      "    - **Preuve test√©e** :\n",
      "      ```lean\n",
      "      theorem mul_comm_manual (m n : Nat) : m * n = n * m := by\n",
      "        simp\n",
      "      ```\n",
      "    - **Erreur Lean** : `simp made no progress`\n",
      "    - **But restant** : `m * n = n * m`\n",
      "    J‚Äôenregistre l‚Äô√©chec et je d√©l√®gue √† **CriticAgent** : il faut d√©marrer une r√©currence (p. ex. `induction n`) puis `simp` avec `Nat.mul_succ`, `Nat.succ_mul`, etc.\n",
      "\n",
      "[+258.64s] PREUVE TROUVEE!\n",
      "  Tactique finale: rfl\n",
      "\n",
      "======================================================================\n",
      "SESSION TERMINEE (duree totale: 258.64s)\n",
      "  Iterations: 14\n",
      "  Lemmes decouverts: 12\n",
      "  Tactiques essayees: 10\n",
      "======================================================================\n",
      "\n",
      "Resultat DEMO_3:\n",
      "  - Success: True\n",
      "  - Iterations: 14 (attendu: 8)\n",
      "  - Proof: rfl\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEMO_3 : MUL_COMM\n",
    "# =============================================================================\n",
    "\n",
    "# Definition inline pour iteration independante\n",
    "demo_3 = {\n",
    "    \"name\": \"DEMO_3_MUL_COMM\",\n",
    "    \"theorem\": \"theorem mul_comm_manual (m n : Nat) : m * n = n * m\",\n",
    "    \"expected_iterations\": 8,\n",
    "    \"expected_lemmas\": 2,\n",
    "    \"complexity\": \"Intermediaire - necessite lemme de commutativite\",\n",
    "    \"strategy\": \"exact Nat.mul_comm m n\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"DEMO 3/4: {demo_3['name']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Theoreme: {demo_3['theorem']}\")\n",
    "print(f\"Complexite: {demo_3['complexity']}\")\n",
    "print(f\"Iterations attendues: {demo_3['expected_iterations']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result_3 = prove_with_multi_agents(\n",
    "    theorem=demo_3[\"theorem\"],\n",
    "    max_iterations=20,\n",
    "    verbose=True,\n",
    "    use_simulation=not USE_LLM_MODE\n",
    ")\n",
    "\n",
    "print(f\"\\nResultat DEMO_3:\")\n",
    "print(f\"  - Success: {result_3['success']}\")\n",
    "print(f\"  - Iterations: {result_3['iterations']} (attendu: {demo_3['expected_iterations']})\")\n",
    "print(f\"  - Proof: {result_3['final_proof']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. DEMO_4 : Addition des Puissances\n",
    "\n",
    "**Objectif** : Stresser le systeme avec induction et lemmes multiples\n",
    "\n",
    "**Theoreme** : `theorem pow_add_manual (a m n : Nat) : a ^ (m + n) = a ^ m * a ^ n`\n",
    "\n",
    "**Comportement attendu** : 12-18 iterations : induction sur n, decouverte de pow_succ, mul_assoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEMO 4/4: DEMO_4_POWER_ADD\n",
      "======================================================================\n",
      "Theoreme: theorem pow_add_manual (a m n : Nat) : a ^ (m + n) = a ^ m * a ^ n\n",
      "Complexite: Avancee - induction et lemmes auxiliaires multiples\n",
      "Iterations attendues: 15\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Configuration LLM Service: OpenAI\n",
      "============================================================\n",
      "[LLM Provider] OpenAI - Model: gpt-5.2\n",
      "\n",
      "Crees 5 agents SK avec provider OpenAI et modele gpt-5.2\n",
      "Mode: Semantic Kernel\n",
      "======================================================================\n",
      "SESSION MULTI-AGENTS (SK)\n",
      "Theoreme: Prouver: theorem pow_add_manual (a m n : Nat) : a ^ (m + n) = a ^ m * a ^ n...\n",
      "======================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+0.00s] TOUR 2 | Agent: SearchAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (617 chars, 20.04s):\n",
      "    Delegate to TacticAgent.\n",
      "    Lemmes Mathlib pertinents trouv√©s pour `theorem pow_add_manual (a m n : Nat) : a ^ (m + n) = a ^ m * a ^ n` :\n",
      "    1) **Lemme direct (exactement le but)**  \n",
      "    - `Nat.pow_add (a m n : Nat) : a ^ (m + n) = a ^ m * a ^ n`  \n",
      "    ‚Üí La preuve peut √™tre `exact Nat.pow_add a m n`.\n",
      "    2) **Pour une preuve par r√©currence (si ‚Äúmanual‚Äù = sans utiliser `Nat.pow_add`)**  \n",
      "    - `Nat.pow_succ (n m : Nat) : n ^ m.succ = n ^ m * n`  \n",
      "    - `Nat.pow_zero (n : Nat) : n ^ 0 = 1`  \n",
      "    (√©ventuellement avec `Nat.add_zero`, `Nat.add_succ`, `Nat.mul_assoc` pour r√©arranger)\n",
      "    Je d√©l√®gue √† **TacticAgent** pour assembler la preuve Lean.\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+20.04s] TOUR 3 | Agent: TacticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (52 chars, 16.90s):\n",
      "    Prochaine tactique Lean √† essayer :\n",
      "    ```lean\n",
      "    simp\n",
      "    ```\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+36.94s] TOUR 4 | Agent: VerifierAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (223 chars, 10.84s):\n",
      "    √âchec de la tactique propos√©e (`simp`) :\n",
      "    - Erreur Lean : `simp made no progress`\n",
      "    - But restant : `a ^ (m + n) = a ^ m * a ^ n`\n",
      "    Je d√©l√®gue √† **CriticAgent** pour analyser l‚Äô√©chec et proposer une tactique/proof alternative.\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+47.79s] TOUR 5 | Agent: CriticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (821 chars, 10.37s):\n",
      "    `simp` ne peut rien faire ici car il n‚Äôa pas, par d√©faut, de r√®gle de r√©√©criture pour d√©velopper `a ^ (m + n)` en produit.\n",
      "    Deux directions possibles :\n",
      "    1) **Preuve directe (recommand√©e) avec le lemme standard**  \n",
      "    Utiliser `Nat.pow_add` (c‚Äôest exactement le but).  \n",
      "    ‚Üí D√©l√©guer √† **TacticAgent** : essayer `simpa using (Nat.pow_add a m n)` ou `exact Nat.pow_add a m n`.\n",
      "    2) **Si ‚Äúmanual‚Äù = sans `Nat.pow_add`**  \n",
      "    Faire une r√©currence sur `n` et utiliser `Nat.pow_succ`, `Nat.add_succ`, et l‚Äôassociativit√© de `(*)` :\n",
      "    - base `n = 0` : `m + 0`, `a^0 = 1`\n",
      "    - step `n ‚Üí n+1` : transformer `a^(m+(n+1))` via `Nat.add_succ` puis `Nat.pow_succ`, r√©√©crire avec l‚Äôhypoth√®se d‚Äôinduction, puis `mul_assoc`.\n",
      "    J‚Äôai d√©sign√© **TacticAgent** pour produire le script Lean correspondant (en commen√ßant par la solution 1, sinon la r√©currence).\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+58.16s] TOUR 6 | Agent: TacticAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (101 chars, 9.19s):\n",
      "    Prochaine tactique Lean √† essayer (3e √©tape : lemme de Mathlib) :\n",
      "    ```lean\n",
      "    exact Nat.pow_add a m n\n",
      "    ```\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[+67.35s] TOUR 7 | Agent: VerifierAgent\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Reponse (141 chars, 14.45s):\n",
      "    La preuve compile avec Lean :\n",
      "    ```lean\n",
      "    theorem pow_add_manual (a m n : Nat) : a ^ (m + n) = a ^ m * a ^ n := by\n",
      "      exact Nat.pow_add a m n\n",
      "    ```\n",
      "\n",
      "[+81.80s] PREUVE TROUVEE!\n",
      "  Tactique finale: theorem pow_add_manual (a m n : Nat) : a ^ (m + n) = a ^ m * a ^ n := by\n",
      "  exact Nat.pow_add a m n\n",
      "\n",
      "======================================================================\n",
      "SESSION TERMINEE (duree totale: 81.80s)\n",
      "  Iterations: 7\n",
      "  Lemmes decouverts: 8\n",
      "  Tactiques essayees: 5\n",
      "======================================================================\n",
      "\n",
      "Resultat DEMO_4:\n",
      "  - Success: True\n",
      "  - Iterations: 7 (attendu: 15)\n",
      "  - Proof: theorem pow_add_manual (a m n : Nat) : a ^ (m + n) = a ^ m * a ^ n := by\n",
      "  exact Nat.pow_add a m n\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEMO_4 : POWER_ADD\n",
    "# =============================================================================\n",
    "\n",
    "# Definition inline pour iteration independante\n",
    "demo_4 = {\n",
    "    \"name\": \"DEMO_4_POWER_ADD\",\n",
    "    \"theorem\": \"theorem pow_add_manual (a m n : Nat) : a ^ (m + n) = a ^ m * a ^ n\",\n",
    "    \"expected_iterations\": 15,\n",
    "    \"expected_lemmas\": 4,\n",
    "    \"complexity\": \"Avancee - induction et lemmes auxiliaires multiples\",\n",
    "    \"strategy\": \"induction n avec Nat.pow_succ, Nat.mul_assoc\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"DEMO 4/4: {demo_4['name']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Theoreme: {demo_4['theorem']}\")\n",
    "print(f\"Complexite: {demo_4['complexity']}\")\n",
    "print(f\"Iterations attendues: {demo_4['expected_iterations']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result_4 = prove_with_multi_agents(\n",
    "    theorem=demo_4[\"theorem\"],\n",
    "    max_iterations=20,\n",
    "    verbose=True,\n",
    "    use_simulation=not USE_LLM_MODE\n",
    ")\n",
    "\n",
    "print(f\"\\nResultat DEMO_4:\")\n",
    "print(f\"  - Success: {result_4['success']}\")\n",
    "print(f\"  - Iterations: {result_4['iterations']} (attendu: {demo_4['expected_iterations']})\")\n",
    "print(f\"  - Proof: {result_4['final_proof']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Analyse Comparative des Resultats\n",
    "\n",
    "**Objectif** : Comparer les resultats observes avec les attentes.\n",
    "\n",
    "#### Pourquoi les resultats sont-ils plus rapides que prevu ?\n",
    "\n",
    "Les demos se terminent en 3-4 iterations au lieu de 10-20 car :\n",
    "\n",
    "1. **Mathlib contient les lemmes exacts** : `Nat.add_right_cancel`, `Nat.mul_add`, `List.length_append`\n",
    "2. **SearchAgent trouve immediatement** le bon lemme (pas de recherche exploratoire)\n",
    "3. **TacticAgent applique directement** `simpa using <lemme>` sans essayer d'autres approches\n",
    "4. **CriticAgent/CoordinatorAgent jamais actives** car aucun echec a corriger\n",
    "\n",
    "#### Implications pedagogiques\n",
    "\n",
    "| Aspect | Simulation actuelle | Systeme reel (LLM) |\n",
    "|--------|--------------------|--------------------|\n",
    "| **Recherche** | Base indexee, O(1) | Embedding similarity, exploration |\n",
    "| **Tactiques** | Pattern matching | Generation creative, essais multiples |\n",
    "| **Verification** | Heuristique simple | Lean 4 reel, erreurs detaillees |\n",
    "| **Iterations** | 3-4 (deterministe) | 10-20 (stochastique) |\n",
    "\n",
    "#### Pour observer la vraie complexite\n",
    "\n",
    "```python\n",
    "# Option 1: Mode LLM (necessite API key)\n",
    "USE_LLM_MODE = True  # Active les vraies generations\n",
    "\n",
    "# Option 2: Theoremes sans lemme direct\n",
    "theorem_custom = \"theorem custom (n m k : Nat) : (n + m) * k = n * k + m * k\"\n",
    "# Mathlib a Nat.add_mul mais pas dans notre base de simulation\n",
    "\n",
    "# Option 3: Desactiver lemmes specifiques\n",
    "# Modifier SIMULATION_LEMMAS pour exclure List.length_append\n",
    "```\n",
    "\n",
    "**Conclusion** : La simulation demontre l'*architecture* multi-agents, pas la *difficulte* reelle du theorem proving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARAISON DES RESULTATS\n",
      "======================================================================\n",
      "Demo                      Success    Iter         Lemmas     Status         \n",
      "------------------------------------------------------------------------\n",
      "DEMO_1_REFLEXIVITY        OK         4/2          0/0        Slow           \n",
      "DEMO_2_DISTRIBUTIVITY     OK         5/5          0/1        Optimal        \n",
      "DEMO_3_MUL_COMM           OK         14/8         0/2        Slow           \n",
      "DEMO_4_POWER_ADD          OK         7/15         0/4        Optimal        \n",
      "------------------------------------------------------------------------\n",
      "Total: 4/4 reussies\n",
      "\n",
      "======================================================================\n",
      "PROGRESSION DE COMPLEXITE\n",
      "======================================================================\n",
      "DEMO_1: Reflexivite     -> Pipeline validation (rfl)\n",
      "DEMO_2: Distributivite  -> Recherche + reecriture inversee\n",
      "DEMO_3: Commutativite   -> Propriete fondamentale (exact)\n",
      "DEMO_4: Puissances      -> Induction + lemmes multiples\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Comparaison des Resultats\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARAISON DES RESULTATS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Definitions inline pour iteration independante\n",
    "demos_info = [\n",
    "    {\"name\": \"DEMO_1_REFLEXIVITY\", \"theorem\": \"n = n\", \"expected_iter\": 2, \"expected_lemmas\": 0},\n",
    "    {\"name\": \"DEMO_2_DISTRIBUTIVITY\", \"theorem\": \"a*c + b*c = (a+b)*c\", \"expected_iter\": 5, \"expected_lemmas\": 1},\n",
    "    {\"name\": \"DEMO_3_MUL_COMM\", \"theorem\": \"m * n = n * m\", \"expected_iter\": 8, \"expected_lemmas\": 2},\n",
    "    {\"name\": \"DEMO_4_POWER_ADD\", \"theorem\": \"a^(m+n) = a^m * a^n\", \"expected_iter\": 15, \"expected_lemmas\": 4},\n",
    "]\n",
    "\n",
    "results = [result_1, result_2, result_3, result_4]\n",
    "\n",
    "print(f\"{'Demo':<25} {'Success':<10} {'Iter':<12} {'Lemmas':<10} {'Status':<15}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for i, (demo, result) in enumerate(zip(demos_info, results), 1):\n",
    "    success_str = \"OK\" if result[\"success\"] else \"FAILED\"\n",
    "    iter_str = f\"{result['iterations']}/{demo['expected_iter']}\"\n",
    "    lemmas_str = f\"{result.get('lemmas_found', 0)}/{demo['expected_lemmas']}\"\n",
    "\n",
    "    if result[\"success\"]:\n",
    "        if result[\"iterations\"] <= demo[\"expected_iter\"]:\n",
    "            status = \"Optimal\"\n",
    "        else:\n",
    "            status = \"Slow\"\n",
    "    else:\n",
    "        status = \"Failed\"\n",
    "\n",
    "    print(f\"{demo['name']:<25} {success_str:<10} {iter_str:<12} {lemmas_str:<10} {status:<15}\")\n",
    "\n",
    "print(\"-\" * 72)\n",
    "total_success = sum(1 for r in results if r[\"success\"])\n",
    "print(f\"Total: {total_success}/4 reussies\")\n",
    "\n",
    "# Resume de la progression\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PROGRESSION DE COMPLEXITE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMO_1: Reflexivite     -> Pipeline validation (rfl)\")\n",
    "print(\"DEMO_2: Distributivite  -> Recherche + reecriture inversee\")\n",
    "print(\"DEMO_3: Commutativite   -> Propriete fondamentale (exact)\")\n",
    "print(\"DEMO_4: Puissances      -> Induction + lemmes multiples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion et Points Cles\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "#### 1. Architecture Multi-Agents pour Theorem Proving\n",
    "\n",
    "| Composant | Role | Implementation SK |\n",
    "|-----------|------|-------------------|\n",
    "| **ProofState** | Etat partage synchronise | `@dataclass` + plugins |\n",
    "| **Plugins** | Fonctionnalites specialisees | `@kernel_function` |\n",
    "| **Agents** | Roles specialises (Search, Tactic, Verify...) | `ChatCompletionAgent` |\n",
    "| **Orchestration** | Delegation dynamique | `AgentGroupChat` + strategies |\n",
    "\n",
    "#### 2. Semantic Kernel vs Implementation Ad-Hoc (Lean-8)\n",
    "\n",
    "| Aspect | Lean-8 (Ad-Hoc) | Lean-9 (Semantic Kernel) |\n",
    "|--------|-----------------|--------------------------|\n",
    "| **Etat** | Variables globales | `ProofState` classe |\n",
    "| **Agents** | Fonctions Python | `ChatCompletionAgent` |\n",
    "| **Communication** | Appels directs | Message passing |\n",
    "| **Extensibilite** | Modifier le code | Ajouter plugins |\n",
    "| **LLM** | OpenAI direct | Abstraction SK |\n",
    "\n",
    "#### 3. Patterns Replicables\n",
    "\n",
    "1. **StateManager Pattern** : Un objet central pour l'etat partage\n",
    "2. **Plugin Pattern** : Fonctions decorees pour l'injection de dependances\n",
    "3. **Delegation Pattern** : Chaque agent designe le suivant\n",
    "4. **Termination Pattern** : Criteres multiples (succes, timeout, max_iter)\n",
    "\n",
    "### Limitations et Perspectives\n",
    "\n",
    "#### Limitations actuelles\n",
    "\n",
    "- **Simulation trop parfaite** : Trouve les lemmes directs immediatement\n",
    "- **Pas de vrai Lean** : Verification heuristique, pas de lean4 reel\n",
    "- **Base de lemmes limitee** : ~50 lemmes vs 100k+ dans Mathlib\n",
    "- **Pas de backtracking** : Premiere tactique qui marche = solution\n",
    "\n",
    "#### Prochaines etapes (Lean-10 LeanDojo)\n",
    "\n",
    "1. **Integration LeanDojo** : Interaction programmatique avec Lean 4\n",
    "2. **Tracing Mathlib** : Extraction des 100k+ lemmes\n",
    "3. **Verification reelle** : Feedback Lean vs heuristique\n",
    "4. **Benchmarks** : MiniF2F, ProofNet, LeanBench\n",
    "\n",
    "### Resume Final\n",
    "\n",
    "Ce notebook a demontre comment construire un systeme multi-agents pour le theorem proving avec Semantic Kernel. Les patterns (StateManager, Plugin, Delegation) sont replicables pour d'autres domaines.\n",
    "\n",
    "**Key Takeaways** :\n",
    "- L'architecture compte plus que les resultats de simulation\n",
    "- Semantic Kernel simplifie l'orchestration multi-agents\n",
    "- Les vrais defis apparaissent avec des theoremes sans lemmes directs\n",
    "- LeanDojo (Notebook 10) permettra la verification reelle\n",
    "\n",
    "---\n",
    "\n",
    "### Navigation\n",
    "\n",
    "| Precedent | Index | Suivant |\n",
    "|-----------|-------|---------|\n",
    "| [Lean-8 (Agents Ad-Hoc)](Lean-8-Agentic-Proving.ipynb) | [Lean-1 (Setup)](Lean-1-Setup.ipynb) | [Lean-10 (LeanDojo)](Lean-10-LeanDojo.ipynb) |\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook complete. Duree estimee: 45-55 minutes.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (WSL)",
   "language": "python",
   "name": "python3-wsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
