{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Analyser un Appel d'Offre avec l'IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas d'usage\n",
    "\n",
    "Dans le monde de l'avant-vente, la réactivité et la pertinence sont des facteurs clés de succès. Répondre à un appel d'offre (RFP - Request for Proposal) est un processus chronophage qui demande de bien cerner les besoins du client pour proposer une solution adaptée. \n",
    "\n",
    "Ce laboratoire démontre comment une IA agentique simple peut accélérer drastiquement ce processus. Nous allons utiliser **LangChain**, un framework puissant qui agit comme un \"chef d'orchestre\" pour les grands modèles de langage (LLMs), afin de :\n",
    "\n",
    "1.  Lire et comprendre un appel d'offre.\n",
    "2.  En extraire les informations stratégiques.\n",
    "3.  Générer une première ébauche de proposition technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 1 : Charger le document d'appel d'offre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./appel_offre.txt')\n",
    "document = loader.load()\n",
    "\n",
    "print(document[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2 : Créer une chaîne d'extraction d'informations\n",
    "\n",
    "Nous allons définir un `PromptTemplate` qui guidera le LLM pour qu'il identifie précisément les points qui nous intéressent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI # Assurez-vous d'avoir configuré votre clé API OpenAI\n",
    "\n",
    "# Initialiser le modèle\n",
    "# Remplacez par votre modèle de prédilection si nécessaire (ex: via Ollama, Mistral, etc.)\n",
    "# Pensez à configurer votre clé API, par exemple avec : \n",
    "# import os\n",
    "# os.environ['OPENAI_API_KEY'] = 'VOTRE_CLE_ICI'\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Créer le template de prompt pour l'extraction\n",
    "extraction_template = \"\"\"\n",
    "Lis attentivement le texte de l'appel d'offre suivant et extrais les informations clés dans un format JSON valide.\n",
    "\n",
    "Texte de l'appel d'offre:\n",
    "--- \n",
    "{document_text}\n",
    "--- \n",
    "\n",
    "Extrais les informations suivantes:\n",
    "1. 'objectif_metier': L'objectif principal que le client cherche à atteindre.\n",
    "2. 'exigences_techniques': Les contraintes ou technologies spécifiques demandées.\n",
    "3. 'date_limite': La date ou période de livraison attendue.\n",
    "\n",
    "Ne retourne que le JSON, sans aucun autre commentaire ou texte d'introduction.\n",
    "\"\"\"\n",
    "\n",
    "extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"document_text\"],\n",
    "    template=extraction_template\n",
    ")\n",
    "\n",
    "extraction_chain = LLMChain(llm=llm, prompt=extraction_prompt)\n",
    "\n",
    "print(\"Chaîne d'extraction créée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3 : Exécuter la chaîne et extraire les points clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "raw_result = extraction_chain.run(document_text=document[0].page_content)\n",
    "extracted_data = json.loads(raw_result)\n",
    "\n",
    "print(\"Informations extraites de l'appel d'offre :\\n\")\n",
    "print(json.dumps(extracted_data, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 4 : Créer une chaîne de génération de proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le template de prompt pour la génération\n",
    "generation_template = \"\"\"\n",
    "Agis en tant qu'architecte de solutions IA. En te basant sur les informations extraites de l'appel d'offre, rédige une ébauche de proposition technique en 3 points clairs et concis.\n",
    "\n",
    "Informations extraites:\n",
    "--- \n",
    "Objectif métier du client: {objectif_metier}\n",
    "Exigences techniques: {exigences_techniques}\n",
    "Date limite: {date_limite}\n",
    "--- \n",
    "\n",
    "Structure ta proposition comme suit:\n",
    "1. **Approche proposée:** Décris brièvement la solution envisagée pour atteindre l'objectif métier.\n",
    "2. **Technologies clés:** Liste les technologies qui seront utilisées, en accord avec les exigences.\n",
    "3. **Livrables:** Précise ce qui sera concrètement livré au client à la date limite.\n",
    "\"\"\"\n",
    "\n",
    "generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"objectif_metier\", \"exigences_techniques\", \"date_limite\"],\n",
    "    template=generation_template\n",
    ")\n",
    "\n",
    "generation_chain = LLMChain(llm=llm, prompt=generation_prompt)\n",
    "\n",
    "print(\"Chaîne de génération créée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécuter la chaîne de génération\n",
    "proposition = generation_chain.run(extracted_data)\n",
    "\n",
    "print(\"--- ÉBAUCHE DE PROPOSITION TECHNIQUE ---\\n\")\n",
    "print(proposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "En quelques minutes, nous avons automatisé une partie significative du travail d'analyse d'un appel d'offre et de rédaction d'une proposition. Les gains sont multiples :\n",
    "\n",
    "*   **Gain de temps :** L'analyse et la première ébauche sont quasi-instantanées.\n",
    "*   **Standardisation :** La structure des réponses est homogène, ce qui facilite la relecture et la validation.\n",
    "*   **Fiabilité :** L'IA est moins susceptible d'oublier une information clé lors de la lecture du document.\n",
    "\n",
    "Dans un vrai projet, cet agent pourrait être enrichi avec des **outils** (`tools`) lui donnant accès à :\n",
    "\n",
    "*   Une recherche web pour se renseigner sur l'entreprise cliente.\n",
    "*   Une base de connaissances interne (via RAG) pour réutiliser des briques de projets précédents.\n",
    "*   Un outil de pricing pour estimer le coût du projet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}