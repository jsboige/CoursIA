{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2 - Analyser un Appel d'Offre avec l'IA\n\n## Objectifs d'apprentissage\n\nA la fin de ce laboratoire, vous saurez :\n1. Utiliser LangChain pour orchestrer des taches d'analyse de documents\n2. Extraire automatiquement des informations structurees d'un texte\n3. Generer une proposition technique basee sur des donnees extraites\n4. Enchaîner plusieurs operations LLM dans un workflow coherent\n\n### Prerequis\n- Python 3.10+\n- Cle API OpenAI configuree (variable d'environnement ou fichier .env)\n- Connaissance de base de LangChain\n\n### Duree estimee : 45-60 minutes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas d'usage\n",
    "\n",
    "Dans le monde de l'avant-vente, la réactivité et la pertinence sont des facteurs clés de succès. Répondre à un appel d'offre (RFP - Request for Proposal) est un processus chronophage qui demande de bien cerner les besoins du client pour proposer une solution adaptée. \n",
    "\n",
    "Ce laboratoire démontre comment une IA agentique simple peut accélérer drastiquement ce processus. Nous allons utiliser **LangChain**, un framework puissant qui agit comme un \"chef d'orchestre\" pour les grands modèles de langage (LLMs), afin de :\n",
    "\n",
    "1.  Lire et comprendre un appel d'offre.\n",
    "2.  En extraire les informations stratégiques.\n",
    "3.  Générer une première ébauche de proposition technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 1 : Charger le document d'appel d'offre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_community.document_loaders import TextLoader\n\nloader = TextLoader('./appel_offre.txt')\ndocument = loader.load()\n\nprint(document[0].page_content)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2 : Créer une chaîne d'extraction d'informations\n",
    "\n",
    "Nous allons définir un `PromptTemplate` qui guidera le LLM pour qu'il identifie précisément les points qui nous intéressent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Initialiser le modèle\n# Remplacez par votre modèle de prédilection si nécessaire (ex: via Ollama, Mistral, etc.)\n# Pensez à configurer votre clé API, par exemple avec : \n# import os\n# os.environ['OPENAI_API_KEY'] = 'VOTRE_CLE_ICI'\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n\n# Créer le template de prompt pour l'extraction avec ChatPromptTemplate (API moderne)\nextraction_template = \"\"\"\nLis attentivement le texte de l'appel d'offre suivant et extrais les informations clés dans un format JSON valide.\n\nTexte de l'appel d'offre:\n--- \n{document_text}\n--- \n\nExtrais les informations suivantes:\n1. 'objectif_metier': L'objectif principal que le client cherche à atteindre.\n2. 'exigences_techniques': Les contraintes ou technologies spécifiques demandées.\n3. 'date_limite': La date ou période de livraison attendue.\n\nNe retourne que le JSON, sans aucun autre commentaire ou texte d'introduction.\n\"\"\"\n\nextraction_prompt = ChatPromptTemplate.from_template(extraction_template)\n\n# Utiliser LCEL (LangChain Expression Language) au lieu de LLMChain\nextraction_chain = extraction_prompt | llm\n\nprint(\"Chaîne d'extraction créée.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3 : Exécuter la chaîne et extraire les points clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n# Utiliser invoke() au lieu de run() et extraire le contenu\nresponse = extraction_chain.invoke({\"document_text\": document[0].page_content})\nraw_result = response.content\nextracted_data = json.loads(raw_result)\n\nprint(\"Informations extraites de l'appel d'offre :\\n\")\nprint(json.dumps(extracted_data, indent=2, ensure_ascii=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 4 : Créer une chaîne de génération de proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Créer le template de prompt pour la génération avec ChatPromptTemplate (API moderne)\ngeneration_template = \"\"\"\nAgis en tant qu'architecte de solutions IA. En te basant sur les informations extraites de l'appel d'offre, rédige une ébauche de proposition technique en 3 points clairs et concis.\n\nInformations extraites:\n--- \nObjectif métier du client: {objectif_metier}\nExigences techniques: {exigences_techniques}\nDate limite: {date_limite}\n--- \n\nStructure ta proposition comme suit:\n1. **Approche proposée:** Décris brièvement la solution envisagée pour atteindre l'objectif métier.\n2. **Technologies clés:** Liste les technologies qui seront utilisées, en accord avec les exigences.\n3. **Livrables:** Précise ce qui sera concrètement livré au client à la date limite.\n\"\"\"\n\ngeneration_prompt = ChatPromptTemplate.from_template(generation_template)\n\n# Utiliser LCEL (LangChain Expression Language) au lieu de LLMChain\ngeneration_chain = generation_prompt | llm\n\nprint(\"Chaîne de génération créée.\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Etape 5 : Execution de la generation\n\nMaintenant que notre chaine de generation est configuree, nous allons l'executer avec les donnees extraites precedemment. Le LLM va synthetiser une proposition structuree en trois volets (approche, technologies, livrables) adaptee aux besoins specifiques du client.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exécuter la chaîne de génération avec invoke() et extraire le contenu\nresponse = generation_chain.invoke(extracted_data)\nproposition = response.content\n\nprint(\"--- ÉBAUCHE DE PROPOSITION TECHNIQUE ---\\n\")\nprint(proposition)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "En quelques minutes, nous avons automatisé une partie significative du travail d'analyse d'un appel d'offre et de rédaction d'une proposition. Les gains sont multiples :\n",
    "\n",
    "*   **Gain de temps :** L'analyse et la première ébauche sont quasi-instantanées.\n",
    "*   **Standardisation :** La structure des réponses est homogène, ce qui facilite la relecture et la validation.\n",
    "*   **Fiabilité :** L'IA est moins susceptible d'oublier une information clé lors de la lecture du document.\n",
    "\n",
    "Dans un vrai projet, cet agent pourrait être enrichi avec des **outils** (`tools`) lui donnant accès à :\n",
    "\n",
    "*   Une recherche web pour se renseigner sur l'entreprise cliente.\n",
    "*   Une base de connaissances interne (via RAG) pour réutiliser des briques de projets précédents.\n",
    "*   Un outil de pricing pour estimer le coût du projet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}