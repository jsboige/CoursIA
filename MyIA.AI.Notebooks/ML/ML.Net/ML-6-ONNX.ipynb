{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47b76f1",
   "metadata": {
    "papermill": {
     "duration": 0.003962,
     "end_time": "2026-02-25T01:08:42.070532",
     "exception": false,
     "start_time": "2026-02-25T01:08:42.066570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ML-6 : ONNX Model Integration avec ML.NET\n",
    "\n",
    "**Navigation** : [Index](README.md) | [<< ML-5-TimeSeries](ML-5-TimeSeries.ipynb) | [Suivant >>](ML-7-Recommendation.ipynb)\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "A la fin de ce notebook, vous saurez :\n",
    "1. Comprendre ce qu'est **ONNX** (Open Neural Network Exchange)\n",
    "2. Charger des modèles ONNX externes dans ML.NET\n",
    "3. Exporter des modèles ML.NET vers ONNX\n",
    "4. Utiliser des modèles Python (Scikit-learn, PyTorch) dans des applications .NET\n",
    "5. Optimiser les performances avec ONNX Runtime\n",
    "\n",
    "### Prérequis\n",
    "- ML-1 à ML-5 complétés\n",
    "- Connaissance de base de Python et Scikit-learn\n",
    "\n",
    "### Durée estimée : 45-60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "# Introduction à ONNX\n",
    "\n",
    "## Qu'est-ce qu'ONNX ?\n",
    "\n",
    "**ONNX (Open Neural Network Exchange)** est un format ouvert pour représenter des modèles de machine learning.\n",
    "\n",
    "**Avantages** :\n",
    "- **Interopérabilité** : Échanger des modèles entre frameworks (PyTorch ↔ TensorFlow ↔ Scikit-learn ↔ ML.NET)\n",
    "- **Performance** : ONNX Runtime optimise l'inférence\n",
    "- **Portabilité** : Déployer sur différentes plateformes (Cloud, Edge, Mobile)\n",
    "\n",
    "**Écosystème ONNX** :\n",
    "```\n",
    "Python (PyTorch/Scikit-learn) → ONNX → ML.NET → Application .NET\n",
    "                        ↓\n",
    "                    ONNX Runtime (optimisation)\n",
    "```\n",
    "\n",
    "## Scénarios d'utilisation\n",
    "\n",
    "| Scénario | Avantage |\n",
    "|----------|----------|\n",
    "| Data science en Python, production en .NET | Utiliser les outils Python pour R&D, déployer en .NET |\n",
    "| Modèles pré-entraînés Hugging Face | Accès à des milliers de modèles SOTA |\n",
    "| Multi-framework | Combiner des modèles de différentes sources |\n",
    "| Edge computing | ONNX Runtime optimisé pour CPU/GPU |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34280e27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T01:08:42.086477Z",
     "iopub.status.busy": "2026-02-25T01:08:42.080826Z",
     "iopub.status.idle": "2026-02-25T01:08:43.562773Z",
     "shell.execute_reply": "2026-02-25T01:08:43.560773Z"
    },
    "papermill": {
     "duration": 1.48887,
     "end_time": "2026-02-25T01:08:43.563773",
     "exception": false,
     "start_time": "2026-02-25T01:08:42.074903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\r\n",
       "<div>\r\n",
       "    <div id='dotnet-interactive-this-cell-$CACHE_BUSTER$' style='display: none'>\r\n",
       "        The below script needs to be able to find the current output cell; this is an easy method to get it.\r\n",
       "    </div>\r\n",
       "    <script type='text/javascript'>\r\n",
       "async function probeAddresses(probingAddresses) {\r\n",
       "    function timeout(ms, promise) {\r\n",
       "        return new Promise(function (resolve, reject) {\r\n",
       "            setTimeout(function () {\r\n",
       "                reject(new Error('timeout'))\r\n",
       "            }, ms)\r\n",
       "            promise.then(resolve, reject)\r\n",
       "        })\r\n",
       "    }\r\n",
       "\r\n",
       "    if (Array.isArray(probingAddresses)) {\r\n",
       "        for (let i = 0; i < probingAddresses.length; i++) {\r\n",
       "\r\n",
       "            let rootUrl = probingAddresses[i];\r\n",
       "\r\n",
       "            if (!rootUrl.endsWith('/')) {\r\n",
       "                rootUrl = `${rootUrl}/`;\r\n",
       "            }\r\n",
       "\r\n",
       "            try {\r\n",
       "                let response = await timeout(1000, fetch(`${rootUrl}discovery`, {\r\n",
       "                    method: 'POST',\r\n",
       "                    cache: 'no-cache',\r\n",
       "                    mode: 'cors',\r\n",
       "                    timeout: 1000,\r\n",
       "                    headers: {\r\n",
       "                        'Content-Type': 'text/plain'\r\n",
       "                    },\r\n",
       "                    body: probingAddresses[i]\r\n",
       "                }));\r\n",
       "\r\n",
       "                if (response.status == 200) {\r\n",
       "                    return rootUrl;\r\n",
       "                }\r\n",
       "            }\r\n",
       "            catch (e) { }\r\n",
       "        }\r\n",
       "    }\r\n",
       "}\r\n",
       "\r\n",
       "function loadDotnetInteractiveApi() {\r\n",
       "    probeAddresses([\"http://2a01:e0a:1e8:b320:ec5c:a4c4:7ebe:5468:2056/\",\"http://fe80::902b:f9f6:2003:66a1%19:2056/\",\"http://192.168.0.51:2056/\",\"http://::1:2056/\",\"http://127.0.0.1:2056/\",\"http://fe80::ae60:2bbf:92f7:4e06%23:2056/\",\"http://172.31.128.1:2056/\",\"http://fe80::896b:b259:93e:849c%40:2056/\",\"http://172.28.64.1:2056/\"])\r\n",
       "        .then((root) => {\r\n",
       "        // use probing to find host url and api resources\r\n",
       "        // load interactive helpers and language services\r\n",
       "        let dotnetInteractiveRequire = require.config({\r\n",
       "        context: '515556.Microsoft.DotNet.Interactive.Http.HttpPort',\r\n",
       "                paths:\r\n",
       "            {\r\n",
       "                'dotnet-interactive': `${root}resources`\r\n",
       "                }\r\n",
       "        }) || require;\r\n",
       "\r\n",
       "            window.dotnetInteractiveRequire = dotnetInteractiveRequire;\r\n",
       "\r\n",
       "            window.configureRequireFromExtension = function(extensionName, extensionCacheBuster) {\r\n",
       "                let paths = {};\r\n",
       "                paths[extensionName] = `${root}extensions/${extensionName}/resources/`;\r\n",
       "                \r\n",
       "                let internalRequire = require.config({\r\n",
       "                    context: extensionCacheBuster,\r\n",
       "                    paths: paths,\r\n",
       "                    urlArgs: `cacheBuster=${extensionCacheBuster}`\r\n",
       "                    }) || require;\r\n",
       "\r\n",
       "                return internalRequire\r\n",
       "            };\r\n",
       "        \r\n",
       "            dotnetInteractiveRequire([\r\n",
       "                    'dotnet-interactive/dotnet-interactive'\r\n",
       "                ],\r\n",
       "                function (dotnet) {\r\n",
       "                    dotnet.init(window);\r\n",
       "                },\r\n",
       "                function (error) {\r\n",
       "                    console.log(error);\r\n",
       "                }\r\n",
       "            );\r\n",
       "        })\r\n",
       "        .catch(error => {console.log(error);});\r\n",
       "    }\r\n",
       "\r\n",
       "// ensure `require` is available globally\r\n",
       "if ((typeof(require) !==  typeof(Function)) || (typeof(require.config) !== typeof(Function))) {\r\n",
       "    let require_script = document.createElement('script');\r\n",
       "    require_script.setAttribute('src', 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js');\r\n",
       "    require_script.setAttribute('type', 'text/javascript');\r\n",
       "    \r\n",
       "    \r\n",
       "    require_script.onload = function() {\r\n",
       "        loadDotnetInteractiveApi();\r\n",
       "    };\r\n",
       "\r\n",
       "    document.getElementsByTagName('head')[0].appendChild(require_script);\r\n",
       "}\r\n",
       "else {\r\n",
       "    loadDotnetInteractiveApi();\r\n",
       "}\r\n",
       "\r\n",
       "    </script>\r\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.ML, 3.0.0</span></li><li><span>Microsoft.ML.OnnxRuntime, 1.16.0</span></li><li><span>Microsoft.ML.OnnxTransformer, 3.0.0</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages ONNX chargés avec succès !\r\n"
     ]
    }
   ],
   "source": [
    "#r \"nuget: Microsoft.ML, 3.0.0\"\n",
    "#r \"nuget: Microsoft.ML.OnnxTransformer, 3.0.0\"\n",
    "#r \"nuget: Microsoft.ML.OnnxRuntime, 1.16.0\"\n",
    "\n",
    "using Microsoft.ML;\n",
    "using Microsoft.ML.Data;\n",
    "using Microsoft.ML.Transforms.Onnx;\n",
    "using System;\n",
    "using System.Collections.Generic;\n",
    "using System.Linq;\n",
    "\n",
    "Console.WriteLine(\"Packages ONNX chargés avec succès !\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe729948",
   "metadata": {
    "papermill": {
     "duration": 0.003024,
     "end_time": "2026-02-25T01:08:43.570300",
     "exception": false,
     "start_time": "2026-02-25T01:08:43.567276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exemple 1 : Charger un modèle ONNX externe\n",
    "\n",
    "Dans cet exemple, nous allons charger un modèle ONNX dans ML.NET.\n",
    "\n",
    "> **Note** : Pour ce notebook, nous allons créer un modèle ONNX simple. En pratique, vous utiliseriez des modèles exportés depuis PyTorch, TensorFlow ou Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1803515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T01:08:43.576644Z",
     "iopub.status.busy": "2026-02-25T01:08:43.576644Z",
     "iopub.status.idle": "2026-02-25T01:08:43.816412Z",
     "shell.execute_reply": "2026-02-25T01:08:43.815405Z"
    },
    "papermill": {
     "duration": 0.244111,
     "end_time": "2026-02-25T01:08:43.816412",
     "exception": false,
     "start_time": "2026-02-25T01:08:43.572301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données d'exemple créées\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [5,1, 3,5, 1,4, 0,2]\r\n"
     ]
    }
   ],
   "source": [
    "// Créer le contexte ML\n",
    "var mlContext = new MLContext(seed: 42);\n",
    "\n",
    "// Définir les classes de données\n",
    "public class OnnxInputData\n",
    "{\n",
    "    [VectorType(4)]\n",
    "    public float[] Features { get; set; }\n",
    "}\n",
    "\n",
    "public class OnnxOutputData\n",
    "{\n",
    "    public float Prediction { get; set; }\n",
    "    public float[] Probability { get; set; }\n",
    "}\n",
    "\n",
    "// Exemple de données\n",
    "var sampleData = new OnnxInputData\n",
    "{\n",
    "    Features = new float[] { 5.1f, 3.5f, 1.4f, 0.2f }\n",
    "};\n",
    "\n",
    "Console.WriteLine(\"Données d'exemple créées\");\n",
    "Console.WriteLine($\"Features: [{string.Join(\", \", sampleData.Features)}]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f82309",
   "metadata": {
    "papermill": {
     "duration": 0.004001,
     "end_time": "2026-02-25T01:08:43.824047",
     "exception": false,
     "start_time": "2026-02-25T01:08:43.820046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Chargement d'un modèle ONNX\n",
    "\n",
    "Dans un scénario réel, vous auriez un fichier `.onnx` exporté depuis Python.\n",
    "\n",
    "**Exemple d'export depuis Python** :\n",
    "```python\n",
    "# Entrainement avec Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skl2onnx import convert_sklearn\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Export vers ONNX\n",
    "onnx_model = convert_sklearn(model, name='model')\n",
    "with open('model.onnx', 'wb') as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eecec1f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T01:08:43.834666Z",
     "iopub.status.busy": "2026-02-25T01:08:43.833660Z",
     "iopub.status.idle": "2026-02-25T01:08:43.899154Z",
     "shell.execute_reply": "2026-02-25T01:08:43.898148Z"
    },
    "papermill": {
     "duration": 0.071078,
     "end_time": "2026-02-25T01:08:43.899154",
     "exception": false,
     "start_time": "2026-02-25T01:08:43.828076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour utiliser ce code:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Exportez un modèle depuis Python (skl2onnx, torch.onnx)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Placez le fichier .onnx dans le répertoire du notebook\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Décommentez le code ci-dessus et exécutez\r\n"
     ]
    }
   ],
   "source": [
    "// NOTE: Ce code montre comment charger un modèle ONNX.\n",
    "// En pratique, remplacez 'model.onnx' par votre fichier.\n",
    "\n",
    "/*\n",
    "// Chargement du modèle ONNX\n",
    "var onnxModelPath = \"model.onnx\";\n",
    "\n",
    "var pipeline = mlContext.Transforms\n",
    "    .ApplyOnnxModel(\n",
    "        modelFile: onnxModelPath,\n",
    "        outputColumnNames: new[] { \"Prediction\", \"Probability\" },\n",
    "        inputColumnNames: new[] { \"Features\" }\n",
    "    );\n",
    "\n",
    "// Créer le moteur de prédiction\n",
    "var predictionEngine = mlContext.Model.CreatePredictionEngine<OnnxInputData, OnnxOutputData>(pipeline);\n",
    "\n",
    "// Faire une prédiction\n",
    "var prediction = predictionEngine.Predict(sampleData);\n",
    "\n",
    "Console.WriteLine($\"Prediction: {prediction.Prediction}\");\n",
    "Console.WriteLine($\"Probabilities: [{string.Join(\", \", prediction.Probability)}]\");\n",
    "*/\n",
    "\n",
    "Console.WriteLine(\"Pour utiliser ce code:\");\n",
    "Console.WriteLine(\"1. Exportez un modèle depuis Python (skl2onnx, torch.onnx)\");\n",
    "Console.WriteLine(\"2. Placez le fichier .onnx dans le répertoire du notebook\");\n",
    "Console.WriteLine(\"3. Décommentez le code ci-dessus et exécutez\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d87d44",
   "metadata": {
    "papermill": {
     "duration": 0.003138,
     "end_time": "2026-02-25T01:08:43.908296",
     "exception": false,
     "start_time": "2026-02-25T01:08:43.905158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exemple 2 : Exporter un modèle ML.NET vers ONNX\n",
    "\n",
    "ML.NET permet également d'**exporter** des modèles entraînés vers ONNX pour les utiliser dans d'autres frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e94bb138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T01:08:43.921102Z",
     "iopub.status.busy": "2026-02-25T01:08:43.921102Z",
     "iopub.status.idle": "2026-02-25T01:08:45.166341Z",
     "shell.execute_reply": "2026-02-25T01:08:45.165340Z"
    },
    "papermill": {
     "duration": 1.252445,
     "end_time": "2026-02-25T01:08:45.166341",
     "exception": false,
     "start_time": "2026-02-25T01:08:43.913896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle ML.NET entraîné avec succès !\r\n"
     ]
    }
   ],
   "source": [
    "// Entraîner un modèle simple avec ML.NET\n",
    "public class TrainingData\n",
    "{\n",
    "    public float Feature1 { get; set; }\n",
    "    public float Feature2 { get; set; }\n",
    "    public bool Label { get; set; }\n",
    "}\n",
    "\n",
    "// Générer des données d'entraînement\n",
    "var trainingData = new List<TrainingData>();\n",
    "var rand = new Random(42);\n",
    "\n",
    "for (int i = 0; i < 100; i++)\n",
    "{\n",
    "    trainingData.Add(new TrainingData\n",
    "    {\n",
    "        Feature1 = (float)rand.NextDouble(),\n",
    "        Feature2 = (float)rand.NextDouble(),\n",
    "        Label = rand.NextDouble() > 0.5\n",
    "    });\n",
    "}\n",
    "\n",
    "var dataView = mlContext.Data.LoadFromEnumerable(trainingData);\n",
    "\n",
    "// Créer et entraîner un pipeline\n",
    "var pipeline = mlContext.Transforms.Concatenate(\"Features\", \"Feature1\", \"Feature2\")\n",
    "    .Append(mlContext.BinaryClassification.Trainers.SdcaLogisticRegression(\n",
    "        labelColumnName: \"Label\",\n",
    "        featureColumnName: \"Features\"\n",
    "    ));\n",
    "\n",
    "var model = pipeline.Fit(dataView);\n",
    "\n",
    "Console.WriteLine(\"Modèle ML.NET entraîné avec succès !\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9950f78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T01:08:45.178059Z",
     "iopub.status.busy": "2026-02-25T01:08:45.178059Z",
     "iopub.status.idle": "2026-02-25T01:08:45.223621Z",
     "shell.execute_reply": "2026-02-25T01:08:45.222113Z"
    },
    "papermill": {
     "duration": 0.0531,
     "end_time": "2026-02-25T01:08:45.223621",
     "exception": false,
     "start_time": "2026-02-25T01:08:45.170521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: L'export ONNX dans ML.NET est expérimental.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consultez la documentation pour les modèles supportés.\r\n"
     ]
    }
   ],
   "source": [
    "// Exporter le modèle vers ONNX\n",
    "/*\n",
    "using Microsoft.ML.Onnx;\n",
    "\n",
    "var onnxExportPath = \"model_exported.onnx\";\n",
    "\n",
    "// Convertir le modèle ML.NET en ONNX\n",
    "mlContext.Model.ConvertToOnnx(\n",
    "    model: model,\n",
    "    input: dataView,\n",
    "    outputFilePath: onnxExportPath\n",
    ");\n",
    "\n",
    "Console.WriteLine($\"Modèle exporté vers {onnxExportPath}\");\n",
    "\n",
    "// Le modèle ONNX peut maintenant être utilisé dans :\n",
    "// - Python (onnxruntime)\n",
    "// - JavaScript (onnxruntime-web)\n",
    "// - C++ (onnxruntime)\n",
    "// - Java (onnxruntime)\n",
    "*/\n",
    "\n",
    "Console.WriteLine(\"NOTE: L'export ONNX dans ML.NET est expérimental.\");\n",
    "Console.WriteLine(\"Consultez la documentation pour les modèles supportés.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc53c4",
   "metadata": {
    "papermill": {
     "duration": 0.00404,
     "end_time": "2026-02-25T01:08:45.235181",
     "exception": false,
     "start_time": "2026-02-25T01:08:45.231141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exemple 3 : Utiliser des modèles Hugging Face avec ONNX\n",
    "\n",
    "**Hugging Face** propose des milliers de modèles pré-entraînels compatibles ONNX.\n",
    "\n",
    "**Modèles populaires** :\n",
    "- **BERT** : Classification de texte, NER, Question-Answering\n",
    "- **GPT** : Génération de texte\n",
    "- **Whisper** : Reconnaissance vocale\n",
    "- **ViT** : Classification d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978d2f2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T01:08:45.248971Z",
     "iopub.status.busy": "2026-02-25T01:08:45.247967Z",
     "iopub.status.idle": "2026-02-25T01:08:45.287750Z",
     "shell.execute_reply": "2026-02-25T01:08:45.287750Z"
    },
    "papermill": {
     "duration": 0.046348,
     "end_time": "2026-02-25T01:08:45.287750",
     "exception": false,
     "start_time": "2026-02-25T01:08:45.241402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèles Hugging Face supportés:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- bert-base-uncased (classification)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- distilbert-base-uncased (classification rapide)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- whisper-small (reconnaissance vocale)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pour convertir: transformers-cli export --model <nom> --onnx\r\n"
     ]
    }
   ],
   "source": [
    "// Exemple conceptuel d'utilisation d'un modèle BERT ONNX\n",
    "\n",
    "/*\n",
    "// 1. Télécharger le modèle BERT depuis Hugging Face\n",
    "// python -m transformers bert-base-uncased --onnx\n",
    "\n",
    "// 2. Charger dans ML.NET\n",
    "public class BertInput\n",
    "{\n",
    "    [VectorType(512)]\n",
    "    public long[] InputIds { get; set; }\n",
    "    \n",
    "    [VectorType(512)]\n",
    "    public long[] AttentionMask { get; set; }\n",
    "    \n",
    "    [VectorType(512)]\n",
    "    public long[] TokenTypeIds { get; set; }\n",
    "}\n",
    "\n",
    "public class BertOutput\n",
    "{\n",
    "    [VectorType(2)]\n",
    "    public float[] Output { get; set; }  // Probabilités de classification\n",
    "}\n",
    "\n",
    "var bertPipeline = mlContext.Transforms.ApplyOnnxModel(\n",
    "    modelFile: \"bert-base-uncased.onnx\",\n",
    "    outputColumnNames: new[] { \"output\" },\n",
    "    inputColumnNames: new[] { \"input_ids\", \"attention_mask\", \"token_type_ids\" }\n",
    ");\n",
    "*/\n",
    "\n",
    "Console.WriteLine(\"Modèles Hugging Face supportés:\");\n",
    "Console.WriteLine(\"- bert-base-uncased (classification)\");\n",
    "Console.WriteLine(\"- distilbert-base-uncased (classification rapide)\");\n",
    "Console.WriteLine(\"- whisper-small (reconnaissance vocale)\");\n",
    "Console.WriteLine(\"\\nPour convertir: transformers-cli export --model <nom> --onnx\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b65f6a",
   "metadata": {
    "papermill": {
     "duration": 0.003265,
     "end_time": "2026-02-25T01:08:45.296213",
     "exception": false,
     "start_time": "2026-02-25T01:08:45.292948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exemple 4 : Workflow complet Python → ONNX → ML.NET\n",
    "\n",
    "Voici un workflow typique de Data Science en Python avec déploiement en .NET :\n",
    "\n",
    "### Étape 1 : Python (R&D)\n",
    "```python\n",
    "# research_model.py\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from skl2onnx import convert_sklearn\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('data.csv')\n",
    "X, y = df.drop('target', axis=1), df['target']\n",
    "\n",
    "# Entraîner le modèle\n",
    "model = GradientBoostingClassifier(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Exporter vers ONNX\n",
    "onnx_model = convert_sklearn(\n",
    "    model,\n",
    "    initial_types=[('input', FloatTensorType([None, X.shape[1]]))]\n",
    ")\n",
    "\n",
    "with open('gradient_boosting_model.onnx', 'wb') as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "```\n",
    "\n",
    "### Étape 2 : .NET (Production)\n",
    "```csharp\n",
    "using Microsoft.ML;\n",
    "using Microsoft.ML.Data;\n",
    "\n",
    "var mlContext = new MLContext();\n",
    "\n",
    "// Charger le modèle ONNX\n",
    "var pipeline = mlContext.Transforms.ApplyOnnxModel(\n",
    "    modelFile: \"gradient_boosting_model.onnx\",\n",
    "    outputColumnNames: new[] { \"output_label\" },\n",
    "    inputColumnNames: new[] { \"input\" }\n",
    ");\n",
    "\n",
    "// Créer l'API de prédiction\n",
    "var engine = mlContext.Model.CreatePredictionEngine<Input, Output>(pipeline);\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- R&D rapide avec Python (pandas, scikit-learn)\n",
    "- Déploiement robuste avec .NET (ASP.NET, Azure Functions)\n",
    "- Performance optimisée avec ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c43051",
   "metadata": {
    "papermill": {
     "duration": 0.001998,
     "end_time": "2026-02-25T01:08:45.302240",
     "exception": false,
     "start_time": "2026-02-25T01:08:45.300242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Optimisation des performances\n",
    "\n",
    "### ONNX Runtime\n",
    "\n",
    "**ONNX Runtime** est un moteur d'inférence haute performance :\n",
    "\n",
    "| Optimisation | Impact |\n",
    "|--------------|--------|\n",
    "| **Graph optimization** | Fusion des opérations, élimination des nœuds inutiles |\n",
    "| **Quantization** | INT8 au lieu de FP32 (4x moins de mémoire) |\n",
    "| **Parallel execution** | Multi-threading automatique |\n",
    "| **GPU acceleration** | CUDA, TensorRT, OpenVINO |\n",
    "\n",
    "### Comparaison des performances\n",
    "\n",
    "| Framework | Latence (ms) | Throughput (req/s) |\n",
    "|-----------|--------------|-------------------|\n",
    "| Scikit-learn (Python) | 15 | 66 |\n",
    "| PyTorch (CPU) | 10 | 100 |\n",
    "| ONNX Runtime (CPU) | 5 | 200 |\n",
    "| ONNX Runtime (GPU) | 1 | 1000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c20d8e",
   "metadata": {
    "papermill": {
     "duration": 0.003571,
     "end_time": "2026-02-25T01:08:45.309856",
     "exception": false,
     "start_time": "2026-02-25T01:08:45.306285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Résumé et conclusion\n",
    "\n",
    "Ce notebook a présenté l'intégration **ONNX** dans ML.NET :\n",
    "\n",
    "| Concept | Clé |\n",
    "|---------|-----|\n",
    "| **ONNX** | Format ouvert pour échanger des modèles entre frameworks |\n",
    "| **ApplyOnnxModel** | Transformer ML.NET pour charger des modèles ONNX |\n",
    "| **skl2onnx** | Outil Python pour exporter Scikit-learn vers ONNX |\n",
    "| **torch.onnx** | Export PyTorch vers ONNX |\n",
    "| **ONNX Runtime** | Moteur d'inférence haute performance |\n",
    "\n",
    "**Workflow recommandé** :\n",
    "```\n",
    "1. R&D en Python (pandas, scikit-learn, PyTorch)\n",
    "2. Export vers ONNX (skl2onnx, torch.onnx)\n",
    "3. Import dans ML.NET (ApplyOnnxModel)\n",
    "4. Déploiement .NET (ASP.NET, Azure Functions)\n",
    "```\n",
    "\n",
    "**Points clés** :\n",
    "1. ONNX permet d'utiliser des modèles Python dans des applications .NET\n",
    "2. ONNX Runtime optimise automatiquement les performances\n",
    "3. Supporte CPU, GPU et divers accélérateurs matériels\n",
    "4. Hugging Face propose des milliers de modèles ONNX pré-entraînés\n",
    "\n",
    "**Limitations** :\n",
    "- Tous les opérateurs ML.NET ne sont pas exportables vers ONNX\n",
    "- Les modèles personnalisés nécessitent une conversion manuelle\n",
    "- La compatibilité dépend des versions ONNX\n",
    "\n",
    "**Pour aller plus loin** :\n",
    "- [ONNX Runtime Documentation](https://onnxruntime.ai/docs/)\n",
    "- [Hugging Face ONNX Hub](https://huggingface.co/models?library=onnx)\n",
    "- [skl2onnx Tutorial](https://onnx.ai/sklearn-onnx/)\n",
    "\n",
    "---\n",
    "\n",
    "**Navigation** : [<< ML-5-TimeSeries](ML-5-TimeSeries.ipynb) | [Suivant >> ML-7-Recommendation](ML-7-Recommendation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "file_extension": ".cs",
   "mimetype": "text/x-csharp",
   "name": "C#",
   "pygments_lexer": "csharp",
   "version": "13.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.599758,
   "end_time": "2026-02-25T01:08:45.540543",
   "environment_variables": {},
   "exception": null,
   "input_path": "ML-6-ONNX.ipynb",
   "output_path": "ML-6-ONNX.ipynb",
   "parameters": {},
   "start_time": "2026-02-25T01:08:39.940785",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}