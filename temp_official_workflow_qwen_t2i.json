{
  "id": "qwen-text-to-image-2509",
  "nodes": [
    {
      "id": 3,
      "type": "VAELoader",
      "inputs": [],
      "outputs": [
        {
          "name": "VAE",
          "type": "VAE",
          "links": [14, 17]
        }
      ],
      "widgets_values": ["qwen_image_vae.safetensors"]
    },
    {
      "id": 1,
      "type": "QwenVLCLIPLoader",
      "inputs": [],
      "outputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "links": [16]
        }
      ],
      "widgets_values": ["qwen_2.5_vl_7b.safetensors"]
    },
    {
      "id": 2,
      "type": "UNETLoader",
      "inputs": [],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [25]
        }
      ],
      "widgets_values": ["qwen_image_edit_2509_fp8_e4m3fn.safetensors", "default"]
    },
    {
      "id": 14,
      "type": "QwenVLTextEncoder",
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 16
        },
        {
          "name": "edit_image",
          "type": "IMAGE",
          "link": null
        },
        {
          "name": "vae",
          "type": "VAE",
          "link": 17
        },
        {
          "name": "text",
          "type": "STRING",
          "link": 18
        }
      ],
      "outputs": [
        {
          "name": "conditioning",
          "type": "CONDITIONING",
          "links": [21, 31]
        }
      ],
      "widgets_values": ["A beautiful landscape", "text_to_image", "", true, "analyze", "test"]
    },
    {
      "id": 12,
      "type": "ConditioningZeroOut",
      "inputs": [
        {
          "name": "conditioning",
          "type": "CONDITIONING",
          "link": 21
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "links": [30]
        }
      ],
      "widgets_values": []
    },
    {
      "id": 6,
      "type": "QwenVLEmptyLatent",
      "inputs": [],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "links": [33]
        }
      ],
      "widgets_values": [1024, 1024, 1]
    },
    {
      "id": 18,
      "type": "LoraLoaderModelOnly",
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 25
        }
      ],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [26]
        }
      ],
      "widgets_values": ["Qwen-Image-Edit-Lightning-8steps-V1.0.safetensors", 1]
    },
    {
      "id": 16,
      "type": "ModelSamplingAuraFlow",
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 26
        }
      ],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [24]
        }
      ],
      "widgets_values": [3]
    },
    {
      "id": 17,
      "type": "CFGNorm",
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 24
        }
      ],
      "outputs": [
        {
          "name": "patched_model",
          "type": "MODEL",
          "links": [32]
        }
      ],
      "widgets_values": [1]
    },
    {
      "id": 19,
      "type": "KSampler",
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 32
        },
        {
          "name": "positive",
          "type": "CONDITIONING",
          "link": 31
        },
        {
          "name": "negative",
          "type": "CONDITIONING",
          "link": 30
        },
        {
          "name": "latent_image",
          "type": "LATENT",
          "link": 33
        }
      ],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "links": [34]
        }
      ],
      "widgets_values": [355677472862408, "randomize", 5, 1, "res_2s", "beta57", 1]
    },
    {
      "id": 9,
      "type": "VAEDecode",
      "inputs": [
        {
          "name": "samples",
          "type": "LATENT",
          "link": 34
        },
        {
          "name": "vae",
          "type": "VAE",
          "link": 14
        }
      ],
      "outputs": [
        {
          "name": "IMAGE",
          "type": "IMAGE",
          "links": [10]
        }
      ],
      "widgets_values": []
    },
    {
      "id": 10,
      "type": "SaveImage",
      "inputs": [
        {
          "name": "images",
          "type": "IMAGE",
          "link": 10
        }
      ],
      "outputs": [],
      "widgets_values": ["qwen_t2i_2509"]
    }
  ],
  "links": [
    [10, 9, 0, 10, 0, "IMAGE"],
    [14, 3, 0, 9, 1, "VAE"],
    [16, 1, 0, 14, 0, "CLIP"],
    [17, 3, 0, 14, 2, "VAE"],
    [18, 13, 0, 14, 3, "STRING"],
    [21, 14, 0, 12, 0, "CONDITIONING"],
    [24, 16, 0, 17, 0, "MODEL"],
    [25, 2, 0, 18, 0, "MODEL"],
    [26, 18, 0, 16, 0, "MODEL"],
    [30, 12, 0, 19, 2, "CONDITIONING"],
    [31, 14, 0, 19, 1, "CONDITIONING"],
    [32, 17, 0, 19, 0, "MODEL"],
    [33, 6, 0, 19, 3, "LATENT"],
    [34, 19, 0, 9, 0, "LATENT"]
  ]
}